Debug: False
Config: Config: -----
batchnorm_track: False
restart_from_best: False
data_mean: []
out_dir: /scratch/shared/slow/xuji/iid_private/640
use_second_opt: False
second_opt_mult: 2.0
dataset: CIFAR10
lamb: 1.0
num_epochs: 2000
lr_schedule: []
eval_mode: hung
output_k_B: 10
save_freq: 10
output_k_A: 70
lr_mult: 0.1
input_sz: 32
dataloader_batch_sz: 220
rand_crop_szs_tf: []
in_channels: 2
lr: 0.0001
cutout: False
gt_k: 10
opt: Adam
stl_leave_out_unlabelled: False
double_eval: False
output_k: 10
num_dataloaders: 3
fluid_warp: False
head_B_epochs: 2
num_sub_heads: 5
crop_orig: True
mix_train: False
test_code: False
twohead: True
per_img_demean: False
dataset_root: /scratch/local/ssd/xuji/CIFAR
arch: ClusterNet5gTwoHead
head_A_first: True
restart: False
batch_sz: 660
out_root: /scratch/shared/slow/xuji/iid_private
model_ind: 640
data_std: []
head_A_epochs: 1
include_rgb: False
demean: False
mode: IID
rand_crop_sz: 20
cutout_max_box: 0.5
rot_val: 0.0
cutout_p: 0.5
----------
(_sobel_multioutput_make_transforms) config.include_rgb: False
not using cutout
not demeaning data
not per image demeaning data
Making datasets with <class 'torchvision.datasets.cifar.CIFAR10'> and None
Creating auxiliary dataloader ind 0 out of 3 time 2019-02-15 20:28:02.990878
Creating auxiliary dataloader ind 1 out of 3 time 2019-02-15 20:28:03.944556
Creating auxiliary dataloader ind 2 out of 3 time 2019-02-15 20:28:04.877098
Length of datasets vector 4
Number of batches per epoch: 273
Creating auxiliary dataloader ind 0 out of 3 time 2019-02-15 20:28:06.752067
Creating auxiliary dataloader ind 1 out of 3 time 2019-02-15 20:28:07.691147
Creating auxiliary dataloader ind 2 out of 3 time 2019-02-15 20:28:08.634542
Length of datasets vector 4
Number of batches per epoch: 273
avg_pool_sz 3
semisup: False
Pre: time 2019-02-15 20:28:40.176345: 
 	std: 0.008561338
	best_train_sub_head_match: [(0, 7), (1, 6), (2, 3), (3, 4), (4, 8), (5, 9), (6, 2), (7, 0), (8, 1), (9, 5)]
	test_accs: [0.10023333, 0.12435, 0.11045, 0.12156667, 0.11445]
	train_accs: [0.10023333, 0.12435, 0.11045, 0.12156667, 0.11445]
	best_train_sub_head: 1
	worst: 0.10023333
	avg: 0.114209995
	best: 0.12435

Starting e_i: 1
Model ind 640 epoch 1 head A head_i_epoch 0 batch 0: avg loss -0.000004 avg loss no lamb -0.000004 time 2019-02-15 20:28:40.970749
Model ind 640 epoch 1 head A head_i_epoch 0 batch 1: avg loss -0.000007 avg loss no lamb -0.000007 time 2019-02-15 20:28:42.406269
Model ind 640 epoch 1 head A head_i_epoch 0 batch 2: avg loss -0.000024 avg loss no lamb -0.000024 time 2019-02-15 20:28:43.780571
Model ind 640 epoch 1 head A head_i_epoch 0 batch 3: avg loss -0.000080 avg loss no lamb -0.000080 time 2019-02-15 20:28:45.160228
Model ind 640 epoch 1 head A head_i_epoch 0 batch 4: avg loss -0.001006 avg loss no lamb -0.001006 time 2019-02-15 20:28:46.539996
Model ind 640 epoch 1 head A head_i_epoch 0 batch 5: avg loss -0.003279 avg loss no lamb -0.003279 time 2019-02-15 20:28:47.917722
Model ind 640 epoch 1 head A head_i_epoch 0 batch 6: avg loss -0.006190 avg loss no lamb -0.006190 time 2019-02-15 20:28:49.297319
Model ind 640 epoch 1 head A head_i_epoch 0 batch 7: avg loss -0.003200 avg loss no lamb -0.003200 time 2019-02-15 20:28:50.669914
Model ind 640 epoch 1 head A head_i_epoch 0 batch 8: avg loss -0.009238 avg loss no lamb -0.009238 time 2019-02-15 20:28:52.040517
Model ind 640 epoch 1 head A head_i_epoch 0 batch 9: avg loss -0.010463 avg loss no lamb -0.010463 time 2019-02-15 20:28:53.419510
Model ind 640 epoch 1 head A head_i_epoch 0 batch 100: avg loss -0.343355 avg loss no lamb -0.343355 time 2019-02-15 20:30:59.402528
Model ind 640 epoch 1 head A head_i_epoch 0 batch 200: avg loss -0.665296 avg loss no lamb -0.665296 time 2019-02-15 20:33:18.548672
last batch sz 160
Model ind 640 epoch 1 head B head_i_epoch 0 batch 0: avg loss -0.001789 avg loss no lamb -0.001789 time 2019-02-15 20:34:59.394185
Model ind 640 epoch 1 head B head_i_epoch 0 batch 1: avg loss -0.002883 avg loss no lamb -0.002883 time 2019-02-15 20:35:00.825268
Model ind 640 epoch 1 head B head_i_epoch 0 batch 2: avg loss -0.003899 avg loss no lamb -0.003899 time 2019-02-15 20:35:02.237258
Model ind 640 epoch 1 head B head_i_epoch 0 batch 3: avg loss -0.006453 avg loss no lamb -0.006453 time 2019-02-15 20:35:03.668301
Model ind 640 epoch 1 head B head_i_epoch 0 batch 4: avg loss -0.009509 avg loss no lamb -0.009509 time 2019-02-15 20:35:05.086182
Model ind 640 epoch 1 head B head_i_epoch 0 batch 5: avg loss -0.012997 avg loss no lamb -0.012997 time 2019-02-15 20:35:06.502106
Model ind 640 epoch 1 head B head_i_epoch 0 batch 6: avg loss -0.021510 avg loss no lamb -0.021510 time 2019-02-15 20:35:07.923866
Model ind 640 epoch 1 head B head_i_epoch 0 batch 7: avg loss -0.031351 avg loss no lamb -0.031351 time 2019-02-15 20:35:09.344856
Model ind 640 epoch 1 head B head_i_epoch 0 batch 8: avg loss -0.048270 avg loss no lamb -0.048270 time 2019-02-15 20:35:10.773017
Model ind 640 epoch 1 head B head_i_epoch 0 batch 9: avg loss -0.068043 avg loss no lamb -0.068043 time 2019-02-15 20:35:12.192941
Model ind 640 epoch 1 head B head_i_epoch 0 batch 100: avg loss -0.589692 avg loss no lamb -0.589692 time 2019-02-15 20:37:20.946510
Model ind 640 epoch 1 head B head_i_epoch 0 batch 200: avg loss -0.700491 avg loss no lamb -0.700491 time 2019-02-15 20:39:42.317329
last batch sz 160
Model ind 640 epoch 1 head B head_i_epoch 1 batch 0: avg loss -0.654158 avg loss no lamb -0.654158 time 2019-02-15 20:41:25.563006
Model ind 640 epoch 1 head B head_i_epoch 1 batch 1: avg loss -0.722221 avg loss no lamb -0.722221 time 2019-02-15 20:41:27.039432
Model ind 640 epoch 1 head B head_i_epoch 1 batch 2: avg loss -0.662169 avg loss no lamb -0.662169 time 2019-02-15 20:41:28.459811
Model ind 640 epoch 1 head B head_i_epoch 1 batch 3: avg loss -0.696408 avg loss no lamb -0.696408 time 2019-02-15 20:41:29.884387
Model ind 640 epoch 1 head B head_i_epoch 1 batch 4: avg loss -0.703226 avg loss no lamb -0.703226 time 2019-02-15 20:41:31.304483
Model ind 640 epoch 1 head B head_i_epoch 1 batch 5: avg loss -0.583526 avg loss no lamb -0.583526 time 2019-02-15 20:41:32.731092
Model ind 640 epoch 1 head B head_i_epoch 1 batch 6: avg loss -0.701671 avg loss no lamb -0.701671 time 2019-02-15 20:41:34.151414
Model ind 640 epoch 1 head B head_i_epoch 1 batch 7: avg loss -0.665870 avg loss no lamb -0.665870 time 2019-02-15 20:41:35.578821
Model ind 640 epoch 1 head B head_i_epoch 1 batch 8: avg loss -0.658266 avg loss no lamb -0.658266 time 2019-02-15 20:41:37.004801
Model ind 640 epoch 1 head B head_i_epoch 1 batch 9: avg loss -0.696509 avg loss no lamb -0.696509 time 2019-02-15 20:41:38.433486
Model ind 640 epoch 1 head B head_i_epoch 1 batch 100: avg loss -0.715744 avg loss no lamb -0.715744 time 2019-02-15 20:43:47.040625
Model ind 640 epoch 1 head B head_i_epoch 1 batch 200: avg loss -0.707453 avg loss no lamb -0.707453 time 2019-02-15 20:46:08.080213
last batch sz 160
Pre: time 2019-02-15 20:48:15.053227: 
 	std: 0.0044311346
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 3), (3, 8), (4, 0), (5, 1), (6, 6), (7, 5), (8, 7), (9, 2)]
	test_accs: [0.2397, 0.2348, 0.23955, 0.23636666, 0.22756666]
	train_accs: [0.2397, 0.2348, 0.23955, 0.23636666, 0.22756666]
	best_train_sub_head: 0
	worst: 0.22756666
	avg: 0.23559666
	best: 0.2397

Starting e_i: 2
Model ind 640 epoch 2 head A head_i_epoch 0 batch 0: avg loss -0.652623 avg loss no lamb -0.652623 time 2019-02-15 20:48:21.708020
Model ind 640 epoch 2 head A head_i_epoch 0 batch 100: avg loss -0.724365 avg loss no lamb -0.724365 time 2019-02-15 20:50:44.048761
Model ind 640 epoch 2 head A head_i_epoch 0 batch 200: avg loss -0.908415 avg loss no lamb -0.908415 time 2019-02-15 20:53:06.733667
last batch sz 160
Model ind 640 epoch 2 head B head_i_epoch 0 batch 0: avg loss -0.674576 avg loss no lamb -0.674576 time 2019-02-15 20:54:50.796415
Model ind 640 epoch 2 head B head_i_epoch 0 batch 100: avg loss -0.778074 avg loss no lamb -0.778074 time 2019-02-15 20:57:12.522843
Model ind 640 epoch 2 head B head_i_epoch 0 batch 200: avg loss -0.764799 avg loss no lamb -0.764799 time 2019-02-15 20:59:34.685538
last batch sz 160
Model ind 640 epoch 2 head B head_i_epoch 1 batch 0: avg loss -0.639441 avg loss no lamb -0.639441 time 2019-02-15 21:01:18.204481
Model ind 640 epoch 2 head B head_i_epoch 1 batch 100: avg loss -0.746834 avg loss no lamb -0.746834 time 2019-02-15 21:03:40.533237
Model ind 640 epoch 2 head B head_i_epoch 1 batch 200: avg loss -0.843977 avg loss no lamb -0.843977 time 2019-02-15 21:06:02.969227
last batch sz 160
Pre: time 2019-02-15 21:08:10.866544: 
 	std: 0.0031771266
	best_train_sub_head_match: [(0, 7), (1, 1), (2, 9), (3, 2), (4, 5), (5, 8), (6, 6), (7, 0), (8, 4), (9, 3)]
	test_accs: [0.24816667, 0.24638334, 0.24891667, 0.24571666, 0.2399]
	train_accs: [0.24816667, 0.24638334, 0.24891667, 0.24571666, 0.2399]
	best_train_sub_head: 2
	worst: 0.2399
	avg: 0.24581666
	best: 0.24891667

Starting e_i: 3
Model ind 640 epoch 3 head A head_i_epoch 0 batch 0: avg loss -0.757272 avg loss no lamb -0.757272 time 2019-02-15 21:08:16.204193
Model ind 640 epoch 3 head A head_i_epoch 0 batch 100: avg loss -1.166269 avg loss no lamb -1.166269 time 2019-02-15 21:10:38.813417
Model ind 640 epoch 3 head A head_i_epoch 0 batch 200: avg loss -1.197872 avg loss no lamb -1.197872 time 2019-02-15 21:13:01.829891
last batch sz 160
Model ind 640 epoch 3 head B head_i_epoch 0 batch 0: avg loss -0.736579 avg loss no lamb -0.736579 time 2019-02-15 21:14:45.902559
Model ind 640 epoch 3 head B head_i_epoch 0 batch 100: avg loss -0.832099 avg loss no lamb -0.832099 time 2019-02-15 21:17:08.789790
Model ind 640 epoch 3 head B head_i_epoch 0 batch 200: avg loss -0.837730 avg loss no lamb -0.837730 time 2019-02-15 21:19:30.841880
last batch sz 160
Model ind 640 epoch 3 head B head_i_epoch 1 batch 0: avg loss -0.844423 avg loss no lamb -0.844423 time 2019-02-15 21:21:14.271054
Model ind 640 epoch 3 head B head_i_epoch 1 batch 100: avg loss -0.810687 avg loss no lamb -0.810687 time 2019-02-15 21:23:35.376036
Model ind 640 epoch 3 head B head_i_epoch 1 batch 200: avg loss -0.928934 avg loss no lamb -0.928934 time 2019-02-15 21:25:56.725469
last batch sz 160
Pre: time 2019-02-15 21:28:03.636654: 
 	std: 0.01035317
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 7), (3, 2), (4, 6), (5, 0), (6, 4), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.2676, 0.29283333, 0.27875, 0.29161668, 0.27096668]
	train_accs: [0.2676, 0.29283333, 0.27875, 0.29161668, 0.27096668]
	best_train_sub_head: 1
	worst: 0.2676
	avg: 0.28035334
	best: 0.29283333

Starting e_i: 4
Model ind 640 epoch 4 head A head_i_epoch 0 batch 0: avg loss -1.073389 avg loss no lamb -1.073389 time 2019-02-15 21:28:09.234392
Model ind 640 epoch 4 head A head_i_epoch 0 batch 100: avg loss -1.233869 avg loss no lamb -1.233869 time 2019-02-15 21:30:31.260271
Model ind 640 epoch 4 head A head_i_epoch 0 batch 200: avg loss -1.302682 avg loss no lamb -1.302682 time 2019-02-15 21:32:53.229666
last batch sz 160
Model ind 640 epoch 4 head B head_i_epoch 0 batch 0: avg loss -0.844582 avg loss no lamb -0.844582 time 2019-02-15 21:34:37.569803
Model ind 640 epoch 4 head B head_i_epoch 0 batch 100: avg loss -0.896921 avg loss no lamb -0.896921 time 2019-02-15 21:36:59.028606
Model ind 640 epoch 4 head B head_i_epoch 0 batch 200: avg loss -0.938011 avg loss no lamb -0.938011 time 2019-02-15 21:39:20.807821
last batch sz 160
Model ind 640 epoch 4 head B head_i_epoch 1 batch 0: avg loss -0.880413 avg loss no lamb -0.880413 time 2019-02-15 21:41:04.674982
Model ind 640 epoch 4 head B head_i_epoch 1 batch 100: avg loss -0.815220 avg loss no lamb -0.815220 time 2019-02-15 21:43:27.368620
Model ind 640 epoch 4 head B head_i_epoch 1 batch 200: avg loss -1.024852 avg loss no lamb -1.024852 time 2019-02-15 21:45:50.124197
last batch sz 160
Pre: time 2019-02-15 21:47:57.852451: 
 	std: 0.0052124774
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 2), (3, 7), (4, 5), (5, 4), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.28263333, 0.29555, 0.2921, 0.29568332, 0.28613332]
	train_accs: [0.28263333, 0.29555, 0.2921, 0.29568332, 0.28613332]
	best_train_sub_head: 3
	worst: 0.28263333
	avg: 0.29042
	best: 0.29568332

Starting e_i: 5
Model ind 640 epoch 5 head A head_i_epoch 0 batch 0: avg loss -1.180439 avg loss no lamb -1.180439 time 2019-02-15 21:48:03.787287
Model ind 640 epoch 5 head A head_i_epoch 0 batch 100: avg loss -1.382783 avg loss no lamb -1.382783 time 2019-02-15 21:50:26.128528
Model ind 640 epoch 5 head A head_i_epoch 0 batch 200: avg loss -1.355237 avg loss no lamb -1.355237 time 2019-02-15 21:52:48.610378
last batch sz 160
Model ind 640 epoch 5 head B head_i_epoch 0 batch 0: avg loss -0.831250 avg loss no lamb -0.831250 time 2019-02-15 21:54:32.681467
Model ind 640 epoch 5 head B head_i_epoch 0 batch 100: avg loss -0.943172 avg loss no lamb -0.943172 time 2019-02-15 21:56:54.918201
Model ind 640 epoch 5 head B head_i_epoch 0 batch 200: avg loss -1.052746 avg loss no lamb -1.052746 time 2019-02-15 21:59:15.937813
last batch sz 160
Model ind 640 epoch 5 head B head_i_epoch 1 batch 0: avg loss -0.975363 avg loss no lamb -0.975363 time 2019-02-15 22:00:58.701706
Model ind 640 epoch 5 head B head_i_epoch 1 batch 100: avg loss -0.940813 avg loss no lamb -0.940813 time 2019-02-15 22:03:20.431460
Model ind 640 epoch 5 head B head_i_epoch 1 batch 200: avg loss -0.946050 avg loss no lamb -0.946050 time 2019-02-15 22:05:41.315657
last batch sz 160
Pre: time 2019-02-15 22:07:47.371677: 
 	std: 0.001394667
	best_train_sub_head_match: [(0, 9), (1, 8), (2, 1), (3, 7), (4, 3), (5, 0), (6, 6), (7, 5), (8, 2), (9, 4)]
	test_accs: [0.30285, 0.30358332, 0.30551666, 0.30353335, 0.30118334]
	train_accs: [0.30285, 0.30358332, 0.30551666, 0.30353335, 0.30118334]
	best_train_sub_head: 2
	worst: 0.30118334
	avg: 0.30333334
	best: 0.30551666

Starting e_i: 6
Model ind 640 epoch 6 head A head_i_epoch 0 batch 0: avg loss -1.331344 avg loss no lamb -1.331344 time 2019-02-15 22:07:53.604834
Model ind 640 epoch 6 head A head_i_epoch 0 batch 100: avg loss -1.387643 avg loss no lamb -1.387643 time 2019-02-15 22:10:15.596052
Model ind 640 epoch 6 head A head_i_epoch 0 batch 200: avg loss -1.399271 avg loss no lamb -1.399271 time 2019-02-15 22:12:38.941587
last batch sz 160
Model ind 640 epoch 6 head B head_i_epoch 0 batch 0: avg loss -0.906185 avg loss no lamb -0.906185 time 2019-02-15 22:14:21.768523
Model ind 640 epoch 6 head B head_i_epoch 0 batch 100: avg loss -0.925822 avg loss no lamb -0.925822 time 2019-02-15 22:16:43.579505
Model ind 640 epoch 6 head B head_i_epoch 0 batch 200: avg loss -0.998982 avg loss no lamb -0.998982 time 2019-02-15 22:19:06.421817
last batch sz 160
Model ind 640 epoch 6 head B head_i_epoch 1 batch 0: avg loss -0.910387 avg loss no lamb -0.910387 time 2019-02-15 22:20:49.893232
Model ind 640 epoch 6 head B head_i_epoch 1 batch 100: avg loss -0.934085 avg loss no lamb -0.934085 time 2019-02-15 22:23:11.979286
Model ind 640 epoch 6 head B head_i_epoch 1 batch 200: avg loss -1.004178 avg loss no lamb -1.004178 time 2019-02-15 22:25:34.243854
last batch sz 160
Pre: time 2019-02-15 22:27:41.996459: 
 	std: 0.010275927
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 2), (3, 7), (4, 5), (5, 4), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.29641667, 0.31946668, 0.30701667, 0.31981668, 0.29696667]
	train_accs: [0.29641667, 0.31946668, 0.30701667, 0.31981668, 0.29696667]
	best_train_sub_head: 3
	worst: 0.29641667
	avg: 0.30793667
	best: 0.31981668

Starting e_i: 7
Model ind 640 epoch 7 head A head_i_epoch 0 batch 0: avg loss -1.345760 avg loss no lamb -1.345760 time 2019-02-15 22:27:47.967055
Model ind 640 epoch 7 head A head_i_epoch 0 batch 100: avg loss -1.455305 avg loss no lamb -1.455305 time 2019-02-15 22:30:10.544695
Model ind 640 epoch 7 head A head_i_epoch 0 batch 200: avg loss -1.484073 avg loss no lamb -1.484073 time 2019-02-15 22:32:32.219324
last batch sz 160
Model ind 640 epoch 7 head B head_i_epoch 0 batch 0: avg loss -0.948130 avg loss no lamb -0.948130 time 2019-02-15 22:34:15.249998
Model ind 640 epoch 7 head B head_i_epoch 0 batch 100: avg loss -0.992544 avg loss no lamb -0.992544 time 2019-02-15 22:36:36.554174
Model ind 640 epoch 7 head B head_i_epoch 0 batch 200: avg loss -0.959638 avg loss no lamb -0.959638 time 2019-02-15 22:38:58.876361
last batch sz 160
Model ind 640 epoch 7 head B head_i_epoch 1 batch 0: avg loss -0.953669 avg loss no lamb -0.953669 time 2019-02-15 22:40:42.318959
Model ind 640 epoch 7 head B head_i_epoch 1 batch 100: avg loss -0.908049 avg loss no lamb -0.908049 time 2019-02-15 22:43:05.322724
Model ind 640 epoch 7 head B head_i_epoch 1 batch 200: avg loss -1.044758 avg loss no lamb -1.044758 time 2019-02-15 22:45:27.024429
last batch sz 160
Pre: time 2019-02-15 22:47:33.955500: 
 	std: 0.0041361083
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 2), (3, 7), (4, 5), (5, 4), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.30136666, 0.30846667, 0.3069, 0.31155, 0.30085]
	train_accs: [0.30136666, 0.30846667, 0.3069, 0.31155, 0.30085]
	best_train_sub_head: 3
	worst: 0.30085
	avg: 0.3058267
	best: 0.31155

Starting e_i: 8
Model ind 640 epoch 8 head A head_i_epoch 0 batch 0: avg loss -1.385454 avg loss no lamb -1.385454 time 2019-02-15 22:47:35.700788
Model ind 640 epoch 8 head A head_i_epoch 0 batch 100: avg loss -1.485322 avg loss no lamb -1.485322 time 2019-02-15 22:49:57.830049
Model ind 640 epoch 8 head A head_i_epoch 0 batch 200: avg loss -1.629736 avg loss no lamb -1.629736 time 2019-02-15 22:52:18.826245
last batch sz 160
Model ind 640 epoch 8 head B head_i_epoch 0 batch 0: avg loss -0.907750 avg loss no lamb -0.907750 time 2019-02-15 22:54:01.245001
Model ind 640 epoch 8 head B head_i_epoch 0 batch 100: avg loss -0.942072 avg loss no lamb -0.942072 time 2019-02-15 22:56:22.094417
Model ind 640 epoch 8 head B head_i_epoch 0 batch 200: avg loss -0.984716 avg loss no lamb -0.984716 time 2019-02-15 22:58:44.654258
last batch sz 160
Model ind 640 epoch 8 head B head_i_epoch 1 batch 0: avg loss -0.905999 avg loss no lamb -0.905999 time 2019-02-15 23:00:28.285158
Model ind 640 epoch 8 head B head_i_epoch 1 batch 100: avg loss -0.958715 avg loss no lamb -0.958715 time 2019-02-15 23:02:50.268263
Model ind 640 epoch 8 head B head_i_epoch 1 batch 200: avg loss -1.037174 avg loss no lamb -1.037174 time 2019-02-15 23:05:11.267131
last batch sz 160
Pre: time 2019-02-15 23:07:17.595997: 
 	std: 0.0055028563
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 2), (3, 7), (4, 6), (5, 0), (6, 4), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.29515, 0.30736667, 0.3045, 0.3069, 0.29525]
	train_accs: [0.29515, 0.30736667, 0.3045, 0.3069, 0.29525]
	best_train_sub_head: 1
	worst: 0.29515
	avg: 0.30183333
	best: 0.30736667

Starting e_i: 9
Model ind 640 epoch 9 head A head_i_epoch 0 batch 0: avg loss -1.514086 avg loss no lamb -1.514086 time 2019-02-15 23:07:19.384678
Model ind 640 epoch 9 head A head_i_epoch 0 batch 100: avg loss -1.640318 avg loss no lamb -1.640318 time 2019-02-15 23:09:41.203744
Model ind 640 epoch 9 head A head_i_epoch 0 batch 200: avg loss -1.772748 avg loss no lamb -1.772748 time 2019-02-15 23:12:02.891100
last batch sz 160
Model ind 640 epoch 9 head B head_i_epoch 0 batch 0: avg loss -0.896034 avg loss no lamb -0.896034 time 2019-02-15 23:13:46.257256
Model ind 640 epoch 9 head B head_i_epoch 0 batch 100: avg loss -0.969088 avg loss no lamb -0.969088 time 2019-02-15 23:16:08.477572
Model ind 640 epoch 9 head B head_i_epoch 0 batch 200: avg loss -1.017437 avg loss no lamb -1.017437 time 2019-02-15 23:18:29.215939
last batch sz 160
Model ind 640 epoch 9 head B head_i_epoch 1 batch 0: avg loss -0.992856 avg loss no lamb -0.992856 time 2019-02-15 23:20:11.978250
Model ind 640 epoch 9 head B head_i_epoch 1 batch 100: avg loss -0.967124 avg loss no lamb -0.967124 time 2019-02-15 23:22:33.567284
Model ind 640 epoch 9 head B head_i_epoch 1 batch 200: avg loss -1.057369 avg loss no lamb -1.057369 time 2019-02-15 23:24:54.517079
last batch sz 160
Pre: time 2019-02-15 23:27:01.706080: 
 	std: 0.0024344085
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 2), (8, 7), (9, 3)]
	test_accs: [0.3112, 0.30955, 0.31518334, 0.31263334, 0.30818334]
	train_accs: [0.3112, 0.30955, 0.31518334, 0.31263334, 0.30818334]
	best_train_sub_head: 2
	worst: 0.30818334
	avg: 0.31135
	best: 0.31518334

Starting e_i: 10
Model ind 640 epoch 10 head A head_i_epoch 0 batch 0: avg loss -1.628743 avg loss no lamb -1.628743 time 2019-02-15 23:27:03.461117
Model ind 640 epoch 10 head A head_i_epoch 0 batch 100: avg loss -1.815807 avg loss no lamb -1.815807 time 2019-02-15 23:29:28.125272
Model ind 640 epoch 10 head A head_i_epoch 0 batch 200: avg loss -1.816200 avg loss no lamb -1.816200 time 2019-02-15 23:31:49.765498
last batch sz 160
Model ind 640 epoch 10 head B head_i_epoch 0 batch 0: avg loss -0.934539 avg loss no lamb -0.934539 time 2019-02-15 23:33:33.118255
Model ind 640 epoch 10 head B head_i_epoch 0 batch 100: avg loss -0.994995 avg loss no lamb -0.994995 time 2019-02-15 23:35:55.028273
Model ind 640 epoch 10 head B head_i_epoch 0 batch 200: avg loss -1.102244 avg loss no lamb -1.102244 time 2019-02-15 23:38:17.182238
last batch sz 160
Model ind 640 epoch 10 head B head_i_epoch 1 batch 0: avg loss -0.956706 avg loss no lamb -0.956706 time 2019-02-15 23:40:00.605377
Model ind 640 epoch 10 head B head_i_epoch 1 batch 100: avg loss -1.021307 avg loss no lamb -1.021307 time 2019-02-15 23:42:22.674421
Model ind 640 epoch 10 head B head_i_epoch 1 batch 200: avg loss -1.061882 avg loss no lamb -1.061882 time 2019-02-15 23:44:45.815402
last batch sz 160
Pre: time 2019-02-15 23:46:54.439399: 
 	std: 0.008975435
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.30586666, 0.3247, 0.3257, 0.32485, 0.30776668]
	train_accs: [0.30586666, 0.3247, 0.3257, 0.32485, 0.30776668]
	best_train_sub_head: 2
	worst: 0.30586666
	avg: 0.31777665
	best: 0.3257

Starting e_i: 11
Model ind 640 epoch 11 head A head_i_epoch 0 batch 0: avg loss -1.609095 avg loss no lamb -1.609095 time 2019-02-15 23:47:02.630193
Model ind 640 epoch 11 head A head_i_epoch 0 batch 100: avg loss -1.788902 avg loss no lamb -1.788902 time 2019-02-15 23:49:25.259438
Model ind 640 epoch 11 head A head_i_epoch 0 batch 200: avg loss -1.930929 avg loss no lamb -1.930929 time 2019-02-15 23:51:48.079423
last batch sz 160
Model ind 640 epoch 11 head B head_i_epoch 0 batch 0: avg loss -0.949265 avg loss no lamb -0.949265 time 2019-02-15 23:53:31.925569
Model ind 640 epoch 11 head B head_i_epoch 0 batch 100: avg loss -0.997291 avg loss no lamb -0.997291 time 2019-02-15 23:55:54.046926
Model ind 640 epoch 11 head B head_i_epoch 0 batch 200: avg loss -1.097177 avg loss no lamb -1.097177 time 2019-02-15 23:58:16.127118
last batch sz 160
Model ind 640 epoch 11 head B head_i_epoch 1 batch 0: avg loss -1.017128 avg loss no lamb -1.017128 time 2019-02-15 23:59:59.496333
Model ind 640 epoch 11 head B head_i_epoch 1 batch 100: avg loss -0.973179 avg loss no lamb -0.973179 time 2019-02-16 00:02:22.053355
Model ind 640 epoch 11 head B head_i_epoch 1 batch 200: avg loss -0.963838 avg loss no lamb -0.963838 time 2019-02-16 00:04:44.757563
last batch sz 160
Pre: time 2019-02-16 00:06:52.180429: 
 	std: 0.01201023
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 2), (9, 3)]
	test_accs: [0.31711668, 0.33473334, 0.34955, 0.33381668, 0.31823334]
	train_accs: [0.31711668, 0.33473334, 0.34955, 0.33381668, 0.31823334]
	best_train_sub_head: 2
	worst: 0.31711668
	avg: 0.33069
	best: 0.34955

Starting e_i: 12
Model ind 640 epoch 12 head A head_i_epoch 0 batch 0: avg loss -1.773289 avg loss no lamb -1.773289 time 2019-02-16 00:06:56.941457
Model ind 640 epoch 12 head A head_i_epoch 0 batch 100: avg loss -1.890935 avg loss no lamb -1.890935 time 2019-02-16 00:09:18.255866
Model ind 640 epoch 12 head A head_i_epoch 0 batch 200: avg loss -1.952319 avg loss no lamb -1.952319 time 2019-02-16 00:11:40.495948
last batch sz 160
Model ind 640 epoch 12 head B head_i_epoch 0 batch 0: avg loss -0.987466 avg loss no lamb -0.987466 time 2019-02-16 00:13:24.667606
Model ind 640 epoch 12 head B head_i_epoch 0 batch 100: avg loss -1.013365 avg loss no lamb -1.013365 time 2019-02-16 00:15:46.440944
Model ind 640 epoch 12 head B head_i_epoch 0 batch 200: avg loss -1.052271 avg loss no lamb -1.052271 time 2019-02-16 00:18:08.621518
last batch sz 160
Model ind 640 epoch 12 head B head_i_epoch 1 batch 0: avg loss -0.971772 avg loss no lamb -0.971772 time 2019-02-16 00:19:51.421323
Model ind 640 epoch 12 head B head_i_epoch 1 batch 100: avg loss -0.983677 avg loss no lamb -0.983677 time 2019-02-16 00:22:12.261109
Model ind 640 epoch 12 head B head_i_epoch 1 batch 200: avg loss -1.119063 avg loss no lamb -1.119063 time 2019-02-16 00:24:32.774352
last batch sz 160
Pre: time 2019-02-16 00:26:38.846722: 
 	std: 0.020033421
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 2), (3, 7), (4, 6), (5, 0), (6, 4), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.33825, 0.37916666, 0.37678334, 0.37858334, 0.33641666]
	train_accs: [0.33825, 0.37916666, 0.37678334, 0.37858334, 0.33641666]
	best_train_sub_head: 1
	worst: 0.33641666
	avg: 0.36184
	best: 0.37916666

Starting e_i: 13
Model ind 640 epoch 13 head A head_i_epoch 0 batch 0: avg loss -1.737805 avg loss no lamb -1.737805 time 2019-02-16 00:26:43.821767
Model ind 640 epoch 13 head A head_i_epoch 0 batch 100: avg loss -1.985066 avg loss no lamb -1.985066 time 2019-02-16 00:29:06.530777
Model ind 640 epoch 13 head A head_i_epoch 0 batch 200: avg loss -1.912868 avg loss no lamb -1.912868 time 2019-02-16 00:31:29.441682
last batch sz 160
Model ind 640 epoch 13 head B head_i_epoch 0 batch 0: avg loss -0.965446 avg loss no lamb -0.965446 time 2019-02-16 00:33:14.007498
Model ind 640 epoch 13 head B head_i_epoch 0 batch 100: avg loss -1.023103 avg loss no lamb -1.023103 time 2019-02-16 00:35:35.514100
Model ind 640 epoch 13 head B head_i_epoch 0 batch 200: avg loss -1.128551 avg loss no lamb -1.128551 time 2019-02-16 00:37:56.549383
last batch sz 160
Model ind 640 epoch 13 head B head_i_epoch 1 batch 0: avg loss -0.984401 avg loss no lamb -0.984401 time 2019-02-16 00:39:39.458981
Model ind 640 epoch 13 head B head_i_epoch 1 batch 100: avg loss -0.965766 avg loss no lamb -0.965766 time 2019-02-16 00:42:00.479105
Model ind 640 epoch 13 head B head_i_epoch 1 batch 200: avg loss -1.135115 avg loss no lamb -1.135115 time 2019-02-16 00:44:22.280649
last batch sz 160
Pre: time 2019-02-16 00:46:30.080276: 
 	std: 0.01699639
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 2), (9, 3)]
	test_accs: [0.34418333, 0.37628335, 0.3828, 0.37785, 0.34495]
	train_accs: [0.34418333, 0.37628335, 0.3828, 0.37785, 0.34495]
	best_train_sub_head: 2
	worst: 0.34418333
	avg: 0.36521333
	best: 0.3828

Starting e_i: 14
Model ind 640 epoch 14 head A head_i_epoch 0 batch 0: avg loss -1.808916 avg loss no lamb -1.808916 time 2019-02-16 00:46:35.103683
Model ind 640 epoch 14 head A head_i_epoch 0 batch 100: avg loss -2.037819 avg loss no lamb -2.037819 time 2019-02-16 00:48:58.067159
Model ind 640 epoch 14 head A head_i_epoch 0 batch 200: avg loss -2.067920 avg loss no lamb -2.067920 time 2019-02-16 00:51:20.622022
last batch sz 160
Model ind 640 epoch 14 head B head_i_epoch 0 batch 0: avg loss -1.067924 avg loss no lamb -1.067924 time 2019-02-16 00:53:03.705698
Model ind 640 epoch 14 head B head_i_epoch 0 batch 100: avg loss -0.965336 avg loss no lamb -0.965336 time 2019-02-16 00:55:25.347073
Model ind 640 epoch 14 head B head_i_epoch 0 batch 200: avg loss -1.133111 avg loss no lamb -1.133111 time 2019-02-16 00:57:47.547461
last batch sz 160
Model ind 640 epoch 14 head B head_i_epoch 1 batch 0: avg loss -1.093961 avg loss no lamb -1.093961 time 2019-02-16 00:59:31.345691
Model ind 640 epoch 14 head B head_i_epoch 1 batch 100: avg loss -1.027190 avg loss no lamb -1.027190 time 2019-02-16 01:01:53.824949
Model ind 640 epoch 14 head B head_i_epoch 1 batch 200: avg loss -1.149390 avg loss no lamb -1.149390 time 2019-02-16 01:04:15.170157
last batch sz 160
Pre: time 2019-02-16 01:06:23.090979: 
 	std: 0.013846297
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 2), (9, 3)]
	test_accs: [0.3618, 0.38753334, 0.39411667, 0.38821667, 0.36236668]
	train_accs: [0.3618, 0.38753334, 0.39411667, 0.38821667, 0.36236668]
	best_train_sub_head: 2
	worst: 0.3618
	avg: 0.37880668
	best: 0.39411667

Starting e_i: 15
Model ind 640 epoch 15 head A head_i_epoch 0 batch 0: avg loss -1.857382 avg loss no lamb -1.857382 time 2019-02-16 01:06:29.701764
Model ind 640 epoch 15 head A head_i_epoch 0 batch 100: avg loss -2.055650 avg loss no lamb -2.055650 time 2019-02-16 01:08:51.905904
Model ind 640 epoch 15 head A head_i_epoch 0 batch 200: avg loss -2.127105 avg loss no lamb -2.127105 time 2019-02-16 01:11:13.469818
last batch sz 160
Model ind 640 epoch 15 head B head_i_epoch 0 batch 0: avg loss -1.031430 avg loss no lamb -1.031430 time 2019-02-16 01:12:56.656019
Model ind 640 epoch 15 head B head_i_epoch 0 batch 100: avg loss -1.045403 avg loss no lamb -1.045403 time 2019-02-16 01:15:18.340497
Model ind 640 epoch 15 head B head_i_epoch 0 batch 200: avg loss -1.161266 avg loss no lamb -1.161266 time 2019-02-16 01:17:39.641820
last batch sz 160
Model ind 640 epoch 15 head B head_i_epoch 1 batch 0: avg loss -0.997526 avg loss no lamb -0.997526 time 2019-02-16 01:19:23.077788
Model ind 640 epoch 15 head B head_i_epoch 1 batch 100: avg loss -1.033000 avg loss no lamb -1.033000 time 2019-02-16 01:21:45.354890
Model ind 640 epoch 15 head B head_i_epoch 1 batch 200: avg loss -1.196169 avg loss no lamb -1.196169 time 2019-02-16 01:24:06.464543
last batch sz 160
Pre: time 2019-02-16 01:26:13.524030: 
 	std: 0.014894355
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.36536667, 0.39373332, 0.39988333, 0.3944, 0.36648333]
	train_accs: [0.36536667, 0.39373332, 0.39988333, 0.3944, 0.36648333]
	best_train_sub_head: 2
	worst: 0.36536667
	avg: 0.3839733
	best: 0.39988333

Starting e_i: 16
Model ind 640 epoch 16 head A head_i_epoch 0 batch 0: avg loss -1.974723 avg loss no lamb -1.974723 time 2019-02-16 01:26:18.762817
Model ind 640 epoch 16 head A head_i_epoch 0 batch 100: avg loss -2.038322 avg loss no lamb -2.038322 time 2019-02-16 01:28:40.611509
Model ind 640 epoch 16 head A head_i_epoch 0 batch 200: avg loss -2.116676 avg loss no lamb -2.116676 time 2019-02-16 01:31:03.760776
last batch sz 160
Model ind 640 epoch 16 head B head_i_epoch 0 batch 0: avg loss -1.023457 avg loss no lamb -1.023457 time 2019-02-16 01:32:47.429774
Model ind 640 epoch 16 head B head_i_epoch 0 batch 100: avg loss -1.052827 avg loss no lamb -1.052827 time 2019-02-16 01:35:09.388409
Model ind 640 epoch 16 head B head_i_epoch 0 batch 200: avg loss -1.137785 avg loss no lamb -1.137785 time 2019-02-16 01:37:31.748913
last batch sz 160
Model ind 640 epoch 16 head B head_i_epoch 1 batch 0: avg loss -1.045205 avg loss no lamb -1.045205 time 2019-02-16 01:39:14.889323
Model ind 640 epoch 16 head B head_i_epoch 1 batch 100: avg loss -1.064496 avg loss no lamb -1.064496 time 2019-02-16 01:41:36.954509
Model ind 640 epoch 16 head B head_i_epoch 1 batch 200: avg loss -1.196322 avg loss no lamb -1.196322 time 2019-02-16 01:43:58.264573
last batch sz 160
Pre: time 2019-02-16 01:46:04.834515: 
 	std: 0.020010531
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 2), (9, 3)]
	test_accs: [0.35285, 0.39123333, 0.39883333, 0.3909, 0.3536]
	train_accs: [0.35285, 0.39123333, 0.39883333, 0.3909, 0.3536]
	best_train_sub_head: 2
	worst: 0.35285
	avg: 0.37748334
	best: 0.39883333

Starting e_i: 17
Model ind 640 epoch 17 head A head_i_epoch 0 batch 0: avg loss -1.894326 avg loss no lamb -1.894326 time 2019-02-16 01:46:06.630918
Model ind 640 epoch 17 head A head_i_epoch 0 batch 100: avg loss -2.084488 avg loss no lamb -2.084488 time 2019-02-16 01:48:28.819083
Model ind 640 epoch 17 head A head_i_epoch 0 batch 200: avg loss -2.147060 avg loss no lamb -2.147060 time 2019-02-16 01:50:50.190740
last batch sz 160
Model ind 640 epoch 17 head B head_i_epoch 0 batch 0: avg loss -1.027368 avg loss no lamb -1.027368 time 2019-02-16 01:52:33.288452
Model ind 640 epoch 17 head B head_i_epoch 0 batch 100: avg loss -1.083719 avg loss no lamb -1.083719 time 2019-02-16 01:54:54.461185
Model ind 640 epoch 17 head B head_i_epoch 0 batch 200: avg loss -1.090750 avg loss no lamb -1.090750 time 2019-02-16 01:57:16.695931
last batch sz 160
Model ind 640 epoch 17 head B head_i_epoch 1 batch 0: avg loss -0.989893 avg loss no lamb -0.989893 time 2019-02-16 01:59:00.215790
Model ind 640 epoch 17 head B head_i_epoch 1 batch 100: avg loss -1.095573 avg loss no lamb -1.095573 time 2019-02-16 02:01:23.024669
Model ind 640 epoch 17 head B head_i_epoch 1 batch 200: avg loss -1.192684 avg loss no lamb -1.192684 time 2019-02-16 02:03:45.949503
last batch sz 160
Pre: time 2019-02-16 02:05:54.351664: 
 	std: 0.017043818
	best_train_sub_head_match: [(0, 0), (1, 8), (2, 1), (3, 9), (4, 5), (5, 2), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.35848334, 0.38983333, 0.39691666, 0.39035, 0.35743332]
	train_accs: [0.35848334, 0.38983333, 0.39691666, 0.39035, 0.35743332]
	best_train_sub_head: 2
	worst: 0.35743332
	avg: 0.3786033
	best: 0.39691666

Starting e_i: 18
Model ind 640 epoch 18 head A head_i_epoch 0 batch 0: avg loss -1.914708 avg loss no lamb -1.914708 time 2019-02-16 02:05:56.115603
Model ind 640 epoch 18 head A head_i_epoch 0 batch 100: avg loss -2.165689 avg loss no lamb -2.165689 time 2019-02-16 02:08:18.798520
Model ind 640 epoch 18 head A head_i_epoch 0 batch 200: avg loss -2.237217 avg loss no lamb -2.237217 time 2019-02-16 02:10:40.667715
last batch sz 160
Model ind 640 epoch 18 head B head_i_epoch 0 batch 0: avg loss -1.076313 avg loss no lamb -1.076313 time 2019-02-16 02:12:24.318678
Model ind 640 epoch 18 head B head_i_epoch 0 batch 100: avg loss -1.045339 avg loss no lamb -1.045339 time 2019-02-16 02:14:45.346996
Model ind 640 epoch 18 head B head_i_epoch 0 batch 200: avg loss -1.194600 avg loss no lamb -1.194600 time 2019-02-16 02:17:06.801222
last batch sz 160
Model ind 640 epoch 18 head B head_i_epoch 1 batch 0: avg loss -1.113353 avg loss no lamb -1.113353 time 2019-02-16 02:18:49.525284
Model ind 640 epoch 18 head B head_i_epoch 1 batch 100: avg loss -1.076754 avg loss no lamb -1.076754 time 2019-02-16 02:21:11.623145
Model ind 640 epoch 18 head B head_i_epoch 1 batch 200: avg loss -1.271131 avg loss no lamb -1.271131 time 2019-02-16 02:23:32.651553
last batch sz 160
Pre: time 2019-02-16 02:25:39.500722: 
 	std: 0.019936431
	best_train_sub_head_match: [(0, 0), (1, 8), (2, 1), (3, 9), (4, 5), (5, 2), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.37073332, 0.40833333, 0.41643333, 0.40795, 0.37063333]
	train_accs: [0.37073332, 0.40833333, 0.41643333, 0.40795, 0.37063333]
	best_train_sub_head: 2
	worst: 0.37063333
	avg: 0.3948167
	best: 0.41643333

Starting e_i: 19
Model ind 640 epoch 19 head A head_i_epoch 0 batch 0: avg loss -2.092748 avg loss no lamb -2.092748 time 2019-02-16 02:25:44.974854
Model ind 640 epoch 19 head A head_i_epoch 0 batch 100: avg loss -2.246143 avg loss no lamb -2.246143 time 2019-02-16 02:28:07.435368
Model ind 640 epoch 19 head A head_i_epoch 0 batch 200: avg loss -2.291400 avg loss no lamb -2.291400 time 2019-02-16 02:30:28.689370
last batch sz 160
Model ind 640 epoch 19 head B head_i_epoch 0 batch 0: avg loss -1.000676 avg loss no lamb -1.000676 time 2019-02-16 02:32:11.722647
Model ind 640 epoch 19 head B head_i_epoch 0 batch 100: avg loss -1.048551 avg loss no lamb -1.048551 time 2019-02-16 02:34:32.918989
Model ind 640 epoch 19 head B head_i_epoch 0 batch 200: avg loss -1.214788 avg loss no lamb -1.214788 time 2019-02-16 02:36:54.406250
last batch sz 160
Model ind 640 epoch 19 head B head_i_epoch 1 batch 0: avg loss -1.100704 avg loss no lamb -1.100704 time 2019-02-16 02:38:38.210226
Model ind 640 epoch 19 head B head_i_epoch 1 batch 100: avg loss -1.107913 avg loss no lamb -1.107913 time 2019-02-16 02:41:00.410114
Model ind 640 epoch 19 head B head_i_epoch 1 batch 200: avg loss -1.189134 avg loss no lamb -1.189134 time 2019-02-16 02:43:22.107810
last batch sz 160
Pre: time 2019-02-16 02:45:29.427894: 
 	std: 0.020370787
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 2), (9, 3)]
	test_accs: [0.37988332, 0.41685, 0.42806667, 0.41855, 0.38075]
	train_accs: [0.37988332, 0.41685, 0.42806667, 0.41855, 0.38075]
	best_train_sub_head: 2
	worst: 0.37988332
	avg: 0.40482002
	best: 0.42806667

Starting e_i: 20
Model ind 640 epoch 20 head A head_i_epoch 0 batch 0: avg loss -2.070307 avg loss no lamb -2.070307 time 2019-02-16 02:45:34.285356
Model ind 640 epoch 20 head A head_i_epoch 0 batch 100: avg loss -2.226213 avg loss no lamb -2.226213 time 2019-02-16 02:47:56.944126
Model ind 640 epoch 20 head A head_i_epoch 0 batch 200: avg loss -2.274748 avg loss no lamb -2.274748 time 2019-02-16 02:50:19.432729
last batch sz 160
Model ind 640 epoch 20 head B head_i_epoch 0 batch 0: avg loss -0.981934 avg loss no lamb -0.981934 time 2019-02-16 02:52:03.323929
Model ind 640 epoch 20 head B head_i_epoch 0 batch 100: avg loss -1.079957 avg loss no lamb -1.079957 time 2019-02-16 02:54:26.367352
Model ind 640 epoch 20 head B head_i_epoch 0 batch 200: avg loss -1.170372 avg loss no lamb -1.170372 time 2019-02-16 02:56:47.785481
last batch sz 160
Model ind 640 epoch 20 head B head_i_epoch 1 batch 0: avg loss -1.059846 avg loss no lamb -1.059846 time 2019-02-16 02:58:30.560637
Model ind 640 epoch 20 head B head_i_epoch 1 batch 100: avg loss -1.149963 avg loss no lamb -1.149963 time 2019-02-16 03:00:51.745893
Model ind 640 epoch 20 head B head_i_epoch 1 batch 200: avg loss -1.171953 avg loss no lamb -1.171953 time 2019-02-16 03:03:13.562593
last batch sz 160
Pre: time 2019-02-16 03:05:20.770307: 
 	std: 0.02787317
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.37433332, 0.43068334, 0.43356666, 0.42981666, 0.3747]
	train_accs: [0.37433332, 0.43068334, 0.43356666, 0.42981666, 0.3747]
	best_train_sub_head: 2
	worst: 0.37433332
	avg: 0.40862003
	best: 0.43356666

Starting e_i: 21
Model ind 640 epoch 21 head A head_i_epoch 0 batch 0: avg loss -2.094407 avg loss no lamb -2.094407 time 2019-02-16 03:05:29.517001
Model ind 640 epoch 21 head A head_i_epoch 0 batch 100: avg loss -2.198956 avg loss no lamb -2.198956 time 2019-02-16 03:07:52.247388
Model ind 640 epoch 21 head A head_i_epoch 0 batch 200: avg loss -2.255339 avg loss no lamb -2.255339 time 2019-02-16 03:10:15.429258
last batch sz 160
Model ind 640 epoch 21 head B head_i_epoch 0 batch 0: avg loss -1.056269 avg loss no lamb -1.056269 time 2019-02-16 03:11:59.198836
Model ind 640 epoch 21 head B head_i_epoch 0 batch 100: avg loss -1.030133 avg loss no lamb -1.030133 time 2019-02-16 03:14:20.232698
Model ind 640 epoch 21 head B head_i_epoch 0 batch 200: avg loss -1.211792 avg loss no lamb -1.211792 time 2019-02-16 03:16:41.532196
last batch sz 160
Model ind 640 epoch 21 head B head_i_epoch 1 batch 0: avg loss -1.036234 avg loss no lamb -1.036234 time 2019-02-16 03:18:24.685046
Model ind 640 epoch 21 head B head_i_epoch 1 batch 100: avg loss -1.124703 avg loss no lamb -1.124703 time 2019-02-16 03:20:47.223551
Model ind 640 epoch 21 head B head_i_epoch 1 batch 200: avg loss -1.214826 avg loss no lamb -1.214826 time 2019-02-16 03:23:09.304661
last batch sz 160
Pre: time 2019-02-16 03:25:16.340540: 
 	std: 0.02993208
	best_train_sub_head_match: [(0, 0), (1, 8), (2, 1), (3, 9), (4, 5), (5, 2), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.37386668, 0.43508333, 0.43598333, 0.4337, 0.37381667]
	train_accs: [0.37386668, 0.43508333, 0.43598333, 0.4337, 0.37381667]
	best_train_sub_head: 2
	worst: 0.37381667
	avg: 0.41048998
	best: 0.43598333

Starting e_i: 22
Model ind 640 epoch 22 head A head_i_epoch 0 batch 0: avg loss -2.201204 avg loss no lamb -2.201204 time 2019-02-16 03:25:21.788904
Model ind 640 epoch 22 head A head_i_epoch 0 batch 100: avg loss -2.220234 avg loss no lamb -2.220234 time 2019-02-16 03:27:44.085216
Model ind 640 epoch 22 head A head_i_epoch 0 batch 200: avg loss -2.290075 avg loss no lamb -2.290075 time 2019-02-16 03:30:06.842713
last batch sz 160
Model ind 640 epoch 22 head B head_i_epoch 0 batch 0: avg loss -1.022592 avg loss no lamb -1.022592 time 2019-02-16 03:31:50.584239
Model ind 640 epoch 22 head B head_i_epoch 0 batch 100: avg loss -1.111316 avg loss no lamb -1.111316 time 2019-02-16 03:34:13.162819
Model ind 640 epoch 22 head B head_i_epoch 0 batch 200: avg loss -1.183620 avg loss no lamb -1.183620 time 2019-02-16 03:36:34.460373
last batch sz 160
Model ind 640 epoch 22 head B head_i_epoch 1 batch 0: avg loss -1.076167 avg loss no lamb -1.076167 time 2019-02-16 03:38:17.362185
Model ind 640 epoch 22 head B head_i_epoch 1 batch 100: avg loss -1.116019 avg loss no lamb -1.116019 time 2019-02-16 03:40:38.995996
Model ind 640 epoch 22 head B head_i_epoch 1 batch 200: avg loss -1.243768 avg loss no lamb -1.243768 time 2019-02-16 03:43:00.335505
last batch sz 160
Pre: time 2019-02-16 03:45:06.972252: 
 	std: 0.022609396
	best_train_sub_head_match: [(0, 0), (1, 8), (2, 1), (3, 9), (4, 5), (5, 2), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.3753, 0.42068332, 0.42285, 0.42045, 0.37511668]
	train_accs: [0.3753, 0.42068332, 0.42285, 0.42045, 0.37511668]
	best_train_sub_head: 2
	worst: 0.37511668
	avg: 0.40288
	best: 0.42285

Starting e_i: 23
Model ind 640 epoch 23 head A head_i_epoch 0 batch 0: avg loss -2.204401 avg loss no lamb -2.204401 time 2019-02-16 03:45:08.657096
Model ind 640 epoch 23 head A head_i_epoch 0 batch 100: avg loss -2.271996 avg loss no lamb -2.271996 time 2019-02-16 03:47:29.972788
Model ind 640 epoch 23 head A head_i_epoch 0 batch 200: avg loss -2.254324 avg loss no lamb -2.254324 time 2019-02-16 03:49:51.486522
last batch sz 160
Model ind 640 epoch 23 head B head_i_epoch 0 batch 0: avg loss -1.073908 avg loss no lamb -1.073908 time 2019-02-16 03:51:34.452698
Model ind 640 epoch 23 head B head_i_epoch 0 batch 100: avg loss -1.115673 avg loss no lamb -1.115673 time 2019-02-16 03:53:55.494051
Model ind 640 epoch 23 head B head_i_epoch 0 batch 200: avg loss -1.204798 avg loss no lamb -1.204798 time 2019-02-16 03:56:16.521930
last batch sz 160
Model ind 640 epoch 23 head B head_i_epoch 1 batch 0: avg loss -1.102258 avg loss no lamb -1.102258 time 2019-02-16 03:57:59.235482
Model ind 640 epoch 23 head B head_i_epoch 1 batch 100: avg loss -1.120597 avg loss no lamb -1.120597 time 2019-02-16 04:00:21.507280
Model ind 640 epoch 23 head B head_i_epoch 1 batch 200: avg loss -1.272427 avg loss no lamb -1.272427 time 2019-02-16 04:02:43.455508
last batch sz 160
Pre: time 2019-02-16 04:04:50.972185: 
 	std: 0.032226384
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.37405, 0.43906668, 0.44068334, 0.43898332, 0.37356666]
	train_accs: [0.37405, 0.43906668, 0.44068334, 0.43898332, 0.37356666]
	best_train_sub_head: 2
	worst: 0.37356666
	avg: 0.41327
	best: 0.44068334

Starting e_i: 24
Model ind 640 epoch 24 head A head_i_epoch 0 batch 0: avg loss -2.259617 avg loss no lamb -2.259617 time 2019-02-16 04:04:55.743635
Model ind 640 epoch 24 head A head_i_epoch 0 batch 100: avg loss -2.352300 avg loss no lamb -2.352300 time 2019-02-16 04:07:18.282475
Model ind 640 epoch 24 head A head_i_epoch 0 batch 200: avg loss -2.349314 avg loss no lamb -2.349314 time 2019-02-16 04:09:41.200718
last batch sz 160
Model ind 640 epoch 24 head B head_i_epoch 0 batch 0: avg loss -1.108491 avg loss no lamb -1.108491 time 2019-02-16 04:11:25.320283
Model ind 640 epoch 24 head B head_i_epoch 0 batch 100: avg loss -1.042721 avg loss no lamb -1.042721 time 2019-02-16 04:13:46.933165
Model ind 640 epoch 24 head B head_i_epoch 0 batch 200: avg loss -1.239333 avg loss no lamb -1.239333 time 2019-02-16 04:16:08.295812
last batch sz 160
Model ind 640 epoch 24 head B head_i_epoch 1 batch 0: avg loss -1.102121 avg loss no lamb -1.102121 time 2019-02-16 04:17:51.078964
Model ind 640 epoch 24 head B head_i_epoch 1 batch 100: avg loss -1.187251 avg loss no lamb -1.187251 time 2019-02-16 04:20:12.353448
Model ind 640 epoch 24 head B head_i_epoch 1 batch 200: avg loss -1.232275 avg loss no lamb -1.232275 time 2019-02-16 04:22:33.758186
last batch sz 160
Pre: time 2019-02-16 04:24:40.240665: 
 	std: 0.037426487
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.37713334, 0.4532, 0.455, 0.4531, 0.37763333]
	train_accs: [0.37713334, 0.4532, 0.455, 0.4531, 0.37763333]
	best_train_sub_head: 2
	worst: 0.37713334
	avg: 0.42321333
	best: 0.455

Starting e_i: 25
Model ind 640 epoch 25 head A head_i_epoch 0 batch 0: avg loss -2.212327 avg loss no lamb -2.212327 time 2019-02-16 04:24:45.438946
Model ind 640 epoch 25 head A head_i_epoch 0 batch 100: avg loss -2.327725 avg loss no lamb -2.327725 time 2019-02-16 04:27:06.529767
Model ind 640 epoch 25 head A head_i_epoch 0 batch 200: avg loss -2.366333 avg loss no lamb -2.366333 time 2019-02-16 04:29:28.004953
last batch sz 160
Model ind 640 epoch 25 head B head_i_epoch 0 batch 0: avg loss -1.128337 avg loss no lamb -1.128337 time 2019-02-16 04:31:11.504194
Model ind 640 epoch 25 head B head_i_epoch 0 batch 100: avg loss -1.201623 avg loss no lamb -1.201623 time 2019-02-16 04:33:33.991606
Model ind 640 epoch 25 head B head_i_epoch 0 batch 200: avg loss -1.247701 avg loss no lamb -1.247701 time 2019-02-16 04:35:56.764059
last batch sz 160
Model ind 640 epoch 25 head B head_i_epoch 1 batch 0: avg loss -1.091861 avg loss no lamb -1.091861 time 2019-02-16 04:37:40.171726
Model ind 640 epoch 25 head B head_i_epoch 1 batch 100: avg loss -1.119516 avg loss no lamb -1.119516 time 2019-02-16 04:40:02.058970
Model ind 640 epoch 25 head B head_i_epoch 1 batch 200: avg loss -1.264900 avg loss no lamb -1.264900 time 2019-02-16 04:42:23.242668
last batch sz 160
Pre: time 2019-02-16 04:44:30.021676: 
 	std: 0.032342374
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.39331666, 0.45988333, 0.45866665, 0.45926666, 0.3932]
	train_accs: [0.39331666, 0.45988333, 0.45866665, 0.45926666, 0.3932]
	best_train_sub_head: 1
	worst: 0.3932
	avg: 0.43286666
	best: 0.45988333

Starting e_i: 26
Model ind 640 epoch 26 head A head_i_epoch 0 batch 0: avg loss -2.236787 avg loss no lamb -2.236787 time 2019-02-16 04:44:34.927984
Model ind 640 epoch 26 head A head_i_epoch 0 batch 100: avg loss -2.319214 avg loss no lamb -2.319214 time 2019-02-16 04:46:56.591778
Model ind 640 epoch 26 head A head_i_epoch 0 batch 200: avg loss -2.415320 avg loss no lamb -2.415320 time 2019-02-16 04:49:18.961637
last batch sz 160
Model ind 640 epoch 26 head B head_i_epoch 0 batch 0: avg loss -1.146857 avg loss no lamb -1.146857 time 2019-02-16 04:51:01.921180
Model ind 640 epoch 26 head B head_i_epoch 0 batch 100: avg loss -1.074973 avg loss no lamb -1.074973 time 2019-02-16 04:53:23.427192
Model ind 640 epoch 26 head B head_i_epoch 0 batch 200: avg loss -1.266425 avg loss no lamb -1.266425 time 2019-02-16 04:55:45.704292
last batch sz 160
Model ind 640 epoch 26 head B head_i_epoch 1 batch 0: avg loss -1.132919 avg loss no lamb -1.132919 time 2019-02-16 04:57:29.114092
Model ind 640 epoch 26 head B head_i_epoch 1 batch 100: avg loss -1.221985 avg loss no lamb -1.221985 time 2019-02-16 04:59:51.435492
Model ind 640 epoch 26 head B head_i_epoch 1 batch 200: avg loss -1.289451 avg loss no lamb -1.289451 time 2019-02-16 05:02:13.956715
last batch sz 160
Pre: time 2019-02-16 05:04:20.851016: 
 	std: 0.034677304
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 0), (6, 9), (7, 3), (8, 6), (9, 2)]
	test_accs: [0.37835, 0.44926667, 0.44775, 0.44996667, 0.3781]
	train_accs: [0.37835, 0.44926667, 0.44775, 0.44996667, 0.3781]
	best_train_sub_head: 3
	worst: 0.3781
	avg: 0.42068666
	best: 0.44996667

Starting e_i: 27
Model ind 640 epoch 27 head A head_i_epoch 0 batch 0: avg loss -2.258475 avg loss no lamb -2.258475 time 2019-02-16 05:04:22.549558
Model ind 640 epoch 27 head A head_i_epoch 0 batch 100: avg loss -2.334221 avg loss no lamb -2.334221 time 2019-02-16 05:06:43.583027
Model ind 640 epoch 27 head A head_i_epoch 0 batch 200: avg loss -2.329085 avg loss no lamb -2.329085 time 2019-02-16 05:09:05.642048
last batch sz 160
Model ind 640 epoch 27 head B head_i_epoch 0 batch 0: avg loss -1.109985 avg loss no lamb -1.109985 time 2019-02-16 05:10:49.223713
Model ind 640 epoch 27 head B head_i_epoch 0 batch 100: avg loss -1.105887 avg loss no lamb -1.105887 time 2019-02-16 05:13:11.243080
Model ind 640 epoch 27 head B head_i_epoch 0 batch 200: avg loss -1.285156 avg loss no lamb -1.285156 time 2019-02-16 05:15:33.672243
last batch sz 160
Model ind 640 epoch 27 head B head_i_epoch 1 batch 0: avg loss -1.217757 avg loss no lamb -1.217757 time 2019-02-16 05:17:17.587022
Model ind 640 epoch 27 head B head_i_epoch 1 batch 100: avg loss -1.166121 avg loss no lamb -1.166121 time 2019-02-16 05:19:40.602050
Model ind 640 epoch 27 head B head_i_epoch 1 batch 200: avg loss -1.211661 avg loss no lamb -1.211661 time 2019-02-16 05:22:02.611331
last batch sz 160
Pre: time 2019-02-16 05:24:09.905197: 
 	std: 0.03977497
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.3848, 0.46645, 0.46638334, 0.46608335, 0.38543335]
	train_accs: [0.3848, 0.46645, 0.46638334, 0.46608335, 0.38543335]
	best_train_sub_head: 1
	worst: 0.3848
	avg: 0.43383002
	best: 0.46645

Starting e_i: 28
Model ind 640 epoch 28 head A head_i_epoch 0 batch 0: avg loss -2.221383 avg loss no lamb -2.221383 time 2019-02-16 05:24:15.336893
Model ind 640 epoch 28 head A head_i_epoch 0 batch 100: avg loss -2.335358 avg loss no lamb -2.335358 time 2019-02-16 05:26:38.115605
Model ind 640 epoch 28 head A head_i_epoch 0 batch 200: avg loss -2.381619 avg loss no lamb -2.381619 time 2019-02-16 05:29:01.106103
last batch sz 160
Model ind 640 epoch 28 head B head_i_epoch 0 batch 0: avg loss -1.146068 avg loss no lamb -1.146068 time 2019-02-16 05:30:45.196827
Model ind 640 epoch 28 head B head_i_epoch 0 batch 100: avg loss -1.150626 avg loss no lamb -1.150626 time 2019-02-16 05:33:07.408391
Model ind 640 epoch 28 head B head_i_epoch 0 batch 200: avg loss -1.251589 avg loss no lamb -1.251589 time 2019-02-16 05:35:28.571639
last batch sz 160
Model ind 640 epoch 28 head B head_i_epoch 1 batch 0: avg loss -1.178584 avg loss no lamb -1.178584 time 2019-02-16 05:37:11.226871
Model ind 640 epoch 28 head B head_i_epoch 1 batch 100: avg loss -1.138075 avg loss no lamb -1.138075 time 2019-02-16 05:39:32.378618
Model ind 640 epoch 28 head B head_i_epoch 1 batch 200: avg loss -1.147300 avg loss no lamb -1.147300 time 2019-02-16 05:41:53.491089
last batch sz 160
Pre: time 2019-02-16 05:44:00.191616: 
 	std: 0.03925198
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.37745, 0.45916668, 0.45858333, 0.45795, 0.37946665]
	train_accs: [0.37745, 0.45916668, 0.45858333, 0.45795, 0.37946665]
	best_train_sub_head: 1
	worst: 0.37745
	avg: 0.4265233
	best: 0.45916668

Starting e_i: 29
Model ind 640 epoch 29 head A head_i_epoch 0 batch 0: avg loss -2.303352 avg loss no lamb -2.303352 time 2019-02-16 05:44:01.989937
Model ind 640 epoch 29 head A head_i_epoch 0 batch 100: avg loss -2.335377 avg loss no lamb -2.335377 time 2019-02-16 05:46:24.012293
Model ind 640 epoch 29 head A head_i_epoch 0 batch 200: avg loss -2.385916 avg loss no lamb -2.385916 time 2019-02-16 05:48:46.389411
last batch sz 160
Model ind 640 epoch 29 head B head_i_epoch 0 batch 0: avg loss -1.159102 avg loss no lamb -1.159102 time 2019-02-16 05:50:29.953211
Model ind 640 epoch 29 head B head_i_epoch 0 batch 100: avg loss -1.115497 avg loss no lamb -1.115497 time 2019-02-16 05:52:51.476925
Model ind 640 epoch 29 head B head_i_epoch 0 batch 200: avg loss -1.387085 avg loss no lamb -1.387085 time 2019-02-16 05:55:12.745639
last batch sz 160
Model ind 640 epoch 29 head B head_i_epoch 1 batch 0: avg loss -1.187925 avg loss no lamb -1.187925 time 2019-02-16 05:56:55.365860
Model ind 640 epoch 29 head B head_i_epoch 1 batch 100: avg loss -1.177529 avg loss no lamb -1.177529 time 2019-02-16 05:59:16.902726
Model ind 640 epoch 29 head B head_i_epoch 1 batch 200: avg loss -1.316555 avg loss no lamb -1.316555 time 2019-02-16 06:01:39.162922
last batch sz 160
Pre: time 2019-02-16 06:03:46.756772: 
 	std: 0.0392009
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.38051668, 0.46138334, 0.46046665, 0.46011665, 0.38076666]
	train_accs: [0.38051668, 0.46138334, 0.46046665, 0.46011665, 0.38076666]
	best_train_sub_head: 1
	worst: 0.38051668
	avg: 0.42865
	best: 0.46138334

Starting e_i: 30
Model ind 640 epoch 30 head A head_i_epoch 0 batch 0: avg loss -2.277416 avg loss no lamb -2.277416 time 2019-02-16 06:03:48.534892
Model ind 640 epoch 30 head A head_i_epoch 0 batch 100: avg loss -2.408311 avg loss no lamb -2.408311 time 2019-02-16 06:06:10.268352
Model ind 640 epoch 30 head A head_i_epoch 0 batch 200: avg loss -2.400281 avg loss no lamb -2.400281 time 2019-02-16 06:08:33.093695
last batch sz 160
Model ind 640 epoch 30 head B head_i_epoch 0 batch 0: avg loss -1.111230 avg loss no lamb -1.111230 time 2019-02-16 06:10:16.943483
Model ind 640 epoch 30 head B head_i_epoch 0 batch 100: avg loss -1.179707 avg loss no lamb -1.179707 time 2019-02-16 06:12:39.169740
Model ind 640 epoch 30 head B head_i_epoch 0 batch 200: avg loss -1.191160 avg loss no lamb -1.191160 time 2019-02-16 06:15:01.825977
last batch sz 160
Model ind 640 epoch 30 head B head_i_epoch 1 batch 0: avg loss -1.214799 avg loss no lamb -1.214799 time 2019-02-16 06:16:45.311928
Model ind 640 epoch 30 head B head_i_epoch 1 batch 100: avg loss -1.273386 avg loss no lamb -1.273386 time 2019-02-16 06:19:06.687712
Model ind 640 epoch 30 head B head_i_epoch 1 batch 200: avg loss -1.251058 avg loss no lamb -1.251058 time 2019-02-16 06:21:27.825755
last batch sz 160
Pre: time 2019-02-16 06:23:35.114275: 
 	std: 0.04720059
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.36425, 0.46168333, 0.46065, 0.4621, 0.36603335]
	train_accs: [0.36425, 0.46168333, 0.46065, 0.4621, 0.36603335]
	best_train_sub_head: 3
	worst: 0.36425
	avg: 0.42294335
	best: 0.4621

Starting e_i: 31
Model ind 640 epoch 31 head A head_i_epoch 0 batch 0: avg loss -2.239425 avg loss no lamb -2.239425 time 2019-02-16 06:23:40.122418
Model ind 640 epoch 31 head A head_i_epoch 0 batch 100: avg loss -2.418556 avg loss no lamb -2.418556 time 2019-02-16 06:26:02.472159
Model ind 640 epoch 31 head A head_i_epoch 0 batch 200: avg loss -2.372719 avg loss no lamb -2.372719 time 2019-02-16 06:28:25.281169
last batch sz 160
Model ind 640 epoch 31 head B head_i_epoch 0 batch 0: avg loss -1.141407 avg loss no lamb -1.141407 time 2019-02-16 06:30:08.827561
Model ind 640 epoch 31 head B head_i_epoch 0 batch 100: avg loss -1.235943 avg loss no lamb -1.235943 time 2019-02-16 06:32:30.985887
Model ind 640 epoch 31 head B head_i_epoch 0 batch 200: avg loss -1.263951 avg loss no lamb -1.263951 time 2019-02-16 06:34:53.175285
last batch sz 160
Model ind 640 epoch 31 head B head_i_epoch 1 batch 0: avg loss -1.229797 avg loss no lamb -1.229797 time 2019-02-16 06:36:36.136127
Model ind 640 epoch 31 head B head_i_epoch 1 batch 100: avg loss -1.222001 avg loss no lamb -1.222001 time 2019-02-16 06:38:57.479529
Model ind 640 epoch 31 head B head_i_epoch 1 batch 200: avg loss -1.339121 avg loss no lamb -1.339121 time 2019-02-16 06:41:18.790225
last batch sz 160
Pre: time 2019-02-16 06:43:25.180465: 
 	std: 0.039297264
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 0), (6, 9), (7, 3), (8, 6), (9, 2)]
	test_accs: [0.38443333, 0.46535, 0.46301666, 0.46556666, 0.38446668]
	train_accs: [0.38443333, 0.46535, 0.46301666, 0.46556666, 0.38446668]
	best_train_sub_head: 3
	worst: 0.38443333
	avg: 0.43256664
	best: 0.46556666

Starting e_i: 32
Model ind 640 epoch 32 head A head_i_epoch 0 batch 0: avg loss -2.212824 avg loss no lamb -2.212824 time 2019-02-16 06:43:26.946405
Model ind 640 epoch 32 head A head_i_epoch 0 batch 100: avg loss -2.427163 avg loss no lamb -2.427163 time 2019-02-16 06:45:48.921780
Model ind 640 epoch 32 head A head_i_epoch 0 batch 200: avg loss -2.419421 avg loss no lamb -2.419421 time 2019-02-16 06:48:11.224141
last batch sz 160
Model ind 640 epoch 32 head B head_i_epoch 0 batch 0: avg loss -1.189681 avg loss no lamb -1.189681 time 2019-02-16 06:49:54.910722
Model ind 640 epoch 32 head B head_i_epoch 0 batch 100: avg loss -1.207576 avg loss no lamb -1.207576 time 2019-02-16 06:52:17.190415
Model ind 640 epoch 32 head B head_i_epoch 0 batch 200: avg loss -1.242970 avg loss no lamb -1.242970 time 2019-02-16 06:54:39.406369
last batch sz 160
Model ind 640 epoch 32 head B head_i_epoch 1 batch 0: avg loss -1.137801 avg loss no lamb -1.137801 time 2019-02-16 06:56:22.750848
Model ind 640 epoch 32 head B head_i_epoch 1 batch 100: avg loss -1.236777 avg loss no lamb -1.236777 time 2019-02-16 06:58:44.905259
Model ind 640 epoch 32 head B head_i_epoch 1 batch 200: avg loss -1.213802 avg loss no lamb -1.213802 time 2019-02-16 07:01:07.017120
last batch sz 160
Pre: time 2019-02-16 07:03:14.529147: 
 	std: 0.040830076
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.38725, 0.47285, 0.46996668, 0.47223333, 0.3895]
	train_accs: [0.38725, 0.47285, 0.46996668, 0.47223333, 0.3895]
	best_train_sub_head: 1
	worst: 0.38725
	avg: 0.43835998
	best: 0.47285

Starting e_i: 33
Model ind 640 epoch 33 head A head_i_epoch 0 batch 0: avg loss -2.325454 avg loss no lamb -2.325454 time 2019-02-16 07:03:19.731128
Model ind 640 epoch 33 head A head_i_epoch 0 batch 100: avg loss -2.481795 avg loss no lamb -2.481795 time 2019-02-16 07:05:42.254117
Model ind 640 epoch 33 head A head_i_epoch 0 batch 200: avg loss -2.342173 avg loss no lamb -2.342173 time 2019-02-16 07:08:05.582429
last batch sz 160
Model ind 640 epoch 33 head B head_i_epoch 0 batch 0: avg loss -1.162664 avg loss no lamb -1.162664 time 2019-02-16 07:09:49.851952
Model ind 640 epoch 33 head B head_i_epoch 0 batch 100: avg loss -1.185428 avg loss no lamb -1.185428 time 2019-02-16 07:12:11.777723
Model ind 640 epoch 33 head B head_i_epoch 0 batch 200: avg loss -1.228077 avg loss no lamb -1.228077 time 2019-02-16 07:14:32.997321
last batch sz 160
Model ind 640 epoch 33 head B head_i_epoch 1 batch 0: avg loss -1.228361 avg loss no lamb -1.228361 time 2019-02-16 07:16:15.614510
Model ind 640 epoch 33 head B head_i_epoch 1 batch 100: avg loss -1.249903 avg loss no lamb -1.249903 time 2019-02-16 07:18:37.497241
Model ind 640 epoch 33 head B head_i_epoch 1 batch 200: avg loss -1.324200 avg loss no lamb -1.324200 time 2019-02-16 07:20:59.012380
last batch sz 160
Pre: time 2019-02-16 07:23:05.901821: 
 	std: 0.03287934
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.39616665, 0.46388334, 0.46248335, 0.46426666, 0.39671665]
	train_accs: [0.39616665, 0.46388334, 0.46248335, 0.46426666, 0.39671665]
	best_train_sub_head: 3
	worst: 0.39616665
	avg: 0.43670335
	best: 0.46426666

Starting e_i: 34
Model ind 640 epoch 34 head A head_i_epoch 0 batch 0: avg loss -2.285928 avg loss no lamb -2.285928 time 2019-02-16 07:23:07.649662
Model ind 640 epoch 34 head A head_i_epoch 0 batch 100: avg loss -2.337756 avg loss no lamb -2.337756 time 2019-02-16 07:25:29.189512
Model ind 640 epoch 34 head A head_i_epoch 0 batch 200: avg loss -2.394500 avg loss no lamb -2.394500 time 2019-02-16 07:27:50.640494
last batch sz 160
Model ind 640 epoch 34 head B head_i_epoch 0 batch 0: avg loss -1.139390 avg loss no lamb -1.139390 time 2019-02-16 07:29:33.715237
Model ind 640 epoch 34 head B head_i_epoch 0 batch 100: avg loss -1.205839 avg loss no lamb -1.205839 time 2019-02-16 07:31:56.431927
Model ind 640 epoch 34 head B head_i_epoch 0 batch 200: avg loss -1.262496 avg loss no lamb -1.262496 time 2019-02-16 07:34:18.973823
last batch sz 160
Model ind 640 epoch 34 head B head_i_epoch 1 batch 0: avg loss -1.120576 avg loss no lamb -1.120576 time 2019-02-16 07:36:01.965875
Model ind 640 epoch 34 head B head_i_epoch 1 batch 100: avg loss -1.164130 avg loss no lamb -1.164130 time 2019-02-16 07:38:22.834669
Model ind 640 epoch 34 head B head_i_epoch 1 batch 200: avg loss -1.302441 avg loss no lamb -1.302441 time 2019-02-16 07:40:44.757167
last batch sz 160
Pre: time 2019-02-16 07:42:53.015991: 
 	std: 0.03761571
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.39683333, 0.4754, 0.4734, 0.4751, 0.39891666]
	train_accs: [0.39683333, 0.4754, 0.4734, 0.4751, 0.39891666]
	best_train_sub_head: 1
	worst: 0.39683333
	avg: 0.44393
	best: 0.4754

Starting e_i: 35
Model ind 640 epoch 35 head A head_i_epoch 0 batch 0: avg loss -2.322642 avg loss no lamb -2.322642 time 2019-02-16 07:42:58.300450
Model ind 640 epoch 35 head A head_i_epoch 0 batch 100: avg loss -2.447437 avg loss no lamb -2.447437 time 2019-02-16 07:45:20.400480
Model ind 640 epoch 35 head A head_i_epoch 0 batch 200: avg loss -2.389513 avg loss no lamb -2.389513 time 2019-02-16 07:47:41.808217
last batch sz 160
Model ind 640 epoch 35 head B head_i_epoch 0 batch 0: avg loss -1.240166 avg loss no lamb -1.240166 time 2019-02-16 07:49:24.685070
Model ind 640 epoch 35 head B head_i_epoch 0 batch 100: avg loss -1.233310 avg loss no lamb -1.233310 time 2019-02-16 07:51:45.731812
Model ind 640 epoch 35 head B head_i_epoch 0 batch 200: avg loss -1.242546 avg loss no lamb -1.242546 time 2019-02-16 07:54:07.732298
last batch sz 160
Model ind 640 epoch 35 head B head_i_epoch 1 batch 0: avg loss -1.083950 avg loss no lamb -1.083950 time 2019-02-16 07:55:50.398226
Model ind 640 epoch 35 head B head_i_epoch 1 batch 100: avg loss -1.205531 avg loss no lamb -1.205531 time 2019-02-16 07:58:12.590074
Model ind 640 epoch 35 head B head_i_epoch 1 batch 200: avg loss -1.361127 avg loss no lamb -1.361127 time 2019-02-16 08:00:33.937881
last batch sz 160
Pre: time 2019-02-16 08:02:40.362677: 
 	std: 0.03918302
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.40451667, 0.48535, 0.48455, 0.4855, 0.4058]
	train_accs: [0.40451667, 0.48535, 0.48455, 0.4855, 0.4058]
	best_train_sub_head: 3
	worst: 0.40451667
	avg: 0.45314336
	best: 0.4855

Starting e_i: 36
Model ind 640 epoch 36 head A head_i_epoch 0 batch 0: avg loss -2.364208 avg loss no lamb -2.364208 time 2019-02-16 08:02:45.638709
Model ind 640 epoch 36 head A head_i_epoch 0 batch 100: avg loss -2.427125 avg loss no lamb -2.427125 time 2019-02-16 08:05:06.941691
Model ind 640 epoch 36 head A head_i_epoch 0 batch 200: avg loss -2.453422 avg loss no lamb -2.453422 time 2019-02-16 08:07:28.706779
last batch sz 160
Model ind 640 epoch 36 head B head_i_epoch 0 batch 0: avg loss -1.201750 avg loss no lamb -1.201750 time 2019-02-16 08:09:11.745235
Model ind 640 epoch 36 head B head_i_epoch 0 batch 100: avg loss -1.183720 avg loss no lamb -1.183720 time 2019-02-16 08:11:32.997718
Model ind 640 epoch 36 head B head_i_epoch 0 batch 200: avg loss -1.191846 avg loss no lamb -1.191846 time 2019-02-16 08:13:54.228151
last batch sz 160
Model ind 640 epoch 36 head B head_i_epoch 1 batch 0: avg loss -1.154302 avg loss no lamb -1.154302 time 2019-02-16 08:15:37.193506
Model ind 640 epoch 36 head B head_i_epoch 1 batch 100: avg loss -1.156138 avg loss no lamb -1.156138 time 2019-02-16 08:17:58.957614
Model ind 640 epoch 36 head B head_i_epoch 1 batch 200: avg loss -1.330708 avg loss no lamb -1.330708 time 2019-02-16 08:20:21.200007
last batch sz 160
Pre: time 2019-02-16 08:22:29.146426: 
 	std: 0.044163395
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.38945, 0.47995, 0.47795, 0.47943333, 0.3885]
	train_accs: [0.38945, 0.47995, 0.47795, 0.47943333, 0.3885]
	best_train_sub_head: 1
	worst: 0.3885
	avg: 0.44305667
	best: 0.47995

Starting e_i: 37
Model ind 640 epoch 37 head A head_i_epoch 0 batch 0: avg loss -2.324704 avg loss no lamb -2.324704 time 2019-02-16 08:22:30.965081
Model ind 640 epoch 37 head A head_i_epoch 0 batch 100: avg loss -2.502067 avg loss no lamb -2.502067 time 2019-02-16 08:24:53.235214
Model ind 640 epoch 37 head A head_i_epoch 0 batch 200: avg loss -2.448372 avg loss no lamb -2.448372 time 2019-02-16 08:27:14.690634
last batch sz 160
Model ind 640 epoch 37 head B head_i_epoch 0 batch 0: avg loss -1.216852 avg loss no lamb -1.216852 time 2019-02-16 08:28:58.371752
Model ind 640 epoch 37 head B head_i_epoch 0 batch 100: avg loss -1.129148 avg loss no lamb -1.129148 time 2019-02-16 08:31:20.624231
Model ind 640 epoch 37 head B head_i_epoch 0 batch 200: avg loss -1.245051 avg loss no lamb -1.245051 time 2019-02-16 08:33:43.143149
last batch sz 160
Model ind 640 epoch 37 head B head_i_epoch 1 batch 0: avg loss -1.219644 avg loss no lamb -1.219644 time 2019-02-16 08:35:26.782370
Model ind 640 epoch 37 head B head_i_epoch 1 batch 100: avg loss -1.222752 avg loss no lamb -1.222752 time 2019-02-16 08:37:49.233128
Model ind 640 epoch 37 head B head_i_epoch 1 batch 200: avg loss -1.253681 avg loss no lamb -1.253681 time 2019-02-16 08:40:11.654636
last batch sz 160
Pre: time 2019-02-16 08:42:18.647130: 
 	std: 0.03802693
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.39951667, 0.47758332, 0.47621667, 0.47715, 0.39921665]
	train_accs: [0.39951667, 0.47758332, 0.47621667, 0.47715, 0.39921665]
	best_train_sub_head: 1
	worst: 0.39921665
	avg: 0.44593668
	best: 0.47758332

Starting e_i: 38
Model ind 640 epoch 38 head A head_i_epoch 0 batch 0: avg loss -2.379628 avg loss no lamb -2.379628 time 2019-02-16 08:42:20.449456
Model ind 640 epoch 38 head A head_i_epoch 0 batch 100: avg loss -2.490570 avg loss no lamb -2.490570 time 2019-02-16 08:44:42.386155
Model ind 640 epoch 38 head A head_i_epoch 0 batch 200: avg loss -2.460510 avg loss no lamb -2.460510 time 2019-02-16 08:47:04.598753
last batch sz 160
Model ind 640 epoch 38 head B head_i_epoch 0 batch 0: avg loss -1.219385 avg loss no lamb -1.219385 time 2019-02-16 08:48:48.006400
Model ind 640 epoch 38 head B head_i_epoch 0 batch 100: avg loss -1.257606 avg loss no lamb -1.257606 time 2019-02-16 08:51:09.167602
Model ind 640 epoch 38 head B head_i_epoch 0 batch 200: avg loss -1.313426 avg loss no lamb -1.313426 time 2019-02-16 08:53:30.055558
last batch sz 160
Model ind 640 epoch 38 head B head_i_epoch 1 batch 0: avg loss -1.267908 avg loss no lamb -1.267908 time 2019-02-16 08:55:13.220140
Model ind 640 epoch 38 head B head_i_epoch 1 batch 100: avg loss -1.250395 avg loss no lamb -1.250395 time 2019-02-16 08:57:35.639470
Model ind 640 epoch 38 head B head_i_epoch 1 batch 200: avg loss -1.236499 avg loss no lamb -1.236499 time 2019-02-16 08:59:56.579826
last batch sz 160
Pre: time 2019-02-16 09:02:04.200909: 
 	std: 0.039996702
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40063334, 0.48191667, 0.48126668, 0.4814, 0.39915]
	train_accs: [0.40063334, 0.48191667, 0.48126668, 0.4814, 0.39915]
	best_train_sub_head: 1
	worst: 0.39915
	avg: 0.44887334
	best: 0.48191667

Starting e_i: 39
Model ind 640 epoch 39 head A head_i_epoch 0 batch 0: avg loss -2.340671 avg loss no lamb -2.340671 time 2019-02-16 09:02:06.028262
Model ind 640 epoch 39 head A head_i_epoch 0 batch 100: avg loss -2.421901 avg loss no lamb -2.421901 time 2019-02-16 09:04:29.166308
Model ind 640 epoch 39 head A head_i_epoch 0 batch 200: avg loss -2.407492 avg loss no lamb -2.407492 time 2019-02-16 09:06:51.537193
last batch sz 160
Model ind 640 epoch 39 head B head_i_epoch 0 batch 0: avg loss -1.220776 avg loss no lamb -1.220776 time 2019-02-16 09:08:34.788872
Model ind 640 epoch 39 head B head_i_epoch 0 batch 100: avg loss -1.302916 avg loss no lamb -1.302916 time 2019-02-16 09:10:56.140357
Model ind 640 epoch 39 head B head_i_epoch 0 batch 200: avg loss -1.220504 avg loss no lamb -1.220504 time 2019-02-16 09:13:17.539429
last batch sz 160
Model ind 640 epoch 39 head B head_i_epoch 1 batch 0: avg loss -1.175277 avg loss no lamb -1.175277 time 2019-02-16 09:15:00.859783
Model ind 640 epoch 39 head B head_i_epoch 1 batch 100: avg loss -1.201153 avg loss no lamb -1.201153 time 2019-02-16 09:17:23.599855
Model ind 640 epoch 39 head B head_i_epoch 1 batch 200: avg loss -1.414780 avg loss no lamb -1.414780 time 2019-02-16 09:19:46.406855
last batch sz 160
Pre: time 2019-02-16 09:21:54.705662: 
 	std: 0.039697286
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40153334, 0.48423332, 0.48148334, 0.48231667, 0.4018]
	train_accs: [0.40153334, 0.48423332, 0.48148334, 0.48231667, 0.4018]
	best_train_sub_head: 1
	worst: 0.40153334
	avg: 0.45027333
	best: 0.48423332

Starting e_i: 40
Model ind 640 epoch 40 head A head_i_epoch 0 batch 0: avg loss -2.404997 avg loss no lamb -2.404997 time 2019-02-16 09:21:56.687757
Model ind 640 epoch 40 head A head_i_epoch 0 batch 100: avg loss -2.481248 avg loss no lamb -2.481248 time 2019-02-16 09:24:19.941462
Model ind 640 epoch 40 head A head_i_epoch 0 batch 200: avg loss -2.502953 avg loss no lamb -2.502953 time 2019-02-16 09:26:43.434276
last batch sz 160
Model ind 640 epoch 40 head B head_i_epoch 0 batch 0: avg loss -1.167472 avg loss no lamb -1.167472 time 2019-02-16 09:28:27.677845
Model ind 640 epoch 40 head B head_i_epoch 0 batch 100: avg loss -1.301792 avg loss no lamb -1.301792 time 2019-02-16 09:30:49.560523
Model ind 640 epoch 40 head B head_i_epoch 0 batch 200: avg loss -1.246341 avg loss no lamb -1.246341 time 2019-02-16 09:33:10.841893
last batch sz 160
Model ind 640 epoch 40 head B head_i_epoch 1 batch 0: avg loss -1.212925 avg loss no lamb -1.212925 time 2019-02-16 09:34:53.603080
Model ind 640 epoch 40 head B head_i_epoch 1 batch 100: avg loss -1.208018 avg loss no lamb -1.208018 time 2019-02-16 09:37:14.770091
Model ind 640 epoch 40 head B head_i_epoch 1 batch 200: avg loss -1.253496 avg loss no lamb -1.253496 time 2019-02-16 09:39:35.980862
last batch sz 160
Pre: time 2019-02-16 09:41:42.469272: 
 	std: 0.040202435
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.39871666, 0.48173332, 0.47981668, 0.48118332, 0.399]
	train_accs: [0.39871666, 0.48173332, 0.47981668, 0.48118332, 0.399]
	best_train_sub_head: 1
	worst: 0.39871666
	avg: 0.44809
	best: 0.48173332

Starting e_i: 41
Model ind 640 epoch 41 head A head_i_epoch 0 batch 0: avg loss -2.366240 avg loss no lamb -2.366240 time 2019-02-16 09:41:49.829940
Model ind 640 epoch 41 head A head_i_epoch 0 batch 100: avg loss -2.489048 avg loss no lamb -2.489048 time 2019-02-16 09:44:12.567801
Model ind 640 epoch 41 head A head_i_epoch 0 batch 200: avg loss -2.544747 avg loss no lamb -2.544747 time 2019-02-16 09:46:35.375839
last batch sz 160
Model ind 640 epoch 41 head B head_i_epoch 0 batch 0: avg loss -1.211642 avg loss no lamb -1.211642 time 2019-02-16 09:48:18.403281
Model ind 640 epoch 41 head B head_i_epoch 0 batch 100: avg loss -1.184793 avg loss no lamb -1.184793 time 2019-02-16 09:50:39.646247
Model ind 640 epoch 41 head B head_i_epoch 0 batch 200: avg loss -1.389463 avg loss no lamb -1.389463 time 2019-02-16 09:53:00.737645
last batch sz 160
Model ind 640 epoch 41 head B head_i_epoch 1 batch 0: avg loss -1.284718 avg loss no lamb -1.284718 time 2019-02-16 09:54:44.079271
Model ind 640 epoch 41 head B head_i_epoch 1 batch 100: avg loss -1.171034 avg loss no lamb -1.171034 time 2019-02-16 09:57:06.329746
Model ind 640 epoch 41 head B head_i_epoch 1 batch 200: avg loss -1.339343 avg loss no lamb -1.339343 time 2019-02-16 09:59:28.495513
last batch sz 160
Pre: time 2019-02-16 10:01:35.222044: 
 	std: 0.041310634
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.39366665, 0.48035, 0.47505, 0.47895, 0.39406666]
	train_accs: [0.39366665, 0.48035, 0.47505, 0.47895, 0.39406666]
	best_train_sub_head: 1
	worst: 0.39366665
	avg: 0.44441667
	best: 0.48035

Starting e_i: 42
Model ind 640 epoch 42 head A head_i_epoch 0 batch 0: avg loss -2.385140 avg loss no lamb -2.385140 time 2019-02-16 10:01:37.006785
Model ind 640 epoch 42 head A head_i_epoch 0 batch 100: avg loss -2.420408 avg loss no lamb -2.420408 time 2019-02-16 10:03:59.225783
Model ind 640 epoch 42 head A head_i_epoch 0 batch 200: avg loss -2.458233 avg loss no lamb -2.458233 time 2019-02-16 10:06:21.482792
last batch sz 160
Model ind 640 epoch 42 head B head_i_epoch 0 batch 0: avg loss -1.219684 avg loss no lamb -1.219684 time 2019-02-16 10:08:05.170903
Model ind 640 epoch 42 head B head_i_epoch 0 batch 100: avg loss -1.223497 avg loss no lamb -1.223497 time 2019-02-16 10:10:27.192659
Model ind 640 epoch 42 head B head_i_epoch 0 batch 200: avg loss -1.367228 avg loss no lamb -1.367228 time 2019-02-16 10:12:49.369451
last batch sz 160
Model ind 640 epoch 42 head B head_i_epoch 1 batch 0: avg loss -1.192142 avg loss no lamb -1.192142 time 2019-02-16 10:14:32.867703
Model ind 640 epoch 42 head B head_i_epoch 1 batch 100: avg loss -1.267494 avg loss no lamb -1.267494 time 2019-02-16 10:16:55.255557
Model ind 640 epoch 42 head B head_i_epoch 1 batch 200: avg loss -1.386746 avg loss no lamb -1.386746 time 2019-02-16 10:19:17.507698
last batch sz 160
Pre: time 2019-02-16 10:21:25.553075: 
 	std: 0.03812964
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.40151668, 0.47926667, 0.47923332, 0.47981668, 0.4017]
	train_accs: [0.40151668, 0.47926667, 0.47923332, 0.47981668, 0.4017]
	best_train_sub_head: 3
	worst: 0.40151668
	avg: 0.44830665
	best: 0.47981668

Starting e_i: 43
Model ind 640 epoch 43 head A head_i_epoch 0 batch 0: avg loss -2.412802 avg loss no lamb -2.412802 time 2019-02-16 10:21:27.305997
Model ind 640 epoch 43 head A head_i_epoch 0 batch 100: avg loss -2.449132 avg loss no lamb -2.449132 time 2019-02-16 10:23:49.858317
Model ind 640 epoch 43 head A head_i_epoch 0 batch 200: avg loss -2.456481 avg loss no lamb -2.456481 time 2019-02-16 10:26:13.216845
last batch sz 160
Model ind 640 epoch 43 head B head_i_epoch 0 batch 0: avg loss -1.172227 avg loss no lamb -1.172227 time 2019-02-16 10:27:56.526822
Model ind 640 epoch 43 head B head_i_epoch 0 batch 100: avg loss -1.265981 avg loss no lamb -1.265981 time 2019-02-16 10:30:18.158975
Model ind 640 epoch 43 head B head_i_epoch 0 batch 200: avg loss -1.384824 avg loss no lamb -1.384824 time 2019-02-16 10:32:39.855622
last batch sz 160
Model ind 640 epoch 43 head B head_i_epoch 1 batch 0: avg loss -1.199311 avg loss no lamb -1.199311 time 2019-02-16 10:34:23.478728
Model ind 640 epoch 43 head B head_i_epoch 1 batch 100: avg loss -1.246736 avg loss no lamb -1.246736 time 2019-02-16 10:36:46.630922
Model ind 640 epoch 43 head B head_i_epoch 1 batch 200: avg loss -1.290702 avg loss no lamb -1.290702 time 2019-02-16 10:39:09.584758
last batch sz 160
Pre: time 2019-02-16 10:41:16.594712: 
 	std: 0.03576821
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4225, 0.49585, 0.49485, 0.49565, 0.42238334]
	train_accs: [0.4225, 0.49585, 0.49485, 0.49565, 0.42238334]
	best_train_sub_head: 1
	worst: 0.42238334
	avg: 0.46624666
	best: 0.49585

Starting e_i: 44
Model ind 640 epoch 44 head A head_i_epoch 0 batch 0: avg loss -2.442215 avg loss no lamb -2.442215 time 2019-02-16 10:41:21.610409
Model ind 640 epoch 44 head A head_i_epoch 0 batch 100: avg loss -2.406313 avg loss no lamb -2.406313 time 2019-02-16 10:43:44.158141
Model ind 640 epoch 44 head A head_i_epoch 0 batch 200: avg loss -2.425796 avg loss no lamb -2.425796 time 2019-02-16 10:46:06.513431
last batch sz 160
Model ind 640 epoch 44 head B head_i_epoch 0 batch 0: avg loss -1.222180 avg loss no lamb -1.222180 time 2019-02-16 10:47:50.436360
Model ind 640 epoch 44 head B head_i_epoch 0 batch 100: avg loss -1.236442 avg loss no lamb -1.236442 time 2019-02-16 10:50:12.650191
Model ind 640 epoch 44 head B head_i_epoch 0 batch 200: avg loss -1.277840 avg loss no lamb -1.277840 time 2019-02-16 10:52:34.967237
last batch sz 160
Model ind 640 epoch 44 head B head_i_epoch 1 batch 0: avg loss -1.175410 avg loss no lamb -1.175410 time 2019-02-16 10:54:18.188811
Model ind 640 epoch 44 head B head_i_epoch 1 batch 100: avg loss -1.241280 avg loss no lamb -1.241280 time 2019-02-16 10:56:40.667243
Model ind 640 epoch 44 head B head_i_epoch 1 batch 200: avg loss -1.329778 avg loss no lamb -1.329778 time 2019-02-16 10:59:02.467188
last batch sz 160
Pre: time 2019-02-16 11:01:10.060114: 
 	std: 0.04135239
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.40898332, 0.49283335, 0.49358332, 0.49358332, 0.40886667]
	train_accs: [0.40898332, 0.49283335, 0.49358332, 0.49358332, 0.40886667]
	best_train_sub_head: 2
	worst: 0.40886667
	avg: 0.45957002
	best: 0.49358332

Starting e_i: 45
Model ind 640 epoch 45 head A head_i_epoch 0 batch 0: avg loss -2.428688 avg loss no lamb -2.428688 time 2019-02-16 11:01:11.850644
Model ind 640 epoch 45 head A head_i_epoch 0 batch 100: avg loss -2.406364 avg loss no lamb -2.406364 time 2019-02-16 11:03:34.575094
Model ind 640 epoch 45 head A head_i_epoch 0 batch 200: avg loss -2.477716 avg loss no lamb -2.477716 time 2019-02-16 11:05:58.051461
last batch sz 160
Model ind 640 epoch 45 head B head_i_epoch 0 batch 0: avg loss -1.176324 avg loss no lamb -1.176324 time 2019-02-16 11:07:41.684604
Model ind 640 epoch 45 head B head_i_epoch 0 batch 100: avg loss -1.214206 avg loss no lamb -1.214206 time 2019-02-16 11:10:02.957384
Model ind 640 epoch 45 head B head_i_epoch 0 batch 200: avg loss -1.343050 avg loss no lamb -1.343050 time 2019-02-16 11:12:24.333771
last batch sz 160
Model ind 640 epoch 45 head B head_i_epoch 1 batch 0: avg loss -1.230792 avg loss no lamb -1.230792 time 2019-02-16 11:14:08.016132
Model ind 640 epoch 45 head B head_i_epoch 1 batch 100: avg loss -1.224081 avg loss no lamb -1.224081 time 2019-02-16 11:16:29.149578
Model ind 640 epoch 45 head B head_i_epoch 1 batch 200: avg loss -1.271586 avg loss no lamb -1.271586 time 2019-02-16 11:18:50.777287
last batch sz 160
Pre: time 2019-02-16 11:20:58.010866: 
 	std: 0.04313734
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40453333, 0.49271667, 0.49256667, 0.49166667, 0.404]
	train_accs: [0.40453333, 0.49271667, 0.49256667, 0.49166667, 0.404]
	best_train_sub_head: 1
	worst: 0.404
	avg: 0.45709667
	best: 0.49271667

Starting e_i: 46
Model ind 640 epoch 46 head A head_i_epoch 0 batch 0: avg loss -2.385306 avg loss no lamb -2.385306 time 2019-02-16 11:20:59.761249
Model ind 640 epoch 46 head A head_i_epoch 0 batch 100: avg loss -2.473198 avg loss no lamb -2.473198 time 2019-02-16 11:23:21.891497
Model ind 640 epoch 46 head A head_i_epoch 0 batch 200: avg loss -2.510751 avg loss no lamb -2.510751 time 2019-02-16 11:25:45.375103
last batch sz 160
Model ind 640 epoch 46 head B head_i_epoch 0 batch 0: avg loss -1.209332 avg loss no lamb -1.209332 time 2019-02-16 11:27:29.733302
Model ind 640 epoch 46 head B head_i_epoch 0 batch 100: avg loss -1.235505 avg loss no lamb -1.235505 time 2019-02-16 11:29:51.681224
Model ind 640 epoch 46 head B head_i_epoch 0 batch 200: avg loss -1.386949 avg loss no lamb -1.386949 time 2019-02-16 11:32:14.308650
last batch sz 160
Model ind 640 epoch 46 head B head_i_epoch 1 batch 0: avg loss -1.219778 avg loss no lamb -1.219778 time 2019-02-16 11:33:57.559499
Model ind 640 epoch 46 head B head_i_epoch 1 batch 100: avg loss -1.175783 avg loss no lamb -1.175783 time 2019-02-16 11:36:19.094060
Model ind 640 epoch 46 head B head_i_epoch 1 batch 200: avg loss -1.300387 avg loss no lamb -1.300387 time 2019-02-16 11:38:40.503354
last batch sz 160
Pre: time 2019-02-16 11:40:47.176143: 
 	std: 0.039623983
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.40098333, 0.4825, 0.4826, 0.48063335, 0.4011]
	train_accs: [0.40098333, 0.4825, 0.4826, 0.48063335, 0.4011]
	best_train_sub_head: 2
	worst: 0.40098333
	avg: 0.44956332
	best: 0.4826

Starting e_i: 47
Model ind 640 epoch 47 head A head_i_epoch 0 batch 0: avg loss -2.334460 avg loss no lamb -2.334460 time 2019-02-16 11:40:49.217245
Model ind 640 epoch 47 head A head_i_epoch 0 batch 100: avg loss -2.465214 avg loss no lamb -2.465214 time 2019-02-16 11:43:10.998489
Model ind 640 epoch 47 head A head_i_epoch 0 batch 200: avg loss -2.505384 avg loss no lamb -2.505384 time 2019-02-16 11:45:33.963869
last batch sz 160
Model ind 640 epoch 47 head B head_i_epoch 0 batch 0: avg loss -1.241305 avg loss no lamb -1.241305 time 2019-02-16 11:47:17.697399
Model ind 640 epoch 47 head B head_i_epoch 0 batch 100: avg loss -1.252071 avg loss no lamb -1.252071 time 2019-02-16 11:49:39.532110
Model ind 640 epoch 47 head B head_i_epoch 0 batch 200: avg loss -1.351714 avg loss no lamb -1.351714 time 2019-02-16 11:52:01.696338
last batch sz 160
Model ind 640 epoch 47 head B head_i_epoch 1 batch 0: avg loss -1.298928 avg loss no lamb -1.298928 time 2019-02-16 11:53:44.478814
Model ind 640 epoch 47 head B head_i_epoch 1 batch 100: avg loss -1.206015 avg loss no lamb -1.206015 time 2019-02-16 11:56:05.874132
Model ind 640 epoch 47 head B head_i_epoch 1 batch 200: avg loss -1.324957 avg loss no lamb -1.324957 time 2019-02-16 11:58:26.872054
last batch sz 160
Pre: time 2019-02-16 12:00:33.716101: 
 	std: 0.041709386
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40823334, 0.49363333, 0.49345, 0.49265, 0.40798333]
	train_accs: [0.40823334, 0.49363333, 0.49345, 0.49265, 0.40798333]
	best_train_sub_head: 1
	worst: 0.40798333
	avg: 0.45918998
	best: 0.49363333

Starting e_i: 48
Model ind 640 epoch 48 head A head_i_epoch 0 batch 0: avg loss -2.384279 avg loss no lamb -2.384279 time 2019-02-16 12:00:35.546820
Model ind 640 epoch 48 head A head_i_epoch 0 batch 100: avg loss -2.458141 avg loss no lamb -2.458141 time 2019-02-16 12:02:57.400983
Model ind 640 epoch 48 head A head_i_epoch 0 batch 200: avg loss -2.529159 avg loss no lamb -2.529159 time 2019-02-16 12:05:19.865112
last batch sz 160
Model ind 640 epoch 48 head B head_i_epoch 0 batch 0: avg loss -1.232790 avg loss no lamb -1.232790 time 2019-02-16 12:07:03.594015
Model ind 640 epoch 48 head B head_i_epoch 0 batch 100: avg loss -1.287251 avg loss no lamb -1.287251 time 2019-02-16 12:09:25.201994
Model ind 640 epoch 48 head B head_i_epoch 0 batch 200: avg loss -1.391225 avg loss no lamb -1.391225 time 2019-02-16 12:11:47.363488
last batch sz 160
Model ind 640 epoch 48 head B head_i_epoch 1 batch 0: avg loss -1.205837 avg loss no lamb -1.205837 time 2019-02-16 12:13:31.152348
Model ind 640 epoch 48 head B head_i_epoch 1 batch 100: avg loss -1.265831 avg loss no lamb -1.265831 time 2019-02-16 12:15:53.672337
Model ind 640 epoch 48 head B head_i_epoch 1 batch 200: avg loss -1.376671 avg loss no lamb -1.376671 time 2019-02-16 12:18:16.319245
last batch sz 160
Pre: time 2019-02-16 12:20:23.459447: 
 	std: 0.043029997
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.40076667, 0.48793334, 0.48905, 0.48731667, 0.39978334]
	train_accs: [0.40076667, 0.48793334, 0.48905, 0.48731667, 0.39978334]
	best_train_sub_head: 2
	worst: 0.39978334
	avg: 0.45297003
	best: 0.48905

Starting e_i: 49
Model ind 640 epoch 49 head A head_i_epoch 0 batch 0: avg loss -2.482419 avg loss no lamb -2.482419 time 2019-02-16 12:20:25.215952
Model ind 640 epoch 49 head A head_i_epoch 0 batch 100: avg loss -2.567771 avg loss no lamb -2.567771 time 2019-02-16 12:22:47.410326
Model ind 640 epoch 49 head A head_i_epoch 0 batch 200: avg loss -2.496520 avg loss no lamb -2.496520 time 2019-02-16 12:25:10.024216
last batch sz 160
Model ind 640 epoch 49 head B head_i_epoch 0 batch 0: avg loss -1.269656 avg loss no lamb -1.269656 time 2019-02-16 12:26:53.372443
Model ind 640 epoch 49 head B head_i_epoch 0 batch 100: avg loss -1.226663 avg loss no lamb -1.226663 time 2019-02-16 12:29:15.855298
Model ind 640 epoch 49 head B head_i_epoch 0 batch 200: avg loss -1.327885 avg loss no lamb -1.327885 time 2019-02-16 12:31:37.136001
last batch sz 160
Model ind 640 epoch 49 head B head_i_epoch 1 batch 0: avg loss -1.244097 avg loss no lamb -1.244097 time 2019-02-16 12:33:20.604988
Model ind 640 epoch 49 head B head_i_epoch 1 batch 100: avg loss -1.201047 avg loss no lamb -1.201047 time 2019-02-16 12:35:41.774273
Model ind 640 epoch 49 head B head_i_epoch 1 batch 200: avg loss -1.339374 avg loss no lamb -1.339374 time 2019-02-16 12:38:03.111725
last batch sz 160
Pre: time 2019-02-16 12:40:09.679444: 
 	std: 0.045815125
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.40291667, 0.49623334, 0.49798334, 0.49526668, 0.40306666]
	train_accs: [0.40291667, 0.49623334, 0.49798334, 0.49526668, 0.40306666]
	best_train_sub_head: 2
	worst: 0.40291667
	avg: 0.45909333
	best: 0.49798334

Starting e_i: 50
Model ind 640 epoch 50 head A head_i_epoch 0 batch 0: avg loss -2.475785 avg loss no lamb -2.475785 time 2019-02-16 12:40:14.547244
Model ind 640 epoch 50 head A head_i_epoch 0 batch 100: avg loss -2.501660 avg loss no lamb -2.501660 time 2019-02-16 12:42:36.936176
Model ind 640 epoch 50 head A head_i_epoch 0 batch 200: avg loss -2.522060 avg loss no lamb -2.522060 time 2019-02-16 12:44:59.917490
last batch sz 160
Model ind 640 epoch 50 head B head_i_epoch 0 batch 0: avg loss -1.257733 avg loss no lamb -1.257733 time 2019-02-16 12:46:43.550221
Model ind 640 epoch 50 head B head_i_epoch 0 batch 100: avg loss -1.228891 avg loss no lamb -1.228891 time 2019-02-16 12:49:06.022667
Model ind 640 epoch 50 head B head_i_epoch 0 batch 200: avg loss -1.400730 avg loss no lamb -1.400730 time 2019-02-16 12:51:28.544982
last batch sz 160
Model ind 640 epoch 50 head B head_i_epoch 1 batch 0: avg loss -1.216375 avg loss no lamb -1.216375 time 2019-02-16 12:53:11.622442
Model ind 640 epoch 50 head B head_i_epoch 1 batch 100: avg loss -1.305944 avg loss no lamb -1.305944 time 2019-02-16 12:55:33.333297
Model ind 640 epoch 50 head B head_i_epoch 1 batch 200: avg loss -1.397212 avg loss no lamb -1.397212 time 2019-02-16 12:57:56.171393
last batch sz 160
Pre: time 2019-02-16 13:00:04.849381: 
 	std: 0.040416684
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.41445, 0.4961, 0.49828333, 0.49601668, 0.41418332]
	train_accs: [0.41445, 0.4961, 0.49828333, 0.49601668, 0.41418332]
	best_train_sub_head: 2
	worst: 0.41418332
	avg: 0.4638067
	best: 0.49828333

Starting e_i: 51
Model ind 640 epoch 51 head A head_i_epoch 0 batch 0: avg loss -2.430132 avg loss no lamb -2.430132 time 2019-02-16 13:00:14.572692
Model ind 640 epoch 51 head A head_i_epoch 0 batch 100: avg loss -2.482847 avg loss no lamb -2.482847 time 2019-02-16 13:02:37.996129
Model ind 640 epoch 51 head A head_i_epoch 0 batch 200: avg loss -2.592347 avg loss no lamb -2.592347 time 2019-02-16 13:05:00.890948
last batch sz 160
Model ind 640 epoch 51 head B head_i_epoch 0 batch 0: avg loss -1.213509 avg loss no lamb -1.213509 time 2019-02-16 13:06:44.342995
Model ind 640 epoch 51 head B head_i_epoch 0 batch 100: avg loss -1.222921 avg loss no lamb -1.222921 time 2019-02-16 13:09:07.287481
Model ind 640 epoch 51 head B head_i_epoch 0 batch 200: avg loss -1.363522 avg loss no lamb -1.363522 time 2019-02-16 13:11:30.807646
last batch sz 160
Model ind 640 epoch 51 head B head_i_epoch 1 batch 0: avg loss -1.292637 avg loss no lamb -1.292637 time 2019-02-16 13:13:14.822123
Model ind 640 epoch 51 head B head_i_epoch 1 batch 100: avg loss -1.231073 avg loss no lamb -1.231073 time 2019-02-16 13:15:37.969877
Model ind 640 epoch 51 head B head_i_epoch 1 batch 200: avg loss -1.472458 avg loss no lamb -1.472458 time 2019-02-16 13:18:00.409521
last batch sz 160
Pre: time 2019-02-16 13:20:08.175226: 
 	std: 0.04787497
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40705, 0.5050833, 0.5046, 0.50413334, 0.40671667]
	train_accs: [0.40705, 0.5050833, 0.5046, 0.50413334, 0.40671667]
	best_train_sub_head: 1
	worst: 0.40671667
	avg: 0.46551666
	best: 0.5050833

Starting e_i: 52
Model ind 640 epoch 52 head A head_i_epoch 0 batch 0: avg loss -2.410138 avg loss no lamb -2.410138 time 2019-02-16 13:20:13.411646
Model ind 640 epoch 52 head A head_i_epoch 0 batch 100: avg loss -2.475473 avg loss no lamb -2.475473 time 2019-02-16 13:22:35.890711
Model ind 640 epoch 52 head A head_i_epoch 0 batch 200: avg loss -2.467699 avg loss no lamb -2.467699 time 2019-02-16 13:24:58.166535
last batch sz 160
Model ind 640 epoch 52 head B head_i_epoch 0 batch 0: avg loss -1.283478 avg loss no lamb -1.283478 time 2019-02-16 13:26:41.189495
Model ind 640 epoch 52 head B head_i_epoch 0 batch 100: avg loss -1.302226 avg loss no lamb -1.302226 time 2019-02-16 13:29:03.673874
Model ind 640 epoch 52 head B head_i_epoch 0 batch 200: avg loss -1.325965 avg loss no lamb -1.325965 time 2019-02-16 13:31:26.178252
last batch sz 160
Model ind 640 epoch 52 head B head_i_epoch 1 batch 0: avg loss -1.301405 avg loss no lamb -1.301405 time 2019-02-16 13:33:09.846200
Model ind 640 epoch 52 head B head_i_epoch 1 batch 100: avg loss -1.285709 avg loss no lamb -1.285709 time 2019-02-16 13:35:32.384731
Model ind 640 epoch 52 head B head_i_epoch 1 batch 200: avg loss -1.386598 avg loss no lamb -1.386598 time 2019-02-16 13:37:54.123870
last batch sz 160
Pre: time 2019-02-16 13:40:01.406571: 
 	std: 0.043202862
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.40856665, 0.4969, 0.4977, 0.49655, 0.40916666]
	train_accs: [0.40856665, 0.4969, 0.4977, 0.49655, 0.40916666]
	best_train_sub_head: 2
	worst: 0.40856665
	avg: 0.46177664
	best: 0.4977

Starting e_i: 53
Model ind 640 epoch 53 head A head_i_epoch 0 batch 0: avg loss -2.434531 avg loss no lamb -2.434531 time 2019-02-16 13:40:03.152675
Model ind 640 epoch 53 head A head_i_epoch 0 batch 100: avg loss -2.475375 avg loss no lamb -2.475375 time 2019-02-16 13:42:25.855581
Model ind 640 epoch 53 head A head_i_epoch 0 batch 200: avg loss -2.513604 avg loss no lamb -2.513604 time 2019-02-16 13:44:47.487573
last batch sz 160
Model ind 640 epoch 53 head B head_i_epoch 0 batch 0: avg loss -1.301537 avg loss no lamb -1.301537 time 2019-02-16 13:46:30.376775
Model ind 640 epoch 53 head B head_i_epoch 0 batch 100: avg loss -1.343272 avg loss no lamb -1.343272 time 2019-02-16 13:48:52.544846
Model ind 640 epoch 53 head B head_i_epoch 0 batch 200: avg loss -1.367729 avg loss no lamb -1.367729 time 2019-02-16 13:51:15.139689
last batch sz 160
Model ind 640 epoch 53 head B head_i_epoch 1 batch 0: avg loss -1.220233 avg loss no lamb -1.220233 time 2019-02-16 13:52:59.055140
Model ind 640 epoch 53 head B head_i_epoch 1 batch 100: avg loss -1.245204 avg loss no lamb -1.245204 time 2019-02-16 13:55:21.761348
Model ind 640 epoch 53 head B head_i_epoch 1 batch 200: avg loss -1.344227 avg loss no lamb -1.344227 time 2019-02-16 13:57:43.256171
last batch sz 160
Pre: time 2019-02-16 13:59:49.925461: 
 	std: 0.04902902
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41015, 0.51173335, 0.5114667, 0.51115, 0.41261667]
	train_accs: [0.41015, 0.51173335, 0.5114667, 0.51115, 0.41261667]
	best_train_sub_head: 1
	worst: 0.41015
	avg: 0.47142333
	best: 0.51173335

Starting e_i: 54
Model ind 640 epoch 54 head A head_i_epoch 0 batch 0: avg loss -2.460994 avg loss no lamb -2.460994 time 2019-02-16 13:59:54.850314
Model ind 640 epoch 54 head A head_i_epoch 0 batch 100: avg loss -2.561660 avg loss no lamb -2.561660 time 2019-02-16 14:02:16.237039
Model ind 640 epoch 54 head A head_i_epoch 0 batch 200: avg loss -2.572306 avg loss no lamb -2.572306 time 2019-02-16 14:04:37.529066
last batch sz 160
Model ind 640 epoch 54 head B head_i_epoch 0 batch 0: avg loss -1.316308 avg loss no lamb -1.316308 time 2019-02-16 14:06:20.441612
Model ind 640 epoch 54 head B head_i_epoch 0 batch 100: avg loss -1.200782 avg loss no lamb -1.200782 time 2019-02-16 14:08:41.596235
Model ind 640 epoch 54 head B head_i_epoch 0 batch 200: avg loss -1.305014 avg loss no lamb -1.305014 time 2019-02-16 14:11:02.687427
last batch sz 160
Model ind 640 epoch 54 head B head_i_epoch 1 batch 0: avg loss -1.232923 avg loss no lamb -1.232923 time 2019-02-16 14:12:45.451337
Model ind 640 epoch 54 head B head_i_epoch 1 batch 100: avg loss -1.289173 avg loss no lamb -1.289173 time 2019-02-16 14:15:06.964377
Model ind 640 epoch 54 head B head_i_epoch 1 batch 200: avg loss -1.396671 avg loss no lamb -1.396671 time 2019-02-16 14:17:28.916616
last batch sz 160
Pre: time 2019-02-16 14:19:36.252283: 
 	std: 0.046728995
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4108, 0.50505, 0.5064, 0.5053333, 0.40963334]
	train_accs: [0.4108, 0.50505, 0.5064, 0.5053333, 0.40963334]
	best_train_sub_head: 2
	worst: 0.40963334
	avg: 0.46744332
	best: 0.5064

Starting e_i: 55
Model ind 640 epoch 55 head A head_i_epoch 0 batch 0: avg loss -2.433767 avg loss no lamb -2.433767 time 2019-02-16 14:19:38.075062
Model ind 640 epoch 55 head A head_i_epoch 0 batch 100: avg loss -2.468680 avg loss no lamb -2.468680 time 2019-02-16 14:21:59.308301
Model ind 640 epoch 55 head A head_i_epoch 0 batch 200: avg loss -2.571925 avg loss no lamb -2.571925 time 2019-02-16 14:24:20.892030
last batch sz 160
Model ind 640 epoch 55 head B head_i_epoch 0 batch 0: avg loss -1.282093 avg loss no lamb -1.282093 time 2019-02-16 14:26:04.764959
Model ind 640 epoch 55 head B head_i_epoch 0 batch 100: avg loss -1.275859 avg loss no lamb -1.275859 time 2019-02-16 14:28:26.389053
Model ind 640 epoch 55 head B head_i_epoch 0 batch 200: avg loss -1.419741 avg loss no lamb -1.419741 time 2019-02-16 14:30:49.000682
last batch sz 160
Model ind 640 epoch 55 head B head_i_epoch 1 batch 0: avg loss -1.234565 avg loss no lamb -1.234565 time 2019-02-16 14:32:32.594382
Model ind 640 epoch 55 head B head_i_epoch 1 batch 100: avg loss -1.288282 avg loss no lamb -1.288282 time 2019-02-16 14:34:55.094781
Model ind 640 epoch 55 head B head_i_epoch 1 batch 200: avg loss -1.408070 avg loss no lamb -1.408070 time 2019-02-16 14:37:18.360077
last batch sz 160
Pre: time 2019-02-16 14:39:26.882795: 
 	std: 0.047774907
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.41295, 0.5085667, 0.50916666, 0.5079, 0.40916666]
	train_accs: [0.41295, 0.5085667, 0.50916666, 0.5079, 0.40916666]
	best_train_sub_head: 2
	worst: 0.40916666
	avg: 0.46954998
	best: 0.50916666

Starting e_i: 56
Model ind 640 epoch 56 head A head_i_epoch 0 batch 0: avg loss -2.445216 avg loss no lamb -2.445216 time 2019-02-16 14:39:28.728325
Model ind 640 epoch 56 head A head_i_epoch 0 batch 100: avg loss -2.563895 avg loss no lamb -2.563895 time 2019-02-16 14:41:50.360532
Model ind 640 epoch 56 head A head_i_epoch 0 batch 200: avg loss -2.556359 avg loss no lamb -2.556359 time 2019-02-16 14:44:11.991249
last batch sz 160
Model ind 640 epoch 56 head B head_i_epoch 0 batch 0: avg loss -1.266419 avg loss no lamb -1.266419 time 2019-02-16 14:45:54.824208
Model ind 640 epoch 56 head B head_i_epoch 0 batch 100: avg loss -1.311330 avg loss no lamb -1.311330 time 2019-02-16 14:48:16.409067
Model ind 640 epoch 56 head B head_i_epoch 0 batch 200: avg loss -1.354661 avg loss no lamb -1.354661 time 2019-02-16 14:50:37.083914
last batch sz 160
Model ind 640 epoch 56 head B head_i_epoch 1 batch 0: avg loss -1.189387 avg loss no lamb -1.189387 time 2019-02-16 14:52:19.572193
Model ind 640 epoch 56 head B head_i_epoch 1 batch 100: avg loss -1.274895 avg loss no lamb -1.274895 time 2019-02-16 14:54:41.554253
Model ind 640 epoch 56 head B head_i_epoch 1 batch 200: avg loss -1.352544 avg loss no lamb -1.352544 time 2019-02-16 14:57:02.787752
last batch sz 160
Pre: time 2019-02-16 14:59:10.926876: 
 	std: 0.045863576
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42413333, 0.5163, 0.51661664, 0.51603335, 0.4213]
	train_accs: [0.42413333, 0.5163, 0.51661664, 0.51603335, 0.4213]
	best_train_sub_head: 2
	worst: 0.4213
	avg: 0.47887668
	best: 0.51661664

Starting e_i: 57
Model ind 640 epoch 57 head A head_i_epoch 0 batch 0: avg loss -2.471250 avg loss no lamb -2.471250 time 2019-02-16 14:59:15.784245
Model ind 640 epoch 57 head A head_i_epoch 0 batch 100: avg loss -2.518774 avg loss no lamb -2.518774 time 2019-02-16 15:01:38.241601
Model ind 640 epoch 57 head A head_i_epoch 0 batch 200: avg loss -2.559444 avg loss no lamb -2.559444 time 2019-02-16 15:04:00.488150
last batch sz 160
Model ind 640 epoch 57 head B head_i_epoch 0 batch 0: avg loss -1.222599 avg loss no lamb -1.222599 time 2019-02-16 15:05:43.867113
Model ind 640 epoch 57 head B head_i_epoch 0 batch 100: avg loss -1.262990 avg loss no lamb -1.262990 time 2019-02-16 15:08:05.257388
Model ind 640 epoch 57 head B head_i_epoch 0 batch 200: avg loss -1.276442 avg loss no lamb -1.276442 time 2019-02-16 15:10:26.649685
last batch sz 160
Model ind 640 epoch 57 head B head_i_epoch 1 batch 0: avg loss -1.291220 avg loss no lamb -1.291220 time 2019-02-16 15:12:09.797229
Model ind 640 epoch 57 head B head_i_epoch 1 batch 100: avg loss -1.287527 avg loss no lamb -1.287527 time 2019-02-16 15:14:31.192905
Model ind 640 epoch 57 head B head_i_epoch 1 batch 200: avg loss -1.384925 avg loss no lamb -1.384925 time 2019-02-16 15:16:52.274208
last batch sz 160
Pre: time 2019-02-16 15:18:58.751935: 
 	std: 0.048384525
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 0), (6, 9), (7, 3), (8, 6), (9, 2)]
	test_accs: [0.40521666, 0.50325, 0.5028833, 0.50416666, 0.40413332]
	train_accs: [0.40521666, 0.50325, 0.5028833, 0.50416666, 0.40413332]
	best_train_sub_head: 3
	worst: 0.40413332
	avg: 0.46392998
	best: 0.50416666

Starting e_i: 58
Model ind 640 epoch 58 head A head_i_epoch 0 batch 0: avg loss -2.467632 avg loss no lamb -2.467632 time 2019-02-16 15:19:00.614271
Model ind 640 epoch 58 head A head_i_epoch 0 batch 100: avg loss -2.489599 avg loss no lamb -2.489599 time 2019-02-16 15:21:23.192489
Model ind 640 epoch 58 head A head_i_epoch 0 batch 200: avg loss -2.510596 avg loss no lamb -2.510596 time 2019-02-16 15:23:45.611988
last batch sz 160
Model ind 640 epoch 58 head B head_i_epoch 0 batch 0: avg loss -1.246949 avg loss no lamb -1.246949 time 2019-02-16 15:25:28.597414
Model ind 640 epoch 58 head B head_i_epoch 0 batch 100: avg loss -1.195646 avg loss no lamb -1.195646 time 2019-02-16 15:27:51.064347
Model ind 640 epoch 58 head B head_i_epoch 0 batch 200: avg loss -1.377696 avg loss no lamb -1.377696 time 2019-02-16 15:30:13.121349
last batch sz 160
Model ind 640 epoch 58 head B head_i_epoch 1 batch 0: avg loss -1.306412 avg loss no lamb -1.306412 time 2019-02-16 15:31:55.834382
Model ind 640 epoch 58 head B head_i_epoch 1 batch 100: avg loss -1.251199 avg loss no lamb -1.251199 time 2019-02-16 15:34:17.038613
Model ind 640 epoch 58 head B head_i_epoch 1 batch 200: avg loss -1.396716 avg loss no lamb -1.396716 time 2019-02-16 15:36:38.247286
last batch sz 160
Pre: time 2019-02-16 15:38:44.868314: 
 	std: 0.047989663
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 2), (6, 0), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40531668, 0.5024167, 0.50235, 0.50208336, 0.40335]
	train_accs: [0.40531668, 0.5024167, 0.50235, 0.50208336, 0.40335]
	best_train_sub_head: 1
	worst: 0.40335
	avg: 0.4631033
	best: 0.5024167

Starting e_i: 59
Model ind 640 epoch 59 head A head_i_epoch 0 batch 0: avg loss -2.435576 avg loss no lamb -2.435576 time 2019-02-16 15:38:46.613544
Model ind 640 epoch 59 head A head_i_epoch 0 batch 100: avg loss -2.537317 avg loss no lamb -2.537317 time 2019-02-16 15:41:08.331471
Model ind 640 epoch 59 head A head_i_epoch 0 batch 200: avg loss -2.473412 avg loss no lamb -2.473412 time 2019-02-16 15:43:30.690606
last batch sz 160
Model ind 640 epoch 59 head B head_i_epoch 0 batch 0: avg loss -1.296428 avg loss no lamb -1.296428 time 2019-02-16 15:45:14.462366
Model ind 640 epoch 59 head B head_i_epoch 0 batch 100: avg loss -1.222936 avg loss no lamb -1.222936 time 2019-02-16 15:47:35.813346
Model ind 640 epoch 59 head B head_i_epoch 0 batch 200: avg loss -1.366359 avg loss no lamb -1.366359 time 2019-02-16 15:49:57.284599
last batch sz 160
Model ind 640 epoch 59 head B head_i_epoch 1 batch 0: avg loss -1.281015 avg loss no lamb -1.281015 time 2019-02-16 15:51:40.159122
Model ind 640 epoch 59 head B head_i_epoch 1 batch 100: avg loss -1.180487 avg loss no lamb -1.180487 time 2019-02-16 15:54:03.029362
Model ind 640 epoch 59 head B head_i_epoch 1 batch 200: avg loss -1.400568 avg loss no lamb -1.400568 time 2019-02-16 15:56:25.784641
last batch sz 160
Pre: time 2019-02-16 15:58:34.297891: 
 	std: 0.045925174
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.41181666, 0.5039833, 0.504, 0.5048, 0.40925]
	train_accs: [0.41181666, 0.5039833, 0.504, 0.5048, 0.40925]
	best_train_sub_head: 3
	worst: 0.40925
	avg: 0.46677002
	best: 0.5048

Starting e_i: 60
Model ind 640 epoch 60 head A head_i_epoch 0 batch 0: avg loss -2.481913 avg loss no lamb -2.481913 time 2019-02-16 15:58:36.108800
Model ind 640 epoch 60 head A head_i_epoch 0 batch 100: avg loss -2.480105 avg loss no lamb -2.480105 time 2019-02-16 16:00:59.073113
Model ind 640 epoch 60 head A head_i_epoch 0 batch 200: avg loss -2.541805 avg loss no lamb -2.541805 time 2019-02-16 16:03:21.232875
last batch sz 160
Model ind 640 epoch 60 head B head_i_epoch 0 batch 0: avg loss -1.280193 avg loss no lamb -1.280193 time 2019-02-16 16:05:04.769014
Model ind 640 epoch 60 head B head_i_epoch 0 batch 100: avg loss -1.212117 avg loss no lamb -1.212117 time 2019-02-16 16:07:26.982117
Model ind 640 epoch 60 head B head_i_epoch 0 batch 200: avg loss -1.338341 avg loss no lamb -1.338341 time 2019-02-16 16:09:49.659925
last batch sz 160
Model ind 640 epoch 60 head B head_i_epoch 1 batch 0: avg loss -1.196498 avg loss no lamb -1.196498 time 2019-02-16 16:11:33.787898
Model ind 640 epoch 60 head B head_i_epoch 1 batch 100: avg loss -1.248805 avg loss no lamb -1.248805 time 2019-02-16 16:13:56.777071
Model ind 640 epoch 60 head B head_i_epoch 1 batch 200: avg loss -1.434924 avg loss no lamb -1.434924 time 2019-02-16 16:16:19.029311
last batch sz 160
Pre: time 2019-02-16 16:18:26.688979: 
 	std: 0.047837276
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41226667, 0.5100333, 0.50905, 0.50995, 0.4118]
	train_accs: [0.41226667, 0.5100333, 0.50905, 0.50995, 0.4118]
	best_train_sub_head: 1
	worst: 0.4118
	avg: 0.47061998
	best: 0.5100333

Starting e_i: 61
Model ind 640 epoch 61 head A head_i_epoch 0 batch 0: avg loss -2.533947 avg loss no lamb -2.533947 time 2019-02-16 16:18:31.795750
Model ind 640 epoch 61 head A head_i_epoch 0 batch 100: avg loss -2.494095 avg loss no lamb -2.494095 time 2019-02-16 16:20:54.234900
Model ind 640 epoch 61 head A head_i_epoch 0 batch 200: avg loss -2.508117 avg loss no lamb -2.508117 time 2019-02-16 16:23:16.481804
last batch sz 160
Model ind 640 epoch 61 head B head_i_epoch 0 batch 0: avg loss -1.268975 avg loss no lamb -1.268975 time 2019-02-16 16:24:59.414421
Model ind 640 epoch 61 head B head_i_epoch 0 batch 100: avg loss -1.249334 avg loss no lamb -1.249334 time 2019-02-16 16:27:21.928896
Model ind 640 epoch 61 head B head_i_epoch 0 batch 200: avg loss -1.362814 avg loss no lamb -1.362814 time 2019-02-16 16:29:44.537706
last batch sz 160
Model ind 640 epoch 61 head B head_i_epoch 1 batch 0: avg loss -1.380965 avg loss no lamb -1.380965 time 2019-02-16 16:31:28.183220
Model ind 640 epoch 61 head B head_i_epoch 1 batch 100: avg loss -1.355300 avg loss no lamb -1.355300 time 2019-02-16 16:33:50.664958
Model ind 640 epoch 61 head B head_i_epoch 1 batch 200: avg loss -1.402405 avg loss no lamb -1.402405 time 2019-02-16 16:36:13.443300
last batch sz 160
Pre: time 2019-02-16 16:38:21.658840: 
 	std: 0.048074063
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.40775, 0.5067667, 0.50615, 0.5061167, 0.40868333]
	train_accs: [0.40775, 0.5067667, 0.50615, 0.5061167, 0.40868333]
	best_train_sub_head: 1
	worst: 0.40775
	avg: 0.46709332
	best: 0.5067667

Starting e_i: 62
Model ind 640 epoch 62 head A head_i_epoch 0 batch 0: avg loss -2.498799 avg loss no lamb -2.498799 time 2019-02-16 16:38:23.458833
Model ind 640 epoch 62 head A head_i_epoch 0 batch 100: avg loss -2.569567 avg loss no lamb -2.569567 time 2019-02-16 16:40:46.645337
Model ind 640 epoch 62 head A head_i_epoch 0 batch 200: avg loss -2.562788 avg loss no lamb -2.562788 time 2019-02-16 16:43:09.514747
last batch sz 160
Model ind 640 epoch 62 head B head_i_epoch 0 batch 0: avg loss -1.330030 avg loss no lamb -1.330030 time 2019-02-16 16:44:53.255334
Model ind 640 epoch 62 head B head_i_epoch 0 batch 100: avg loss -1.260157 avg loss no lamb -1.260157 time 2019-02-16 16:47:15.672202
Model ind 640 epoch 62 head B head_i_epoch 0 batch 200: avg loss -1.321674 avg loss no lamb -1.321674 time 2019-02-16 16:49:38.410444
last batch sz 160
Model ind 640 epoch 62 head B head_i_epoch 1 batch 0: avg loss -1.282826 avg loss no lamb -1.282826 time 2019-02-16 16:51:22.064346
Model ind 640 epoch 62 head B head_i_epoch 1 batch 100: avg loss -1.271291 avg loss no lamb -1.271291 time 2019-02-16 16:53:43.660797
Model ind 640 epoch 62 head B head_i_epoch 1 batch 200: avg loss -1.352770 avg loss no lamb -1.352770 time 2019-02-16 16:56:04.959305
last batch sz 160
Pre: time 2019-02-16 16:58:13.283096: 
 	std: 0.053306438
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.40798333, 0.5168333, 0.51561666, 0.5172167, 0.40751666]
	train_accs: [0.40798333, 0.5168333, 0.51561666, 0.5172167, 0.40751666]
	best_train_sub_head: 3
	worst: 0.40751666
	avg: 0.47303334
	best: 0.5172167

Starting e_i: 63
Model ind 640 epoch 63 head A head_i_epoch 0 batch 0: avg loss -2.407040 avg loss no lamb -2.407040 time 2019-02-16 16:58:19.949985
Model ind 640 epoch 63 head A head_i_epoch 0 batch 100: avg loss -2.528890 avg loss no lamb -2.528890 time 2019-02-16 17:00:41.903903
Model ind 640 epoch 63 head A head_i_epoch 0 batch 200: avg loss -2.668723 avg loss no lamb -2.668723 time 2019-02-16 17:03:03.897963
last batch sz 160
Model ind 640 epoch 63 head B head_i_epoch 0 batch 0: avg loss -1.291370 avg loss no lamb -1.291370 time 2019-02-16 17:04:47.239473
Model ind 640 epoch 63 head B head_i_epoch 0 batch 100: avg loss -1.278953 avg loss no lamb -1.278953 time 2019-02-16 17:07:08.735649
Model ind 640 epoch 63 head B head_i_epoch 0 batch 200: avg loss -1.413992 avg loss no lamb -1.413992 time 2019-02-16 17:09:30.183652
last batch sz 160
Model ind 640 epoch 63 head B head_i_epoch 1 batch 0: avg loss -1.211560 avg loss no lamb -1.211560 time 2019-02-16 17:11:12.824458
Model ind 640 epoch 63 head B head_i_epoch 1 batch 100: avg loss -1.208345 avg loss no lamb -1.208345 time 2019-02-16 17:13:34.002363
Model ind 640 epoch 63 head B head_i_epoch 1 batch 200: avg loss -1.346324 avg loss no lamb -1.346324 time 2019-02-16 17:15:55.419357
last batch sz 160
Pre: time 2019-02-16 17:18:03.180986: 
 	std: 0.04897097
	best_train_sub_head_match: [(0, 0), (1, 8), (2, 1), (3, 9), (4, 5), (5, 2), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4021, 0.50221664, 0.50238335, 0.5022333, 0.40253332]
	train_accs: [0.4021, 0.50221664, 0.50238335, 0.5022333, 0.40253332]
	best_train_sub_head: 2
	worst: 0.4021
	avg: 0.46229333
	best: 0.50238335

Starting e_i: 64
Model ind 640 epoch 64 head A head_i_epoch 0 batch 0: avg loss -2.462790 avg loss no lamb -2.462790 time 2019-02-16 17:18:05.124659
Model ind 640 epoch 64 head A head_i_epoch 0 batch 100: avg loss -2.520333 avg loss no lamb -2.520333 time 2019-02-16 17:20:28.172284
Model ind 640 epoch 64 head A head_i_epoch 0 batch 200: avg loss -2.516710 avg loss no lamb -2.516710 time 2019-02-16 17:22:51.052801
last batch sz 160
Model ind 640 epoch 64 head B head_i_epoch 0 batch 0: avg loss -1.262186 avg loss no lamb -1.262186 time 2019-02-16 17:24:35.013511
Model ind 640 epoch 64 head B head_i_epoch 0 batch 100: avg loss -1.336702 avg loss no lamb -1.336702 time 2019-02-16 17:26:56.795041
Model ind 640 epoch 64 head B head_i_epoch 0 batch 200: avg loss -1.394985 avg loss no lamb -1.394985 time 2019-02-16 17:29:18.926262
last batch sz 160
Model ind 640 epoch 64 head B head_i_epoch 1 batch 0: avg loss -1.339263 avg loss no lamb -1.339263 time 2019-02-16 17:31:02.394067
Model ind 640 epoch 64 head B head_i_epoch 1 batch 100: avg loss -1.341359 avg loss no lamb -1.341359 time 2019-02-16 17:33:24.578497
Model ind 640 epoch 64 head B head_i_epoch 1 batch 200: avg loss -1.403247 avg loss no lamb -1.403247 time 2019-02-16 17:35:46.202974
last batch sz 160
Pre: time 2019-02-16 17:37:53.900819: 
 	std: 0.050543718
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4197, 0.52251667, 0.52236664, 0.52215, 0.41865]
	train_accs: [0.4197, 0.52251667, 0.52236664, 0.52215, 0.41865]
	best_train_sub_head: 1
	worst: 0.41865
	avg: 0.48107663
	best: 0.52251667

Starting e_i: 65
Model ind 640 epoch 65 head A head_i_epoch 0 batch 0: avg loss -2.527093 avg loss no lamb -2.527093 time 2019-02-16 17:37:59.358943
Model ind 640 epoch 65 head A head_i_epoch 0 batch 100: avg loss -2.512131 avg loss no lamb -2.512131 time 2019-02-16 17:40:22.731749
Model ind 640 epoch 65 head A head_i_epoch 0 batch 200: avg loss -2.560609 avg loss no lamb -2.560609 time 2019-02-16 17:42:45.587703
last batch sz 160
Model ind 640 epoch 65 head B head_i_epoch 0 batch 0: avg loss -1.288503 avg loss no lamb -1.288503 time 2019-02-16 17:44:29.456314
Model ind 640 epoch 65 head B head_i_epoch 0 batch 100: avg loss -1.327281 avg loss no lamb -1.327281 time 2019-02-16 17:46:51.835349
Model ind 640 epoch 65 head B head_i_epoch 0 batch 200: avg loss -1.386795 avg loss no lamb -1.386795 time 2019-02-16 17:49:14.296160
last batch sz 160
Model ind 640 epoch 65 head B head_i_epoch 1 batch 0: avg loss -1.294142 avg loss no lamb -1.294142 time 2019-02-16 17:50:57.522675
Model ind 640 epoch 65 head B head_i_epoch 1 batch 100: avg loss -1.288163 avg loss no lamb -1.288163 time 2019-02-16 17:53:19.285071
Model ind 640 epoch 65 head B head_i_epoch 1 batch 200: avg loss -1.385977 avg loss no lamb -1.385977 time 2019-02-16 17:55:41.831121
last batch sz 160
Pre: time 2019-02-16 17:57:50.292332: 
 	std: 0.052053776
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41306666, 0.51905, 0.5179833, 0.51805, 0.41116667]
	train_accs: [0.41306666, 0.51905, 0.5179833, 0.51805, 0.41116667]
	best_train_sub_head: 1
	worst: 0.41116667
	avg: 0.4758633
	best: 0.51905

Starting e_i: 66
Model ind 640 epoch 66 head A head_i_epoch 0 batch 0: avg loss -2.516388 avg loss no lamb -2.516388 time 2019-02-16 17:57:52.122360
Model ind 640 epoch 66 head A head_i_epoch 0 batch 100: avg loss -2.554347 avg loss no lamb -2.554347 time 2019-02-16 18:00:15.279572
Model ind 640 epoch 66 head A head_i_epoch 0 batch 200: avg loss -2.574709 avg loss no lamb -2.574709 time 2019-02-16 18:02:37.914689
last batch sz 160
Model ind 640 epoch 66 head B head_i_epoch 0 batch 0: avg loss -1.292745 avg loss no lamb -1.292745 time 2019-02-16 18:04:21.035733
Model ind 640 epoch 66 head B head_i_epoch 0 batch 100: avg loss -1.214205 avg loss no lamb -1.214205 time 2019-02-16 18:06:42.689703
Model ind 640 epoch 66 head B head_i_epoch 0 batch 200: avg loss -1.367911 avg loss no lamb -1.367911 time 2019-02-16 18:09:05.472469
last batch sz 160
Model ind 640 epoch 66 head B head_i_epoch 1 batch 0: avg loss -1.222320 avg loss no lamb -1.222320 time 2019-02-16 18:10:48.635008
Model ind 640 epoch 66 head B head_i_epoch 1 batch 100: avg loss -1.245059 avg loss no lamb -1.245059 time 2019-02-16 18:13:09.967099
Model ind 640 epoch 66 head B head_i_epoch 1 batch 200: avg loss -1.449944 avg loss no lamb -1.449944 time 2019-02-16 18:15:31.642223
last batch sz 160
Pre: time 2019-02-16 18:17:39.855369: 
 	std: 0.053803653
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4132, 0.52166665, 0.52176666, 0.52178335, 0.41065]
	train_accs: [0.4132, 0.52166665, 0.52176666, 0.52178335, 0.41065]
	best_train_sub_head: 3
	worst: 0.41065
	avg: 0.47781333
	best: 0.52178335

Starting e_i: 67
Model ind 640 epoch 67 head A head_i_epoch 0 batch 0: avg loss -2.533604 avg loss no lamb -2.533604 time 2019-02-16 18:17:41.765208
Model ind 640 epoch 67 head A head_i_epoch 0 batch 100: avg loss -2.463674 avg loss no lamb -2.463674 time 2019-02-16 18:20:04.720264
Model ind 640 epoch 67 head A head_i_epoch 0 batch 200: avg loss -2.535485 avg loss no lamb -2.535485 time 2019-02-16 18:22:27.629436
last batch sz 160
Model ind 640 epoch 67 head B head_i_epoch 0 batch 0: avg loss -1.310898 avg loss no lamb -1.310898 time 2019-02-16 18:24:12.011845
Model ind 640 epoch 67 head B head_i_epoch 0 batch 100: avg loss -1.247860 avg loss no lamb -1.247860 time 2019-02-16 18:26:34.948964
Model ind 640 epoch 67 head B head_i_epoch 0 batch 200: avg loss -1.428353 avg loss no lamb -1.428353 time 2019-02-16 18:28:56.903182
last batch sz 160
Model ind 640 epoch 67 head B head_i_epoch 1 batch 0: avg loss -1.241057 avg loss no lamb -1.241057 time 2019-02-16 18:30:40.043742
Model ind 640 epoch 67 head B head_i_epoch 1 batch 100: avg loss -1.281602 avg loss no lamb -1.281602 time 2019-02-16 18:33:02.547154
Model ind 640 epoch 67 head B head_i_epoch 1 batch 200: avg loss -1.381454 avg loss no lamb -1.381454 time 2019-02-16 18:35:25.146471
last batch sz 160
Pre: time 2019-02-16 18:37:33.639418: 
 	std: 0.04980673
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4215, 0.5213, 0.52065, 0.5218833, 0.41778332]
	train_accs: [0.4215, 0.5213, 0.52065, 0.5218833, 0.41778332]
	best_train_sub_head: 3
	worst: 0.41778332
	avg: 0.48062333
	best: 0.5218833

Starting e_i: 68
Model ind 640 epoch 68 head A head_i_epoch 0 batch 0: avg loss -2.561159 avg loss no lamb -2.561159 time 2019-02-16 18:37:35.520914
Model ind 640 epoch 68 head A head_i_epoch 0 batch 100: avg loss -2.600905 avg loss no lamb -2.600905 time 2019-02-16 18:39:58.065059
Model ind 640 epoch 68 head A head_i_epoch 0 batch 200: avg loss -2.617177 avg loss no lamb -2.617177 time 2019-02-16 18:42:20.000840
last batch sz 160
Model ind 640 epoch 68 head B head_i_epoch 0 batch 0: avg loss -1.273173 avg loss no lamb -1.273173 time 2019-02-16 18:44:02.827526
Model ind 640 epoch 68 head B head_i_epoch 0 batch 100: avg loss -1.339014 avg loss no lamb -1.339014 time 2019-02-16 18:46:24.142627
Model ind 640 epoch 68 head B head_i_epoch 0 batch 200: avg loss -1.376183 avg loss no lamb -1.376183 time 2019-02-16 18:48:45.973041
last batch sz 160
Model ind 640 epoch 68 head B head_i_epoch 1 batch 0: avg loss -1.310290 avg loss no lamb -1.310290 time 2019-02-16 18:50:30.124563
Model ind 640 epoch 68 head B head_i_epoch 1 batch 100: avg loss -1.228053 avg loss no lamb -1.228053 time 2019-02-16 18:52:53.243342
Model ind 640 epoch 68 head B head_i_epoch 1 batch 200: avg loss -1.326215 avg loss no lamb -1.326215 time 2019-02-16 18:55:16.126130
last batch sz 160
Pre: time 2019-02-16 18:57:24.463390: 
 	std: 0.051431812
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41208333, 0.51678336, 0.5165667, 0.51601666, 0.41086668]
	train_accs: [0.41208333, 0.51678336, 0.5165667, 0.51601666, 0.41086668]
	best_train_sub_head: 1
	worst: 0.41086668
	avg: 0.47446337
	best: 0.51678336

Starting e_i: 69
Model ind 640 epoch 69 head A head_i_epoch 0 batch 0: avg loss -2.517673 avg loss no lamb -2.517673 time 2019-02-16 18:57:26.289030
Model ind 640 epoch 69 head A head_i_epoch 0 batch 100: avg loss -2.510417 avg loss no lamb -2.510417 time 2019-02-16 18:59:49.932873
Model ind 640 epoch 69 head A head_i_epoch 0 batch 200: avg loss -2.576397 avg loss no lamb -2.576397 time 2019-02-16 19:02:13.070160
last batch sz 160
Model ind 640 epoch 69 head B head_i_epoch 0 batch 0: avg loss -1.252711 avg loss no lamb -1.252711 time 2019-02-16 19:03:57.699828
Model ind 640 epoch 69 head B head_i_epoch 0 batch 100: avg loss -1.191914 avg loss no lamb -1.191914 time 2019-02-16 19:06:20.326835
Model ind 640 epoch 69 head B head_i_epoch 0 batch 200: avg loss -1.432920 avg loss no lamb -1.432920 time 2019-02-16 19:08:41.881951
last batch sz 160
Model ind 640 epoch 69 head B head_i_epoch 1 batch 0: avg loss -1.317339 avg loss no lamb -1.317339 time 2019-02-16 19:10:25.507164
Model ind 640 epoch 69 head B head_i_epoch 1 batch 100: avg loss -1.257882 avg loss no lamb -1.257882 time 2019-02-16 19:12:47.898977
Model ind 640 epoch 69 head B head_i_epoch 1 batch 200: avg loss -1.438395 avg loss no lamb -1.438395 time 2019-02-16 19:15:10.102462
last batch sz 160
Pre: time 2019-02-16 19:17:17.863892: 
 	std: 0.052073304
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42481667, 0.5301, 0.5312833, 0.53033334, 0.42375]
	train_accs: [0.42481667, 0.5301, 0.5312833, 0.53033334, 0.42375]
	best_train_sub_head: 2
	worst: 0.42375
	avg: 0.48805666
	best: 0.5312833

Starting e_i: 70
Model ind 640 epoch 70 head A head_i_epoch 0 batch 0: avg loss -2.534613 avg loss no lamb -2.534613 time 2019-02-16 19:17:24.141968
Model ind 640 epoch 70 head A head_i_epoch 0 batch 100: avg loss -2.592836 avg loss no lamb -2.592836 time 2019-02-16 19:19:46.014316
Model ind 640 epoch 70 head A head_i_epoch 0 batch 200: avg loss -2.608598 avg loss no lamb -2.608598 time 2019-02-16 19:22:09.274636
last batch sz 160
Model ind 640 epoch 70 head B head_i_epoch 0 batch 0: avg loss -1.325966 avg loss no lamb -1.325966 time 2019-02-16 19:23:53.519483
Model ind 640 epoch 70 head B head_i_epoch 0 batch 100: avg loss -1.283496 avg loss no lamb -1.283496 time 2019-02-16 19:26:16.555931
Model ind 640 epoch 70 head B head_i_epoch 0 batch 200: avg loss -1.451671 avg loss no lamb -1.451671 time 2019-02-16 19:28:39.537114
last batch sz 160
Model ind 640 epoch 70 head B head_i_epoch 1 batch 0: avg loss -1.292763 avg loss no lamb -1.292763 time 2019-02-16 19:30:23.553053
Model ind 640 epoch 70 head B head_i_epoch 1 batch 100: avg loss -1.219146 avg loss no lamb -1.219146 time 2019-02-16 19:32:46.313651
Model ind 640 epoch 70 head B head_i_epoch 1 batch 200: avg loss -1.350594 avg loss no lamb -1.350594 time 2019-02-16 19:35:08.251449
last batch sz 160
Pre: time 2019-02-16 19:37:16.008174: 
 	std: 0.05236781
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41883335, 0.52606666, 0.52601665, 0.5255, 0.4191]
	train_accs: [0.41883335, 0.52606666, 0.52601665, 0.5255, 0.4191]
	best_train_sub_head: 1
	worst: 0.41883335
	avg: 0.48310336
	best: 0.52606666

Starting e_i: 71
Model ind 640 epoch 71 head A head_i_epoch 0 batch 0: avg loss -2.458037 avg loss no lamb -2.458037 time 2019-02-16 19:37:21.036958
Model ind 640 epoch 71 head A head_i_epoch 0 batch 100: avg loss -2.538833 avg loss no lamb -2.538833 time 2019-02-16 19:39:44.695718
Model ind 640 epoch 71 head A head_i_epoch 0 batch 200: avg loss -2.582765 avg loss no lamb -2.582765 time 2019-02-16 19:42:08.569166
last batch sz 160
Model ind 640 epoch 71 head B head_i_epoch 0 batch 0: avg loss -1.328665 avg loss no lamb -1.328665 time 2019-02-16 19:43:53.215042
Model ind 640 epoch 71 head B head_i_epoch 0 batch 100: avg loss -1.259298 avg loss no lamb -1.259298 time 2019-02-16 19:46:15.250744
Model ind 640 epoch 71 head B head_i_epoch 0 batch 200: avg loss -1.390101 avg loss no lamb -1.390101 time 2019-02-16 19:48:36.760792
last batch sz 160
Model ind 640 epoch 71 head B head_i_epoch 1 batch 0: avg loss -1.209801 avg loss no lamb -1.209801 time 2019-02-16 19:50:19.489517
Model ind 640 epoch 71 head B head_i_epoch 1 batch 100: avg loss -1.291402 avg loss no lamb -1.291402 time 2019-02-16 19:52:40.931937
Model ind 640 epoch 71 head B head_i_epoch 1 batch 200: avg loss -1.433185 avg loss no lamb -1.433185 time 2019-02-16 19:55:02.751611
last batch sz 160
Pre: time 2019-02-16 19:57:10.462723: 
 	std: 0.049979527
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.42721668, 0.52963334, 0.5292, 0.5283667, 0.42688334]
	train_accs: [0.42721668, 0.52963334, 0.5292, 0.5283667, 0.42688334]
	best_train_sub_head: 1
	worst: 0.42688334
	avg: 0.48825997
	best: 0.52963334

Starting e_i: 72
Model ind 640 epoch 72 head A head_i_epoch 0 batch 0: avg loss -2.565463 avg loss no lamb -2.565463 time 2019-02-16 19:57:12.300109
Model ind 640 epoch 72 head A head_i_epoch 0 batch 100: avg loss -2.600477 avg loss no lamb -2.600477 time 2019-02-16 19:59:35.636900
Model ind 640 epoch 72 head A head_i_epoch 0 batch 200: avg loss -2.537819 avg loss no lamb -2.537819 time 2019-02-16 20:01:58.130222
last batch sz 160
Model ind 640 epoch 72 head B head_i_epoch 0 batch 0: avg loss -1.322738 avg loss no lamb -1.322738 time 2019-02-16 20:03:42.259153
Model ind 640 epoch 72 head B head_i_epoch 0 batch 100: avg loss -1.332639 avg loss no lamb -1.332639 time 2019-02-16 20:06:04.688042
Model ind 640 epoch 72 head B head_i_epoch 0 batch 200: avg loss -1.432779 avg loss no lamb -1.432779 time 2019-02-16 20:08:26.782323
last batch sz 160
Model ind 640 epoch 72 head B head_i_epoch 1 batch 0: avg loss -1.338216 avg loss no lamb -1.338216 time 2019-02-16 20:10:10.323821
Model ind 640 epoch 72 head B head_i_epoch 1 batch 100: avg loss -1.289227 avg loss no lamb -1.289227 time 2019-02-16 20:12:32.649938
Model ind 640 epoch 72 head B head_i_epoch 1 batch 200: avg loss -1.398327 avg loss no lamb -1.398327 time 2019-02-16 20:14:55.443321
last batch sz 160
Pre: time 2019-02-16 20:17:03.222269: 
 	std: 0.053291053
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42306668, 0.5326833, 0.5335, 0.53235, 0.42508334]
	train_accs: [0.42306668, 0.5326833, 0.5335, 0.53235, 0.42508334]
	best_train_sub_head: 2
	worst: 0.42306668
	avg: 0.48933667
	best: 0.5335

Starting e_i: 73
Model ind 640 epoch 73 head A head_i_epoch 0 batch 0: avg loss -2.520150 avg loss no lamb -2.520150 time 2019-02-16 20:17:09.375753
Model ind 640 epoch 73 head A head_i_epoch 0 batch 100: avg loss -2.564011 avg loss no lamb -2.564011 time 2019-02-16 20:19:31.495823
Model ind 640 epoch 73 head A head_i_epoch 0 batch 200: avg loss -2.574023 avg loss no lamb -2.574023 time 2019-02-16 20:21:53.268674
last batch sz 160
Model ind 640 epoch 73 head B head_i_epoch 0 batch 0: avg loss -1.304621 avg loss no lamb -1.304621 time 2019-02-16 20:23:36.516299
Model ind 640 epoch 73 head B head_i_epoch 0 batch 100: avg loss -1.304881 avg loss no lamb -1.304881 time 2019-02-16 20:25:57.996142
Model ind 640 epoch 73 head B head_i_epoch 0 batch 200: avg loss -1.443181 avg loss no lamb -1.443181 time 2019-02-16 20:28:19.797770
last batch sz 160
Model ind 640 epoch 73 head B head_i_epoch 1 batch 0: avg loss -1.256662 avg loss no lamb -1.256662 time 2019-02-16 20:30:02.854864
Model ind 640 epoch 73 head B head_i_epoch 1 batch 100: avg loss -1.248104 avg loss no lamb -1.248104 time 2019-02-16 20:32:25.152702
Model ind 640 epoch 73 head B head_i_epoch 1 batch 200: avg loss -1.544554 avg loss no lamb -1.544554 time 2019-02-16 20:34:47.626637
last batch sz 160
Pre: time 2019-02-16 20:36:55.546462: 
 	std: 0.04778196
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42066666, 0.5171833, 0.5182833, 0.5181, 0.41998333]
	train_accs: [0.42066666, 0.5171833, 0.5182833, 0.5181, 0.41998333]
	best_train_sub_head: 2
	worst: 0.41998333
	avg: 0.47884336
	best: 0.5182833

Starting e_i: 74
Model ind 640 epoch 74 head A head_i_epoch 0 batch 0: avg loss -2.464782 avg loss no lamb -2.464782 time 2019-02-16 20:36:57.479075
Model ind 640 epoch 74 head A head_i_epoch 0 batch 100: avg loss -2.537136 avg loss no lamb -2.537136 time 2019-02-16 20:39:19.403854
Model ind 640 epoch 74 head A head_i_epoch 0 batch 200: avg loss -2.555632 avg loss no lamb -2.555632 time 2019-02-16 20:41:42.294928
last batch sz 160
Model ind 640 epoch 74 head B head_i_epoch 0 batch 0: avg loss -1.341866 avg loss no lamb -1.341866 time 2019-02-16 20:43:26.405622
Model ind 640 epoch 74 head B head_i_epoch 0 batch 100: avg loss -1.274773 avg loss no lamb -1.274773 time 2019-02-16 20:45:48.866712
Model ind 640 epoch 74 head B head_i_epoch 0 batch 200: avg loss -1.370160 avg loss no lamb -1.370160 time 2019-02-16 20:48:11.226427
last batch sz 160
Model ind 640 epoch 74 head B head_i_epoch 1 batch 0: avg loss -1.323655 avg loss no lamb -1.323655 time 2019-02-16 20:49:55.125613
Model ind 640 epoch 74 head B head_i_epoch 1 batch 100: avg loss -1.181800 avg loss no lamb -1.181800 time 2019-02-16 20:52:17.585568
Model ind 640 epoch 74 head B head_i_epoch 1 batch 200: avg loss -1.341725 avg loss no lamb -1.341725 time 2019-02-16 20:54:40.568145
last batch sz 160
Pre: time 2019-02-16 20:56:47.941653: 
 	std: 0.047694966
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43513334, 0.53275, 0.5307, 0.53145, 0.43345]
	train_accs: [0.43513334, 0.53275, 0.5307, 0.53145, 0.43345]
	best_train_sub_head: 1
	worst: 0.43345
	avg: 0.49269667
	best: 0.53275

Starting e_i: 75
Model ind 640 epoch 75 head A head_i_epoch 0 batch 0: avg loss -2.563181 avg loss no lamb -2.563181 time 2019-02-16 20:56:49.780250
Model ind 640 epoch 75 head A head_i_epoch 0 batch 100: avg loss -2.554003 avg loss no lamb -2.554003 time 2019-02-16 20:59:13.122841
Model ind 640 epoch 75 head A head_i_epoch 0 batch 200: avg loss -2.572625 avg loss no lamb -2.572625 time 2019-02-16 21:01:36.373330
last batch sz 160
Model ind 640 epoch 75 head B head_i_epoch 0 batch 0: avg loss -1.257475 avg loss no lamb -1.257475 time 2019-02-16 21:03:20.684622
Model ind 640 epoch 75 head B head_i_epoch 0 batch 100: avg loss -1.298395 avg loss no lamb -1.298395 time 2019-02-16 21:05:44.097790
Model ind 640 epoch 75 head B head_i_epoch 0 batch 200: avg loss -1.442083 avg loss no lamb -1.442083 time 2019-02-16 21:08:06.028818
last batch sz 160
Model ind 640 epoch 75 head B head_i_epoch 1 batch 0: avg loss -1.222477 avg loss no lamb -1.222477 time 2019-02-16 21:09:48.996088
Model ind 640 epoch 75 head B head_i_epoch 1 batch 100: avg loss -1.293176 avg loss no lamb -1.293176 time 2019-02-16 21:12:10.518297
Model ind 640 epoch 75 head B head_i_epoch 1 batch 200: avg loss -1.365273 avg loss no lamb -1.365273 time 2019-02-16 21:14:33.380699
last batch sz 160
Pre: time 2019-02-16 21:16:41.753543: 
 	std: 0.05486481
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.42896667, 0.54031664, 0.54, 0.5394833, 0.42693335]
	train_accs: [0.42896667, 0.54031664, 0.54, 0.5394833, 0.42693335]
	best_train_sub_head: 1
	worst: 0.42693335
	avg: 0.49514
	best: 0.54031664

Starting e_i: 76
Model ind 640 epoch 76 head A head_i_epoch 0 batch 0: avg loss -2.510372 avg loss no lamb -2.510372 time 2019-02-16 21:16:47.284049
Model ind 640 epoch 76 head A head_i_epoch 0 batch 100: avg loss -2.548560 avg loss no lamb -2.548560 time 2019-02-16 21:19:10.510622
Model ind 640 epoch 76 head A head_i_epoch 0 batch 200: avg loss -2.615335 avg loss no lamb -2.615335 time 2019-02-16 21:21:34.163353
last batch sz 160
Model ind 640 epoch 76 head B head_i_epoch 0 batch 0: avg loss -1.386784 avg loss no lamb -1.386784 time 2019-02-16 21:23:18.675912
Model ind 640 epoch 76 head B head_i_epoch 0 batch 100: avg loss -1.272474 avg loss no lamb -1.272474 time 2019-02-16 21:25:40.179541
Model ind 640 epoch 76 head B head_i_epoch 0 batch 200: avg loss -1.434397 avg loss no lamb -1.434397 time 2019-02-16 21:28:02.103153
last batch sz 160
Model ind 640 epoch 76 head B head_i_epoch 1 batch 0: avg loss -1.384910 avg loss no lamb -1.384910 time 2019-02-16 21:29:44.965761
Model ind 640 epoch 76 head B head_i_epoch 1 batch 100: avg loss -1.367377 avg loss no lamb -1.367377 time 2019-02-16 21:32:06.232077
Model ind 640 epoch 76 head B head_i_epoch 1 batch 200: avg loss -1.462677 avg loss no lamb -1.462677 time 2019-02-16 21:34:27.317531
last batch sz 160
Pre: time 2019-02-16 21:36:33.982838: 
 	std: 0.05469745
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.42171666, 0.5337167, 0.53243333, 0.5323833, 0.42068332]
	train_accs: [0.42171666, 0.5337167, 0.53243333, 0.5323833, 0.42068332]
	best_train_sub_head: 1
	worst: 0.42068332
	avg: 0.48818666
	best: 0.5337167

Starting e_i: 77
Model ind 640 epoch 77 head A head_i_epoch 0 batch 0: avg loss -2.498311 avg loss no lamb -2.498311 time 2019-02-16 21:36:35.879073
Model ind 640 epoch 77 head A head_i_epoch 0 batch 100: avg loss -2.478929 avg loss no lamb -2.478929 time 2019-02-16 21:38:58.115599
Model ind 640 epoch 77 head A head_i_epoch 0 batch 200: avg loss -2.536943 avg loss no lamb -2.536943 time 2019-02-16 21:41:20.033827
last batch sz 160
Model ind 640 epoch 77 head B head_i_epoch 0 batch 0: avg loss -1.259077 avg loss no lamb -1.259077 time 2019-02-16 21:43:02.672525
Model ind 640 epoch 77 head B head_i_epoch 0 batch 100: avg loss -1.238729 avg loss no lamb -1.238729 time 2019-02-16 21:45:23.690530
Model ind 640 epoch 77 head B head_i_epoch 0 batch 200: avg loss -1.434717 avg loss no lamb -1.434717 time 2019-02-16 21:47:44.678159
last batch sz 160
Model ind 640 epoch 77 head B head_i_epoch 1 batch 0: avg loss -1.255029 avg loss no lamb -1.255029 time 2019-02-16 21:49:27.588681
Model ind 640 epoch 77 head B head_i_epoch 1 batch 100: avg loss -1.223554 avg loss no lamb -1.223554 time 2019-02-16 21:51:50.026481
Model ind 640 epoch 77 head B head_i_epoch 1 batch 200: avg loss -1.386362 avg loss no lamb -1.386362 time 2019-02-16 21:54:12.566235
last batch sz 160
Pre: time 2019-02-16 21:56:20.771316: 
 	std: 0.051377196
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.41713333, 0.5233833, 0.5222833, 0.52208334, 0.4183]
	train_accs: [0.41713333, 0.5233833, 0.5222833, 0.52208334, 0.4183]
	best_train_sub_head: 1
	worst: 0.41713333
	avg: 0.48063666
	best: 0.5233833

Starting e_i: 78
Model ind 640 epoch 78 head A head_i_epoch 0 batch 0: avg loss -2.504328 avg loss no lamb -2.504328 time 2019-02-16 21:56:22.595846
Model ind 640 epoch 78 head A head_i_epoch 0 batch 100: avg loss -2.593191 avg loss no lamb -2.593191 time 2019-02-16 21:58:45.577716
Model ind 640 epoch 78 head A head_i_epoch 0 batch 200: avg loss -2.588913 avg loss no lamb -2.588913 time 2019-02-16 22:01:08.831386
last batch sz 160
Model ind 640 epoch 78 head B head_i_epoch 0 batch 0: avg loss -1.338046 avg loss no lamb -1.338046 time 2019-02-16 22:02:52.018078
Model ind 640 epoch 78 head B head_i_epoch 0 batch 100: avg loss -1.304652 avg loss no lamb -1.304652 time 2019-02-16 22:05:14.803294
Model ind 640 epoch 78 head B head_i_epoch 0 batch 200: avg loss -1.542958 avg loss no lamb -1.542958 time 2019-02-16 22:07:37.216920
last batch sz 160
Model ind 640 epoch 78 head B head_i_epoch 1 batch 0: avg loss -1.328405 avg loss no lamb -1.328405 time 2019-02-16 22:09:20.925636
Model ind 640 epoch 78 head B head_i_epoch 1 batch 100: avg loss -1.289510 avg loss no lamb -1.289510 time 2019-02-16 22:11:43.772602
Model ind 640 epoch 78 head B head_i_epoch 1 batch 200: avg loss -1.401663 avg loss no lamb -1.401663 time 2019-02-16 22:14:05.209450
last batch sz 160
Pre: time 2019-02-16 22:16:12.157760: 
 	std: 0.054362398
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4291, 0.53898335, 0.53921664, 0.5394833, 0.42743334]
	train_accs: [0.4291, 0.53898335, 0.53921664, 0.5394833, 0.42743334]
	best_train_sub_head: 3
	worst: 0.42743334
	avg: 0.49484333
	best: 0.5394833

Starting e_i: 79
Model ind 640 epoch 79 head A head_i_epoch 0 batch 0: avg loss -2.551018 avg loss no lamb -2.551018 time 2019-02-16 22:16:14.338622
Model ind 640 epoch 79 head A head_i_epoch 0 batch 100: avg loss -2.573573 avg loss no lamb -2.573573 time 2019-02-16 22:18:37.320552
Model ind 640 epoch 79 head A head_i_epoch 0 batch 200: avg loss -2.636418 avg loss no lamb -2.636418 time 2019-02-16 22:21:00.364195
last batch sz 160
Model ind 640 epoch 79 head B head_i_epoch 0 batch 0: avg loss -1.271183 avg loss no lamb -1.271183 time 2019-02-16 22:22:43.479800
Model ind 640 epoch 79 head B head_i_epoch 0 batch 100: avg loss -1.266542 avg loss no lamb -1.266542 time 2019-02-16 22:25:06.202221
Model ind 640 epoch 79 head B head_i_epoch 0 batch 200: avg loss -1.416745 avg loss no lamb -1.416745 time 2019-02-16 22:27:28.800484
last batch sz 160
Model ind 640 epoch 79 head B head_i_epoch 1 batch 0: avg loss -1.255742 avg loss no lamb -1.255742 time 2019-02-16 22:29:12.433780
Model ind 640 epoch 79 head B head_i_epoch 1 batch 100: avg loss -1.310489 avg loss no lamb -1.310489 time 2019-02-16 22:31:35.115697
Model ind 640 epoch 79 head B head_i_epoch 1 batch 200: avg loss -1.417114 avg loss no lamb -1.417114 time 2019-02-16 22:33:58.241876
last batch sz 160
Pre: time 2019-02-16 22:36:06.900722: 
 	std: 0.0509897
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4199, 0.5244667, 0.52356666, 0.52383333, 0.41985]
	train_accs: [0.4199, 0.5244667, 0.52356666, 0.52383333, 0.41985]
	best_train_sub_head: 1
	worst: 0.41985
	avg: 0.48232335
	best: 0.5244667

Starting e_i: 80
Model ind 640 epoch 80 head A head_i_epoch 0 batch 0: avg loss -2.595805 avg loss no lamb -2.595805 time 2019-02-16 22:36:08.934972
Model ind 640 epoch 80 head A head_i_epoch 0 batch 100: avg loss -2.578356 avg loss no lamb -2.578356 time 2019-02-16 22:38:32.410137
Model ind 640 epoch 80 head A head_i_epoch 0 batch 200: avg loss -2.587905 avg loss no lamb -2.587905 time 2019-02-16 22:40:54.836746
last batch sz 160
Model ind 640 epoch 80 head B head_i_epoch 0 batch 0: avg loss -1.331494 avg loss no lamb -1.331494 time 2019-02-16 22:42:38.742894
Model ind 640 epoch 80 head B head_i_epoch 0 batch 100: avg loss -1.309655 avg loss no lamb -1.309655 time 2019-02-16 22:45:01.341475
Model ind 640 epoch 80 head B head_i_epoch 0 batch 200: avg loss -1.494764 avg loss no lamb -1.494764 time 2019-02-16 22:47:23.254726
last batch sz 160
Model ind 640 epoch 80 head B head_i_epoch 1 batch 0: avg loss -1.340061 avg loss no lamb -1.340061 time 2019-02-16 22:49:06.276702
Model ind 640 epoch 80 head B head_i_epoch 1 batch 100: avg loss -1.320032 avg loss no lamb -1.320032 time 2019-02-16 22:51:28.714698
Model ind 640 epoch 80 head B head_i_epoch 1 batch 200: avg loss -1.383469 avg loss no lamb -1.383469 time 2019-02-16 22:53:51.830275
last batch sz 160
Pre: time 2019-02-16 22:56:00.489572: 
 	std: 0.051397003
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.42571667, 0.52853334, 0.528, 0.5287167, 0.42136666]
	train_accs: [0.42571667, 0.52853334, 0.528, 0.5287167, 0.42136666]
	best_train_sub_head: 3
	worst: 0.42136666
	avg: 0.4864667
	best: 0.5287167

Starting e_i: 81
Model ind 640 epoch 81 head A head_i_epoch 0 batch 0: avg loss -2.497030 avg loss no lamb -2.497030 time 2019-02-16 22:56:06.005230
Model ind 640 epoch 81 head A head_i_epoch 0 batch 100: avg loss -2.610833 avg loss no lamb -2.610833 time 2019-02-16 22:58:27.723345
Model ind 640 epoch 81 head A head_i_epoch 0 batch 200: avg loss -2.525634 avg loss no lamb -2.525634 time 2019-02-16 23:00:50.470948
last batch sz 160
Model ind 640 epoch 81 head B head_i_epoch 0 batch 0: avg loss -1.370087 avg loss no lamb -1.370087 time 2019-02-16 23:02:34.504172
Model ind 640 epoch 81 head B head_i_epoch 0 batch 100: avg loss -1.337451 avg loss no lamb -1.337451 time 2019-02-16 23:04:57.158816
Model ind 640 epoch 81 head B head_i_epoch 0 batch 200: avg loss -1.446464 avg loss no lamb -1.446464 time 2019-02-16 23:07:20.759950
last batch sz 160
Model ind 640 epoch 81 head B head_i_epoch 1 batch 0: avg loss -1.299496 avg loss no lamb -1.299496 time 2019-02-16 23:09:04.729856
Model ind 640 epoch 81 head B head_i_epoch 1 batch 100: avg loss -1.223911 avg loss no lamb -1.223911 time 2019-02-16 23:11:26.629415
Model ind 640 epoch 81 head B head_i_epoch 1 batch 200: avg loss -1.522441 avg loss no lamb -1.522441 time 2019-02-16 23:13:49.455151
last batch sz 160
Pre: time 2019-02-16 23:15:57.825259: 
 	std: 0.04901614
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43511668, 0.5355167, 0.5355167, 0.535, 0.43546668]
	train_accs: [0.43511668, 0.5355167, 0.5355167, 0.535, 0.43546668]
	best_train_sub_head: 1
	worst: 0.43511668
	avg: 0.49532336
	best: 0.5355167

Starting e_i: 82
Model ind 640 epoch 82 head A head_i_epoch 0 batch 0: avg loss -2.513603 avg loss no lamb -2.513603 time 2019-02-16 23:15:59.657325
Model ind 640 epoch 82 head A head_i_epoch 0 batch 100: avg loss -2.564489 avg loss no lamb -2.564489 time 2019-02-16 23:18:21.306748
Model ind 640 epoch 82 head A head_i_epoch 0 batch 200: avg loss -2.661310 avg loss no lamb -2.661310 time 2019-02-16 23:20:43.803249
last batch sz 160
Model ind 640 epoch 82 head B head_i_epoch 0 batch 0: avg loss -1.330565 avg loss no lamb -1.330565 time 2019-02-16 23:22:27.670242
Model ind 640 epoch 82 head B head_i_epoch 0 batch 100: avg loss -1.285197 avg loss no lamb -1.285197 time 2019-02-16 23:24:50.212796
Model ind 640 epoch 82 head B head_i_epoch 0 batch 200: avg loss -1.348393 avg loss no lamb -1.348393 time 2019-02-16 23:27:12.821050
last batch sz 160
Model ind 640 epoch 82 head B head_i_epoch 1 batch 0: avg loss -1.315926 avg loss no lamb -1.315926 time 2019-02-16 23:28:56.735059
Model ind 640 epoch 82 head B head_i_epoch 1 batch 100: avg loss -1.376119 avg loss no lamb -1.376119 time 2019-02-16 23:31:19.498238
Model ind 640 epoch 82 head B head_i_epoch 1 batch 200: avg loss -1.495377 avg loss no lamb -1.495377 time 2019-02-16 23:33:40.995913
last batch sz 160
Pre: time 2019-02-16 23:35:48.242658: 
 	std: 0.05049304
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4347, 0.53608334, 0.53718334, 0.5373833, 0.43295]
	train_accs: [0.4347, 0.53608334, 0.53718334, 0.5373833, 0.43295]
	best_train_sub_head: 3
	worst: 0.43295
	avg: 0.49566
	best: 0.5373833

Starting e_i: 83
Model ind 640 epoch 83 head A head_i_epoch 0 batch 0: avg loss -2.544900 avg loss no lamb -2.544900 time 2019-02-16 23:35:50.192797
Model ind 640 epoch 83 head A head_i_epoch 0 batch 100: avg loss -2.552148 avg loss no lamb -2.552148 time 2019-02-16 23:38:13.304968
Model ind 640 epoch 83 head A head_i_epoch 0 batch 200: avg loss -2.547070 avg loss no lamb -2.547070 time 2019-02-16 23:40:35.830771
last batch sz 160
Model ind 640 epoch 83 head B head_i_epoch 0 batch 0: avg loss -1.327196 avg loss no lamb -1.327196 time 2019-02-16 23:42:20.620924
Model ind 640 epoch 83 head B head_i_epoch 0 batch 100: avg loss -1.323393 avg loss no lamb -1.323393 time 2019-02-16 23:44:46.813608
Model ind 640 epoch 83 head B head_i_epoch 0 batch 200: avg loss -1.410517 avg loss no lamb -1.410517 time 2019-02-16 23:47:12.271890
last batch sz 160
Model ind 640 epoch 83 head B head_i_epoch 1 batch 0: avg loss -1.340444 avg loss no lamb -1.340444 time 2019-02-16 23:48:58.127833
Model ind 640 epoch 83 head B head_i_epoch 1 batch 100: avg loss -1.312654 avg loss no lamb -1.312654 time 2019-02-16 23:51:19.486402
Model ind 640 epoch 83 head B head_i_epoch 1 batch 200: avg loss -1.379333 avg loss no lamb -1.379333 time 2019-02-16 23:53:40.672325
last batch sz 160
Pre: time 2019-02-16 23:55:47.930173: 
 	std: 0.053994395
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42616665, 0.53473336, 0.53581667, 0.53503335, 0.42381668]
	train_accs: [0.42616665, 0.53473336, 0.53581667, 0.53503335, 0.42381668]
	best_train_sub_head: 2
	worst: 0.42381668
	avg: 0.49111336
	best: 0.53581667

Starting e_i: 84
Model ind 640 epoch 84 head A head_i_epoch 0 batch 0: avg loss -2.599576 avg loss no lamb -2.599576 time 2019-02-16 23:55:49.942347
Model ind 640 epoch 84 head A head_i_epoch 0 batch 100: avg loss -2.578386 avg loss no lamb -2.578386 time 2019-02-16 23:58:12.280429
Model ind 640 epoch 84 head A head_i_epoch 0 batch 200: avg loss -2.522252 avg loss no lamb -2.522252 time 2019-02-17 00:00:35.175575
last batch sz 160
Model ind 640 epoch 84 head B head_i_epoch 0 batch 0: avg loss -1.265562 avg loss no lamb -1.265562 time 2019-02-17 00:02:18.640678
Model ind 640 epoch 84 head B head_i_epoch 0 batch 100: avg loss -1.310443 avg loss no lamb -1.310443 time 2019-02-17 00:04:41.360036
Model ind 640 epoch 84 head B head_i_epoch 0 batch 200: avg loss -1.383585 avg loss no lamb -1.383585 time 2019-02-17 00:07:03.346642
last batch sz 160
Model ind 640 epoch 84 head B head_i_epoch 1 batch 0: avg loss -1.381086 avg loss no lamb -1.381086 time 2019-02-17 00:08:46.833435
Model ind 640 epoch 84 head B head_i_epoch 1 batch 100: avg loss -1.311075 avg loss no lamb -1.311075 time 2019-02-17 00:11:09.477874
Model ind 640 epoch 84 head B head_i_epoch 1 batch 200: avg loss -1.464581 avg loss no lamb -1.464581 time 2019-02-17 00:13:31.530309
last batch sz 160
Pre: time 2019-02-17 00:15:39.070666: 
 	std: 0.050053094
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43815, 0.54078335, 0.5406333, 0.54031664, 0.43866667]
	train_accs: [0.43815, 0.54078335, 0.5406333, 0.54031664, 0.43866667]
	best_train_sub_head: 1
	worst: 0.43815
	avg: 0.49971
	best: 0.54078335

Starting e_i: 85
Model ind 640 epoch 85 head A head_i_epoch 0 batch 0: avg loss -2.562728 avg loss no lamb -2.562728 time 2019-02-17 00:15:45.207302
Model ind 640 epoch 85 head A head_i_epoch 0 batch 100: avg loss -2.604854 avg loss no lamb -2.604854 time 2019-02-17 00:18:07.155040
Model ind 640 epoch 85 head A head_i_epoch 0 batch 200: avg loss -2.692679 avg loss no lamb -2.692679 time 2019-02-17 00:20:29.990090
last batch sz 160
Model ind 640 epoch 85 head B head_i_epoch 0 batch 0: avg loss -1.336996 avg loss no lamb -1.336996 time 2019-02-17 00:22:13.198460
Model ind 640 epoch 85 head B head_i_epoch 0 batch 100: avg loss -1.379027 avg loss no lamb -1.379027 time 2019-02-17 00:24:34.914125
Model ind 640 epoch 85 head B head_i_epoch 0 batch 200: avg loss -1.421321 avg loss no lamb -1.421321 time 2019-02-17 00:26:56.555307
last batch sz 160
Model ind 640 epoch 85 head B head_i_epoch 1 batch 0: avg loss -1.345237 avg loss no lamb -1.345237 time 2019-02-17 00:28:39.418920
Model ind 640 epoch 85 head B head_i_epoch 1 batch 100: avg loss -1.237178 avg loss no lamb -1.237178 time 2019-02-17 00:31:00.821733
Model ind 640 epoch 85 head B head_i_epoch 1 batch 200: avg loss -1.392481 avg loss no lamb -1.392481 time 2019-02-17 00:33:22.899430
last batch sz 160
Pre: time 2019-02-17 00:35:30.265300: 
 	std: 0.049548566
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42898333, 0.52958333, 0.5301833, 0.52965, 0.42835]
	train_accs: [0.42898333, 0.52958333, 0.5301833, 0.52965, 0.42835]
	best_train_sub_head: 2
	worst: 0.42835
	avg: 0.48935
	best: 0.5301833

Starting e_i: 86
Model ind 640 epoch 86 head A head_i_epoch 0 batch 0: avg loss -2.518187 avg loss no lamb -2.518187 time 2019-02-17 00:35:32.002213
Model ind 640 epoch 86 head A head_i_epoch 0 batch 100: avg loss -2.602288 avg loss no lamb -2.602288 time 2019-02-17 00:37:53.705021
Model ind 640 epoch 86 head A head_i_epoch 0 batch 200: avg loss -2.593330 avg loss no lamb -2.593330 time 2019-02-17 00:40:15.980003
last batch sz 160
Model ind 640 epoch 86 head B head_i_epoch 0 batch 0: avg loss -1.337075 avg loss no lamb -1.337075 time 2019-02-17 00:41:59.471138
Model ind 640 epoch 86 head B head_i_epoch 0 batch 100: avg loss -1.312929 avg loss no lamb -1.312929 time 2019-02-17 00:44:21.318947
Model ind 640 epoch 86 head B head_i_epoch 0 batch 200: avg loss -1.442456 avg loss no lamb -1.442456 time 2019-02-17 00:46:43.044784
last batch sz 160
Model ind 640 epoch 86 head B head_i_epoch 1 batch 0: avg loss -1.271128 avg loss no lamb -1.271128 time 2019-02-17 00:48:27.086100
Model ind 640 epoch 86 head B head_i_epoch 1 batch 100: avg loss -1.327305 avg loss no lamb -1.327305 time 2019-02-17 00:50:50.418745
Model ind 640 epoch 86 head B head_i_epoch 1 batch 200: avg loss -1.538705 avg loss no lamb -1.538705 time 2019-02-17 00:53:12.102526
last batch sz 160
Pre: time 2019-02-17 00:55:18.934935: 
 	std: 0.05292295
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.42993334, 0.53893334, 0.5395333, 0.5394, 0.43261668]
	train_accs: [0.42993334, 0.53893334, 0.5395333, 0.5394, 0.43261668]
	best_train_sub_head: 2
	worst: 0.42993334
	avg: 0.49608335
	best: 0.5395333

Starting e_i: 87
Model ind 640 epoch 87 head A head_i_epoch 0 batch 0: avg loss -2.542767 avg loss no lamb -2.542767 time 2019-02-17 00:55:20.699351
Model ind 640 epoch 87 head A head_i_epoch 0 batch 100: avg loss -2.601317 avg loss no lamb -2.601317 time 2019-02-17 00:57:43.257483
Model ind 640 epoch 87 head A head_i_epoch 0 batch 200: avg loss -2.634926 avg loss no lamb -2.634926 time 2019-02-17 01:00:05.403740
last batch sz 160
Model ind 640 epoch 87 head B head_i_epoch 0 batch 0: avg loss -1.321867 avg loss no lamb -1.321867 time 2019-02-17 01:01:49.053769
Model ind 640 epoch 87 head B head_i_epoch 0 batch 100: avg loss -1.387944 avg loss no lamb -1.387944 time 2019-02-17 01:04:11.109698
Model ind 640 epoch 87 head B head_i_epoch 0 batch 200: avg loss -1.338599 avg loss no lamb -1.338599 time 2019-02-17 01:06:33.213625
last batch sz 160
Model ind 640 epoch 87 head B head_i_epoch 1 batch 0: avg loss -1.381644 avg loss no lamb -1.381644 time 2019-02-17 01:08:16.338085
Model ind 640 epoch 87 head B head_i_epoch 1 batch 100: avg loss -1.311489 avg loss no lamb -1.311489 time 2019-02-17 01:10:38.633921
Model ind 640 epoch 87 head B head_i_epoch 1 batch 200: avg loss -1.415502 avg loss no lamb -1.415502 time 2019-02-17 01:13:01.418995
last batch sz 160
Pre: time 2019-02-17 01:15:09.499981: 
 	std: 0.0543146
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.43636668, 0.5469667, 0.54795, 0.54693335, 0.43646666]
	train_accs: [0.43636668, 0.5469667, 0.54795, 0.54693335, 0.43646666]
	best_train_sub_head: 2
	worst: 0.43636668
	avg: 0.5029367
	best: 0.54795

Starting e_i: 88
Model ind 640 epoch 88 head A head_i_epoch 0 batch 0: avg loss -2.608885 avg loss no lamb -2.608885 time 2019-02-17 01:15:14.472170
Model ind 640 epoch 88 head A head_i_epoch 0 batch 100: avg loss -2.569026 avg loss no lamb -2.569026 time 2019-02-17 01:17:36.831334
Model ind 640 epoch 88 head A head_i_epoch 0 batch 200: avg loss -2.609620 avg loss no lamb -2.609620 time 2019-02-17 01:19:58.656242
last batch sz 160
Model ind 640 epoch 88 head B head_i_epoch 0 batch 0: avg loss -1.348177 avg loss no lamb -1.348177 time 2019-02-17 01:21:41.914535
Model ind 640 epoch 88 head B head_i_epoch 0 batch 100: avg loss -1.269114 avg loss no lamb -1.269114 time 2019-02-17 01:24:04.818531
Model ind 640 epoch 88 head B head_i_epoch 0 batch 200: avg loss -1.399244 avg loss no lamb -1.399244 time 2019-02-17 01:26:27.467975
last batch sz 160
Model ind 640 epoch 88 head B head_i_epoch 1 batch 0: avg loss -1.316086 avg loss no lamb -1.316086 time 2019-02-17 01:28:10.890650
Model ind 640 epoch 88 head B head_i_epoch 1 batch 100: avg loss -1.374830 avg loss no lamb -1.374830 time 2019-02-17 01:30:32.616136
Model ind 640 epoch 88 head B head_i_epoch 1 batch 200: avg loss -1.441547 avg loss no lamb -1.441547 time 2019-02-17 01:32:54.097066
last batch sz 160
Pre: time 2019-02-17 01:35:00.832904: 
 	std: 0.0519655
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4342, 0.54088336, 0.54078335, 0.54035, 0.435]
	train_accs: [0.4342, 0.54088336, 0.54078335, 0.54035, 0.435]
	best_train_sub_head: 1
	worst: 0.4342
	avg: 0.49824333
	best: 0.54088336

Starting e_i: 89
Model ind 640 epoch 89 head A head_i_epoch 0 batch 0: avg loss -2.596769 avg loss no lamb -2.596769 time 2019-02-17 01:35:02.607801
Model ind 640 epoch 89 head A head_i_epoch 0 batch 100: avg loss -2.593222 avg loss no lamb -2.593222 time 2019-02-17 01:37:24.767281
Model ind 640 epoch 89 head A head_i_epoch 0 batch 200: avg loss -2.661311 avg loss no lamb -2.661311 time 2019-02-17 01:39:47.091697
last batch sz 160
Model ind 640 epoch 89 head B head_i_epoch 0 batch 0: avg loss -1.328952 avg loss no lamb -1.328952 time 2019-02-17 01:41:31.134001
Model ind 640 epoch 89 head B head_i_epoch 0 batch 100: avg loss -1.357073 avg loss no lamb -1.357073 time 2019-02-17 01:43:54.711195
Model ind 640 epoch 89 head B head_i_epoch 0 batch 200: avg loss -1.324725 avg loss no lamb -1.324725 time 2019-02-17 01:46:17.900719
last batch sz 160
Model ind 640 epoch 89 head B head_i_epoch 1 batch 0: avg loss -1.371960 avg loss no lamb -1.371960 time 2019-02-17 01:48:01.978535
Model ind 640 epoch 89 head B head_i_epoch 1 batch 100: avg loss -1.267540 avg loss no lamb -1.267540 time 2019-02-17 01:50:25.142465
Model ind 640 epoch 89 head B head_i_epoch 1 batch 200: avg loss -1.428234 avg loss no lamb -1.428234 time 2019-02-17 01:52:48.564441
last batch sz 160
Pre: time 2019-02-17 01:54:57.249267: 
 	std: 0.051438235
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43785, 0.54298335, 0.5427667, 0.54246664, 0.43763334]
	train_accs: [0.43785, 0.54298335, 0.5427667, 0.54246664, 0.43763334]
	best_train_sub_head: 1
	worst: 0.43763334
	avg: 0.50074
	best: 0.54298335

Starting e_i: 90
Model ind 640 epoch 90 head A head_i_epoch 0 batch 0: avg loss -2.544354 avg loss no lamb -2.544354 time 2019-02-17 01:54:59.025718
Model ind 640 epoch 90 head A head_i_epoch 0 batch 100: avg loss -2.679425 avg loss no lamb -2.679425 time 2019-02-17 01:57:22.057943
Model ind 640 epoch 90 head A head_i_epoch 0 batch 200: avg loss -2.663189 avg loss no lamb -2.663189 time 2019-02-17 01:59:45.133989
last batch sz 160
Model ind 640 epoch 90 head B head_i_epoch 0 batch 0: avg loss -1.355665 avg loss no lamb -1.355665 time 2019-02-17 02:01:28.568626
Model ind 640 epoch 90 head B head_i_epoch 0 batch 100: avg loss -1.346615 avg loss no lamb -1.346615 time 2019-02-17 02:03:50.131179
Model ind 640 epoch 90 head B head_i_epoch 0 batch 200: avg loss -1.425167 avg loss no lamb -1.425167 time 2019-02-17 02:06:12.648408
last batch sz 160
Model ind 640 epoch 90 head B head_i_epoch 1 batch 0: avg loss -1.339650 avg loss no lamb -1.339650 time 2019-02-17 02:07:57.393481
Model ind 640 epoch 90 head B head_i_epoch 1 batch 100: avg loss -1.290209 avg loss no lamb -1.290209 time 2019-02-17 02:10:20.319146
Model ind 640 epoch 90 head B head_i_epoch 1 batch 200: avg loss -1.480261 avg loss no lamb -1.480261 time 2019-02-17 02:12:42.141731
last batch sz 160
Pre: time 2019-02-17 02:14:50.442544: 
 	std: 0.051992662
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43946666, 0.54588336, 0.5438, 0.54433334, 0.43765]
	train_accs: [0.43946666, 0.54588336, 0.5438, 0.54433334, 0.43765]
	best_train_sub_head: 1
	worst: 0.43765
	avg: 0.5022267
	best: 0.54588336

Starting e_i: 91
Model ind 640 epoch 91 head A head_i_epoch 0 batch 0: avg loss -2.618453 avg loss no lamb -2.618453 time 2019-02-17 02:14:55.389167
Model ind 640 epoch 91 head A head_i_epoch 0 batch 100: avg loss -2.586313 avg loss no lamb -2.586313 time 2019-02-17 02:17:18.962991
Model ind 640 epoch 91 head A head_i_epoch 0 batch 200: avg loss -2.632041 avg loss no lamb -2.632041 time 2019-02-17 02:19:41.493181
last batch sz 160
Model ind 640 epoch 91 head B head_i_epoch 0 batch 0: avg loss -1.371421 avg loss no lamb -1.371421 time 2019-02-17 02:21:25.864134
Model ind 640 epoch 91 head B head_i_epoch 0 batch 100: avg loss -1.327989 avg loss no lamb -1.327989 time 2019-02-17 02:23:49.059805
Model ind 640 epoch 91 head B head_i_epoch 0 batch 200: avg loss -1.393867 avg loss no lamb -1.393867 time 2019-02-17 02:26:12.068515
last batch sz 160
Model ind 640 epoch 91 head B head_i_epoch 1 batch 0: avg loss -1.303324 avg loss no lamb -1.303324 time 2019-02-17 02:27:55.317087
Model ind 640 epoch 91 head B head_i_epoch 1 batch 100: avg loss -1.321483 avg loss no lamb -1.321483 time 2019-02-17 02:30:17.587665
Model ind 640 epoch 91 head B head_i_epoch 1 batch 200: avg loss -1.351961 avg loss no lamb -1.351961 time 2019-02-17 02:32:39.878373
last batch sz 160
Pre: time 2019-02-17 02:34:47.633414: 
 	std: 0.052981738
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44655, 0.55385, 0.5531833, 0.55301666, 0.44388333]
	train_accs: [0.44655, 0.55385, 0.5531833, 0.55301666, 0.44388333]
	best_train_sub_head: 1
	worst: 0.44388333
	avg: 0.51009667
	best: 0.55385

Starting e_i: 92
Model ind 640 epoch 92 head A head_i_epoch 0 batch 0: avg loss -2.562930 avg loss no lamb -2.562930 time 2019-02-17 02:34:52.543082
Model ind 640 epoch 92 head A head_i_epoch 0 batch 100: avg loss -2.589396 avg loss no lamb -2.589396 time 2019-02-17 02:37:14.226601
Model ind 640 epoch 92 head A head_i_epoch 0 batch 200: avg loss -2.594235 avg loss no lamb -2.594235 time 2019-02-17 02:39:36.207547
last batch sz 160
Model ind 640 epoch 92 head B head_i_epoch 0 batch 0: avg loss -1.364697 avg loss no lamb -1.364697 time 2019-02-17 02:41:19.823531
Model ind 640 epoch 92 head B head_i_epoch 0 batch 100: avg loss -1.324519 avg loss no lamb -1.324519 time 2019-02-17 02:43:41.802956
Model ind 640 epoch 92 head B head_i_epoch 0 batch 200: avg loss -1.406653 avg loss no lamb -1.406653 time 2019-02-17 02:46:04.160946
last batch sz 160
Model ind 640 epoch 92 head B head_i_epoch 1 batch 0: avg loss -1.391904 avg loss no lamb -1.391904 time 2019-02-17 02:47:48.231032
Model ind 640 epoch 92 head B head_i_epoch 1 batch 100: avg loss -1.405565 avg loss no lamb -1.405565 time 2019-02-17 02:50:10.704168
Model ind 640 epoch 92 head B head_i_epoch 1 batch 200: avg loss -1.412201 avg loss no lamb -1.412201 time 2019-02-17 02:52:32.215802
last batch sz 160
Pre: time 2019-02-17 02:54:39.291429: 
 	std: 0.05364927
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43385, 0.5420667, 0.5418, 0.5412, 0.43055]
	train_accs: [0.43385, 0.5420667, 0.5418, 0.5412, 0.43055]
	best_train_sub_head: 1
	worst: 0.43055
	avg: 0.49789333
	best: 0.5420667

Starting e_i: 93
Model ind 640 epoch 93 head A head_i_epoch 0 batch 0: avg loss -2.556684 avg loss no lamb -2.556684 time 2019-02-17 02:54:41.123230
Model ind 640 epoch 93 head A head_i_epoch 0 batch 100: avg loss -2.582845 avg loss no lamb -2.582845 time 2019-02-17 02:57:04.223268
Model ind 640 epoch 93 head A head_i_epoch 0 batch 200: avg loss -2.610543 avg loss no lamb -2.610543 time 2019-02-17 02:59:27.071008
last batch sz 160
Model ind 640 epoch 93 head B head_i_epoch 0 batch 0: avg loss -1.350653 avg loss no lamb -1.350653 time 2019-02-17 03:01:10.507701
Model ind 640 epoch 93 head B head_i_epoch 0 batch 100: avg loss -1.290757 avg loss no lamb -1.290757 time 2019-02-17 03:03:31.956011
Model ind 640 epoch 93 head B head_i_epoch 0 batch 200: avg loss -1.421205 avg loss no lamb -1.421205 time 2019-02-17 03:05:53.491174
last batch sz 160
Model ind 640 epoch 93 head B head_i_epoch 1 batch 0: avg loss -1.301770 avg loss no lamb -1.301770 time 2019-02-17 03:07:36.463004
Model ind 640 epoch 93 head B head_i_epoch 1 batch 100: avg loss -1.315577 avg loss no lamb -1.315577 time 2019-02-17 03:09:57.956599
Model ind 640 epoch 93 head B head_i_epoch 1 batch 200: avg loss -1.499962 avg loss no lamb -1.499962 time 2019-02-17 03:12:20.752670
last batch sz 160
Pre: time 2019-02-17 03:14:28.793458: 
 	std: 0.052491207
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.43673334, 0.54175, 0.54295, 0.54186666, 0.4334]
	train_accs: [0.43673334, 0.54175, 0.54295, 0.54186666, 0.4334]
	best_train_sub_head: 2
	worst: 0.4334
	avg: 0.49934
	best: 0.54295

Starting e_i: 94
Model ind 640 epoch 94 head A head_i_epoch 0 batch 0: avg loss -2.599572 avg loss no lamb -2.599572 time 2019-02-17 03:14:30.858204
Model ind 640 epoch 94 head A head_i_epoch 0 batch 100: avg loss -2.567509 avg loss no lamb -2.567509 time 2019-02-17 03:16:53.395284
Model ind 640 epoch 94 head A head_i_epoch 0 batch 200: avg loss -2.648352 avg loss no lamb -2.648352 time 2019-02-17 03:19:15.720279
last batch sz 160
Model ind 640 epoch 94 head B head_i_epoch 0 batch 0: avg loss -1.392349 avg loss no lamb -1.392349 time 2019-02-17 03:20:58.784701
Model ind 640 epoch 94 head B head_i_epoch 0 batch 100: avg loss -1.360993 avg loss no lamb -1.360993 time 2019-02-17 03:23:20.104204
Model ind 640 epoch 94 head B head_i_epoch 0 batch 200: avg loss -1.451438 avg loss no lamb -1.451438 time 2019-02-17 03:25:41.293059
last batch sz 160
Model ind 640 epoch 94 head B head_i_epoch 1 batch 0: avg loss -1.313279 avg loss no lamb -1.313279 time 2019-02-17 03:27:24.624989
Model ind 640 epoch 94 head B head_i_epoch 1 batch 100: avg loss -1.381616 avg loss no lamb -1.381616 time 2019-02-17 03:29:47.213886
Model ind 640 epoch 94 head B head_i_epoch 1 batch 200: avg loss -1.408241 avg loss no lamb -1.408241 time 2019-02-17 03:32:09.876779
last batch sz 160
Pre: time 2019-02-17 03:34:17.675933: 
 	std: 0.052198797
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.43608335, 0.5409, 0.54175, 0.541, 0.43328333]
	train_accs: [0.43608335, 0.5409, 0.54175, 0.541, 0.43328333]
	best_train_sub_head: 2
	worst: 0.43328333
	avg: 0.49860334
	best: 0.54175

Starting e_i: 95
Model ind 640 epoch 95 head A head_i_epoch 0 batch 0: avg loss -2.601259 avg loss no lamb -2.601259 time 2019-02-17 03:34:19.452548
Model ind 640 epoch 95 head A head_i_epoch 0 batch 100: avg loss -2.632624 avg loss no lamb -2.632624 time 2019-02-17 03:36:41.946262
Model ind 640 epoch 95 head A head_i_epoch 0 batch 200: avg loss -2.633302 avg loss no lamb -2.633302 time 2019-02-17 03:39:04.704991
last batch sz 160
Model ind 640 epoch 95 head B head_i_epoch 0 batch 0: avg loss -1.325436 avg loss no lamb -1.325436 time 2019-02-17 03:40:48.750327
Model ind 640 epoch 95 head B head_i_epoch 0 batch 100: avg loss -1.421977 avg loss no lamb -1.421977 time 2019-02-17 03:43:11.333231
Model ind 640 epoch 95 head B head_i_epoch 0 batch 200: avg loss -1.413377 avg loss no lamb -1.413377 time 2019-02-17 03:45:34.163487
last batch sz 160
Model ind 640 epoch 95 head B head_i_epoch 1 batch 0: avg loss -1.375023 avg loss no lamb -1.375023 time 2019-02-17 03:47:18.429449
Model ind 640 epoch 95 head B head_i_epoch 1 batch 100: avg loss -1.304117 avg loss no lamb -1.304117 time 2019-02-17 03:49:41.651967
Model ind 640 epoch 95 head B head_i_epoch 1 batch 200: avg loss -1.399616 avg loss no lamb -1.399616 time 2019-02-17 03:52:05.143480
last batch sz 160
Pre: time 2019-02-17 03:54:13.704442: 
 	std: 0.052341007
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44701666, 0.55273336, 0.55375, 0.5534, 0.4459]
	train_accs: [0.44701666, 0.55273336, 0.55375, 0.5534, 0.4459]
	best_train_sub_head: 2
	worst: 0.4459
	avg: 0.51056
	best: 0.55375

Starting e_i: 96
Model ind 640 epoch 96 head A head_i_epoch 0 batch 0: avg loss -2.661453 avg loss no lamb -2.661453 time 2019-02-17 03:54:15.694000
Model ind 640 epoch 96 head A head_i_epoch 0 batch 100: avg loss -2.659487 avg loss no lamb -2.659487 time 2019-02-17 03:56:39.258105
Model ind 640 epoch 96 head A head_i_epoch 0 batch 200: avg loss -2.798586 avg loss no lamb -2.798586 time 2019-02-17 03:59:02.402935
last batch sz 160
Model ind 640 epoch 96 head B head_i_epoch 0 batch 0: avg loss -1.365857 avg loss no lamb -1.365857 time 2019-02-17 04:00:46.024065
Model ind 640 epoch 96 head B head_i_epoch 0 batch 100: avg loss -1.353248 avg loss no lamb -1.353248 time 2019-02-17 04:03:07.140536
Model ind 640 epoch 96 head B head_i_epoch 0 batch 200: avg loss -1.552164 avg loss no lamb -1.552164 time 2019-02-17 04:05:28.172009
last batch sz 160
Model ind 640 epoch 96 head B head_i_epoch 1 batch 0: avg loss -1.364195 avg loss no lamb -1.364195 time 2019-02-17 04:07:10.899364
Model ind 640 epoch 96 head B head_i_epoch 1 batch 100: avg loss -1.280447 avg loss no lamb -1.280447 time 2019-02-17 04:09:32.738537
Model ind 640 epoch 96 head B head_i_epoch 1 batch 200: avg loss -1.412645 avg loss no lamb -1.412645 time 2019-02-17 04:11:55.069489
last batch sz 160
Pre: time 2019-02-17 04:14:03.190267: 
 	std: 0.052622832
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44065, 0.5472, 0.54825, 0.5468, 0.43936667]
	train_accs: [0.44065, 0.5472, 0.54825, 0.5468, 0.43936667]
	best_train_sub_head: 2
	worst: 0.43936667
	avg: 0.5044533
	best: 0.54825

Starting e_i: 97
Model ind 640 epoch 97 head A head_i_epoch 0 batch 0: avg loss -2.651344 avg loss no lamb -2.651344 time 2019-02-17 04:14:04.950192
Model ind 640 epoch 97 head A head_i_epoch 0 batch 100: avg loss -2.667327 avg loss no lamb -2.667327 time 2019-02-17 04:16:26.640587
Model ind 640 epoch 97 head A head_i_epoch 0 batch 200: avg loss -2.691083 avg loss no lamb -2.691083 time 2019-02-17 04:18:49.504820
last batch sz 160
Model ind 640 epoch 97 head B head_i_epoch 0 batch 0: avg loss -1.386681 avg loss no lamb -1.386681 time 2019-02-17 04:20:33.701043
Model ind 640 epoch 97 head B head_i_epoch 0 batch 100: avg loss -1.324705 avg loss no lamb -1.324705 time 2019-02-17 04:22:56.666462
Model ind 640 epoch 97 head B head_i_epoch 0 batch 200: avg loss -1.402094 avg loss no lamb -1.402094 time 2019-02-17 04:25:19.617176
last batch sz 160
Model ind 640 epoch 97 head B head_i_epoch 1 batch 0: avg loss -1.324182 avg loss no lamb -1.324182 time 2019-02-17 04:27:02.766625
Model ind 640 epoch 97 head B head_i_epoch 1 batch 100: avg loss -1.283775 avg loss no lamb -1.283775 time 2019-02-17 04:29:25.516231
Model ind 640 epoch 97 head B head_i_epoch 1 batch 200: avg loss -1.413601 avg loss no lamb -1.413601 time 2019-02-17 04:31:48.310503
last batch sz 160
Pre: time 2019-02-17 04:33:56.281641: 
 	std: 0.05265985
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43443334, 0.5420667, 0.542, 0.5411, 0.43403333]
	train_accs: [0.43443334, 0.5420667, 0.542, 0.5411, 0.43403333]
	best_train_sub_head: 1
	worst: 0.43403333
	avg: 0.4987267
	best: 0.5420667

Starting e_i: 98
Model ind 640 epoch 98 head A head_i_epoch 0 batch 0: avg loss -2.636426 avg loss no lamb -2.636426 time 2019-02-17 04:33:58.077672
Model ind 640 epoch 98 head A head_i_epoch 0 batch 100: avg loss -2.611897 avg loss no lamb -2.611897 time 2019-02-17 04:36:20.558274
Model ind 640 epoch 98 head A head_i_epoch 0 batch 200: avg loss -2.641262 avg loss no lamb -2.641262 time 2019-02-17 04:38:43.783643
last batch sz 160
Model ind 640 epoch 98 head B head_i_epoch 0 batch 0: avg loss -1.385361 avg loss no lamb -1.385361 time 2019-02-17 04:40:27.912076
Model ind 640 epoch 98 head B head_i_epoch 0 batch 100: avg loss -1.300218 avg loss no lamb -1.300218 time 2019-02-17 04:42:49.468108
Model ind 640 epoch 98 head B head_i_epoch 0 batch 200: avg loss -1.417475 avg loss no lamb -1.417475 time 2019-02-17 04:45:11.012378
last batch sz 160
Model ind 640 epoch 98 head B head_i_epoch 1 batch 0: avg loss -1.382358 avg loss no lamb -1.382358 time 2019-02-17 04:46:54.140689
Model ind 640 epoch 98 head B head_i_epoch 1 batch 100: avg loss -1.276136 avg loss no lamb -1.276136 time 2019-02-17 04:49:15.747327
Model ind 640 epoch 98 head B head_i_epoch 1 batch 200: avg loss -1.380636 avg loss no lamb -1.380636 time 2019-02-17 04:51:37.439749
last batch sz 160
Pre: time 2019-02-17 04:53:45.000196: 
 	std: 0.054456092
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4296, 0.5414, 0.54148334, 0.54005, 0.43005]
	train_accs: [0.4296, 0.5414, 0.54148334, 0.54005, 0.43005]
	best_train_sub_head: 2
	worst: 0.4296
	avg: 0.49651664
	best: 0.54148334

Starting e_i: 99
Model ind 640 epoch 99 head A head_i_epoch 0 batch 0: avg loss -2.605462 avg loss no lamb -2.605462 time 2019-02-17 04:53:46.777936
Model ind 640 epoch 99 head A head_i_epoch 0 batch 100: avg loss -2.596693 avg loss no lamb -2.596693 time 2019-02-17 04:56:08.381767
Model ind 640 epoch 99 head A head_i_epoch 0 batch 200: avg loss -2.642787 avg loss no lamb -2.642787 time 2019-02-17 04:58:31.593821
last batch sz 160
Model ind 640 epoch 99 head B head_i_epoch 0 batch 0: avg loss -1.421328 avg loss no lamb -1.421328 time 2019-02-17 05:00:14.932161
Model ind 640 epoch 99 head B head_i_epoch 0 batch 100: avg loss -1.245789 avg loss no lamb -1.245789 time 2019-02-17 05:02:36.599950
Model ind 640 epoch 99 head B head_i_epoch 0 batch 200: avg loss -1.450417 avg loss no lamb -1.450417 time 2019-02-17 05:04:59.065990
last batch sz 160
Model ind 640 epoch 99 head B head_i_epoch 1 batch 0: avg loss -1.348832 avg loss no lamb -1.348832 time 2019-02-17 05:06:42.904615
Model ind 640 epoch 99 head B head_i_epoch 1 batch 100: avg loss -1.333617 avg loss no lamb -1.333617 time 2019-02-17 05:09:04.334721
Model ind 640 epoch 99 head B head_i_epoch 1 batch 200: avg loss -1.475140 avg loss no lamb -1.475140 time 2019-02-17 05:11:25.701213
last batch sz 160
Pre: time 2019-02-17 05:13:32.596198: 
 	std: 0.05164338
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44721666, 0.5520333, 0.5528, 0.5515, 0.44618332]
	train_accs: [0.44721666, 0.5520333, 0.5528, 0.5515, 0.44618332]
	best_train_sub_head: 2
	worst: 0.44618332
	avg: 0.50994664
	best: 0.5528

Starting e_i: 100
Model ind 640 epoch 100 head A head_i_epoch 0 batch 0: avg loss -2.579443 avg loss no lamb -2.579443 time 2019-02-17 05:13:34.375421
Model ind 640 epoch 100 head A head_i_epoch 0 batch 100: avg loss -2.681290 avg loss no lamb -2.681290 time 2019-02-17 05:15:56.004274
Model ind 640 epoch 100 head A head_i_epoch 0 batch 200: avg loss -2.612519 avg loss no lamb -2.612519 time 2019-02-17 05:18:17.943949
last batch sz 160
Model ind 640 epoch 100 head B head_i_epoch 0 batch 0: avg loss -1.327879 avg loss no lamb -1.327879 time 2019-02-17 05:20:01.751937
Model ind 640 epoch 100 head B head_i_epoch 0 batch 100: avg loss -1.425491 avg loss no lamb -1.425491 time 2019-02-17 05:22:23.293989
Model ind 640 epoch 100 head B head_i_epoch 0 batch 200: avg loss -1.457922 avg loss no lamb -1.457922 time 2019-02-17 05:24:45.156629
last batch sz 160
Model ind 640 epoch 100 head B head_i_epoch 1 batch 0: avg loss -1.401154 avg loss no lamb -1.401154 time 2019-02-17 05:26:28.389988
Model ind 640 epoch 100 head B head_i_epoch 1 batch 100: avg loss -1.240150 avg loss no lamb -1.240150 time 2019-02-17 05:28:50.341378
Model ind 640 epoch 100 head B head_i_epoch 1 batch 200: avg loss -1.429002 avg loss no lamb -1.429002 time 2019-02-17 05:31:13.217743
last batch sz 160
Pre: time 2019-02-17 05:33:21.286003: 
 	std: 0.053426463
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4551, 0.5629, 0.5636333, 0.56255, 0.45286667]
	train_accs: [0.4551, 0.5629, 0.5636333, 0.56255, 0.45286667]
	best_train_sub_head: 2
	worst: 0.45286667
	avg: 0.51941
	best: 0.5636333

Starting e_i: 101
Model ind 640 epoch 101 head A head_i_epoch 0 batch 0: avg loss -2.634386 avg loss no lamb -2.634386 time 2019-02-17 05:33:30.233745
Model ind 640 epoch 101 head A head_i_epoch 0 batch 100: avg loss -2.587810 avg loss no lamb -2.587810 time 2019-02-17 05:35:52.934680
Model ind 640 epoch 101 head A head_i_epoch 0 batch 200: avg loss -2.688779 avg loss no lamb -2.688779 time 2019-02-17 05:38:15.811787
last batch sz 160
Model ind 640 epoch 101 head B head_i_epoch 0 batch 0: avg loss -1.356159 avg loss no lamb -1.356159 time 2019-02-17 05:39:59.838832
Model ind 640 epoch 101 head B head_i_epoch 0 batch 100: avg loss -1.355181 avg loss no lamb -1.355181 time 2019-02-17 05:42:22.152813
Model ind 640 epoch 101 head B head_i_epoch 0 batch 200: avg loss -1.464849 avg loss no lamb -1.464849 time 2019-02-17 05:44:44.354699
last batch sz 160
Model ind 640 epoch 101 head B head_i_epoch 1 batch 0: avg loss -1.322428 avg loss no lamb -1.322428 time 2019-02-17 05:46:27.433819
Model ind 640 epoch 101 head B head_i_epoch 1 batch 100: avg loss -1.317312 avg loss no lamb -1.317312 time 2019-02-17 05:48:49.577418
Model ind 640 epoch 101 head B head_i_epoch 1 batch 200: avg loss -1.515986 avg loss no lamb -1.515986 time 2019-02-17 05:51:12.592413
last batch sz 160
Pre: time 2019-02-17 05:53:20.022393: 
 	std: 0.05464281
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44148332, 0.55303335, 0.5533, 0.55303335, 0.44168332]
	train_accs: [0.44148332, 0.55303335, 0.5533, 0.55303335, 0.44168332]
	best_train_sub_head: 2
	worst: 0.44148332
	avg: 0.50850666
	best: 0.5533

Starting e_i: 102
Model ind 640 epoch 102 head A head_i_epoch 0 batch 0: avg loss -2.635434 avg loss no lamb -2.635434 time 2019-02-17 05:53:21.818745
Model ind 640 epoch 102 head A head_i_epoch 0 batch 100: avg loss -2.591053 avg loss no lamb -2.591053 time 2019-02-17 05:55:44.412669
Model ind 640 epoch 102 head A head_i_epoch 0 batch 200: avg loss -2.613829 avg loss no lamb -2.613829 time 2019-02-17 05:58:07.348141
last batch sz 160
Model ind 640 epoch 102 head B head_i_epoch 0 batch 0: avg loss -1.266457 avg loss no lamb -1.266457 time 2019-02-17 05:59:51.900836
Model ind 640 epoch 102 head B head_i_epoch 0 batch 100: avg loss -1.334930 avg loss no lamb -1.334930 time 2019-02-17 06:02:15.335877
Model ind 640 epoch 102 head B head_i_epoch 0 batch 200: avg loss -1.434883 avg loss no lamb -1.434883 time 2019-02-17 06:04:38.516251
last batch sz 160
Model ind 640 epoch 102 head B head_i_epoch 1 batch 0: avg loss -1.339146 avg loss no lamb -1.339146 time 2019-02-17 06:06:22.817399
Model ind 640 epoch 102 head B head_i_epoch 1 batch 100: avg loss -1.271706 avg loss no lamb -1.271706 time 2019-02-17 06:08:45.574608
Model ind 640 epoch 102 head B head_i_epoch 1 batch 200: avg loss -1.400909 avg loss no lamb -1.400909 time 2019-02-17 06:11:07.925505
last batch sz 160
Pre: time 2019-02-17 06:13:16.386960: 
 	std: 0.05238284
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44571668, 0.5521333, 0.5516667, 0.55221665, 0.44445]
	train_accs: [0.44571668, 0.5521333, 0.5516667, 0.55221665, 0.44445]
	best_train_sub_head: 3
	worst: 0.44445
	avg: 0.50923663
	best: 0.55221665

Starting e_i: 103
Model ind 640 epoch 103 head A head_i_epoch 0 batch 0: avg loss -2.650982 avg loss no lamb -2.650982 time 2019-02-17 06:13:18.188174
Model ind 640 epoch 103 head A head_i_epoch 0 batch 100: avg loss -2.615706 avg loss no lamb -2.615706 time 2019-02-17 06:15:41.123671
Model ind 640 epoch 103 head A head_i_epoch 0 batch 200: avg loss -2.652628 avg loss no lamb -2.652628 time 2019-02-17 06:18:04.993793
last batch sz 160
Model ind 640 epoch 103 head B head_i_epoch 0 batch 0: avg loss -1.356233 avg loss no lamb -1.356233 time 2019-02-17 06:19:48.673838
Model ind 640 epoch 103 head B head_i_epoch 0 batch 100: avg loss -1.299972 avg loss no lamb -1.299972 time 2019-02-17 06:22:10.813494
Model ind 640 epoch 103 head B head_i_epoch 0 batch 200: avg loss -1.453072 avg loss no lamb -1.453072 time 2019-02-17 06:24:34.620539
last batch sz 160
Model ind 640 epoch 103 head B head_i_epoch 1 batch 0: avg loss -1.370081 avg loss no lamb -1.370081 time 2019-02-17 06:26:19.122477
Model ind 640 epoch 103 head B head_i_epoch 1 batch 100: avg loss -1.331538 avg loss no lamb -1.331538 time 2019-02-17 06:28:41.208460
Model ind 640 epoch 103 head B head_i_epoch 1 batch 200: avg loss -1.513271 avg loss no lamb -1.513271 time 2019-02-17 06:31:04.055614
last batch sz 160
Pre: time 2019-02-17 06:33:12.951982: 
 	std: 0.0542169
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44268334, 0.55495, 0.55518335, 0.55466664, 0.44588333]
	train_accs: [0.44268334, 0.55495, 0.55518335, 0.55466664, 0.44588333]
	best_train_sub_head: 2
	worst: 0.44268334
	avg: 0.51067334
	best: 0.55518335

Starting e_i: 104
Model ind 640 epoch 104 head A head_i_epoch 0 batch 0: avg loss -2.661558 avg loss no lamb -2.661558 time 2019-02-17 06:33:14.797033
Model ind 640 epoch 104 head A head_i_epoch 0 batch 100: avg loss -2.582698 avg loss no lamb -2.582698 time 2019-02-17 06:35:37.859663
Model ind 640 epoch 104 head A head_i_epoch 0 batch 200: avg loss -2.688850 avg loss no lamb -2.688850 time 2019-02-17 06:38:01.118928
last batch sz 160
Model ind 640 epoch 104 head B head_i_epoch 0 batch 0: avg loss -1.325347 avg loss no lamb -1.325347 time 2019-02-17 06:39:45.736091
Model ind 640 epoch 104 head B head_i_epoch 0 batch 100: avg loss -1.402744 avg loss no lamb -1.402744 time 2019-02-17 06:42:09.381698
Model ind 640 epoch 104 head B head_i_epoch 0 batch 200: avg loss -1.380137 avg loss no lamb -1.380137 time 2019-02-17 06:44:32.448117
last batch sz 160
Model ind 640 epoch 104 head B head_i_epoch 1 batch 0: avg loss -1.347571 avg loss no lamb -1.347571 time 2019-02-17 06:46:15.648753
Model ind 640 epoch 104 head B head_i_epoch 1 batch 100: avg loss -1.369129 avg loss no lamb -1.369129 time 2019-02-17 06:48:38.047523
Model ind 640 epoch 104 head B head_i_epoch 1 batch 200: avg loss -1.431297 avg loss no lamb -1.431297 time 2019-02-17 06:51:01.060235
last batch sz 160
Pre: time 2019-02-17 06:53:09.234879: 
 	std: 0.05316563
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.43678334, 0.54538333, 0.5460333, 0.5445167, 0.4368]
	train_accs: [0.43678334, 0.54538333, 0.5460333, 0.5445167, 0.4368]
	best_train_sub_head: 2
	worst: 0.43678334
	avg: 0.50190336
	best: 0.5460333

Starting e_i: 105
Model ind 640 epoch 105 head A head_i_epoch 0 batch 0: avg loss -2.629368 avg loss no lamb -2.629368 time 2019-02-17 06:53:11.065048
Model ind 640 epoch 105 head A head_i_epoch 0 batch 100: avg loss -2.616683 avg loss no lamb -2.616683 time 2019-02-17 06:55:34.214731
Model ind 640 epoch 105 head A head_i_epoch 0 batch 200: avg loss -2.663064 avg loss no lamb -2.663064 time 2019-02-17 06:57:56.619549
last batch sz 160
Model ind 640 epoch 105 head B head_i_epoch 0 batch 0: avg loss -1.375956 avg loss no lamb -1.375956 time 2019-02-17 06:59:39.929680
Model ind 640 epoch 105 head B head_i_epoch 0 batch 100: avg loss -1.373799 avg loss no lamb -1.373799 time 2019-02-17 07:02:02.199119
Model ind 640 epoch 105 head B head_i_epoch 0 batch 200: avg loss -1.396429 avg loss no lamb -1.396429 time 2019-02-17 07:04:24.473394
last batch sz 160
Model ind 640 epoch 105 head B head_i_epoch 1 batch 0: avg loss -1.366245 avg loss no lamb -1.366245 time 2019-02-17 07:06:07.892129
Model ind 640 epoch 105 head B head_i_epoch 1 batch 100: avg loss -1.325645 avg loss no lamb -1.325645 time 2019-02-17 07:08:29.569977
Model ind 640 epoch 105 head B head_i_epoch 1 batch 200: avg loss -1.481240 avg loss no lamb -1.481240 time 2019-02-17 07:10:50.842855
last batch sz 160
Pre: time 2019-02-17 07:12:57.565936: 
 	std: 0.051916044
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45093334, 0.55736667, 0.557, 0.55516666, 0.45016667]
	train_accs: [0.45093334, 0.55736667, 0.557, 0.55516666, 0.45016667]
	best_train_sub_head: 1
	worst: 0.45016667
	avg: 0.51412666
	best: 0.55736667

Starting e_i: 106
Model ind 640 epoch 106 head A head_i_epoch 0 batch 0: avg loss -2.655732 avg loss no lamb -2.655732 time 2019-02-17 07:12:59.487065
Model ind 640 epoch 106 head A head_i_epoch 0 batch 100: avg loss -2.677774 avg loss no lamb -2.677774 time 2019-02-17 07:15:21.113358
Model ind 640 epoch 106 head A head_i_epoch 0 batch 200: avg loss -2.653577 avg loss no lamb -2.653577 time 2019-02-17 07:17:43.448112
last batch sz 160
Model ind 640 epoch 106 head B head_i_epoch 0 batch 0: avg loss -1.384178 avg loss no lamb -1.384178 time 2019-02-17 07:19:26.928809
Model ind 640 epoch 106 head B head_i_epoch 0 batch 100: avg loss -1.300267 avg loss no lamb -1.300267 time 2019-02-17 07:21:49.308924
Model ind 640 epoch 106 head B head_i_epoch 0 batch 200: avg loss -1.494900 avg loss no lamb -1.494900 time 2019-02-17 07:24:11.383468
last batch sz 160
Model ind 640 epoch 106 head B head_i_epoch 1 batch 0: avg loss -1.386982 avg loss no lamb -1.386982 time 2019-02-17 07:25:54.470856
Model ind 640 epoch 106 head B head_i_epoch 1 batch 100: avg loss -1.332117 avg loss no lamb -1.332117 time 2019-02-17 07:28:17.839454
Model ind 640 epoch 106 head B head_i_epoch 1 batch 200: avg loss -1.360114 avg loss no lamb -1.360114 time 2019-02-17 07:30:41.206572
last batch sz 160
Pre: time 2019-02-17 07:32:48.974544: 
 	std: 0.056231193
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44316667, 0.5571167, 0.55855, 0.5563167, 0.44195]
	train_accs: [0.44316667, 0.5571167, 0.55855, 0.5563167, 0.44195]
	best_train_sub_head: 2
	worst: 0.44195
	avg: 0.51142
	best: 0.55855

Starting e_i: 107
Model ind 640 epoch 107 head A head_i_epoch 0 batch 0: avg loss -2.706394 avg loss no lamb -2.706394 time 2019-02-17 07:32:50.763263
Model ind 640 epoch 107 head A head_i_epoch 0 batch 100: avg loss -2.639061 avg loss no lamb -2.639061 time 2019-02-17 07:35:13.640347
Model ind 640 epoch 107 head A head_i_epoch 0 batch 200: avg loss -2.662720 avg loss no lamb -2.662720 time 2019-02-17 07:37:36.883054
last batch sz 160
Model ind 640 epoch 107 head B head_i_epoch 0 batch 0: avg loss -1.383726 avg loss no lamb -1.383726 time 2019-02-17 07:39:21.140126
Model ind 640 epoch 107 head B head_i_epoch 0 batch 100: avg loss -1.368965 avg loss no lamb -1.368965 time 2019-02-17 07:41:43.039579
Model ind 640 epoch 107 head B head_i_epoch 0 batch 200: avg loss -1.507372 avg loss no lamb -1.507372 time 2019-02-17 07:44:05.039236
last batch sz 160
Model ind 640 epoch 107 head B head_i_epoch 1 batch 0: avg loss -1.342924 avg loss no lamb -1.342924 time 2019-02-17 07:45:49.104305
Model ind 640 epoch 107 head B head_i_epoch 1 batch 100: avg loss -1.325704 avg loss no lamb -1.325704 time 2019-02-17 07:48:11.823466
Model ind 640 epoch 107 head B head_i_epoch 1 batch 200: avg loss -1.459669 avg loss no lamb -1.459669 time 2019-02-17 07:50:34.436233
last batch sz 160
Pre: time 2019-02-17 07:52:42.143983: 
 	std: 0.05105309
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4543, 0.55791664, 0.5589833, 0.55795, 0.45385]
	train_accs: [0.4543, 0.55791664, 0.5589833, 0.55795, 0.45385]
	best_train_sub_head: 2
	worst: 0.45385
	avg: 0.5166
	best: 0.5589833

Starting e_i: 108
Model ind 640 epoch 108 head A head_i_epoch 0 batch 0: avg loss -2.608974 avg loss no lamb -2.608974 time 2019-02-17 07:52:43.938076
Model ind 640 epoch 108 head A head_i_epoch 0 batch 100: avg loss -2.675400 avg loss no lamb -2.675400 time 2019-02-17 07:55:06.114176
Model ind 640 epoch 108 head A head_i_epoch 0 batch 200: avg loss -2.734667 avg loss no lamb -2.734667 time 2019-02-17 07:57:28.840845
last batch sz 160
Model ind 640 epoch 108 head B head_i_epoch 0 batch 0: avg loss -1.393683 avg loss no lamb -1.393683 time 2019-02-17 07:59:13.082332
Model ind 640 epoch 108 head B head_i_epoch 0 batch 100: avg loss -1.328658 avg loss no lamb -1.328658 time 2019-02-17 08:01:36.339183
Model ind 640 epoch 108 head B head_i_epoch 0 batch 200: avg loss -1.532241 avg loss no lamb -1.532241 time 2019-02-17 08:03:58.276845
last batch sz 160
Model ind 640 epoch 108 head B head_i_epoch 1 batch 0: avg loss -1.440112 avg loss no lamb -1.440112 time 2019-02-17 08:05:41.601751
Model ind 640 epoch 108 head B head_i_epoch 1 batch 100: avg loss -1.300839 avg loss no lamb -1.300839 time 2019-02-17 08:08:03.748984
Model ind 640 epoch 108 head B head_i_epoch 1 batch 200: avg loss -1.455701 avg loss no lamb -1.455701 time 2019-02-17 08:10:25.527387
last batch sz 160
Pre: time 2019-02-17 08:12:32.606440: 
 	std: 0.055171
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44708332, 0.55971664, 0.56015, 0.5596833, 0.44738334]
	train_accs: [0.44708332, 0.55971664, 0.56015, 0.5596833, 0.44738334]
	best_train_sub_head: 2
	worst: 0.44708332
	avg: 0.5148033
	best: 0.56015

Starting e_i: 109
Model ind 640 epoch 109 head A head_i_epoch 0 batch 0: avg loss -2.691579 avg loss no lamb -2.691579 time 2019-02-17 08:12:34.399573
Model ind 640 epoch 109 head A head_i_epoch 0 batch 100: avg loss -2.690957 avg loss no lamb -2.690957 time 2019-02-17 08:14:56.538866
Model ind 640 epoch 109 head A head_i_epoch 0 batch 200: avg loss -2.670390 avg loss no lamb -2.670390 time 2019-02-17 08:17:18.615945
last batch sz 160
Model ind 640 epoch 109 head B head_i_epoch 0 batch 0: avg loss -1.410740 avg loss no lamb -1.410740 time 2019-02-17 08:19:02.001744
Model ind 640 epoch 109 head B head_i_epoch 0 batch 100: avg loss -1.404120 avg loss no lamb -1.404120 time 2019-02-17 08:21:24.143151
Model ind 640 epoch 109 head B head_i_epoch 0 batch 200: avg loss -1.493347 avg loss no lamb -1.493347 time 2019-02-17 08:23:47.219874
last batch sz 160
Model ind 640 epoch 109 head B head_i_epoch 1 batch 0: avg loss -1.446038 avg loss no lamb -1.446038 time 2019-02-17 08:25:30.218644
Model ind 640 epoch 109 head B head_i_epoch 1 batch 100: avg loss -1.276534 avg loss no lamb -1.276534 time 2019-02-17 08:27:51.850732
Model ind 640 epoch 109 head B head_i_epoch 1 batch 200: avg loss -1.446272 avg loss no lamb -1.446272 time 2019-02-17 08:30:13.581568
last batch sz 160
Pre: time 2019-02-17 08:32:20.911644: 
 	std: 0.053932663
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45461667, 0.56435, 0.5644, 0.56476665, 0.45421666]
	train_accs: [0.45461667, 0.56435, 0.5644, 0.56476665, 0.45421666]
	best_train_sub_head: 3
	worst: 0.45421666
	avg: 0.52047
	best: 0.56476665

Starting e_i: 110
Model ind 640 epoch 110 head A head_i_epoch 0 batch 0: avg loss -2.696330 avg loss no lamb -2.696330 time 2019-02-17 08:32:25.794741
Model ind 640 epoch 110 head A head_i_epoch 0 batch 100: avg loss -2.655077 avg loss no lamb -2.655077 time 2019-02-17 08:34:48.425694
Model ind 640 epoch 110 head A head_i_epoch 0 batch 200: avg loss -2.701330 avg loss no lamb -2.701330 time 2019-02-17 08:37:11.554528
last batch sz 160
Model ind 640 epoch 110 head B head_i_epoch 0 batch 0: avg loss -1.418417 avg loss no lamb -1.418417 time 2019-02-17 08:38:54.926741
Model ind 640 epoch 110 head B head_i_epoch 0 batch 100: avg loss -1.367485 avg loss no lamb -1.367485 time 2019-02-17 08:41:16.770618
Model ind 640 epoch 110 head B head_i_epoch 0 batch 200: avg loss -1.420018 avg loss no lamb -1.420018 time 2019-02-17 08:43:38.666708
last batch sz 160
Model ind 640 epoch 110 head B head_i_epoch 1 batch 0: avg loss -1.415075 avg loss no lamb -1.415075 time 2019-02-17 08:45:21.828023
Model ind 640 epoch 110 head B head_i_epoch 1 batch 100: avg loss -1.304842 avg loss no lamb -1.304842 time 2019-02-17 08:47:44.139607
Model ind 640 epoch 110 head B head_i_epoch 1 batch 200: avg loss -1.474292 avg loss no lamb -1.474292 time 2019-02-17 08:50:05.946724
last batch sz 160
Pre: time 2019-02-17 08:52:13.104691: 
 	std: 0.05420119
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44268334, 0.55333334, 0.5528167, 0.55373335, 0.44263333]
	train_accs: [0.44268334, 0.55333334, 0.5528167, 0.55373335, 0.44263333]
	best_train_sub_head: 3
	worst: 0.44263333
	avg: 0.50904
	best: 0.55373335

Starting e_i: 111
Model ind 640 epoch 111 head A head_i_epoch 0 batch 0: avg loss -2.652732 avg loss no lamb -2.652732 time 2019-02-17 08:52:18.110212
Model ind 640 epoch 111 head A head_i_epoch 0 batch 100: avg loss -2.586770 avg loss no lamb -2.586770 time 2019-02-17 08:54:40.423154
Model ind 640 epoch 111 head A head_i_epoch 0 batch 200: avg loss -2.641182 avg loss no lamb -2.641182 time 2019-02-17 08:57:02.942669
last batch sz 160
Model ind 640 epoch 111 head B head_i_epoch 0 batch 0: avg loss -1.348729 avg loss no lamb -1.348729 time 2019-02-17 08:58:46.273922
Model ind 640 epoch 111 head B head_i_epoch 0 batch 100: avg loss -1.300074 avg loss no lamb -1.300074 time 2019-02-17 09:01:07.807633
Model ind 640 epoch 111 head B head_i_epoch 0 batch 200: avg loss -1.412376 avg loss no lamb -1.412376 time 2019-02-17 09:03:29.433919
last batch sz 160
Model ind 640 epoch 111 head B head_i_epoch 1 batch 0: avg loss -1.329906 avg loss no lamb -1.329906 time 2019-02-17 09:05:13.251322
Model ind 640 epoch 111 head B head_i_epoch 1 batch 100: avg loss -1.350896 avg loss no lamb -1.350896 time 2019-02-17 09:07:35.204516
Model ind 640 epoch 111 head B head_i_epoch 1 batch 200: avg loss -1.513971 avg loss no lamb -1.513971 time 2019-02-17 09:09:57.021122
last batch sz 160
Pre: time 2019-02-17 09:12:04.133878: 
 	std: 0.050041188
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44688332, 0.54891664, 0.54835, 0.5487667, 0.44618332]
	train_accs: [0.44688332, 0.54891664, 0.54835, 0.5487667, 0.44618332]
	best_train_sub_head: 1
	worst: 0.44618332
	avg: 0.50781995
	best: 0.54891664

Starting e_i: 112
Model ind 640 epoch 112 head A head_i_epoch 0 batch 0: avg loss -2.675899 avg loss no lamb -2.675899 time 2019-02-17 09:12:05.926659
Model ind 640 epoch 112 head A head_i_epoch 0 batch 100: avg loss -2.652823 avg loss no lamb -2.652823 time 2019-02-17 09:14:28.276587
Model ind 640 epoch 112 head A head_i_epoch 0 batch 200: avg loss -2.701980 avg loss no lamb -2.701980 time 2019-02-17 09:16:50.809424
last batch sz 160
Model ind 640 epoch 112 head B head_i_epoch 0 batch 0: avg loss -1.357680 avg loss no lamb -1.357680 time 2019-02-17 09:18:34.887969
Model ind 640 epoch 112 head B head_i_epoch 0 batch 100: avg loss -1.346249 avg loss no lamb -1.346249 time 2019-02-17 09:20:58.640691
Model ind 640 epoch 112 head B head_i_epoch 0 batch 200: avg loss -1.423746 avg loss no lamb -1.423746 time 2019-02-17 09:23:22.486320
last batch sz 160
Model ind 640 epoch 112 head B head_i_epoch 1 batch 0: avg loss -1.329088 avg loss no lamb -1.329088 time 2019-02-17 09:25:07.109664
Model ind 640 epoch 112 head B head_i_epoch 1 batch 100: avg loss -1.346168 avg loss no lamb -1.346168 time 2019-02-17 09:27:29.826822
Model ind 640 epoch 112 head B head_i_epoch 1 batch 200: avg loss -1.512802 avg loss no lamb -1.512802 time 2019-02-17 09:29:52.241341
last batch sz 160
Pre: time 2019-02-17 09:32:00.604860: 
 	std: 0.05413688
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.43985, 0.5498667, 0.54895, 0.551, 0.43903333]
	train_accs: [0.43985, 0.5498667, 0.54895, 0.551, 0.43903333]
	best_train_sub_head: 3
	worst: 0.43903333
	avg: 0.50574005
	best: 0.551

Starting e_i: 113
Model ind 640 epoch 113 head A head_i_epoch 0 batch 0: avg loss -2.672728 avg loss no lamb -2.672728 time 2019-02-17 09:32:02.479094
Model ind 640 epoch 113 head A head_i_epoch 0 batch 100: avg loss -2.716760 avg loss no lamb -2.716760 time 2019-02-17 09:34:25.426792
Model ind 640 epoch 113 head A head_i_epoch 0 batch 200: avg loss -2.651593 avg loss no lamb -2.651593 time 2019-02-17 09:36:48.368245
last batch sz 160
Model ind 640 epoch 113 head B head_i_epoch 0 batch 0: avg loss -1.394390 avg loss no lamb -1.394390 time 2019-02-17 09:38:32.400243
Model ind 640 epoch 113 head B head_i_epoch 0 batch 100: avg loss -1.370585 avg loss no lamb -1.370585 time 2019-02-17 09:40:54.169182
Model ind 640 epoch 113 head B head_i_epoch 0 batch 200: avg loss -1.435978 avg loss no lamb -1.435978 time 2019-02-17 09:43:16.627107
last batch sz 160
Model ind 640 epoch 113 head B head_i_epoch 1 batch 0: avg loss -1.378608 avg loss no lamb -1.378608 time 2019-02-17 09:45:00.565987
Model ind 640 epoch 113 head B head_i_epoch 1 batch 100: avg loss -1.317105 avg loss no lamb -1.317105 time 2019-02-17 09:47:23.439125
Model ind 640 epoch 113 head B head_i_epoch 1 batch 200: avg loss -1.495393 avg loss no lamb -1.495393 time 2019-02-17 09:49:47.308519
last batch sz 160
Pre: time 2019-02-17 09:51:55.282539: 
 	std: 0.054614883
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45116666, 0.5621833, 0.56123334, 0.5628333, 0.45005]
	train_accs: [0.45116666, 0.5621833, 0.56123334, 0.5628333, 0.45005]
	best_train_sub_head: 3
	worst: 0.45005
	avg: 0.51749337
	best: 0.5628333

Starting e_i: 114
Model ind 640 epoch 114 head A head_i_epoch 0 batch 0: avg loss -2.716623 avg loss no lamb -2.716623 time 2019-02-17 09:51:57.082212
Model ind 640 epoch 114 head A head_i_epoch 0 batch 100: avg loss -2.647115 avg loss no lamb -2.647115 time 2019-02-17 09:54:19.885923
Model ind 640 epoch 114 head A head_i_epoch 0 batch 200: avg loss -2.696192 avg loss no lamb -2.696192 time 2019-02-17 09:56:42.864326
last batch sz 160
Model ind 640 epoch 114 head B head_i_epoch 0 batch 0: avg loss -1.388367 avg loss no lamb -1.388367 time 2019-02-17 09:58:26.838893
Model ind 640 epoch 114 head B head_i_epoch 0 batch 100: avg loss -1.346765 avg loss no lamb -1.346765 time 2019-02-17 10:00:49.669312
Model ind 640 epoch 114 head B head_i_epoch 0 batch 200: avg loss -1.518268 avg loss no lamb -1.518268 time 2019-02-17 10:03:12.795517
last batch sz 160
Model ind 640 epoch 114 head B head_i_epoch 1 batch 0: avg loss -1.369205 avg loss no lamb -1.369205 time 2019-02-17 10:04:57.050463
Model ind 640 epoch 114 head B head_i_epoch 1 batch 100: avg loss -1.337743 avg loss no lamb -1.337743 time 2019-02-17 10:07:19.726436
Model ind 640 epoch 114 head B head_i_epoch 1 batch 200: avg loss -1.438129 avg loss no lamb -1.438129 time 2019-02-17 10:09:42.021608
last batch sz 160
Pre: time 2019-02-17 10:11:49.787160: 
 	std: 0.0549769
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4445, 0.55696666, 0.55736667, 0.55655, 0.44498333]
	train_accs: [0.4445, 0.55696666, 0.55736667, 0.55655, 0.44498333]
	best_train_sub_head: 2
	worst: 0.4445
	avg: 0.51207334
	best: 0.55736667

Starting e_i: 115
Model ind 640 epoch 115 head A head_i_epoch 0 batch 0: avg loss -2.557946 avg loss no lamb -2.557946 time 2019-02-17 10:11:51.662426
Model ind 640 epoch 115 head A head_i_epoch 0 batch 100: avg loss -2.640324 avg loss no lamb -2.640324 time 2019-02-17 10:14:13.823933
Model ind 640 epoch 115 head A head_i_epoch 0 batch 200: avg loss -2.803309 avg loss no lamb -2.803309 time 2019-02-17 10:16:36.202396
last batch sz 160
Model ind 640 epoch 115 head B head_i_epoch 0 batch 0: avg loss -1.270687 avg loss no lamb -1.270687 time 2019-02-17 10:18:20.885045
Model ind 640 epoch 115 head B head_i_epoch 0 batch 100: avg loss -1.312909 avg loss no lamb -1.312909 time 2019-02-17 10:20:43.286789
Model ind 640 epoch 115 head B head_i_epoch 0 batch 200: avg loss -1.530238 avg loss no lamb -1.530238 time 2019-02-17 10:23:05.500290
last batch sz 160
Model ind 640 epoch 115 head B head_i_epoch 1 batch 0: avg loss -1.345285 avg loss no lamb -1.345285 time 2019-02-17 10:24:49.342142
Model ind 640 epoch 115 head B head_i_epoch 1 batch 100: avg loss -1.349498 avg loss no lamb -1.349498 time 2019-02-17 10:27:11.082958
Model ind 640 epoch 115 head B head_i_epoch 1 batch 200: avg loss -1.487230 avg loss no lamb -1.487230 time 2019-02-17 10:29:34.565235
last batch sz 160
Pre: time 2019-02-17 10:31:43.739235: 
 	std: 0.050629333
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44226667, 0.5454, 0.5449167, 0.54511666, 0.44133332]
	train_accs: [0.44226667, 0.5454, 0.5449167, 0.54511666, 0.44133332]
	best_train_sub_head: 1
	worst: 0.44133332
	avg: 0.5038067
	best: 0.5454

Starting e_i: 116
Model ind 640 epoch 116 head A head_i_epoch 0 batch 0: avg loss -2.642523 avg loss no lamb -2.642523 time 2019-02-17 10:31:45.758623
Model ind 640 epoch 116 head A head_i_epoch 0 batch 100: avg loss -2.635286 avg loss no lamb -2.635286 time 2019-02-17 10:34:09.664067
Model ind 640 epoch 116 head A head_i_epoch 0 batch 200: avg loss -2.693082 avg loss no lamb -2.693082 time 2019-02-17 10:36:33.501501
last batch sz 160
Model ind 640 epoch 116 head B head_i_epoch 0 batch 0: avg loss -1.434572 avg loss no lamb -1.434572 time 2019-02-17 10:38:18.220273
Model ind 640 epoch 116 head B head_i_epoch 0 batch 100: avg loss -1.281477 avg loss no lamb -1.281477 time 2019-02-17 10:40:40.432177
Model ind 640 epoch 116 head B head_i_epoch 0 batch 200: avg loss -1.550814 avg loss no lamb -1.550814 time 2019-02-17 10:43:03.007151
last batch sz 160
Model ind 640 epoch 116 head B head_i_epoch 1 batch 0: avg loss -1.412820 avg loss no lamb -1.412820 time 2019-02-17 10:44:46.610693
Model ind 640 epoch 116 head B head_i_epoch 1 batch 100: avg loss -1.335467 avg loss no lamb -1.335467 time 2019-02-17 10:47:08.497476
Model ind 640 epoch 116 head B head_i_epoch 1 batch 200: avg loss -1.517074 avg loss no lamb -1.517074 time 2019-02-17 10:49:30.914185
last batch sz 160
Pre: time 2019-02-17 10:51:38.744641: 
 	std: 0.0516898
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45566666, 0.5611333, 0.5609667, 0.56098336, 0.45536667]
	train_accs: [0.45566666, 0.5611333, 0.5609667, 0.56098336, 0.45536667]
	best_train_sub_head: 1
	worst: 0.45536667
	avg: 0.5188233
	best: 0.5611333

Starting e_i: 117
Model ind 640 epoch 117 head A head_i_epoch 0 batch 0: avg loss -2.594709 avg loss no lamb -2.594709 time 2019-02-17 10:51:40.588938
Model ind 640 epoch 117 head A head_i_epoch 0 batch 100: avg loss -2.601743 avg loss no lamb -2.601743 time 2019-02-17 10:54:03.331875
Model ind 640 epoch 117 head A head_i_epoch 0 batch 200: avg loss -2.745902 avg loss no lamb -2.745902 time 2019-02-17 10:56:26.419060
last batch sz 160
Model ind 640 epoch 117 head B head_i_epoch 0 batch 0: avg loss -1.415226 avg loss no lamb -1.415226 time 2019-02-17 10:58:10.518856
Model ind 640 epoch 117 head B head_i_epoch 0 batch 100: avg loss -1.361847 avg loss no lamb -1.361847 time 2019-02-17 11:00:33.015316
Model ind 640 epoch 117 head B head_i_epoch 0 batch 200: avg loss -1.486652 avg loss no lamb -1.486652 time 2019-02-17 11:02:54.767439
last batch sz 160
Model ind 640 epoch 117 head B head_i_epoch 1 batch 0: avg loss -1.404671 avg loss no lamb -1.404671 time 2019-02-17 11:04:38.428211
Model ind 640 epoch 117 head B head_i_epoch 1 batch 100: avg loss -1.285511 avg loss no lamb -1.285511 time 2019-02-17 11:07:01.030732
Model ind 640 epoch 117 head B head_i_epoch 1 batch 200: avg loss -1.522927 avg loss no lamb -1.522927 time 2019-02-17 11:09:23.454826
last batch sz 160
Pre: time 2019-02-17 11:11:32.269229: 
 	std: 0.053533006
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44401667, 0.5538667, 0.55301666, 0.55343336, 0.44431666]
	train_accs: [0.44401667, 0.5538667, 0.55301666, 0.55343336, 0.44431666]
	best_train_sub_head: 1
	worst: 0.44401667
	avg: 0.50973
	best: 0.5538667

Starting e_i: 118
Model ind 640 epoch 118 head A head_i_epoch 0 batch 0: avg loss -2.690021 avg loss no lamb -2.690021 time 2019-02-17 11:11:34.107525
Model ind 640 epoch 118 head A head_i_epoch 0 batch 100: avg loss -2.654246 avg loss no lamb -2.654246 time 2019-02-17 11:13:57.139533
Model ind 640 epoch 118 head A head_i_epoch 0 batch 200: avg loss -2.656010 avg loss no lamb -2.656010 time 2019-02-17 11:16:20.109747
last batch sz 160
Model ind 640 epoch 118 head B head_i_epoch 0 batch 0: avg loss -1.392578 avg loss no lamb -1.392578 time 2019-02-17 11:18:04.154040
Model ind 640 epoch 118 head B head_i_epoch 0 batch 100: avg loss -1.335173 avg loss no lamb -1.335173 time 2019-02-17 11:20:27.049276
Model ind 640 epoch 118 head B head_i_epoch 0 batch 200: avg loss -1.473386 avg loss no lamb -1.473386 time 2019-02-17 11:22:49.781219
last batch sz 160
Model ind 640 epoch 118 head B head_i_epoch 1 batch 0: avg loss -1.426003 avg loss no lamb -1.426003 time 2019-02-17 11:24:33.581272
Model ind 640 epoch 118 head B head_i_epoch 1 batch 100: avg loss -1.347099 avg loss no lamb -1.347099 time 2019-02-17 11:26:55.952199
Model ind 640 epoch 118 head B head_i_epoch 1 batch 200: avg loss -1.412887 avg loss no lamb -1.412887 time 2019-02-17 11:29:17.569163
last batch sz 160
Pre: time 2019-02-17 11:31:24.710419: 
 	std: 0.05382455
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4456, 0.5551, 0.5546833, 0.55541664, 0.4448]
	train_accs: [0.4456, 0.5551, 0.5546833, 0.55541664, 0.4448]
	best_train_sub_head: 3
	worst: 0.4448
	avg: 0.51111996
	best: 0.55541664

Starting e_i: 119
Model ind 640 epoch 119 head A head_i_epoch 0 batch 0: avg loss -2.674014 avg loss no lamb -2.674014 time 2019-02-17 11:31:26.538144
Model ind 640 epoch 119 head A head_i_epoch 0 batch 100: avg loss -2.615713 avg loss no lamb -2.615713 time 2019-02-17 11:33:48.803297
Model ind 640 epoch 119 head A head_i_epoch 0 batch 200: avg loss -2.625500 avg loss no lamb -2.625500 time 2019-02-17 11:36:11.241691
last batch sz 160
Model ind 640 epoch 119 head B head_i_epoch 0 batch 0: avg loss -1.359112 avg loss no lamb -1.359112 time 2019-02-17 11:37:54.885678
Model ind 640 epoch 119 head B head_i_epoch 0 batch 100: avg loss -1.372053 avg loss no lamb -1.372053 time 2019-02-17 11:40:17.417420
Model ind 640 epoch 119 head B head_i_epoch 0 batch 200: avg loss -1.608518 avg loss no lamb -1.608518 time 2019-02-17 11:42:40.142945
last batch sz 160
Model ind 640 epoch 119 head B head_i_epoch 1 batch 0: avg loss -1.312292 avg loss no lamb -1.312292 time 2019-02-17 11:44:24.375569
Model ind 640 epoch 119 head B head_i_epoch 1 batch 100: avg loss -1.303774 avg loss no lamb -1.303774 time 2019-02-17 11:46:47.974284
Model ind 640 epoch 119 head B head_i_epoch 1 batch 200: avg loss -1.372929 avg loss no lamb -1.372929 time 2019-02-17 11:49:10.889856
last batch sz 160
Pre: time 2019-02-17 11:51:19.011616: 
 	std: 0.053239845
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4449, 0.5534667, 0.55366665, 0.55326664, 0.44468334]
	train_accs: [0.4449, 0.5534667, 0.55366665, 0.55326664, 0.44468334]
	best_train_sub_head: 2
	worst: 0.44468334
	avg: 0.50999665
	best: 0.55366665

Starting e_i: 120
Model ind 640 epoch 120 head A head_i_epoch 0 batch 0: avg loss -2.738097 avg loss no lamb -2.738097 time 2019-02-17 11:51:20.871814
Model ind 640 epoch 120 head A head_i_epoch 0 batch 100: avg loss -2.662234 avg loss no lamb -2.662234 time 2019-02-17 11:53:44.161510
Model ind 640 epoch 120 head A head_i_epoch 0 batch 200: avg loss -2.704544 avg loss no lamb -2.704544 time 2019-02-17 11:56:06.646536
last batch sz 160
Model ind 640 epoch 120 head B head_i_epoch 0 batch 0: avg loss -1.422785 avg loss no lamb -1.422785 time 2019-02-17 11:57:49.913279
Model ind 640 epoch 120 head B head_i_epoch 0 batch 100: avg loss -1.352097 avg loss no lamb -1.352097 time 2019-02-17 12:00:11.508433
Model ind 640 epoch 120 head B head_i_epoch 0 batch 200: avg loss -1.472998 avg loss no lamb -1.472998 time 2019-02-17 12:02:33.785517
last batch sz 160
Model ind 640 epoch 120 head B head_i_epoch 1 batch 0: avg loss -1.417872 avg loss no lamb -1.417872 time 2019-02-17 12:04:17.054236
Model ind 640 epoch 120 head B head_i_epoch 1 batch 100: avg loss -1.401530 avg loss no lamb -1.401530 time 2019-02-17 12:06:39.679802
Model ind 640 epoch 120 head B head_i_epoch 1 batch 200: avg loss -1.447232 avg loss no lamb -1.447232 time 2019-02-17 12:09:01.356574
last batch sz 160
Pre: time 2019-02-17 12:11:08.201430: 
 	std: 0.0554961
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44425, 0.55661666, 0.55653334, 0.55675, 0.44246668]
	train_accs: [0.44425, 0.55661666, 0.55653334, 0.55675, 0.44246668]
	best_train_sub_head: 3
	worst: 0.44246668
	avg: 0.51132333
	best: 0.55675

Starting e_i: 121
Model ind 640 epoch 121 head A head_i_epoch 0 batch 0: avg loss -2.650253 avg loss no lamb -2.650253 time 2019-02-17 12:11:13.284835
Model ind 640 epoch 121 head A head_i_epoch 0 batch 100: avg loss -2.626468 avg loss no lamb -2.626468 time 2019-02-17 12:13:35.301761
Model ind 640 epoch 121 head A head_i_epoch 0 batch 200: avg loss -2.701212 avg loss no lamb -2.701212 time 2019-02-17 12:15:58.646891
last batch sz 160
Model ind 640 epoch 121 head B head_i_epoch 0 batch 0: avg loss -1.322463 avg loss no lamb -1.322463 time 2019-02-17 12:17:43.019765
Model ind 640 epoch 121 head B head_i_epoch 0 batch 100: avg loss -1.382740 avg loss no lamb -1.382740 time 2019-02-17 12:20:05.969812
Model ind 640 epoch 121 head B head_i_epoch 0 batch 200: avg loss -1.476549 avg loss no lamb -1.476549 time 2019-02-17 12:22:26.862174
last batch sz 160
Model ind 640 epoch 121 head B head_i_epoch 1 batch 0: avg loss -1.415782 avg loss no lamb -1.415782 time 2019-02-17 12:24:10.237191
Model ind 640 epoch 121 head B head_i_epoch 1 batch 100: avg loss -1.433796 avg loss no lamb -1.433796 time 2019-02-17 12:26:32.091213
Model ind 640 epoch 121 head B head_i_epoch 1 batch 200: avg loss -1.536872 avg loss no lamb -1.536872 time 2019-02-17 12:28:54.613244
last batch sz 160
Pre: time 2019-02-17 12:31:03.175490: 
 	std: 0.05493062
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.43968335, 0.5517167, 0.5506333, 0.55151665, 0.43865]
	train_accs: [0.43968335, 0.5517167, 0.5506333, 0.55151665, 0.43865]
	best_train_sub_head: 1
	worst: 0.43865
	avg: 0.50644
	best: 0.5517167

Starting e_i: 122
Model ind 640 epoch 122 head A head_i_epoch 0 batch 0: avg loss -2.713785 avg loss no lamb -2.713785 time 2019-02-17 12:31:05.039646
Model ind 640 epoch 122 head A head_i_epoch 0 batch 100: avg loss -2.670852 avg loss no lamb -2.670852 time 2019-02-17 12:33:27.993349
Model ind 640 epoch 122 head A head_i_epoch 0 batch 200: avg loss -2.669813 avg loss no lamb -2.669813 time 2019-02-17 12:35:50.258415
last batch sz 160
Model ind 640 epoch 122 head B head_i_epoch 0 batch 0: avg loss -1.435703 avg loss no lamb -1.435703 time 2019-02-17 12:37:33.791996
Model ind 640 epoch 122 head B head_i_epoch 0 batch 100: avg loss -1.338956 avg loss no lamb -1.338956 time 2019-02-17 12:39:56.475089
Model ind 640 epoch 122 head B head_i_epoch 0 batch 200: avg loss -1.516253 avg loss no lamb -1.516253 time 2019-02-17 12:42:19.575412
last batch sz 160
Model ind 640 epoch 122 head B head_i_epoch 1 batch 0: avg loss -1.402518 avg loss no lamb -1.402518 time 2019-02-17 12:44:03.580756
Model ind 640 epoch 122 head B head_i_epoch 1 batch 100: avg loss -1.368441 avg loss no lamb -1.368441 time 2019-02-17 12:46:25.427826
Model ind 640 epoch 122 head B head_i_epoch 1 batch 200: avg loss -1.334183 avg loss no lamb -1.334183 time 2019-02-17 12:48:47.736433
last batch sz 160
Pre: time 2019-02-17 12:50:56.173139: 
 	std: 0.05181782
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44373333, 0.5492667, 0.5485333, 0.54943335, 0.44288334]
	train_accs: [0.44373333, 0.5492667, 0.5485333, 0.54943335, 0.44288334]
	best_train_sub_head: 3
	worst: 0.44288334
	avg: 0.50677
	best: 0.54943335

Starting e_i: 123
Model ind 640 epoch 123 head A head_i_epoch 0 batch 0: avg loss -2.563097 avg loss no lamb -2.563097 time 2019-02-17 12:50:58.178876
Model ind 640 epoch 123 head A head_i_epoch 0 batch 100: avg loss -2.609862 avg loss no lamb -2.609862 time 2019-02-17 12:53:21.373982
Model ind 640 epoch 123 head A head_i_epoch 0 batch 200: avg loss -2.653833 avg loss no lamb -2.653833 time 2019-02-17 12:55:43.491660
last batch sz 160
Model ind 640 epoch 123 head B head_i_epoch 0 batch 0: avg loss -1.329837 avg loss no lamb -1.329837 time 2019-02-17 12:57:27.099484
Model ind 640 epoch 123 head B head_i_epoch 0 batch 100: avg loss -1.375103 avg loss no lamb -1.375103 time 2019-02-17 12:59:49.028740
Model ind 640 epoch 123 head B head_i_epoch 0 batch 200: avg loss -1.497929 avg loss no lamb -1.497929 time 2019-02-17 13:02:10.935127
last batch sz 160
Model ind 640 epoch 123 head B head_i_epoch 1 batch 0: avg loss -1.357199 avg loss no lamb -1.357199 time 2019-02-17 13:03:53.985248
Model ind 640 epoch 123 head B head_i_epoch 1 batch 100: avg loss -1.316249 avg loss no lamb -1.316249 time 2019-02-17 13:06:16.681789
Model ind 640 epoch 123 head B head_i_epoch 1 batch 200: avg loss -1.524490 avg loss no lamb -1.524490 time 2019-02-17 13:08:39.255578
last batch sz 160
Pre: time 2019-02-17 13:10:47.187914: 
 	std: 0.05476267
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44843334, 0.5626, 0.5614833, 0.56256664, 0.4525]
	train_accs: [0.44843334, 0.5626, 0.5614833, 0.56256664, 0.4525]
	best_train_sub_head: 1
	worst: 0.44843334
	avg: 0.5175166
	best: 0.5626

Starting e_i: 124
Model ind 640 epoch 124 head A head_i_epoch 0 batch 0: avg loss -2.671508 avg loss no lamb -2.671508 time 2019-02-17 13:10:49.035943
Model ind 640 epoch 124 head A head_i_epoch 0 batch 100: avg loss -2.628062 avg loss no lamb -2.628062 time 2019-02-17 13:13:11.830201
Model ind 640 epoch 124 head A head_i_epoch 0 batch 200: avg loss -2.660062 avg loss no lamb -2.660062 time 2019-02-17 13:15:34.134390
last batch sz 160
Model ind 640 epoch 124 head B head_i_epoch 0 batch 0: avg loss -1.365226 avg loss no lamb -1.365226 time 2019-02-17 13:17:17.979722
Model ind 640 epoch 124 head B head_i_epoch 0 batch 100: avg loss -1.297032 avg loss no lamb -1.297032 time 2019-02-17 13:19:40.549647
Model ind 640 epoch 124 head B head_i_epoch 0 batch 200: avg loss -1.499446 avg loss no lamb -1.499446 time 2019-02-17 13:22:02.334090
last batch sz 160
Model ind 640 epoch 124 head B head_i_epoch 1 batch 0: avg loss -1.460886 avg loss no lamb -1.460886 time 2019-02-17 13:23:45.566864
Model ind 640 epoch 124 head B head_i_epoch 1 batch 100: avg loss -1.425014 avg loss no lamb -1.425014 time 2019-02-17 13:26:07.207090
Model ind 640 epoch 124 head B head_i_epoch 1 batch 200: avg loss -1.462171 avg loss no lamb -1.462171 time 2019-02-17 13:28:29.259442
last batch sz 160
Pre: time 2019-02-17 13:30:37.716919: 
 	std: 0.057557814
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44138333, 0.55981666, 0.5586, 0.55906665, 0.44196665]
	train_accs: [0.44138333, 0.55981666, 0.5586, 0.55906665, 0.44196665]
	best_train_sub_head: 1
	worst: 0.44138333
	avg: 0.5121666
	best: 0.55981666

Starting e_i: 125
Model ind 640 epoch 125 head A head_i_epoch 0 batch 0: avg loss -2.655938 avg loss no lamb -2.655938 time 2019-02-17 13:30:39.656240
Model ind 640 epoch 125 head A head_i_epoch 0 batch 100: avg loss -2.599643 avg loss no lamb -2.599643 time 2019-02-17 13:33:01.906939
Model ind 640 epoch 125 head A head_i_epoch 0 batch 200: avg loss -2.798556 avg loss no lamb -2.798556 time 2019-02-17 13:35:24.441766
last batch sz 160
Model ind 640 epoch 125 head B head_i_epoch 0 batch 0: avg loss -1.444167 avg loss no lamb -1.444167 time 2019-02-17 13:37:08.764840
Model ind 640 epoch 125 head B head_i_epoch 0 batch 100: avg loss -1.408040 avg loss no lamb -1.408040 time 2019-02-17 13:39:31.902269
Model ind 640 epoch 125 head B head_i_epoch 0 batch 200: avg loss -1.568596 avg loss no lamb -1.568596 time 2019-02-17 13:41:54.795707
last batch sz 160
Model ind 640 epoch 125 head B head_i_epoch 1 batch 0: avg loss -1.331272 avg loss no lamb -1.331272 time 2019-02-17 13:43:37.947854
Model ind 640 epoch 125 head B head_i_epoch 1 batch 100: avg loss -1.396957 avg loss no lamb -1.396957 time 2019-02-17 13:45:59.643090
Model ind 640 epoch 125 head B head_i_epoch 1 batch 200: avg loss -1.532448 avg loss no lamb -1.532448 time 2019-02-17 13:48:21.333203
last batch sz 160
Pre: time 2019-02-17 13:50:28.591962: 
 	std: 0.0552968
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45635, 0.5693167, 0.56875, 0.56975, 0.45645]
	train_accs: [0.45635, 0.5693167, 0.56875, 0.56975, 0.45645]
	best_train_sub_head: 3
	worst: 0.45635
	avg: 0.5241233
	best: 0.56975

Starting e_i: 126
Model ind 640 epoch 126 head A head_i_epoch 0 batch 0: avg loss -2.647517 avg loss no lamb -2.647517 time 2019-02-17 13:50:33.946722
Model ind 640 epoch 126 head A head_i_epoch 0 batch 100: avg loss -2.672840 avg loss no lamb -2.672840 time 2019-02-17 13:52:56.447677
Model ind 640 epoch 126 head A head_i_epoch 0 batch 200: avg loss -2.754919 avg loss no lamb -2.754919 time 2019-02-17 13:55:19.846464
last batch sz 160
Model ind 640 epoch 126 head B head_i_epoch 0 batch 0: avg loss -1.489769 avg loss no lamb -1.489769 time 2019-02-17 13:57:04.227434
Model ind 640 epoch 126 head B head_i_epoch 0 batch 100: avg loss -1.367756 avg loss no lamb -1.367756 time 2019-02-17 13:59:27.486574
Model ind 640 epoch 126 head B head_i_epoch 0 batch 200: avg loss -1.529292 avg loss no lamb -1.529292 time 2019-02-17 14:01:50.815629
last batch sz 160
Model ind 640 epoch 126 head B head_i_epoch 1 batch 0: avg loss -1.407072 avg loss no lamb -1.407072 time 2019-02-17 14:03:35.593629
Model ind 640 epoch 126 head B head_i_epoch 1 batch 100: avg loss -1.323273 avg loss no lamb -1.323273 time 2019-02-17 14:05:58.132069
Model ind 640 epoch 126 head B head_i_epoch 1 batch 200: avg loss -1.415910 avg loss no lamb -1.415910 time 2019-02-17 14:08:20.336973
last batch sz 160
Pre: time 2019-02-17 14:10:27.733552: 
 	std: 0.0549497
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.44261667, 0.55543333, 0.55466664, 0.55511665, 0.4432]
	train_accs: [0.44261667, 0.55543333, 0.55466664, 0.55511665, 0.4432]
	best_train_sub_head: 1
	worst: 0.44261667
	avg: 0.5102067
	best: 0.55543333

Starting e_i: 127
Model ind 640 epoch 127 head A head_i_epoch 0 batch 0: avg loss -2.658186 avg loss no lamb -2.658186 time 2019-02-17 14:10:29.558227
Model ind 640 epoch 127 head A head_i_epoch 0 batch 100: avg loss -2.722602 avg loss no lamb -2.722602 time 2019-02-17 14:12:52.391942
Model ind 640 epoch 127 head A head_i_epoch 0 batch 200: avg loss -2.676447 avg loss no lamb -2.676447 time 2019-02-17 14:15:14.437767
last batch sz 160
Model ind 640 epoch 127 head B head_i_epoch 0 batch 0: avg loss -1.462942 avg loss no lamb -1.462942 time 2019-02-17 14:16:58.012364
Model ind 640 epoch 127 head B head_i_epoch 0 batch 100: avg loss -1.259425 avg loss no lamb -1.259425 time 2019-02-17 14:19:20.949027
Model ind 640 epoch 127 head B head_i_epoch 0 batch 200: avg loss -1.422112 avg loss no lamb -1.422112 time 2019-02-17 14:21:43.775526
last batch sz 160
Model ind 640 epoch 127 head B head_i_epoch 1 batch 0: avg loss -1.356866 avg loss no lamb -1.356866 time 2019-02-17 14:23:27.591573
Model ind 640 epoch 127 head B head_i_epoch 1 batch 100: avg loss -1.323345 avg loss no lamb -1.323345 time 2019-02-17 14:25:49.187856
Model ind 640 epoch 127 head B head_i_epoch 1 batch 200: avg loss -1.563201 avg loss no lamb -1.563201 time 2019-02-17 14:28:11.209178
last batch sz 160
Pre: time 2019-02-17 14:30:19.655801: 
 	std: 0.054524623
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45846668, 0.5689167, 0.5682167, 0.56923336, 0.45653334]
	train_accs: [0.45846668, 0.5689167, 0.5682167, 0.56923336, 0.45653334]
	best_train_sub_head: 3
	worst: 0.45653334
	avg: 0.52427334
	best: 0.56923336

Starting e_i: 128
Model ind 640 epoch 128 head A head_i_epoch 0 batch 0: avg loss -2.727379 avg loss no lamb -2.727379 time 2019-02-17 14:30:21.939318
Model ind 640 epoch 128 head A head_i_epoch 0 batch 100: avg loss -2.694672 avg loss no lamb -2.694672 time 2019-02-17 14:32:45.413110
Model ind 640 epoch 128 head A head_i_epoch 0 batch 200: avg loss -2.649955 avg loss no lamb -2.649955 time 2019-02-17 14:35:09.006545
last batch sz 160
Model ind 640 epoch 128 head B head_i_epoch 0 batch 0: avg loss -1.441185 avg loss no lamb -1.441185 time 2019-02-17 14:36:53.496469
Model ind 640 epoch 128 head B head_i_epoch 0 batch 100: avg loss -1.417301 avg loss no lamb -1.417301 time 2019-02-17 14:39:16.867092
Model ind 640 epoch 128 head B head_i_epoch 0 batch 200: avg loss -1.523813 avg loss no lamb -1.523813 time 2019-02-17 14:41:40.355518
last batch sz 160
Model ind 640 epoch 128 head B head_i_epoch 1 batch 0: avg loss -1.412307 avg loss no lamb -1.412307 time 2019-02-17 14:43:24.663724
Model ind 640 epoch 128 head B head_i_epoch 1 batch 100: avg loss -1.343461 avg loss no lamb -1.343461 time 2019-02-17 14:45:47.048035
Model ind 640 epoch 128 head B head_i_epoch 1 batch 200: avg loss -1.498431 avg loss no lamb -1.498431 time 2019-02-17 14:48:08.744099
last batch sz 160
Pre: time 2019-02-17 14:50:16.168529: 
 	std: 0.055375148
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46188334, 0.57603335, 0.5757167, 0.57568336, 0.46368334]
	train_accs: [0.46188334, 0.57603335, 0.5757167, 0.57568336, 0.46368334]
	best_train_sub_head: 1
	worst: 0.46188334
	avg: 0.5306
	best: 0.57603335

Starting e_i: 129
Model ind 640 epoch 129 head A head_i_epoch 0 batch 0: avg loss -2.673146 avg loss no lamb -2.673146 time 2019-02-17 14:50:21.747431
Model ind 640 epoch 129 head A head_i_epoch 0 batch 100: avg loss -2.594987 avg loss no lamb -2.594987 time 2019-02-17 14:52:44.901326
Model ind 640 epoch 129 head A head_i_epoch 0 batch 200: avg loss -2.701008 avg loss no lamb -2.701008 time 2019-02-17 14:55:07.326841
last batch sz 160
Model ind 640 epoch 129 head B head_i_epoch 0 batch 0: avg loss -1.382109 avg loss no lamb -1.382109 time 2019-02-17 14:56:50.657418
Model ind 640 epoch 129 head B head_i_epoch 0 batch 100: avg loss -1.261770 avg loss no lamb -1.261770 time 2019-02-17 14:59:12.224419
Model ind 640 epoch 129 head B head_i_epoch 0 batch 200: avg loss -1.511180 avg loss no lamb -1.511180 time 2019-02-17 15:01:33.578710
last batch sz 160
Model ind 640 epoch 129 head B head_i_epoch 1 batch 0: avg loss -1.453640 avg loss no lamb -1.453640 time 2019-02-17 15:03:16.560092
Model ind 640 epoch 129 head B head_i_epoch 1 batch 100: avg loss -1.447202 avg loss no lamb -1.447202 time 2019-02-17 15:05:38.832850
Model ind 640 epoch 129 head B head_i_epoch 1 batch 200: avg loss -1.597190 avg loss no lamb -1.597190 time 2019-02-17 15:08:01.152048
last batch sz 160
Pre: time 2019-02-17 15:10:09.597297: 
 	std: 0.051309325
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46638334, 0.5721833, 0.57343334, 0.5718, 0.46913335]
	train_accs: [0.46638334, 0.5721833, 0.57343334, 0.5718, 0.46913335]
	best_train_sub_head: 2
	worst: 0.46638334
	avg: 0.53058666
	best: 0.57343334

Starting e_i: 130
Model ind 640 epoch 130 head A head_i_epoch 0 batch 0: avg loss -2.709951 avg loss no lamb -2.709951 time 2019-02-17 15:10:11.622487
Model ind 640 epoch 130 head A head_i_epoch 0 batch 100: avg loss -2.662769 avg loss no lamb -2.662769 time 2019-02-17 15:12:34.796752
Model ind 640 epoch 130 head A head_i_epoch 0 batch 200: avg loss -2.720658 avg loss no lamb -2.720658 time 2019-02-17 15:14:58.111955
last batch sz 160
Model ind 640 epoch 130 head B head_i_epoch 0 batch 0: avg loss -1.458095 avg loss no lamb -1.458095 time 2019-02-17 15:16:42.342002
Model ind 640 epoch 130 head B head_i_epoch 0 batch 100: avg loss -1.394490 avg loss no lamb -1.394490 time 2019-02-17 15:19:04.154886
Model ind 640 epoch 130 head B head_i_epoch 0 batch 200: avg loss -1.412074 avg loss no lamb -1.412074 time 2019-02-17 15:21:25.940375
last batch sz 160
Model ind 640 epoch 130 head B head_i_epoch 1 batch 0: avg loss -1.338436 avg loss no lamb -1.338436 time 2019-02-17 15:23:09.917677
Model ind 640 epoch 130 head B head_i_epoch 1 batch 100: avg loss -1.312028 avg loss no lamb -1.312028 time 2019-02-17 15:25:31.996409
Model ind 640 epoch 130 head B head_i_epoch 1 batch 200: avg loss -1.474034 avg loss no lamb -1.474034 time 2019-02-17 15:27:55.163089
last batch sz 160
Pre: time 2019-02-17 15:30:03.977450: 
 	std: 0.054760125
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.45161667, 0.56186664, 0.5632667, 0.56168336, 0.4494]
	train_accs: [0.45161667, 0.56186664, 0.5632667, 0.56168336, 0.4494]
	best_train_sub_head: 2
	worst: 0.4494
	avg: 0.5175667
	best: 0.5632667

Starting e_i: 131
Model ind 640 epoch 131 head A head_i_epoch 0 batch 0: avg loss -2.715747 avg loss no lamb -2.715747 time 2019-02-17 15:30:09.689799
Model ind 640 epoch 131 head A head_i_epoch 0 batch 100: avg loss -2.686190 avg loss no lamb -2.686190 time 2019-02-17 15:32:33.362355
Model ind 640 epoch 131 head A head_i_epoch 0 batch 200: avg loss -2.717520 avg loss no lamb -2.717520 time 2019-02-17 15:34:57.198934
last batch sz 160
Model ind 640 epoch 131 head B head_i_epoch 0 batch 0: avg loss -1.503437 avg loss no lamb -1.503437 time 2019-02-17 15:36:41.881374
Model ind 640 epoch 131 head B head_i_epoch 0 batch 100: avg loss -1.445312 avg loss no lamb -1.445312 time 2019-02-17 15:39:04.432647
Model ind 640 epoch 131 head B head_i_epoch 0 batch 200: avg loss -1.406779 avg loss no lamb -1.406779 time 2019-02-17 15:41:26.699315
last batch sz 160
Model ind 640 epoch 131 head B head_i_epoch 1 batch 0: avg loss -1.339996 avg loss no lamb -1.339996 time 2019-02-17 15:43:11.348380
Model ind 640 epoch 131 head B head_i_epoch 1 batch 100: avg loss -1.356779 avg loss no lamb -1.356779 time 2019-02-17 15:45:34.878719
Model ind 640 epoch 131 head B head_i_epoch 1 batch 200: avg loss -1.512310 avg loss no lamb -1.512310 time 2019-02-17 15:47:58.213644
last batch sz 160
Pre: time 2019-02-17 15:50:06.081903: 
 	std: 0.051974706
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.45608333, 0.56133336, 0.56255, 0.56163335, 0.45541668]
	train_accs: [0.45608333, 0.56133336, 0.56255, 0.56163335, 0.45541668]
	best_train_sub_head: 2
	worst: 0.45541668
	avg: 0.51940334
	best: 0.56255

Starting e_i: 132
Model ind 640 epoch 132 head A head_i_epoch 0 batch 0: avg loss -2.701500 avg loss no lamb -2.701500 time 2019-02-17 15:50:08.147381
Model ind 640 epoch 132 head A head_i_epoch 0 batch 100: avg loss -2.647960 avg loss no lamb -2.647960 time 2019-02-17 15:52:30.403186
Model ind 640 epoch 132 head A head_i_epoch 0 batch 200: avg loss -2.803383 avg loss no lamb -2.803383 time 2019-02-17 15:54:51.942446
last batch sz 160
Model ind 640 epoch 132 head B head_i_epoch 0 batch 0: avg loss -1.395213 avg loss no lamb -1.395213 time 2019-02-17 15:56:35.832454
Model ind 640 epoch 132 head B head_i_epoch 0 batch 100: avg loss -1.396060 avg loss no lamb -1.396060 time 2019-02-17 15:58:59.097036
Model ind 640 epoch 132 head B head_i_epoch 0 batch 200: avg loss -1.573128 avg loss no lamb -1.573128 time 2019-02-17 16:01:22.524304
last batch sz 160
Model ind 640 epoch 132 head B head_i_epoch 1 batch 0: avg loss -1.268526 avg loss no lamb -1.268526 time 2019-02-17 16:03:06.765272
Model ind 640 epoch 132 head B head_i_epoch 1 batch 100: avg loss -1.310665 avg loss no lamb -1.310665 time 2019-02-17 16:05:29.494985
Model ind 640 epoch 132 head B head_i_epoch 1 batch 200: avg loss -1.514140 avg loss no lamb -1.514140 time 2019-02-17 16:07:52.230274
last batch sz 160
Pre: time 2019-02-17 16:10:00.280096: 
 	std: 0.054955974
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44401667, 0.55655, 0.5571333, 0.55733335, 0.44565]
	train_accs: [0.44401667, 0.55655, 0.5571333, 0.55733335, 0.44565]
	best_train_sub_head: 3
	worst: 0.44401667
	avg: 0.5121367
	best: 0.55733335

Starting e_i: 133
Model ind 640 epoch 133 head A head_i_epoch 0 batch 0: avg loss -2.724836 avg loss no lamb -2.724836 time 2019-02-17 16:10:02.663868
Model ind 640 epoch 133 head A head_i_epoch 0 batch 100: avg loss -2.701145 avg loss no lamb -2.701145 time 2019-02-17 16:12:25.048820
Model ind 640 epoch 133 head A head_i_epoch 0 batch 200: avg loss -2.754169 avg loss no lamb -2.754169 time 2019-02-17 16:14:47.228509
last batch sz 160
Model ind 640 epoch 133 head B head_i_epoch 0 batch 0: avg loss -1.475553 avg loss no lamb -1.475553 time 2019-02-17 16:16:30.390500
Model ind 640 epoch 133 head B head_i_epoch 0 batch 100: avg loss -1.423427 avg loss no lamb -1.423427 time 2019-02-17 16:18:52.258036
Model ind 640 epoch 133 head B head_i_epoch 0 batch 200: avg loss -1.508186 avg loss no lamb -1.508186 time 2019-02-17 16:21:14.367766
last batch sz 160
Model ind 640 epoch 133 head B head_i_epoch 1 batch 0: avg loss -1.409087 avg loss no lamb -1.409087 time 2019-02-17 16:22:57.856624
Model ind 640 epoch 133 head B head_i_epoch 1 batch 100: avg loss -1.389192 avg loss no lamb -1.389192 time 2019-02-17 16:25:20.873801
Model ind 640 epoch 133 head B head_i_epoch 1 batch 200: avg loss -1.448976 avg loss no lamb -1.448976 time 2019-02-17 16:27:44.586687
last batch sz 160
Pre: time 2019-02-17 16:29:53.137325: 
 	std: 0.053954404
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.44843334, 0.55755, 0.5585667, 0.55845, 0.44768333]
	train_accs: [0.44843334, 0.55755, 0.5585667, 0.55845, 0.44768333]
	best_train_sub_head: 2
	worst: 0.44768333
	avg: 0.5141367
	best: 0.5585667

Starting e_i: 134
Model ind 640 epoch 134 head A head_i_epoch 0 batch 0: avg loss -2.614860 avg loss no lamb -2.614860 time 2019-02-17 16:29:55.154018
Model ind 640 epoch 134 head A head_i_epoch 0 batch 100: avg loss -2.657506 avg loss no lamb -2.657506 time 2019-02-17 16:32:18.405472
Model ind 640 epoch 134 head A head_i_epoch 0 batch 200: avg loss -2.723851 avg loss no lamb -2.723851 time 2019-02-17 16:34:42.120768
last batch sz 160
Model ind 640 epoch 134 head B head_i_epoch 0 batch 0: avg loss -1.433510 avg loss no lamb -1.433510 time 2019-02-17 16:36:26.541176
Model ind 640 epoch 134 head B head_i_epoch 0 batch 100: avg loss -1.305375 avg loss no lamb -1.305375 time 2019-02-17 16:38:49.666298
Model ind 640 epoch 134 head B head_i_epoch 0 batch 200: avg loss -1.506577 avg loss no lamb -1.506577 time 2019-02-17 16:41:13.173049
last batch sz 160
Model ind 640 epoch 134 head B head_i_epoch 1 batch 0: avg loss -1.366904 avg loss no lamb -1.366904 time 2019-02-17 16:42:56.692186
Model ind 640 epoch 134 head B head_i_epoch 1 batch 100: avg loss -1.344678 avg loss no lamb -1.344678 time 2019-02-17 16:45:18.118443
Model ind 640 epoch 134 head B head_i_epoch 1 batch 200: avg loss -1.544046 avg loss no lamb -1.544046 time 2019-02-17 16:47:39.619463
last batch sz 160
Pre: time 2019-02-17 16:49:46.738942: 
 	std: 0.05528161
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.45693332, 0.5693833, 0.57011664, 0.5694, 0.45665]
	train_accs: [0.45693332, 0.5693833, 0.57011664, 0.5694, 0.45665]
	best_train_sub_head: 2
	worst: 0.45665
	avg: 0.5244967
	best: 0.57011664

Starting e_i: 135
Model ind 640 epoch 135 head A head_i_epoch 0 batch 0: avg loss -2.691154 avg loss no lamb -2.691154 time 2019-02-17 16:49:48.654018
Model ind 640 epoch 135 head A head_i_epoch 0 batch 100: avg loss -2.660659 avg loss no lamb -2.660659 time 2019-02-17 16:52:11.019431
Model ind 640 epoch 135 head A head_i_epoch 0 batch 200: avg loss -2.768924 avg loss no lamb -2.768924 time 2019-02-17 16:54:33.367611
last batch sz 160
Model ind 640 epoch 135 head B head_i_epoch 0 batch 0: avg loss -1.369449 avg loss no lamb -1.369449 time 2019-02-17 16:56:17.282159
Model ind 640 epoch 135 head B head_i_epoch 0 batch 100: avg loss -1.291327 avg loss no lamb -1.291327 time 2019-02-17 16:58:40.547125
Model ind 640 epoch 135 head B head_i_epoch 0 batch 200: avg loss -1.517156 avg loss no lamb -1.517156 time 2019-02-17 17:01:04.010397
last batch sz 160
Model ind 640 epoch 135 head B head_i_epoch 1 batch 0: avg loss -1.436323 avg loss no lamb -1.436323 time 2019-02-17 17:02:47.457906
Model ind 640 epoch 135 head B head_i_epoch 1 batch 100: avg loss -1.443522 avg loss no lamb -1.443522 time 2019-02-17 17:05:09.254546
Model ind 640 epoch 135 head B head_i_epoch 1 batch 200: avg loss -1.442820 avg loss no lamb -1.442820 time 2019-02-17 17:07:31.346565
last batch sz 160
Pre: time 2019-02-17 17:09:38.892035: 
 	std: 0.054396268
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.43678334, 0.54786664, 0.5491, 0.548, 0.4378]
	train_accs: [0.43678334, 0.54786664, 0.5491, 0.548, 0.4378]
	best_train_sub_head: 2
	worst: 0.43678334
	avg: 0.50390995
	best: 0.5491

Starting e_i: 136
Model ind 640 epoch 136 head A head_i_epoch 0 batch 0: avg loss -2.701642 avg loss no lamb -2.701642 time 2019-02-17 17:09:40.917640
Model ind 640 epoch 136 head A head_i_epoch 0 batch 100: avg loss -2.737387 avg loss no lamb -2.737387 time 2019-02-17 17:12:03.025860
Model ind 640 epoch 136 head A head_i_epoch 0 batch 200: avg loss -2.755135 avg loss no lamb -2.755135 time 2019-02-17 17:14:25.472121
last batch sz 160
Model ind 640 epoch 136 head B head_i_epoch 0 batch 0: avg loss -1.442276 avg loss no lamb -1.442276 time 2019-02-17 17:16:09.969525
Model ind 640 epoch 136 head B head_i_epoch 0 batch 100: avg loss -1.370521 avg loss no lamb -1.370521 time 2019-02-17 17:18:32.741808
Model ind 640 epoch 136 head B head_i_epoch 0 batch 200: avg loss -1.494450 avg loss no lamb -1.494450 time 2019-02-17 17:20:55.932724
last batch sz 160
Model ind 640 epoch 136 head B head_i_epoch 1 batch 0: avg loss -1.392672 avg loss no lamb -1.392672 time 2019-02-17 17:22:40.057078
Model ind 640 epoch 136 head B head_i_epoch 1 batch 100: avg loss -1.431687 avg loss no lamb -1.431687 time 2019-02-17 17:25:03.012724
Model ind 640 epoch 136 head B head_i_epoch 1 batch 200: avg loss -1.434547 avg loss no lamb -1.434547 time 2019-02-17 17:27:26.148320
last batch sz 160
Pre: time 2019-02-17 17:29:34.323558: 
 	std: 0.05494287
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44741666, 0.55971664, 0.56035, 0.56055, 0.4487]
	train_accs: [0.44741666, 0.55971664, 0.56035, 0.56055, 0.4487]
	best_train_sub_head: 3
	worst: 0.44741666
	avg: 0.51534665
	best: 0.56055

Starting e_i: 137
Model ind 640 epoch 137 head A head_i_epoch 0 batch 0: avg loss -2.745871 avg loss no lamb -2.745871 time 2019-02-17 17:29:36.498251
Model ind 640 epoch 137 head A head_i_epoch 0 batch 100: avg loss -2.684613 avg loss no lamb -2.684613 time 2019-02-17 17:31:59.753800
Model ind 640 epoch 137 head A head_i_epoch 0 batch 200: avg loss -2.749648 avg loss no lamb -2.749648 time 2019-02-17 17:34:22.956138
last batch sz 160
Model ind 640 epoch 137 head B head_i_epoch 0 batch 0: avg loss -1.429911 avg loss no lamb -1.429911 time 2019-02-17 17:36:06.786079
Model ind 640 epoch 137 head B head_i_epoch 0 batch 100: avg loss -1.394837 avg loss no lamb -1.394837 time 2019-02-17 17:38:28.145181
Model ind 640 epoch 137 head B head_i_epoch 0 batch 200: avg loss -1.407999 avg loss no lamb -1.407999 time 2019-02-17 17:40:49.892892
last batch sz 160
Model ind 640 epoch 137 head B head_i_epoch 1 batch 0: avg loss -1.424980 avg loss no lamb -1.424980 time 2019-02-17 17:42:33.220626
Model ind 640 epoch 137 head B head_i_epoch 1 batch 100: avg loss -1.336626 avg loss no lamb -1.336626 time 2019-02-17 17:44:54.841993
Model ind 640 epoch 137 head B head_i_epoch 1 batch 200: avg loss -1.436313 avg loss no lamb -1.436313 time 2019-02-17 17:47:16.414385
last batch sz 160
Pre: time 2019-02-17 17:49:23.435274: 
 	std: 0.056268364
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.44305, 0.5583, 0.5584667, 0.55938333, 0.44468334]
	train_accs: [0.44305, 0.5583, 0.5584667, 0.55938333, 0.44468334]
	best_train_sub_head: 3
	worst: 0.44305
	avg: 0.5127767
	best: 0.55938333

Starting e_i: 138
Model ind 640 epoch 138 head A head_i_epoch 0 batch 0: avg loss -2.737584 avg loss no lamb -2.737584 time 2019-02-17 17:49:25.342310
Model ind 640 epoch 138 head A head_i_epoch 0 batch 100: avg loss -2.767886 avg loss no lamb -2.767886 time 2019-02-17 17:51:48.200931
Model ind 640 epoch 138 head A head_i_epoch 0 batch 200: avg loss -2.717438 avg loss no lamb -2.717438 time 2019-02-17 17:54:10.551200
last batch sz 160
Model ind 640 epoch 138 head B head_i_epoch 0 batch 0: avg loss -1.455490 avg loss no lamb -1.455490 time 2019-02-17 17:55:53.825864
Model ind 640 epoch 138 head B head_i_epoch 0 batch 100: avg loss -1.419567 avg loss no lamb -1.419567 time 2019-02-17 17:58:17.199162
Model ind 640 epoch 138 head B head_i_epoch 0 batch 200: avg loss -1.481255 avg loss no lamb -1.481255 time 2019-02-17 18:00:40.567024
last batch sz 160
Model ind 640 epoch 138 head B head_i_epoch 1 batch 0: avg loss -1.419173 avg loss no lamb -1.419173 time 2019-02-17 18:02:24.722285
Model ind 640 epoch 138 head B head_i_epoch 1 batch 100: avg loss -1.434643 avg loss no lamb -1.434643 time 2019-02-17 18:04:46.069613
Model ind 640 epoch 138 head B head_i_epoch 1 batch 200: avg loss -1.514442 avg loss no lamb -1.514442 time 2019-02-17 18:07:08.643970
last batch sz 160
Pre: time 2019-02-17 18:09:16.653102: 
 	std: 0.053024493
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45466667, 0.5642, 0.56418335, 0.56376666, 0.45698333]
	train_accs: [0.45466667, 0.5642, 0.56418335, 0.56376666, 0.45698333]
	best_train_sub_head: 1
	worst: 0.45466667
	avg: 0.52076
	best: 0.5642

Starting e_i: 139
Model ind 640 epoch 139 head A head_i_epoch 0 batch 0: avg loss -2.667857 avg loss no lamb -2.667857 time 2019-02-17 18:09:18.728060
Model ind 640 epoch 139 head A head_i_epoch 0 batch 100: avg loss -2.640503 avg loss no lamb -2.640503 time 2019-02-17 18:11:42.155529
Model ind 640 epoch 139 head A head_i_epoch 0 batch 200: avg loss -2.769968 avg loss no lamb -2.769968 time 2019-02-17 18:14:05.543751
last batch sz 160
Model ind 640 epoch 139 head B head_i_epoch 0 batch 0: avg loss -1.482656 avg loss no lamb -1.482656 time 2019-02-17 18:15:49.174397
Model ind 640 epoch 139 head B head_i_epoch 0 batch 100: avg loss -1.329707 avg loss no lamb -1.329707 time 2019-02-17 18:18:11.179560
Model ind 640 epoch 139 head B head_i_epoch 0 batch 200: avg loss -1.553267 avg loss no lamb -1.553267 time 2019-02-17 18:20:33.182602
last batch sz 160
Model ind 640 epoch 139 head B head_i_epoch 1 batch 0: avg loss -1.479506 avg loss no lamb -1.479506 time 2019-02-17 18:22:16.735707
Model ind 640 epoch 139 head B head_i_epoch 1 batch 100: avg loss -1.394689 avg loss no lamb -1.394689 time 2019-02-17 18:24:39.764528
Model ind 640 epoch 139 head B head_i_epoch 1 batch 200: avg loss -1.612030 avg loss no lamb -1.612030 time 2019-02-17 18:27:02.018492
last batch sz 160
Pre: time 2019-02-17 18:29:09.202965: 
 	std: 0.056374073
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46518335, 0.5803, 0.5800833, 0.58073336, 0.46541667]
	train_accs: [0.46518335, 0.5803, 0.5800833, 0.58073336, 0.46541667]
	best_train_sub_head: 3
	worst: 0.46518335
	avg: 0.53434336
	best: 0.58073336

Starting e_i: 140
Model ind 640 epoch 140 head A head_i_epoch 0 batch 0: avg loss -2.740529 avg loss no lamb -2.740529 time 2019-02-17 18:29:15.645178
Model ind 640 epoch 140 head A head_i_epoch 0 batch 100: avg loss -2.722540 avg loss no lamb -2.722540 time 2019-02-17 18:31:38.914171
Model ind 640 epoch 140 head A head_i_epoch 0 batch 200: avg loss -2.701946 avg loss no lamb -2.701946 time 2019-02-17 18:34:00.706872
last batch sz 160
Model ind 640 epoch 140 head B head_i_epoch 0 batch 0: avg loss -1.385683 avg loss no lamb -1.385683 time 2019-02-17 18:35:43.748407
Model ind 640 epoch 140 head B head_i_epoch 0 batch 100: avg loss -1.401052 avg loss no lamb -1.401052 time 2019-02-17 18:38:06.202446
Model ind 640 epoch 140 head B head_i_epoch 0 batch 200: avg loss -1.606954 avg loss no lamb -1.606954 time 2019-02-17 18:40:28.957969
last batch sz 160
Model ind 640 epoch 140 head B head_i_epoch 1 batch 0: avg loss -1.454735 avg loss no lamb -1.454735 time 2019-02-17 18:42:11.819880
Model ind 640 epoch 140 head B head_i_epoch 1 batch 100: avg loss -1.419770 avg loss no lamb -1.419770 time 2019-02-17 18:44:34.522065
Model ind 640 epoch 140 head B head_i_epoch 1 batch 200: avg loss -1.567806 avg loss no lamb -1.567806 time 2019-02-17 18:46:57.677935
last batch sz 160
Pre: time 2019-02-17 18:49:06.086506: 
 	std: 0.053843297
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46111667, 0.5722, 0.5721167, 0.57283336, 0.46386668]
	train_accs: [0.46111667, 0.5722, 0.5721167, 0.57283336, 0.46386668]
	best_train_sub_head: 3
	worst: 0.46111667
	avg: 0.52842665
	best: 0.57283336

Starting e_i: 141
Model ind 640 epoch 141 head A head_i_epoch 0 batch 0: avg loss -2.697174 avg loss no lamb -2.697174 time 2019-02-17 18:49:13.930845
Model ind 640 epoch 141 head A head_i_epoch 0 batch 100: avg loss -2.662097 avg loss no lamb -2.662097 time 2019-02-17 18:51:37.185544
Model ind 640 epoch 141 head A head_i_epoch 0 batch 200: avg loss -2.703105 avg loss no lamb -2.703105 time 2019-02-17 18:54:00.575261
last batch sz 160
Model ind 640 epoch 141 head B head_i_epoch 0 batch 0: avg loss -1.392563 avg loss no lamb -1.392563 time 2019-02-17 18:55:44.085213
Model ind 640 epoch 141 head B head_i_epoch 0 batch 100: avg loss -1.415889 avg loss no lamb -1.415889 time 2019-02-17 18:58:07.209261
Model ind 640 epoch 141 head B head_i_epoch 0 batch 200: avg loss -1.530741 avg loss no lamb -1.530741 time 2019-02-17 19:00:30.161202
last batch sz 160
Model ind 640 epoch 141 head B head_i_epoch 1 batch 0: avg loss -1.397158 avg loss no lamb -1.397158 time 2019-02-17 19:02:13.913357
Model ind 640 epoch 141 head B head_i_epoch 1 batch 100: avg loss -1.391353 avg loss no lamb -1.391353 time 2019-02-17 19:04:35.649345
Model ind 640 epoch 141 head B head_i_epoch 1 batch 200: avg loss -1.501097 avg loss no lamb -1.501097 time 2019-02-17 19:06:57.188075
last batch sz 160
Pre: time 2019-02-17 19:09:04.543385: 
 	std: 0.055836506
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45916668, 0.5735, 0.57303333, 0.5728667, 0.45915]
	train_accs: [0.45916668, 0.5735, 0.57303333, 0.5728667, 0.45915]
	best_train_sub_head: 1
	worst: 0.45915
	avg: 0.52754337
	best: 0.5735

Starting e_i: 142
Model ind 640 epoch 142 head A head_i_epoch 0 batch 0: avg loss -2.733159 avg loss no lamb -2.733159 time 2019-02-17 19:09:06.541029
Model ind 640 epoch 142 head A head_i_epoch 0 batch 100: avg loss -2.690606 avg loss no lamb -2.690606 time 2019-02-17 19:11:28.930836
Model ind 640 epoch 142 head A head_i_epoch 0 batch 200: avg loss -2.689882 avg loss no lamb -2.689882 time 2019-02-17 19:13:52.343575
last batch sz 160
Model ind 640 epoch 142 head B head_i_epoch 0 batch 0: avg loss -1.348989 avg loss no lamb -1.348989 time 2019-02-17 19:15:36.583574
Model ind 640 epoch 142 head B head_i_epoch 0 batch 100: avg loss -1.351059 avg loss no lamb -1.351059 time 2019-02-17 19:17:59.880494
Model ind 640 epoch 142 head B head_i_epoch 0 batch 200: avg loss -1.423517 avg loss no lamb -1.423517 time 2019-02-17 19:20:23.130518
last batch sz 160
Model ind 640 epoch 142 head B head_i_epoch 1 batch 0: avg loss -1.469957 avg loss no lamb -1.469957 time 2019-02-17 19:22:07.209272
Model ind 640 epoch 142 head B head_i_epoch 1 batch 100: avg loss -1.368036 avg loss no lamb -1.368036 time 2019-02-17 19:24:30.876652
Model ind 640 epoch 142 head B head_i_epoch 1 batch 200: avg loss -1.512053 avg loss no lamb -1.512053 time 2019-02-17 19:26:53.998570
last batch sz 160
Pre: time 2019-02-17 19:29:01.979731: 
 	std: 0.053970236
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4545, 0.56478333, 0.56515, 0.5654333, 0.45541668]
	train_accs: [0.4545, 0.56478333, 0.56515, 0.5654333, 0.45541668]
	best_train_sub_head: 3
	worst: 0.4545
	avg: 0.52105665
	best: 0.5654333

Starting e_i: 143
Model ind 640 epoch 143 head A head_i_epoch 0 batch 0: avg loss -2.646528 avg loss no lamb -2.646528 time 2019-02-17 19:29:04.040598
Model ind 640 epoch 143 head A head_i_epoch 0 batch 100: avg loss -2.703615 avg loss no lamb -2.703615 time 2019-02-17 19:31:26.275245
Model ind 640 epoch 143 head A head_i_epoch 0 batch 200: avg loss -2.750842 avg loss no lamb -2.750842 time 2019-02-17 19:33:48.542358
last batch sz 160
Model ind 640 epoch 143 head B head_i_epoch 0 batch 0: avg loss -1.377606 avg loss no lamb -1.377606 time 2019-02-17 19:35:32.411490
Model ind 640 epoch 143 head B head_i_epoch 0 batch 100: avg loss -1.401480 avg loss no lamb -1.401480 time 2019-02-17 19:37:55.070331
Model ind 640 epoch 143 head B head_i_epoch 0 batch 200: avg loss -1.470720 avg loss no lamb -1.470720 time 2019-02-17 19:40:16.917416
last batch sz 160
Model ind 640 epoch 143 head B head_i_epoch 1 batch 0: avg loss -1.490786 avg loss no lamb -1.490786 time 2019-02-17 19:42:00.293717
Model ind 640 epoch 143 head B head_i_epoch 1 batch 100: avg loss -1.381409 avg loss no lamb -1.381409 time 2019-02-17 19:44:22.246035
Model ind 640 epoch 143 head B head_i_epoch 1 batch 200: avg loss -1.488465 avg loss no lamb -1.488465 time 2019-02-17 19:46:43.948996
last batch sz 160
Pre: time 2019-02-17 19:48:51.695294: 
 	std: 0.052285437
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.45873332, 0.56766665, 0.5678333, 0.56763333, 0.46331668]
	train_accs: [0.45873332, 0.56766665, 0.5678333, 0.56763333, 0.46331668]
	best_train_sub_head: 2
	worst: 0.45873332
	avg: 0.5250367
	best: 0.5678333

Starting e_i: 144
Model ind 640 epoch 144 head A head_i_epoch 0 batch 0: avg loss -2.740115 avg loss no lamb -2.740115 time 2019-02-17 19:48:53.586783
Model ind 640 epoch 144 head A head_i_epoch 0 batch 100: avg loss -2.680287 avg loss no lamb -2.680287 time 2019-02-17 19:51:15.583587
Model ind 640 epoch 144 head A head_i_epoch 0 batch 200: avg loss -2.695170 avg loss no lamb -2.695170 time 2019-02-17 19:53:37.632030
last batch sz 160
Model ind 640 epoch 144 head B head_i_epoch 0 batch 0: avg loss -1.476882 avg loss no lamb -1.476882 time 2019-02-17 19:55:21.434846
Model ind 640 epoch 144 head B head_i_epoch 0 batch 100: avg loss -1.398895 avg loss no lamb -1.398895 time 2019-02-17 19:57:44.262082
Model ind 640 epoch 144 head B head_i_epoch 0 batch 200: avg loss -1.497083 avg loss no lamb -1.497083 time 2019-02-17 20:00:07.227356
last batch sz 160
Model ind 640 epoch 144 head B head_i_epoch 1 batch 0: avg loss -1.386874 avg loss no lamb -1.386874 time 2019-02-17 20:01:51.318929
Model ind 640 epoch 144 head B head_i_epoch 1 batch 100: avg loss -1.383259 avg loss no lamb -1.383259 time 2019-02-17 20:04:13.711178
Model ind 640 epoch 144 head B head_i_epoch 1 batch 200: avg loss -1.553895 avg loss no lamb -1.553895 time 2019-02-17 20:06:35.381535
last batch sz 160
Pre: time 2019-02-17 20:08:42.525658: 
 	std: 0.054136124
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45983332, 0.5708167, 0.57225, 0.57271665, 0.46306667]
	train_accs: [0.45983332, 0.5708167, 0.57225, 0.57271665, 0.46306667]
	best_train_sub_head: 3
	worst: 0.45983332
	avg: 0.52773666
	best: 0.57271665

Starting e_i: 145
Model ind 640 epoch 145 head A head_i_epoch 0 batch 0: avg loss -2.667724 avg loss no lamb -2.667724 time 2019-02-17 20:08:44.534183
Model ind 640 epoch 145 head A head_i_epoch 0 batch 100: avg loss -2.685730 avg loss no lamb -2.685730 time 2019-02-17 20:11:06.335070
Model ind 640 epoch 145 head A head_i_epoch 0 batch 200: avg loss -2.761277 avg loss no lamb -2.761277 time 2019-02-17 20:13:27.838110
last batch sz 160
Model ind 640 epoch 145 head B head_i_epoch 0 batch 0: avg loss -1.419709 avg loss no lamb -1.419709 time 2019-02-17 20:15:10.761907
Model ind 640 epoch 145 head B head_i_epoch 0 batch 100: avg loss -1.340856 avg loss no lamb -1.340856 time 2019-02-17 20:17:32.904865
Model ind 640 epoch 145 head B head_i_epoch 0 batch 200: avg loss -1.649910 avg loss no lamb -1.649910 time 2019-02-17 20:19:55.513549
last batch sz 160
Model ind 640 epoch 145 head B head_i_epoch 1 batch 0: avg loss -1.417526 avg loss no lamb -1.417526 time 2019-02-17 20:21:39.511114
Model ind 640 epoch 145 head B head_i_epoch 1 batch 100: avg loss -1.326392 avg loss no lamb -1.326392 time 2019-02-17 20:24:02.893274
Model ind 640 epoch 145 head B head_i_epoch 1 batch 200: avg loss -1.543427 avg loss no lamb -1.543427 time 2019-02-17 20:26:25.562361
last batch sz 160
Pre: time 2019-02-17 20:28:33.517089: 
 	std: 0.054529544
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.45368335, 0.56535, 0.5658, 0.5653167, 0.45468333]
	train_accs: [0.45368335, 0.56535, 0.5658, 0.5653167, 0.45468333]
	best_train_sub_head: 2
	worst: 0.45368335
	avg: 0.52096665
	best: 0.5658

Starting e_i: 146
Model ind 640 epoch 146 head A head_i_epoch 0 batch 0: avg loss -2.717858 avg loss no lamb -2.717858 time 2019-02-17 20:28:35.543727
Model ind 640 epoch 146 head A head_i_epoch 0 batch 100: avg loss -2.643899 avg loss no lamb -2.643899 time 2019-02-17 20:30:58.489564
Model ind 640 epoch 146 head A head_i_epoch 0 batch 200: avg loss -2.745212 avg loss no lamb -2.745212 time 2019-02-17 20:33:20.739371
last batch sz 160
Model ind 640 epoch 146 head B head_i_epoch 0 batch 0: avg loss -1.386337 avg loss no lamb -1.386337 time 2019-02-17 20:35:04.084524
Model ind 640 epoch 146 head B head_i_epoch 0 batch 100: avg loss -1.394698 avg loss no lamb -1.394698 time 2019-02-17 20:37:26.050639
Model ind 640 epoch 146 head B head_i_epoch 0 batch 200: avg loss -1.505515 avg loss no lamb -1.505515 time 2019-02-17 20:39:47.911065
last batch sz 160
Model ind 640 epoch 146 head B head_i_epoch 1 batch 0: avg loss -1.498167 avg loss no lamb -1.498167 time 2019-02-17 20:41:31.189234
Model ind 640 epoch 146 head B head_i_epoch 1 batch 100: avg loss -1.370267 avg loss no lamb -1.370267 time 2019-02-17 20:43:53.122988
Model ind 640 epoch 146 head B head_i_epoch 1 batch 200: avg loss -1.504531 avg loss no lamb -1.504531 time 2019-02-17 20:46:15.245919
last batch sz 160
Pre: time 2019-02-17 20:48:22.827171: 
 	std: 0.05335386
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.45891666, 0.5672333, 0.568, 0.56885, 0.45933333]
	train_accs: [0.45891666, 0.5672333, 0.568, 0.56885, 0.45933333]
	best_train_sub_head: 3
	worst: 0.45891666
	avg: 0.5244667
	best: 0.56885

Starting e_i: 147
Model ind 640 epoch 147 head A head_i_epoch 0 batch 0: avg loss -2.754848 avg loss no lamb -2.754848 time 2019-02-17 20:48:24.708002
Model ind 640 epoch 147 head A head_i_epoch 0 batch 100: avg loss -2.718283 avg loss no lamb -2.718283 time 2019-02-17 20:50:47.559306
Model ind 640 epoch 147 head A head_i_epoch 0 batch 200: avg loss -2.719141 avg loss no lamb -2.719141 time 2019-02-17 20:53:10.517480
last batch sz 160
Model ind 640 epoch 147 head B head_i_epoch 0 batch 0: avg loss -1.437813 avg loss no lamb -1.437813 time 2019-02-17 20:54:54.753019
Model ind 640 epoch 147 head B head_i_epoch 0 batch 100: avg loss -1.343057 avg loss no lamb -1.343057 time 2019-02-17 20:57:17.851134
Model ind 640 epoch 147 head B head_i_epoch 0 batch 200: avg loss -1.471371 avg loss no lamb -1.471371 time 2019-02-17 20:59:40.287542
last batch sz 160
Model ind 640 epoch 147 head B head_i_epoch 1 batch 0: avg loss -1.415117 avg loss no lamb -1.415117 time 2019-02-17 21:01:24.458566
Model ind 640 epoch 147 head B head_i_epoch 1 batch 100: avg loss -1.385940 avg loss no lamb -1.385940 time 2019-02-17 21:03:47.771290
Model ind 640 epoch 147 head B head_i_epoch 1 batch 200: avg loss -1.511002 avg loss no lamb -1.511002 time 2019-02-17 21:06:10.905418
last batch sz 160
Pre: time 2019-02-17 21:08:19.892797: 
 	std: 0.05361024
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4595, 0.5682, 0.56838334, 0.56986666, 0.45928332]
	train_accs: [0.4595, 0.5682, 0.56838334, 0.56986666, 0.45928332]
	best_train_sub_head: 3
	worst: 0.45928332
	avg: 0.52504665
	best: 0.56986666

Starting e_i: 148
Model ind 640 epoch 148 head A head_i_epoch 0 batch 0: avg loss -2.744967 avg loss no lamb -2.744967 time 2019-02-17 21:08:21.894410
Model ind 640 epoch 148 head A head_i_epoch 0 batch 100: avg loss -2.708064 avg loss no lamb -2.708064 time 2019-02-17 21:10:45.034944
Model ind 640 epoch 148 head A head_i_epoch 0 batch 200: avg loss -2.668158 avg loss no lamb -2.668158 time 2019-02-17 21:13:07.727455
last batch sz 160
Model ind 640 epoch 148 head B head_i_epoch 0 batch 0: avg loss -1.356555 avg loss no lamb -1.356555 time 2019-02-17 21:14:51.366983
Model ind 640 epoch 148 head B head_i_epoch 0 batch 100: avg loss -1.322769 avg loss no lamb -1.322769 time 2019-02-17 21:17:14.866407
Model ind 640 epoch 148 head B head_i_epoch 0 batch 200: avg loss -1.463722 avg loss no lamb -1.463722 time 2019-02-17 21:19:37.845617
last batch sz 160
Model ind 640 epoch 148 head B head_i_epoch 1 batch 0: avg loss -1.442152 avg loss no lamb -1.442152 time 2019-02-17 21:21:22.060295
Model ind 640 epoch 148 head B head_i_epoch 1 batch 100: avg loss -1.352246 avg loss no lamb -1.352246 time 2019-02-17 21:23:46.074552
Model ind 640 epoch 148 head B head_i_epoch 1 batch 200: avg loss -1.466906 avg loss no lamb -1.466906 time 2019-02-17 21:26:09.464312
last batch sz 160
Pre: time 2019-02-17 21:28:17.817848: 
 	std: 0.054528765
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46476668, 0.5758333, 0.57615, 0.5764833, 0.46493334]
	train_accs: [0.46476668, 0.5758333, 0.57615, 0.5764833, 0.46493334]
	best_train_sub_head: 3
	worst: 0.46476668
	avg: 0.5316333
	best: 0.5764833

Starting e_i: 149
Model ind 640 epoch 149 head A head_i_epoch 0 batch 0: avg loss -2.746409 avg loss no lamb -2.746409 time 2019-02-17 21:28:19.850684
Model ind 640 epoch 149 head A head_i_epoch 0 batch 100: avg loss -2.678738 avg loss no lamb -2.678738 time 2019-02-17 21:30:42.112680
Model ind 640 epoch 149 head A head_i_epoch 0 batch 200: avg loss -2.714726 avg loss no lamb -2.714726 time 2019-02-17 21:33:04.585579
last batch sz 160
Model ind 640 epoch 149 head B head_i_epoch 0 batch 0: avg loss -1.436722 avg loss no lamb -1.436722 time 2019-02-17 21:34:48.102971
Model ind 640 epoch 149 head B head_i_epoch 0 batch 100: avg loss -1.398535 avg loss no lamb -1.398535 time 2019-02-17 21:37:09.833116
Model ind 640 epoch 149 head B head_i_epoch 0 batch 200: avg loss -1.498293 avg loss no lamb -1.498293 time 2019-02-17 21:39:31.627562
last batch sz 160
Model ind 640 epoch 149 head B head_i_epoch 1 batch 0: avg loss -1.351863 avg loss no lamb -1.351863 time 2019-02-17 21:41:15.466812
Model ind 640 epoch 149 head B head_i_epoch 1 batch 100: avg loss -1.367542 avg loss no lamb -1.367542 time 2019-02-17 21:43:38.122593
Model ind 640 epoch 149 head B head_i_epoch 1 batch 200: avg loss -1.580750 avg loss no lamb -1.580750 time 2019-02-17 21:46:00.497998
last batch sz 160
Pre: time 2019-02-17 21:48:08.581472: 
 	std: 0.054471973
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46321666, 0.5737333, 0.57441664, 0.57416666, 0.46261665]
	train_accs: [0.46321666, 0.5737333, 0.57441664, 0.57416666, 0.46261665]
	best_train_sub_head: 2
	worst: 0.46261665
	avg: 0.52963
	best: 0.57441664

Starting e_i: 150
Model ind 640 epoch 150 head A head_i_epoch 0 batch 0: avg loss -2.787968 avg loss no lamb -2.787968 time 2019-02-17 21:48:10.733786
Model ind 640 epoch 150 head A head_i_epoch 0 batch 100: avg loss -2.732197 avg loss no lamb -2.732197 time 2019-02-17 21:50:34.894870
Model ind 640 epoch 150 head A head_i_epoch 0 batch 200: avg loss -2.682431 avg loss no lamb -2.682431 time 2019-02-17 21:52:58.422776
last batch sz 160
Model ind 640 epoch 150 head B head_i_epoch 0 batch 0: avg loss -1.468390 avg loss no lamb -1.468390 time 2019-02-17 21:54:42.368162
Model ind 640 epoch 150 head B head_i_epoch 0 batch 100: avg loss -1.414760 avg loss no lamb -1.414760 time 2019-02-17 21:57:04.323188
Model ind 640 epoch 150 head B head_i_epoch 0 batch 200: avg loss -1.520190 avg loss no lamb -1.520190 time 2019-02-17 21:59:26.314642
last batch sz 160
Model ind 640 epoch 150 head B head_i_epoch 1 batch 0: avg loss -1.476415 avg loss no lamb -1.476415 time 2019-02-17 22:01:10.593943
Model ind 640 epoch 150 head B head_i_epoch 1 batch 100: avg loss -1.426878 avg loss no lamb -1.426878 time 2019-02-17 22:03:34.529751
Model ind 640 epoch 150 head B head_i_epoch 1 batch 200: avg loss -1.423862 avg loss no lamb -1.423862 time 2019-02-17 22:05:58.353274
last batch sz 160
Pre: time 2019-02-17 22:08:07.656727: 
 	std: 0.051635385
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48256665, 0.5876833, 0.5879833, 0.58788335, 0.48233333]
	train_accs: [0.48256665, 0.5876833, 0.5879833, 0.58788335, 0.48233333]
	best_train_sub_head: 2
	worst: 0.48233333
	avg: 0.54569
	best: 0.5879833

Starting e_i: 151
Model ind 640 epoch 151 head A head_i_epoch 0 batch 0: avg loss -2.714193 avg loss no lamb -2.714193 time 2019-02-17 22:08:23.513794
Model ind 640 epoch 151 head A head_i_epoch 0 batch 100: avg loss -2.694265 avg loss no lamb -2.694265 time 2019-02-17 22:10:47.166940
Model ind 640 epoch 151 head A head_i_epoch 0 batch 200: avg loss -2.777749 avg loss no lamb -2.777749 time 2019-02-17 22:13:09.611981
last batch sz 160
Model ind 640 epoch 151 head B head_i_epoch 0 batch 0: avg loss -1.407829 avg loss no lamb -1.407829 time 2019-02-17 22:14:52.985474
Model ind 640 epoch 151 head B head_i_epoch 0 batch 100: avg loss -1.340581 avg loss no lamb -1.340581 time 2019-02-17 22:17:16.019829
Model ind 640 epoch 151 head B head_i_epoch 0 batch 200: avg loss -1.551334 avg loss no lamb -1.551334 time 2019-02-17 22:19:38.438970
last batch sz 160
Model ind 640 epoch 151 head B head_i_epoch 1 batch 0: avg loss -1.496003 avg loss no lamb -1.496003 time 2019-02-17 22:21:21.756668
Model ind 640 epoch 151 head B head_i_epoch 1 batch 100: avg loss -1.423570 avg loss no lamb -1.423570 time 2019-02-17 22:23:45.019895
Model ind 640 epoch 151 head B head_i_epoch 1 batch 200: avg loss -1.510504 avg loss no lamb -1.510504 time 2019-02-17 22:26:08.035103
last batch sz 160
Pre: time 2019-02-17 22:28:16.400554: 
 	std: 0.05460217
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45885, 0.5693167, 0.5685667, 0.5689667, 0.45616665]
	train_accs: [0.45885, 0.5693167, 0.5685667, 0.5689667, 0.45616665]
	best_train_sub_head: 1
	worst: 0.45616665
	avg: 0.52437335
	best: 0.5693167

Starting e_i: 152
Model ind 640 epoch 152 head A head_i_epoch 0 batch 0: avg loss -2.754900 avg loss no lamb -2.754900 time 2019-02-17 22:28:18.498329
Model ind 640 epoch 152 head A head_i_epoch 0 batch 100: avg loss -2.593027 avg loss no lamb -2.593027 time 2019-02-17 22:30:41.952669
Model ind 640 epoch 152 head A head_i_epoch 0 batch 200: avg loss -2.743993 avg loss no lamb -2.743993 time 2019-02-17 22:33:04.981451
last batch sz 160
Model ind 640 epoch 152 head B head_i_epoch 0 batch 0: avg loss -1.493102 avg loss no lamb -1.493102 time 2019-02-17 22:34:49.057015
Model ind 640 epoch 152 head B head_i_epoch 0 batch 100: avg loss -1.318330 avg loss no lamb -1.318330 time 2019-02-17 22:37:11.816375
Model ind 640 epoch 152 head B head_i_epoch 0 batch 200: avg loss -1.543653 avg loss no lamb -1.543653 time 2019-02-17 22:39:34.433024
last batch sz 160
Model ind 640 epoch 152 head B head_i_epoch 1 batch 0: avg loss -1.550347 avg loss no lamb -1.550347 time 2019-02-17 22:41:18.580366
Model ind 640 epoch 152 head B head_i_epoch 1 batch 100: avg loss -1.340488 avg loss no lamb -1.340488 time 2019-02-17 22:43:41.226795
Model ind 640 epoch 152 head B head_i_epoch 1 batch 200: avg loss -1.561433 avg loss no lamb -1.561433 time 2019-02-17 22:46:04.115517
last batch sz 160
Pre: time 2019-02-17 22:48:12.428746: 
 	std: 0.0532567
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46976668, 0.57883334, 0.57876664, 0.57905, 0.47058332]
	train_accs: [0.46976668, 0.57883334, 0.57876664, 0.57905, 0.47058332]
	best_train_sub_head: 3
	worst: 0.46976668
	avg: 0.53540003
	best: 0.57905

Starting e_i: 153
Model ind 640 epoch 153 head A head_i_epoch 0 batch 0: avg loss -2.655777 avg loss no lamb -2.655777 time 2019-02-17 22:48:14.643701
Model ind 640 epoch 153 head A head_i_epoch 0 batch 100: avg loss -2.674962 avg loss no lamb -2.674962 time 2019-02-17 22:50:37.377837
Model ind 640 epoch 153 head A head_i_epoch 0 batch 200: avg loss -2.754471 avg loss no lamb -2.754471 time 2019-02-17 22:52:59.721689
last batch sz 160
Model ind 640 epoch 153 head B head_i_epoch 0 batch 0: avg loss -1.407143 avg loss no lamb -1.407143 time 2019-02-17 22:54:43.798687
Model ind 640 epoch 153 head B head_i_epoch 0 batch 100: avg loss -1.340650 avg loss no lamb -1.340650 time 2019-02-17 22:57:05.856591
Model ind 640 epoch 153 head B head_i_epoch 0 batch 200: avg loss -1.445247 avg loss no lamb -1.445247 time 2019-02-17 22:59:28.518828
last batch sz 160
Model ind 640 epoch 153 head B head_i_epoch 1 batch 0: avg loss -1.485529 avg loss no lamb -1.485529 time 2019-02-17 23:01:13.274368
Model ind 640 epoch 153 head B head_i_epoch 1 batch 100: avg loss -1.353426 avg loss no lamb -1.353426 time 2019-02-17 23:03:36.798185
Model ind 640 epoch 153 head B head_i_epoch 1 batch 200: avg loss -1.583257 avg loss no lamb -1.583257 time 2019-02-17 23:06:00.099739
last batch sz 160
Pre: time 2019-02-17 23:08:08.529422: 
 	std: 0.055718675
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45833334, 0.5727, 0.5725333, 0.5724667, 0.45933333]
	train_accs: [0.45833334, 0.5727, 0.5725333, 0.5724667, 0.45933333]
	best_train_sub_head: 1
	worst: 0.45833334
	avg: 0.5270733
	best: 0.5727

Starting e_i: 154
Model ind 640 epoch 154 head A head_i_epoch 0 batch 0: avg loss -2.823683 avg loss no lamb -2.823683 time 2019-02-17 23:08:10.665032
Model ind 640 epoch 154 head A head_i_epoch 0 batch 100: avg loss -2.700242 avg loss no lamb -2.700242 time 2019-02-17 23:10:34.336935
Model ind 640 epoch 154 head A head_i_epoch 0 batch 200: avg loss -2.704266 avg loss no lamb -2.704266 time 2019-02-17 23:12:57.697090
last batch sz 160
Model ind 640 epoch 154 head B head_i_epoch 0 batch 0: avg loss -1.495756 avg loss no lamb -1.495756 time 2019-02-17 23:14:42.007346
Model ind 640 epoch 154 head B head_i_epoch 0 batch 100: avg loss -1.376565 avg loss no lamb -1.376565 time 2019-02-17 23:17:05.069075
Model ind 640 epoch 154 head B head_i_epoch 0 batch 200: avg loss -1.541675 avg loss no lamb -1.541675 time 2019-02-17 23:19:28.106891
last batch sz 160
Model ind 640 epoch 154 head B head_i_epoch 1 batch 0: avg loss -1.441585 avg loss no lamb -1.441585 time 2019-02-17 23:21:12.073317
Model ind 640 epoch 154 head B head_i_epoch 1 batch 100: avg loss -1.478965 avg loss no lamb -1.478965 time 2019-02-17 23:23:34.095911
Model ind 640 epoch 154 head B head_i_epoch 1 batch 200: avg loss -1.507253 avg loss no lamb -1.507253 time 2019-02-17 23:25:56.302400
last batch sz 160
Pre: time 2019-02-17 23:28:04.037350: 
 	std: 0.055070266
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46308333, 0.57451665, 0.5739167, 0.5743667, 0.46065]
	train_accs: [0.46308333, 0.57451665, 0.5739167, 0.5743667, 0.46065]
	best_train_sub_head: 1
	worst: 0.46065
	avg: 0.5293067
	best: 0.57451665

Starting e_i: 155
Model ind 640 epoch 155 head A head_i_epoch 0 batch 0: avg loss -2.757060 avg loss no lamb -2.757060 time 2019-02-17 23:28:05.992394
Model ind 640 epoch 155 head A head_i_epoch 0 batch 100: avg loss -2.689508 avg loss no lamb -2.689508 time 2019-02-17 23:30:28.332542
Model ind 640 epoch 155 head A head_i_epoch 0 batch 200: avg loss -2.783983 avg loss no lamb -2.783983 time 2019-02-17 23:32:50.848195
last batch sz 160
Model ind 640 epoch 155 head B head_i_epoch 0 batch 0: avg loss -1.441718 avg loss no lamb -1.441718 time 2019-02-17 23:34:34.472814
Model ind 640 epoch 155 head B head_i_epoch 0 batch 100: avg loss -1.279939 avg loss no lamb -1.279939 time 2019-02-17 23:36:57.999899
Model ind 640 epoch 155 head B head_i_epoch 0 batch 200: avg loss -1.521237 avg loss no lamb -1.521237 time 2019-02-17 23:39:20.684246
last batch sz 160
Model ind 640 epoch 155 head B head_i_epoch 1 batch 0: avg loss -1.453993 avg loss no lamb -1.453993 time 2019-02-17 23:41:04.700550
Model ind 640 epoch 155 head B head_i_epoch 1 batch 100: avg loss -1.369493 avg loss no lamb -1.369493 time 2019-02-17 23:43:28.092979
Model ind 640 epoch 155 head B head_i_epoch 1 batch 200: avg loss -1.566312 avg loss no lamb -1.566312 time 2019-02-17 23:45:51.070123
last batch sz 160
Pre: time 2019-02-17 23:47:58.394686: 
 	std: 0.053010434
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46945, 0.577, 0.57633334, 0.57696664, 0.46768335]
	train_accs: [0.46945, 0.577, 0.57633334, 0.57696664, 0.46768335]
	best_train_sub_head: 1
	worst: 0.46768335
	avg: 0.53348666
	best: 0.577

Starting e_i: 156
Model ind 640 epoch 156 head A head_i_epoch 0 batch 0: avg loss -2.776931 avg loss no lamb -2.776931 time 2019-02-17 23:48:00.944122
Model ind 640 epoch 156 head A head_i_epoch 0 batch 100: avg loss -2.748929 avg loss no lamb -2.748929 time 2019-02-17 23:50:23.621864
Model ind 640 epoch 156 head A head_i_epoch 0 batch 200: avg loss -2.796286 avg loss no lamb -2.796286 time 2019-02-17 23:52:46.753611
last batch sz 160
Model ind 640 epoch 156 head B head_i_epoch 0 batch 0: avg loss -1.369129 avg loss no lamb -1.369129 time 2019-02-17 23:54:30.990989
Model ind 640 epoch 156 head B head_i_epoch 0 batch 100: avg loss -1.324999 avg loss no lamb -1.324999 time 2019-02-17 23:56:54.098218
Model ind 640 epoch 156 head B head_i_epoch 0 batch 200: avg loss -1.471207 avg loss no lamb -1.471207 time 2019-02-17 23:59:16.719758
last batch sz 160
Model ind 640 epoch 156 head B head_i_epoch 1 batch 0: avg loss -1.400801 avg loss no lamb -1.400801 time 2019-02-18 00:01:01.242538
Model ind 640 epoch 156 head B head_i_epoch 1 batch 100: avg loss -1.319593 avg loss no lamb -1.319593 time 2019-02-18 00:03:24.595008
Model ind 640 epoch 156 head B head_i_epoch 1 batch 200: avg loss -1.607795 avg loss no lamb -1.607795 time 2019-02-18 00:05:47.687789
last batch sz 160
Pre: time 2019-02-18 00:07:56.635931: 
 	std: 0.056049764
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46353334, 0.57636666, 0.5781, 0.5779667, 0.46261665]
	train_accs: [0.46353334, 0.57636666, 0.5781, 0.5779667, 0.46261665]
	best_train_sub_head: 2
	worst: 0.46261665
	avg: 0.5317167
	best: 0.5781

Starting e_i: 157
Model ind 640 epoch 157 head A head_i_epoch 0 batch 0: avg loss -2.688761 avg loss no lamb -2.688761 time 2019-02-18 00:07:58.662613
Model ind 640 epoch 157 head A head_i_epoch 0 batch 100: avg loss -2.713306 avg loss no lamb -2.713306 time 2019-02-18 00:10:22.471599
Model ind 640 epoch 157 head A head_i_epoch 0 batch 200: avg loss -2.780996 avg loss no lamb -2.780996 time 2019-02-18 00:12:45.035234
last batch sz 160
Model ind 640 epoch 157 head B head_i_epoch 0 batch 0: avg loss -1.364795 avg loss no lamb -1.364795 time 2019-02-18 00:14:29.175495
Model ind 640 epoch 157 head B head_i_epoch 0 batch 100: avg loss -1.374897 avg loss no lamb -1.374897 time 2019-02-18 00:16:52.217894
Model ind 640 epoch 157 head B head_i_epoch 0 batch 200: avg loss -1.571625 avg loss no lamb -1.571625 time 2019-02-18 00:19:15.696327
last batch sz 160
Model ind 640 epoch 157 head B head_i_epoch 1 batch 0: avg loss -1.370403 avg loss no lamb -1.370403 time 2019-02-18 00:20:59.592798
Model ind 640 epoch 157 head B head_i_epoch 1 batch 100: avg loss -1.364518 avg loss no lamb -1.364518 time 2019-02-18 00:23:21.084723
Model ind 640 epoch 157 head B head_i_epoch 1 batch 200: avg loss -1.507035 avg loss no lamb -1.507035 time 2019-02-18 00:25:42.740066
last batch sz 160
Pre: time 2019-02-18 00:27:50.227775: 
 	std: 0.0542337
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4649, 0.5742667, 0.5744333, 0.5747833, 0.4627]
	train_accs: [0.4649, 0.5742667, 0.5744333, 0.5747833, 0.4627]
	best_train_sub_head: 3
	worst: 0.4627
	avg: 0.5302167
	best: 0.5747833

Starting e_i: 158
Model ind 640 epoch 158 head A head_i_epoch 0 batch 0: avg loss -2.740824 avg loss no lamb -2.740824 time 2019-02-18 00:27:52.626236
Model ind 640 epoch 158 head A head_i_epoch 0 batch 100: avg loss -2.706712 avg loss no lamb -2.706712 time 2019-02-18 00:30:15.955543
Model ind 640 epoch 158 head A head_i_epoch 0 batch 200: avg loss -2.743175 avg loss no lamb -2.743175 time 2019-02-18 00:32:39.371905
last batch sz 160
Model ind 640 epoch 158 head B head_i_epoch 0 batch 0: avg loss -1.465936 avg loss no lamb -1.465936 time 2019-02-18 00:34:22.675765
Model ind 640 epoch 158 head B head_i_epoch 0 batch 100: avg loss -1.415743 avg loss no lamb -1.415743 time 2019-02-18 00:36:44.246897
Model ind 640 epoch 158 head B head_i_epoch 0 batch 200: avg loss -1.565519 avg loss no lamb -1.565519 time 2019-02-18 00:39:06.153986
last batch sz 160
Model ind 640 epoch 158 head B head_i_epoch 1 batch 0: avg loss -1.485208 avg loss no lamb -1.485208 time 2019-02-18 00:40:49.220488
Model ind 640 epoch 158 head B head_i_epoch 1 batch 100: avg loss -1.386443 avg loss no lamb -1.386443 time 2019-02-18 00:43:10.496417
Model ind 640 epoch 158 head B head_i_epoch 1 batch 200: avg loss -1.494149 avg loss no lamb -1.494149 time 2019-02-18 00:45:31.725611
last batch sz 160
Pre: time 2019-02-18 00:47:38.413895: 
 	std: 0.053444378
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46331668, 0.57338333, 0.57338333, 0.5737333, 0.46551666]
	train_accs: [0.46331668, 0.57338333, 0.57338333, 0.5737333, 0.46551666]
	best_train_sub_head: 3
	worst: 0.46331668
	avg: 0.52986664
	best: 0.5737333

Starting e_i: 159
Model ind 640 epoch 159 head A head_i_epoch 0 batch 0: avg loss -2.764666 avg loss no lamb -2.764666 time 2019-02-18 00:47:40.294225
Model ind 640 epoch 159 head A head_i_epoch 0 batch 100: avg loss -2.635435 avg loss no lamb -2.635435 time 2019-02-18 00:50:03.902716
Model ind 640 epoch 159 head A head_i_epoch 0 batch 200: avg loss -2.751494 avg loss no lamb -2.751494 time 2019-02-18 00:52:27.135012
last batch sz 160
Model ind 640 epoch 159 head B head_i_epoch 0 batch 0: avg loss -1.466472 avg loss no lamb -1.466472 time 2019-02-18 00:54:11.171374
Model ind 640 epoch 159 head B head_i_epoch 0 batch 100: avg loss -1.379680 avg loss no lamb -1.379680 time 2019-02-18 00:56:34.195920
Model ind 640 epoch 159 head B head_i_epoch 0 batch 200: avg loss -1.541850 avg loss no lamb -1.541850 time 2019-02-18 00:58:56.894067
last batch sz 160
Model ind 640 epoch 159 head B head_i_epoch 1 batch 0: avg loss -1.426936 avg loss no lamb -1.426936 time 2019-02-18 01:00:40.859483
Model ind 640 epoch 159 head B head_i_epoch 1 batch 100: avg loss -1.367694 avg loss no lamb -1.367694 time 2019-02-18 01:03:04.315796
Model ind 640 epoch 159 head B head_i_epoch 1 batch 200: avg loss -1.519904 avg loss no lamb -1.519904 time 2019-02-18 01:05:27.819738
last batch sz 160
Pre: time 2019-02-18 01:07:36.243702: 
 	std: 0.0537058
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46373335, 0.57375, 0.5729, 0.57315, 0.46355]
	train_accs: [0.46373335, 0.57375, 0.5729, 0.57315, 0.46355]
	best_train_sub_head: 1
	worst: 0.46355
	avg: 0.5294167
	best: 0.57375

Starting e_i: 160
Model ind 640 epoch 160 head A head_i_epoch 0 batch 0: avg loss -2.754592 avg loss no lamb -2.754592 time 2019-02-18 01:07:38.338013
Model ind 640 epoch 160 head A head_i_epoch 0 batch 100: avg loss -2.725142 avg loss no lamb -2.725142 time 2019-02-18 01:10:01.753987
Model ind 640 epoch 160 head A head_i_epoch 0 batch 200: avg loss -2.726316 avg loss no lamb -2.726316 time 2019-02-18 01:12:25.636514
last batch sz 160
Model ind 640 epoch 160 head B head_i_epoch 0 batch 0: avg loss -1.433434 avg loss no lamb -1.433434 time 2019-02-18 01:14:09.678013
Model ind 640 epoch 160 head B head_i_epoch 0 batch 100: avg loss -1.309876 avg loss no lamb -1.309876 time 2019-02-18 01:16:32.236052
Model ind 640 epoch 160 head B head_i_epoch 0 batch 200: avg loss -1.573256 avg loss no lamb -1.573256 time 2019-02-18 01:18:55.312071
last batch sz 160
Model ind 640 epoch 160 head B head_i_epoch 1 batch 0: avg loss -1.476333 avg loss no lamb -1.476333 time 2019-02-18 01:20:39.332069
Model ind 640 epoch 160 head B head_i_epoch 1 batch 100: avg loss -1.388794 avg loss no lamb -1.388794 time 2019-02-18 01:23:01.364200
Model ind 640 epoch 160 head B head_i_epoch 1 batch 200: avg loss -1.556803 avg loss no lamb -1.556803 time 2019-02-18 01:25:23.108233
last batch sz 160
Pre: time 2019-02-18 01:27:30.275287: 
 	std: 0.053594418
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47136667, 0.58126664, 0.58055, 0.5797667, 0.4709]
	train_accs: [0.47136667, 0.58126664, 0.58055, 0.5797667, 0.4709]
	best_train_sub_head: 1
	worst: 0.4709
	avg: 0.53677
	best: 0.58126664

Starting e_i: 161
Model ind 640 epoch 161 head A head_i_epoch 0 batch 0: avg loss -2.792218 avg loss no lamb -2.792218 time 2019-02-18 01:27:35.504240
Model ind 640 epoch 161 head A head_i_epoch 0 batch 100: avg loss -2.638632 avg loss no lamb -2.638632 time 2019-02-18 01:29:59.029604
Model ind 640 epoch 161 head A head_i_epoch 0 batch 200: avg loss -2.813177 avg loss no lamb -2.813177 time 2019-02-18 01:32:23.014284
last batch sz 160
Model ind 640 epoch 161 head B head_i_epoch 0 batch 0: avg loss -1.463154 avg loss no lamb -1.463154 time 2019-02-18 01:34:07.723899
Model ind 640 epoch 161 head B head_i_epoch 0 batch 100: avg loss -1.355843 avg loss no lamb -1.355843 time 2019-02-18 01:36:31.088986
Model ind 640 epoch 161 head B head_i_epoch 0 batch 200: avg loss -1.532642 avg loss no lamb -1.532642 time 2019-02-18 01:38:54.360748
last batch sz 160
Model ind 640 epoch 161 head B head_i_epoch 1 batch 0: avg loss -1.426794 avg loss no lamb -1.426794 time 2019-02-18 01:40:38.310319
Model ind 640 epoch 161 head B head_i_epoch 1 batch 100: avg loss -1.466679 avg loss no lamb -1.466679 time 2019-02-18 01:43:01.014334
Model ind 640 epoch 161 head B head_i_epoch 1 batch 200: avg loss -1.553776 avg loss no lamb -1.553776 time 2019-02-18 01:45:24.654412
last batch sz 160
Pre: time 2019-02-18 01:47:33.519932: 
 	std: 0.05380465
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46333334, 0.57285, 0.5733333, 0.57325, 0.4633]
	train_accs: [0.46333334, 0.57285, 0.5733333, 0.57325, 0.4633]
	best_train_sub_head: 2
	worst: 0.4633
	avg: 0.5292133
	best: 0.5733333

Starting e_i: 162
Model ind 640 epoch 162 head A head_i_epoch 0 batch 0: avg loss -2.817969 avg loss no lamb -2.817969 time 2019-02-18 01:47:35.436470
Model ind 640 epoch 162 head A head_i_epoch 0 batch 100: avg loss -2.709086 avg loss no lamb -2.709086 time 2019-02-18 01:49:58.908718
Model ind 640 epoch 162 head A head_i_epoch 0 batch 200: avg loss -2.806028 avg loss no lamb -2.806028 time 2019-02-18 01:52:22.476999
last batch sz 160
Model ind 640 epoch 162 head B head_i_epoch 0 batch 0: avg loss -1.413652 avg loss no lamb -1.413652 time 2019-02-18 01:54:05.844671
Model ind 640 epoch 162 head B head_i_epoch 0 batch 100: avg loss -1.333065 avg loss no lamb -1.333065 time 2019-02-18 01:56:27.798825
Model ind 640 epoch 162 head B head_i_epoch 0 batch 200: avg loss -1.550678 avg loss no lamb -1.550678 time 2019-02-18 01:58:49.772463
last batch sz 160
Model ind 640 epoch 162 head B head_i_epoch 1 batch 0: avg loss -1.443884 avg loss no lamb -1.443884 time 2019-02-18 02:00:33.448088
Model ind 640 epoch 162 head B head_i_epoch 1 batch 100: avg loss -1.412449 avg loss no lamb -1.412449 time 2019-02-18 02:02:56.570277
Model ind 640 epoch 162 head B head_i_epoch 1 batch 200: avg loss -1.545065 avg loss no lamb -1.545065 time 2019-02-18 02:05:18.711469
last batch sz 160
Pre: time 2019-02-18 02:07:26.959366: 
 	std: 0.0516484
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46848333, 0.5750167, 0.57481664, 0.57475, 0.4704]
	train_accs: [0.46848333, 0.5750167, 0.57481664, 0.57475, 0.4704]
	best_train_sub_head: 1
	worst: 0.46848333
	avg: 0.5326933
	best: 0.5750167

Starting e_i: 163
Model ind 640 epoch 163 head A head_i_epoch 0 batch 0: avg loss -2.793003 avg loss no lamb -2.793003 time 2019-02-18 02:07:28.888486
Model ind 640 epoch 163 head A head_i_epoch 0 batch 100: avg loss -2.733470 avg loss no lamb -2.733470 time 2019-02-18 02:09:51.584954
Model ind 640 epoch 163 head A head_i_epoch 0 batch 200: avg loss -2.773361 avg loss no lamb -2.773361 time 2019-02-18 02:12:13.736359
last batch sz 160
Model ind 640 epoch 163 head B head_i_epoch 0 batch 0: avg loss -1.408011 avg loss no lamb -1.408011 time 2019-02-18 02:13:57.319707
Model ind 640 epoch 163 head B head_i_epoch 0 batch 100: avg loss -1.350783 avg loss no lamb -1.350783 time 2019-02-18 02:16:19.580078
Model ind 640 epoch 163 head B head_i_epoch 0 batch 200: avg loss -1.537366 avg loss no lamb -1.537366 time 2019-02-18 02:18:42.009736
last batch sz 160
Model ind 640 epoch 163 head B head_i_epoch 1 batch 0: avg loss -1.414299 avg loss no lamb -1.414299 time 2019-02-18 02:20:25.543026
Model ind 640 epoch 163 head B head_i_epoch 1 batch 100: avg loss -1.372396 avg loss no lamb -1.372396 time 2019-02-18 02:22:48.721120
Model ind 640 epoch 163 head B head_i_epoch 1 batch 200: avg loss -1.520298 avg loss no lamb -1.520298 time 2019-02-18 02:25:12.391735
last batch sz 160
Pre: time 2019-02-18 02:27:20.301406: 
 	std: 0.049505472
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47221667, 0.57385, 0.57455, 0.5746, 0.47436666]
	train_accs: [0.47221667, 0.57385, 0.57455, 0.5746, 0.47436666]
	best_train_sub_head: 3
	worst: 0.47221667
	avg: 0.53391665
	best: 0.5746

Starting e_i: 164
Model ind 640 epoch 164 head A head_i_epoch 0 batch 0: avg loss -2.825572 avg loss no lamb -2.825572 time 2019-02-18 02:27:22.226823
Model ind 640 epoch 164 head A head_i_epoch 0 batch 100: avg loss -2.676053 avg loss no lamb -2.676053 time 2019-02-18 02:29:44.526562
Model ind 640 epoch 164 head A head_i_epoch 0 batch 200: avg loss -2.769362 avg loss no lamb -2.769362 time 2019-02-18 02:32:07.133368
last batch sz 160
Model ind 640 epoch 164 head B head_i_epoch 0 batch 0: avg loss -1.529468 avg loss no lamb -1.529468 time 2019-02-18 02:33:50.825693
Model ind 640 epoch 164 head B head_i_epoch 0 batch 100: avg loss -1.445550 avg loss no lamb -1.445550 time 2019-02-18 02:36:13.088447
Model ind 640 epoch 164 head B head_i_epoch 0 batch 200: avg loss -1.558305 avg loss no lamb -1.558305 time 2019-02-18 02:38:36.054637
last batch sz 160
Model ind 640 epoch 164 head B head_i_epoch 1 batch 0: avg loss -1.407488 avg loss no lamb -1.407488 time 2019-02-18 02:40:20.136509
Model ind 640 epoch 164 head B head_i_epoch 1 batch 100: avg loss -1.386907 avg loss no lamb -1.386907 time 2019-02-18 02:42:42.610675
Model ind 640 epoch 164 head B head_i_epoch 1 batch 200: avg loss -1.592701 avg loss no lamb -1.592701 time 2019-02-18 02:45:04.655105
last batch sz 160
Pre: time 2019-02-18 02:47:11.236950: 
 	std: 0.054319292
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47386667, 0.58495, 0.58495, 0.58528334, 0.4745]
	train_accs: [0.47386667, 0.58495, 0.58495, 0.58528334, 0.4745]
	best_train_sub_head: 3
	worst: 0.47386667
	avg: 0.54071
	best: 0.58528334

Starting e_i: 165
Model ind 640 epoch 165 head A head_i_epoch 0 batch 0: avg loss -2.721623 avg loss no lamb -2.721623 time 2019-02-18 02:47:13.119363
Model ind 640 epoch 165 head A head_i_epoch 0 batch 100: avg loss -2.684266 avg loss no lamb -2.684266 time 2019-02-18 02:49:35.880406
Model ind 640 epoch 165 head A head_i_epoch 0 batch 200: avg loss -2.681627 avg loss no lamb -2.681627 time 2019-02-18 02:51:59.088793
last batch sz 160
Model ind 640 epoch 165 head B head_i_epoch 0 batch 0: avg loss -1.446794 avg loss no lamb -1.446794 time 2019-02-18 02:53:42.327098
Model ind 640 epoch 165 head B head_i_epoch 0 batch 100: avg loss -1.399783 avg loss no lamb -1.399783 time 2019-02-18 02:56:04.407929
Model ind 640 epoch 165 head B head_i_epoch 0 batch 200: avg loss -1.498166 avg loss no lamb -1.498166 time 2019-02-18 02:58:27.364802
last batch sz 160
Model ind 640 epoch 165 head B head_i_epoch 1 batch 0: avg loss -1.450583 avg loss no lamb -1.450583 time 2019-02-18 03:00:11.592588
Model ind 640 epoch 165 head B head_i_epoch 1 batch 100: avg loss -1.394128 avg loss no lamb -1.394128 time 2019-02-18 03:02:34.793674
Model ind 640 epoch 165 head B head_i_epoch 1 batch 200: avg loss -1.552222 avg loss no lamb -1.552222 time 2019-02-18 03:04:56.638168
last batch sz 160
Pre: time 2019-02-18 03:07:03.769969: 
 	std: 0.054622583
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46133333, 0.57198334, 0.57131666, 0.5715333, 0.45891666]
	train_accs: [0.46133333, 0.57198334, 0.57131666, 0.5715333, 0.45891666]
	best_train_sub_head: 1
	worst: 0.45891666
	avg: 0.52701664
	best: 0.57198334

Starting e_i: 166
Model ind 640 epoch 166 head A head_i_epoch 0 batch 0: avg loss -2.749340 avg loss no lamb -2.749340 time 2019-02-18 03:07:05.623629
Model ind 640 epoch 166 head A head_i_epoch 0 batch 100: avg loss -2.673444 avg loss no lamb -2.673444 time 2019-02-18 03:09:27.275905
Model ind 640 epoch 166 head A head_i_epoch 0 batch 200: avg loss -2.786391 avg loss no lamb -2.786391 time 2019-02-18 03:11:49.536593
last batch sz 160
Model ind 640 epoch 166 head B head_i_epoch 0 batch 0: avg loss -1.341562 avg loss no lamb -1.341562 time 2019-02-18 03:13:33.743426
Model ind 640 epoch 166 head B head_i_epoch 0 batch 100: avg loss -1.410270 avg loss no lamb -1.410270 time 2019-02-18 03:15:56.833693
Model ind 640 epoch 166 head B head_i_epoch 0 batch 200: avg loss -1.548895 avg loss no lamb -1.548895 time 2019-02-18 03:18:19.805258
last batch sz 160
Model ind 640 epoch 166 head B head_i_epoch 1 batch 0: avg loss -1.384040 avg loss no lamb -1.384040 time 2019-02-18 03:20:03.219532
Model ind 640 epoch 166 head B head_i_epoch 1 batch 100: avg loss -1.318015 avg loss no lamb -1.318015 time 2019-02-18 03:22:25.744348
Model ind 640 epoch 166 head B head_i_epoch 1 batch 200: avg loss -1.538170 avg loss no lamb -1.538170 time 2019-02-18 03:24:49.151389
last batch sz 160
Pre: time 2019-02-18 03:26:57.233370: 
 	std: 0.054543093
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4695, 0.5814667, 0.58063334, 0.58135, 0.47013333]
	train_accs: [0.4695, 0.5814667, 0.58063334, 0.58135, 0.47013333]
	best_train_sub_head: 1
	worst: 0.4695
	avg: 0.5366167
	best: 0.5814667

Starting e_i: 167
Model ind 640 epoch 167 head A head_i_epoch 0 batch 0: avg loss -2.742279 avg loss no lamb -2.742279 time 2019-02-18 03:26:59.300826
Model ind 640 epoch 167 head A head_i_epoch 0 batch 100: avg loss -2.755894 avg loss no lamb -2.755894 time 2019-02-18 03:29:21.915956
Model ind 640 epoch 167 head A head_i_epoch 0 batch 200: avg loss -2.710920 avg loss no lamb -2.710920 time 2019-02-18 03:31:45.439965
last batch sz 160
Model ind 640 epoch 167 head B head_i_epoch 0 batch 0: avg loss -1.534341 avg loss no lamb -1.534341 time 2019-02-18 03:33:29.982249
Model ind 640 epoch 167 head B head_i_epoch 0 batch 100: avg loss -1.355047 avg loss no lamb -1.355047 time 2019-02-18 03:35:52.482951
Model ind 640 epoch 167 head B head_i_epoch 0 batch 200: avg loss -1.492307 avg loss no lamb -1.492307 time 2019-02-18 03:38:14.183387
last batch sz 160
Model ind 640 epoch 167 head B head_i_epoch 1 batch 0: avg loss -1.502704 avg loss no lamb -1.502704 time 2019-02-18 03:39:57.422481
Model ind 640 epoch 167 head B head_i_epoch 1 batch 100: avg loss -1.436003 avg loss no lamb -1.436003 time 2019-02-18 03:42:20.078058
Model ind 640 epoch 167 head B head_i_epoch 1 batch 200: avg loss -1.596356 avg loss no lamb -1.596356 time 2019-02-18 03:44:42.578699
last batch sz 160
Pre: time 2019-02-18 03:46:50.616750: 
 	std: 0.05416564
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46803334, 0.57841665, 0.57813334, 0.57811666, 0.46728334]
	train_accs: [0.46803334, 0.57841665, 0.57813334, 0.57811666, 0.46728334]
	best_train_sub_head: 1
	worst: 0.46728334
	avg: 0.5339967
	best: 0.57841665

Starting e_i: 168
Model ind 640 epoch 168 head A head_i_epoch 0 batch 0: avg loss -2.783301 avg loss no lamb -2.783301 time 2019-02-18 03:46:52.525886
Model ind 640 epoch 168 head A head_i_epoch 0 batch 100: avg loss -2.826391 avg loss no lamb -2.826391 time 2019-02-18 03:49:14.481630
Model ind 640 epoch 168 head A head_i_epoch 0 batch 200: avg loss -2.797068 avg loss no lamb -2.797068 time 2019-02-18 03:51:38.003877
last batch sz 160
Model ind 640 epoch 168 head B head_i_epoch 0 batch 0: avg loss -1.441620 avg loss no lamb -1.441620 time 2019-02-18 03:53:22.905034
Model ind 640 epoch 168 head B head_i_epoch 0 batch 100: avg loss -1.437802 avg loss no lamb -1.437802 time 2019-02-18 03:55:46.950645
Model ind 640 epoch 168 head B head_i_epoch 0 batch 200: avg loss -1.529703 avg loss no lamb -1.529703 time 2019-02-18 03:58:10.913782
last batch sz 160
Model ind 640 epoch 168 head B head_i_epoch 1 batch 0: avg loss -1.396438 avg loss no lamb -1.396438 time 2019-02-18 03:59:55.593171
Model ind 640 epoch 168 head B head_i_epoch 1 batch 100: avg loss -1.306081 avg loss no lamb -1.306081 time 2019-02-18 04:02:18.074706
Model ind 640 epoch 168 head B head_i_epoch 1 batch 200: avg loss -1.465418 avg loss no lamb -1.465418 time 2019-02-18 04:04:40.965619
last batch sz 160
Pre: time 2019-02-18 04:06:49.750520: 
 	std: 0.05407859
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46591666, 0.57586664, 0.57601666, 0.5758, 0.4651]
	train_accs: [0.46591666, 0.57586664, 0.57601666, 0.5758, 0.4651]
	best_train_sub_head: 2
	worst: 0.4651
	avg: 0.53174
	best: 0.57601666

Starting e_i: 169
Model ind 640 epoch 169 head A head_i_epoch 0 batch 0: avg loss -2.736913 avg loss no lamb -2.736913 time 2019-02-18 04:06:51.876339
Model ind 640 epoch 169 head A head_i_epoch 0 batch 100: avg loss -2.706608 avg loss no lamb -2.706608 time 2019-02-18 04:09:15.731494
Model ind 640 epoch 169 head A head_i_epoch 0 batch 200: avg loss -2.804612 avg loss no lamb -2.804612 time 2019-02-18 04:11:38.751231
last batch sz 160
Model ind 640 epoch 169 head B head_i_epoch 0 batch 0: avg loss -1.488016 avg loss no lamb -1.488016 time 2019-02-18 04:13:22.507329
Model ind 640 epoch 169 head B head_i_epoch 0 batch 100: avg loss -1.384605 avg loss no lamb -1.384605 time 2019-02-18 04:15:45.339953
Model ind 640 epoch 169 head B head_i_epoch 0 batch 200: avg loss -1.530919 avg loss no lamb -1.530919 time 2019-02-18 04:18:08.611291
last batch sz 160
Model ind 640 epoch 169 head B head_i_epoch 1 batch 0: avg loss -1.436215 avg loss no lamb -1.436215 time 2019-02-18 04:19:52.592660
Model ind 640 epoch 169 head B head_i_epoch 1 batch 100: avg loss -1.436520 avg loss no lamb -1.436520 time 2019-02-18 04:22:15.115040
Model ind 640 epoch 169 head B head_i_epoch 1 batch 200: avg loss -1.484219 avg loss no lamb -1.484219 time 2019-02-18 04:24:37.164281
last batch sz 160
Pre: time 2019-02-18 04:26:44.677222: 
 	std: 0.056299087
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4531, 0.5675333, 0.5672, 0.5667833, 0.45141667]
	train_accs: [0.4531, 0.5675333, 0.5672, 0.5667833, 0.45141667]
	best_train_sub_head: 1
	worst: 0.45141667
	avg: 0.52120674
	best: 0.5675333

Starting e_i: 170
Model ind 640 epoch 170 head A head_i_epoch 0 batch 0: avg loss -2.832470 avg loss no lamb -2.832470 time 2019-02-18 04:26:46.582019
Model ind 640 epoch 170 head A head_i_epoch 0 batch 100: avg loss -2.680995 avg loss no lamb -2.680995 time 2019-02-18 04:29:09.726069
Model ind 640 epoch 170 head A head_i_epoch 0 batch 200: avg loss -2.803144 avg loss no lamb -2.803144 time 2019-02-18 04:31:33.518034
last batch sz 160
Model ind 640 epoch 170 head B head_i_epoch 0 batch 0: avg loss -1.440990 avg loss no lamb -1.440990 time 2019-02-18 04:33:17.994367
Model ind 640 epoch 170 head B head_i_epoch 0 batch 100: avg loss -1.432902 avg loss no lamb -1.432902 time 2019-02-18 04:35:41.529347
Model ind 640 epoch 170 head B head_i_epoch 0 batch 200: avg loss -1.558685 avg loss no lamb -1.558685 time 2019-02-18 04:38:05.153216
last batch sz 160
Model ind 640 epoch 170 head B head_i_epoch 1 batch 0: avg loss -1.538203 avg loss no lamb -1.538203 time 2019-02-18 04:39:48.868390
Model ind 640 epoch 170 head B head_i_epoch 1 batch 100: avg loss -1.331907 avg loss no lamb -1.331907 time 2019-02-18 04:42:12.065821
Model ind 640 epoch 170 head B head_i_epoch 1 batch 200: avg loss -1.493140 avg loss no lamb -1.493140 time 2019-02-18 04:44:35.333083
last batch sz 160
Pre: time 2019-02-18 04:46:44.050238: 
 	std: 0.04887505
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47406667, 0.57515, 0.57456666, 0.57418334, 0.47568333]
	train_accs: [0.47406667, 0.57515, 0.57456666, 0.57418334, 0.47568333]
	best_train_sub_head: 1
	worst: 0.47406667
	avg: 0.53472996
	best: 0.57515

Starting e_i: 171
Model ind 640 epoch 171 head A head_i_epoch 0 batch 0: avg loss -2.760789 avg loss no lamb -2.760789 time 2019-02-18 04:46:49.624050
Model ind 640 epoch 171 head A head_i_epoch 0 batch 100: avg loss -2.711692 avg loss no lamb -2.711692 time 2019-02-18 04:49:11.893187
Model ind 640 epoch 171 head A head_i_epoch 0 batch 200: avg loss -2.756622 avg loss no lamb -2.756622 time 2019-02-18 04:51:34.461429
last batch sz 160
Model ind 640 epoch 171 head B head_i_epoch 0 batch 0: avg loss -1.451432 avg loss no lamb -1.451432 time 2019-02-18 04:53:18.463004
Model ind 640 epoch 171 head B head_i_epoch 0 batch 100: avg loss -1.452132 avg loss no lamb -1.452132 time 2019-02-18 04:55:41.182635
Model ind 640 epoch 171 head B head_i_epoch 0 batch 200: avg loss -1.584397 avg loss no lamb -1.584397 time 2019-02-18 04:58:04.298659
last batch sz 160
Model ind 640 epoch 171 head B head_i_epoch 1 batch 0: avg loss -1.546345 avg loss no lamb -1.546345 time 2019-02-18 04:59:47.594185
Model ind 640 epoch 171 head B head_i_epoch 1 batch 100: avg loss -1.419518 avg loss no lamb -1.419518 time 2019-02-18 05:02:10.495735
Model ind 640 epoch 171 head B head_i_epoch 1 batch 200: avg loss -1.564647 avg loss no lamb -1.564647 time 2019-02-18 05:04:33.058793
last batch sz 160
Pre: time 2019-02-18 05:06:40.326992: 
 	std: 0.052693177
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4619, 0.57101667, 0.5702, 0.5700833, 0.46386668]
	train_accs: [0.4619, 0.57101667, 0.5702, 0.5700833, 0.46386668]
	best_train_sub_head: 1
	worst: 0.4619
	avg: 0.52741337
	best: 0.57101667

Starting e_i: 172
Model ind 640 epoch 172 head A head_i_epoch 0 batch 0: avg loss -2.779536 avg loss no lamb -2.779536 time 2019-02-18 05:06:42.209563
Model ind 640 epoch 172 head A head_i_epoch 0 batch 100: avg loss -2.730077 avg loss no lamb -2.730077 time 2019-02-18 05:09:04.228284
Model ind 640 epoch 172 head A head_i_epoch 0 batch 200: avg loss -2.730831 avg loss no lamb -2.730831 time 2019-02-18 05:11:26.923094
last batch sz 160
Model ind 640 epoch 172 head B head_i_epoch 0 batch 0: avg loss -1.440628 avg loss no lamb -1.440628 time 2019-02-18 05:13:11.824786
Model ind 640 epoch 172 head B head_i_epoch 0 batch 100: avg loss -1.428275 avg loss no lamb -1.428275 time 2019-02-18 05:15:34.386627
Model ind 640 epoch 172 head B head_i_epoch 0 batch 200: avg loss -1.598006 avg loss no lamb -1.598006 time 2019-02-18 05:17:56.457614
last batch sz 160
Model ind 640 epoch 172 head B head_i_epoch 1 batch 0: avg loss -1.473122 avg loss no lamb -1.473122 time 2019-02-18 05:19:40.471944
Model ind 640 epoch 172 head B head_i_epoch 1 batch 100: avg loss -1.435334 avg loss no lamb -1.435334 time 2019-02-18 05:22:03.013088
Model ind 640 epoch 172 head B head_i_epoch 1 batch 200: avg loss -1.490383 avg loss no lamb -1.490383 time 2019-02-18 05:24:25.936812
last batch sz 160
Pre: time 2019-02-18 05:26:34.772269: 
 	std: 0.05352179
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47435, 0.58321667, 0.5837167, 0.5832667, 0.47395]
	train_accs: [0.47435, 0.58321667, 0.5837167, 0.5832667, 0.47395]
	best_train_sub_head: 2
	worst: 0.47395
	avg: 0.5397
	best: 0.5837167

Starting e_i: 173
Model ind 640 epoch 173 head A head_i_epoch 0 batch 0: avg loss -2.790677 avg loss no lamb -2.790677 time 2019-02-18 05:26:36.748174
Model ind 640 epoch 173 head A head_i_epoch 0 batch 100: avg loss -2.645213 avg loss no lamb -2.645213 time 2019-02-18 05:29:00.028170
Model ind 640 epoch 173 head A head_i_epoch 0 batch 200: avg loss -2.765807 avg loss no lamb -2.765807 time 2019-02-18 05:31:23.053296
last batch sz 160
Model ind 640 epoch 173 head B head_i_epoch 0 batch 0: avg loss -1.456603 avg loss no lamb -1.456603 time 2019-02-18 05:33:07.453991
Model ind 640 epoch 173 head B head_i_epoch 0 batch 100: avg loss -1.434080 avg loss no lamb -1.434080 time 2019-02-18 05:35:31.385263
Model ind 640 epoch 173 head B head_i_epoch 0 batch 200: avg loss -1.564372 avg loss no lamb -1.564372 time 2019-02-18 05:37:54.554416
last batch sz 160
Model ind 640 epoch 173 head B head_i_epoch 1 batch 0: avg loss -1.439044 avg loss no lamb -1.439044 time 2019-02-18 05:39:38.061989
Model ind 640 epoch 173 head B head_i_epoch 1 batch 100: avg loss -1.414578 avg loss no lamb -1.414578 time 2019-02-18 05:41:59.892098
Model ind 640 epoch 173 head B head_i_epoch 1 batch 200: avg loss -1.588154 avg loss no lamb -1.588154 time 2019-02-18 05:44:22.981128
last batch sz 160
Pre: time 2019-02-18 05:46:31.013294: 
 	std: 0.054889206
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46511668, 0.5775167, 0.57711667, 0.5772667, 0.4654]
	train_accs: [0.46511668, 0.5775167, 0.57711667, 0.5772667, 0.4654]
	best_train_sub_head: 1
	worst: 0.46511668
	avg: 0.53248334
	best: 0.5775167

Starting e_i: 174
Model ind 640 epoch 174 head A head_i_epoch 0 batch 0: avg loss -2.652595 avg loss no lamb -2.652595 time 2019-02-18 05:46:32.927312
Model ind 640 epoch 174 head A head_i_epoch 0 batch 100: avg loss -2.812219 avg loss no lamb -2.812219 time 2019-02-18 05:48:56.353494
Model ind 640 epoch 174 head A head_i_epoch 0 batch 200: avg loss -2.829833 avg loss no lamb -2.829833 time 2019-02-18 05:51:20.258835
last batch sz 160
Model ind 640 epoch 174 head B head_i_epoch 0 batch 0: avg loss -1.534538 avg loss no lamb -1.534538 time 2019-02-18 05:53:03.992523
Model ind 640 epoch 174 head B head_i_epoch 0 batch 100: avg loss -1.412272 avg loss no lamb -1.412272 time 2019-02-18 05:55:26.840259
Model ind 640 epoch 174 head B head_i_epoch 0 batch 200: avg loss -1.573475 avg loss no lamb -1.573475 time 2019-02-18 05:57:50.045205
last batch sz 160
Model ind 640 epoch 174 head B head_i_epoch 1 batch 0: avg loss -1.407399 avg loss no lamb -1.407399 time 2019-02-18 05:59:34.268786
Model ind 640 epoch 174 head B head_i_epoch 1 batch 100: avg loss -1.353824 avg loss no lamb -1.353824 time 2019-02-18 06:01:56.664830
Model ind 640 epoch 174 head B head_i_epoch 1 batch 200: avg loss -1.604868 avg loss no lamb -1.604868 time 2019-02-18 06:04:18.944126
last batch sz 160
Pre: time 2019-02-18 06:06:26.184078: 
 	std: 0.05294211
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4796, 0.58678335, 0.58666664, 0.58678335, 0.47776666]
	train_accs: [0.4796, 0.58678335, 0.58666664, 0.58678335, 0.47776666]
	best_train_sub_head: 1
	worst: 0.47776666
	avg: 0.54352003
	best: 0.58678335

Starting e_i: 175
Model ind 640 epoch 175 head A head_i_epoch 0 batch 0: avg loss -2.845375 avg loss no lamb -2.845375 time 2019-02-18 06:06:28.072200
Model ind 640 epoch 175 head A head_i_epoch 0 batch 100: avg loss -2.715543 avg loss no lamb -2.715543 time 2019-02-18 06:08:51.121947
Model ind 640 epoch 175 head A head_i_epoch 0 batch 200: avg loss -2.768921 avg loss no lamb -2.768921 time 2019-02-18 06:11:14.785935
last batch sz 160
Model ind 640 epoch 175 head B head_i_epoch 0 batch 0: avg loss -1.492431 avg loss no lamb -1.492431 time 2019-02-18 06:12:59.602540
Model ind 640 epoch 175 head B head_i_epoch 0 batch 100: avg loss -1.406816 avg loss no lamb -1.406816 time 2019-02-18 06:15:22.372272
Model ind 640 epoch 175 head B head_i_epoch 0 batch 200: avg loss -1.611520 avg loss no lamb -1.611520 time 2019-02-18 06:17:45.089671
last batch sz 160
Model ind 640 epoch 175 head B head_i_epoch 1 batch 0: avg loss -1.483174 avg loss no lamb -1.483174 time 2019-02-18 06:19:29.063894
Model ind 640 epoch 175 head B head_i_epoch 1 batch 100: avg loss -1.383742 avg loss no lamb -1.383742 time 2019-02-18 06:21:51.335941
Model ind 640 epoch 175 head B head_i_epoch 1 batch 200: avg loss -1.456741 avg loss no lamb -1.456741 time 2019-02-18 06:24:14.065180
last batch sz 160
Pre: time 2019-02-18 06:26:22.377274: 
 	std: 0.051074073
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47505, 0.57913333, 0.5782, 0.57881665, 0.47388333]
	train_accs: [0.47505, 0.57913333, 0.5782, 0.57881665, 0.47388333]
	best_train_sub_head: 1
	worst: 0.47388333
	avg: 0.5370167
	best: 0.57913333

Starting e_i: 176
Model ind 640 epoch 176 head A head_i_epoch 0 batch 0: avg loss -2.738950 avg loss no lamb -2.738950 time 2019-02-18 06:26:24.295248
Model ind 640 epoch 176 head A head_i_epoch 0 batch 100: avg loss -2.763941 avg loss no lamb -2.763941 time 2019-02-18 06:28:47.446821
Model ind 640 epoch 176 head A head_i_epoch 0 batch 200: avg loss -2.760992 avg loss no lamb -2.760992 time 2019-02-18 06:31:10.287961
last batch sz 160
Model ind 640 epoch 176 head B head_i_epoch 0 batch 0: avg loss -1.381020 avg loss no lamb -1.381020 time 2019-02-18 06:32:53.518184
Model ind 640 epoch 176 head B head_i_epoch 0 batch 100: avg loss -1.296897 avg loss no lamb -1.296897 time 2019-02-18 06:35:15.568385
Model ind 640 epoch 176 head B head_i_epoch 0 batch 200: avg loss -1.582330 avg loss no lamb -1.582330 time 2019-02-18 06:37:37.649404
last batch sz 160
Model ind 640 epoch 176 head B head_i_epoch 1 batch 0: avg loss -1.476444 avg loss no lamb -1.476444 time 2019-02-18 06:39:21.430862
Model ind 640 epoch 176 head B head_i_epoch 1 batch 100: avg loss -1.315520 avg loss no lamb -1.315520 time 2019-02-18 06:41:43.655885
Model ind 640 epoch 176 head B head_i_epoch 1 batch 200: avg loss -1.623841 avg loss no lamb -1.623841 time 2019-02-18 06:44:05.894040
last batch sz 160
Pre: time 2019-02-18 06:46:13.485272: 
 	std: 0.053229947
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4698, 0.57836664, 0.57776666, 0.57785, 0.46888334]
	train_accs: [0.4698, 0.57836664, 0.57776666, 0.57785, 0.46888334]
	best_train_sub_head: 1
	worst: 0.46888334
	avg: 0.5345333
	best: 0.57836664

Starting e_i: 177
Model ind 640 epoch 177 head A head_i_epoch 0 batch 0: avg loss -2.697298 avg loss no lamb -2.697298 time 2019-02-18 06:46:15.378139
Model ind 640 epoch 177 head A head_i_epoch 0 batch 100: avg loss -2.735395 avg loss no lamb -2.735395 time 2019-02-18 06:48:37.121857
Model ind 640 epoch 177 head A head_i_epoch 0 batch 200: avg loss -2.827734 avg loss no lamb -2.827734 time 2019-02-18 06:50:58.751112
last batch sz 160
Model ind 640 epoch 177 head B head_i_epoch 0 batch 0: avg loss -1.534186 avg loss no lamb -1.534186 time 2019-02-18 06:52:42.098882
Model ind 640 epoch 177 head B head_i_epoch 0 batch 100: avg loss -1.279688 avg loss no lamb -1.279688 time 2019-02-18 06:55:04.115345
Model ind 640 epoch 177 head B head_i_epoch 0 batch 200: avg loss -1.545327 avg loss no lamb -1.545327 time 2019-02-18 06:57:26.264154
last batch sz 160
Model ind 640 epoch 177 head B head_i_epoch 1 batch 0: avg loss -1.456047 avg loss no lamb -1.456047 time 2019-02-18 06:59:10.346829
Model ind 640 epoch 177 head B head_i_epoch 1 batch 100: avg loss -1.391588 avg loss no lamb -1.391588 time 2019-02-18 07:01:33.382324
Model ind 640 epoch 177 head B head_i_epoch 1 batch 200: avg loss -1.506231 avg loss no lamb -1.506231 time 2019-02-18 07:03:55.484894
last batch sz 160
Pre: time 2019-02-18 07:06:03.095813: 
 	std: 0.05287749
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48175, 0.59035, 0.5901167, 0.5908, 0.48323333]
	train_accs: [0.48175, 0.59035, 0.5901167, 0.5908, 0.48323333]
	best_train_sub_head: 3
	worst: 0.48175
	avg: 0.54725
	best: 0.5908

Starting e_i: 178
Model ind 640 epoch 178 head A head_i_epoch 0 batch 0: avg loss -2.749221 avg loss no lamb -2.749221 time 2019-02-18 07:06:08.893839
Model ind 640 epoch 178 head A head_i_epoch 0 batch 100: avg loss -2.698110 avg loss no lamb -2.698110 time 2019-02-18 07:08:31.015159
Model ind 640 epoch 178 head A head_i_epoch 0 batch 200: avg loss -2.755227 avg loss no lamb -2.755227 time 2019-02-18 07:10:53.216829
last batch sz 160
Model ind 640 epoch 178 head B head_i_epoch 0 batch 0: avg loss -1.451467 avg loss no lamb -1.451467 time 2019-02-18 07:12:36.711734
Model ind 640 epoch 178 head B head_i_epoch 0 batch 100: avg loss -1.458877 avg loss no lamb -1.458877 time 2019-02-18 07:14:58.824697
Model ind 640 epoch 178 head B head_i_epoch 0 batch 200: avg loss -1.608736 avg loss no lamb -1.608736 time 2019-02-18 07:17:20.818482
last batch sz 160
Model ind 640 epoch 178 head B head_i_epoch 1 batch 0: avg loss -1.498262 avg loss no lamb -1.498262 time 2019-02-18 07:19:05.469744
Model ind 640 epoch 178 head B head_i_epoch 1 batch 100: avg loss -1.354554 avg loss no lamb -1.354554 time 2019-02-18 07:21:28.942702
Model ind 640 epoch 178 head B head_i_epoch 1 batch 200: avg loss -1.524400 avg loss no lamb -1.524400 time 2019-02-18 07:23:51.868306
last batch sz 160
Pre: time 2019-02-18 07:26:00.481302: 
 	std: 0.053671192
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46585, 0.57563335, 0.57561666, 0.57556665, 0.46625]
	train_accs: [0.46585, 0.57563335, 0.57561666, 0.57556665, 0.46625]
	best_train_sub_head: 1
	worst: 0.46585
	avg: 0.5317833
	best: 0.57563335

Starting e_i: 179
Model ind 640 epoch 179 head A head_i_epoch 0 batch 0: avg loss -2.746532 avg loss no lamb -2.746532 time 2019-02-18 07:26:02.697098
Model ind 640 epoch 179 head A head_i_epoch 0 batch 100: avg loss -2.700866 avg loss no lamb -2.700866 time 2019-02-18 07:28:26.052292
Model ind 640 epoch 179 head A head_i_epoch 0 batch 200: avg loss -2.713604 avg loss no lamb -2.713604 time 2019-02-18 07:30:49.182454
last batch sz 160
Model ind 640 epoch 179 head B head_i_epoch 0 batch 0: avg loss -1.492548 avg loss no lamb -1.492548 time 2019-02-18 07:32:32.706611
Model ind 640 epoch 179 head B head_i_epoch 0 batch 100: avg loss -1.425177 avg loss no lamb -1.425177 time 2019-02-18 07:34:54.672357
Model ind 640 epoch 179 head B head_i_epoch 0 batch 200: avg loss -1.545206 avg loss no lamb -1.545206 time 2019-02-18 07:37:16.567186
last batch sz 160
Model ind 640 epoch 179 head B head_i_epoch 1 batch 0: avg loss -1.491014 avg loss no lamb -1.491014 time 2019-02-18 07:38:59.990086
Model ind 640 epoch 179 head B head_i_epoch 1 batch 100: avg loss -1.416350 avg loss no lamb -1.416350 time 2019-02-18 07:41:21.889999
Model ind 640 epoch 179 head B head_i_epoch 1 batch 200: avg loss -1.571262 avg loss no lamb -1.571262 time 2019-02-18 07:43:43.880184
last batch sz 160
Pre: time 2019-02-18 07:45:51.408390: 
 	std: 0.052908856
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.45913333, 0.56728333, 0.5664667, 0.56656665, 0.45841667]
	train_accs: [0.45913333, 0.56728333, 0.5664667, 0.56656665, 0.45841667]
	best_train_sub_head: 1
	worst: 0.45841667
	avg: 0.52357334
	best: 0.56728333

Starting e_i: 180
Model ind 640 epoch 180 head A head_i_epoch 0 batch 0: avg loss -2.717193 avg loss no lamb -2.717193 time 2019-02-18 07:45:53.526264
Model ind 640 epoch 180 head A head_i_epoch 0 batch 100: avg loss -2.755490 avg loss no lamb -2.755490 time 2019-02-18 07:48:15.878347
Model ind 640 epoch 180 head A head_i_epoch 0 batch 200: avg loss -2.748188 avg loss no lamb -2.748188 time 2019-02-18 07:50:38.010378
last batch sz 160
Model ind 640 epoch 180 head B head_i_epoch 0 batch 0: avg loss -1.424588 avg loss no lamb -1.424588 time 2019-02-18 07:52:21.636581
Model ind 640 epoch 180 head B head_i_epoch 0 batch 100: avg loss -1.386621 avg loss no lamb -1.386621 time 2019-02-18 07:54:43.887978
Model ind 640 epoch 180 head B head_i_epoch 0 batch 200: avg loss -1.505662 avg loss no lamb -1.505662 time 2019-02-18 07:57:06.130837
last batch sz 160
Model ind 640 epoch 180 head B head_i_epoch 1 batch 0: avg loss -1.476819 avg loss no lamb -1.476819 time 2019-02-18 07:58:50.356232
Model ind 640 epoch 180 head B head_i_epoch 1 batch 100: avg loss -1.399341 avg loss no lamb -1.399341 time 2019-02-18 08:01:12.463000
Model ind 640 epoch 180 head B head_i_epoch 1 batch 200: avg loss -1.539808 avg loss no lamb -1.539808 time 2019-02-18 08:03:34.623249
last batch sz 160
Pre: time 2019-02-18 08:05:43.155097: 
 	std: 0.051997565
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47875, 0.5845, 0.58493334, 0.58503336, 0.47861665]
	train_accs: [0.47875, 0.5845, 0.58493334, 0.58503336, 0.47861665]
	best_train_sub_head: 3
	worst: 0.47861665
	avg: 0.5423667
	best: 0.58503336

Starting e_i: 181
Model ind 640 epoch 181 head A head_i_epoch 0 batch 0: avg loss -2.692128 avg loss no lamb -2.692128 time 2019-02-18 08:05:48.523982
Model ind 640 epoch 181 head A head_i_epoch 0 batch 100: avg loss -2.727376 avg loss no lamb -2.727376 time 2019-02-18 08:08:11.598071
Model ind 640 epoch 181 head A head_i_epoch 0 batch 200: avg loss -2.796009 avg loss no lamb -2.796009 time 2019-02-18 08:10:33.555907
last batch sz 160
Model ind 640 epoch 181 head B head_i_epoch 0 batch 0: avg loss -1.448968 avg loss no lamb -1.448968 time 2019-02-18 08:12:17.287111
Model ind 640 epoch 181 head B head_i_epoch 0 batch 100: avg loss -1.462514 avg loss no lamb -1.462514 time 2019-02-18 08:14:39.224056
Model ind 640 epoch 181 head B head_i_epoch 0 batch 200: avg loss -1.574106 avg loss no lamb -1.574106 time 2019-02-18 08:17:01.141562
last batch sz 160
Model ind 640 epoch 181 head B head_i_epoch 1 batch 0: avg loss -1.449499 avg loss no lamb -1.449499 time 2019-02-18 08:18:45.633767
Model ind 640 epoch 181 head B head_i_epoch 1 batch 100: avg loss -1.384728 avg loss no lamb -1.384728 time 2019-02-18 08:21:08.035295
Model ind 640 epoch 181 head B head_i_epoch 1 batch 200: avg loss -1.569750 avg loss no lamb -1.569750 time 2019-02-18 08:23:29.827336
last batch sz 160
Pre: time 2019-02-18 08:25:37.567975: 
 	std: 0.056894038
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46303332, 0.5797167, 0.5790167, 0.57911664, 0.46326667]
	train_accs: [0.46303332, 0.5797167, 0.5790167, 0.57911664, 0.46326667]
	best_train_sub_head: 1
	worst: 0.46303332
	avg: 0.53283
	best: 0.5797167

Starting e_i: 182
Model ind 640 epoch 182 head A head_i_epoch 0 batch 0: avg loss -2.754169 avg loss no lamb -2.754169 time 2019-02-18 08:25:39.753965
Model ind 640 epoch 182 head A head_i_epoch 0 batch 100: avg loss -2.715088 avg loss no lamb -2.715088 time 2019-02-18 08:28:02.526578
Model ind 640 epoch 182 head A head_i_epoch 0 batch 200: avg loss -2.738035 avg loss no lamb -2.738035 time 2019-02-18 08:30:24.447626
last batch sz 160
Model ind 640 epoch 182 head B head_i_epoch 0 batch 0: avg loss -1.547546 avg loss no lamb -1.547546 time 2019-02-18 08:32:07.601084
Model ind 640 epoch 182 head B head_i_epoch 0 batch 100: avg loss -1.415637 avg loss no lamb -1.415637 time 2019-02-18 08:34:29.549662
Model ind 640 epoch 182 head B head_i_epoch 0 batch 200: avg loss -1.575243 avg loss no lamb -1.575243 time 2019-02-18 08:36:52.288702
last batch sz 160
Model ind 640 epoch 182 head B head_i_epoch 1 batch 0: avg loss -1.463949 avg loss no lamb -1.463949 time 2019-02-18 08:38:35.488503
Model ind 640 epoch 182 head B head_i_epoch 1 batch 100: avg loss -1.405630 avg loss no lamb -1.405630 time 2019-02-18 08:40:57.385473
Model ind 640 epoch 182 head B head_i_epoch 1 batch 200: avg loss -1.563872 avg loss no lamb -1.563872 time 2019-02-18 08:43:20.469610
last batch sz 160
Pre: time 2019-02-18 08:45:28.829786: 
 	std: 0.051871177
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4744, 0.5795, 0.5787, 0.57991666, 0.4726]
	train_accs: [0.4744, 0.5795, 0.5787, 0.57991666, 0.4726]
	best_train_sub_head: 3
	worst: 0.4726
	avg: 0.53702337
	best: 0.57991666

Starting e_i: 183
Model ind 640 epoch 183 head A head_i_epoch 0 batch 0: avg loss -2.726989 avg loss no lamb -2.726989 time 2019-02-18 08:45:30.775030
Model ind 640 epoch 183 head A head_i_epoch 0 batch 100: avg loss -2.798373 avg loss no lamb -2.798373 time 2019-02-18 08:47:54.380363
Model ind 640 epoch 183 head A head_i_epoch 0 batch 200: avg loss -2.778667 avg loss no lamb -2.778667 time 2019-02-18 08:50:17.729604
last batch sz 160
Model ind 640 epoch 183 head B head_i_epoch 0 batch 0: avg loss -1.508144 avg loss no lamb -1.508144 time 2019-02-18 08:52:01.950534
Model ind 640 epoch 183 head B head_i_epoch 0 batch 100: avg loss -1.460410 avg loss no lamb -1.460410 time 2019-02-18 08:54:25.181819
Model ind 640 epoch 183 head B head_i_epoch 0 batch 200: avg loss -1.552246 avg loss no lamb -1.552246 time 2019-02-18 08:56:48.382148
last batch sz 160
Model ind 640 epoch 183 head B head_i_epoch 1 batch 0: avg loss -1.542442 avg loss no lamb -1.542442 time 2019-02-18 08:58:32.089728
Model ind 640 epoch 183 head B head_i_epoch 1 batch 100: avg loss -1.296349 avg loss no lamb -1.296349 time 2019-02-18 09:00:54.526360
Model ind 640 epoch 183 head B head_i_epoch 1 batch 200: avg loss -1.548185 avg loss no lamb -1.548185 time 2019-02-18 09:03:16.920608
last batch sz 160
Pre: time 2019-02-18 09:05:26.074205: 
 	std: 0.05510331
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.46538332, 0.57741666, 0.57675, 0.5776333, 0.4642]
	train_accs: [0.46538332, 0.57741666, 0.57675, 0.5776333, 0.4642]
	best_train_sub_head: 3
	worst: 0.4642
	avg: 0.5322767
	best: 0.5776333

Starting e_i: 184
Model ind 640 epoch 184 head A head_i_epoch 0 batch 0: avg loss -2.724056 avg loss no lamb -2.724056 time 2019-02-18 09:05:28.300932
Model ind 640 epoch 184 head A head_i_epoch 0 batch 100: avg loss -2.679026 avg loss no lamb -2.679026 time 2019-02-18 09:07:50.229594
Model ind 640 epoch 184 head A head_i_epoch 0 batch 200: avg loss -2.851706 avg loss no lamb -2.851706 time 2019-02-18 09:10:12.448279
last batch sz 160
Model ind 640 epoch 184 head B head_i_epoch 0 batch 0: avg loss -1.559432 avg loss no lamb -1.559432 time 2019-02-18 09:11:56.526133
Model ind 640 epoch 184 head B head_i_epoch 0 batch 100: avg loss -1.471745 avg loss no lamb -1.471745 time 2019-02-18 09:14:18.697522
Model ind 640 epoch 184 head B head_i_epoch 0 batch 200: avg loss -1.515851 avg loss no lamb -1.515851 time 2019-02-18 09:16:41.651882
last batch sz 160
Model ind 640 epoch 184 head B head_i_epoch 1 batch 0: avg loss -1.432145 avg loss no lamb -1.432145 time 2019-02-18 09:18:25.083817
Model ind 640 epoch 184 head B head_i_epoch 1 batch 100: avg loss -1.435629 avg loss no lamb -1.435629 time 2019-02-18 09:20:46.972399
Model ind 640 epoch 184 head B head_i_epoch 1 batch 200: avg loss -1.552109 avg loss no lamb -1.552109 time 2019-02-18 09:23:08.951650
last batch sz 160
Pre: time 2019-02-18 09:25:17.801227: 
 	std: 0.054181702
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48101667, 0.5906, 0.59001666, 0.5905667, 0.4786]
	train_accs: [0.48101667, 0.5906, 0.59001666, 0.5905667, 0.4786]
	best_train_sub_head: 1
	worst: 0.4786
	avg: 0.54616
	best: 0.5906

Starting e_i: 185
Model ind 640 epoch 185 head A head_i_epoch 0 batch 0: avg loss -2.815821 avg loss no lamb -2.815821 time 2019-02-18 09:25:19.722800
Model ind 640 epoch 185 head A head_i_epoch 0 batch 100: avg loss -2.686326 avg loss no lamb -2.686326 time 2019-02-18 09:27:43.251335
Model ind 640 epoch 185 head A head_i_epoch 0 batch 200: avg loss -2.769613 avg loss no lamb -2.769613 time 2019-02-18 09:30:06.783856
last batch sz 160
Model ind 640 epoch 185 head B head_i_epoch 0 batch 0: avg loss -1.486317 avg loss no lamb -1.486317 time 2019-02-18 09:31:51.134395
Model ind 640 epoch 185 head B head_i_epoch 0 batch 100: avg loss -1.508111 avg loss no lamb -1.508111 time 2019-02-18 09:34:13.376635
Model ind 640 epoch 185 head B head_i_epoch 0 batch 200: avg loss -1.501190 avg loss no lamb -1.501190 time 2019-02-18 09:36:35.436571
last batch sz 160
Model ind 640 epoch 185 head B head_i_epoch 1 batch 0: avg loss -1.447996 avg loss no lamb -1.447996 time 2019-02-18 09:38:18.783148
Model ind 640 epoch 185 head B head_i_epoch 1 batch 100: avg loss -1.436425 avg loss no lamb -1.436425 time 2019-02-18 09:40:40.710612
Model ind 640 epoch 185 head B head_i_epoch 1 batch 200: avg loss -1.514909 avg loss no lamb -1.514909 time 2019-02-18 09:43:02.576189
last batch sz 160
Pre: time 2019-02-18 09:45:09.526676: 
 	std: 0.053889047
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47996667, 0.58966666, 0.5897833, 0.58965, 0.47943333]
	train_accs: [0.47996667, 0.58966666, 0.5897833, 0.58965, 0.47943333]
	best_train_sub_head: 2
	worst: 0.47943333
	avg: 0.54569995
	best: 0.5897833

Starting e_i: 186
Model ind 640 epoch 186 head A head_i_epoch 0 batch 0: avg loss -2.805049 avg loss no lamb -2.805049 time 2019-02-18 09:45:11.614449
Model ind 640 epoch 186 head A head_i_epoch 0 batch 100: avg loss -2.745211 avg loss no lamb -2.745211 time 2019-02-18 09:47:33.342645
Model ind 640 epoch 186 head A head_i_epoch 0 batch 200: avg loss -2.794737 avg loss no lamb -2.794737 time 2019-02-18 09:49:55.463559
last batch sz 160
Model ind 640 epoch 186 head B head_i_epoch 0 batch 0: avg loss -1.502002 avg loss no lamb -1.502002 time 2019-02-18 09:51:38.960393
Model ind 640 epoch 186 head B head_i_epoch 0 batch 100: avg loss -1.315465 avg loss no lamb -1.315465 time 2019-02-18 09:54:01.636366
Model ind 640 epoch 186 head B head_i_epoch 0 batch 200: avg loss -1.536821 avg loss no lamb -1.536821 time 2019-02-18 09:56:24.685664
last batch sz 160
Model ind 640 epoch 186 head B head_i_epoch 1 batch 0: avg loss -1.469826 avg loss no lamb -1.469826 time 2019-02-18 09:58:08.700318
Model ind 640 epoch 186 head B head_i_epoch 1 batch 100: avg loss -1.407613 avg loss no lamb -1.407613 time 2019-02-18 10:00:31.956781
Model ind 640 epoch 186 head B head_i_epoch 1 batch 200: avg loss -1.520826 avg loss no lamb -1.520826 time 2019-02-18 10:02:55.276018
last batch sz 160
Pre: time 2019-02-18 10:05:03.334501: 
 	std: 0.053520363
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47261667, 0.58055, 0.58028334, 0.57925, 0.469]
	train_accs: [0.47261667, 0.58055, 0.58028334, 0.57925, 0.469]
	best_train_sub_head: 1
	worst: 0.469
	avg: 0.53634
	best: 0.58055

Starting e_i: 187
Model ind 640 epoch 187 head A head_i_epoch 0 batch 0: avg loss -2.786338 avg loss no lamb -2.786338 time 2019-02-18 10:05:05.247518
Model ind 640 epoch 187 head A head_i_epoch 0 batch 100: avg loss -2.720221 avg loss no lamb -2.720221 time 2019-02-18 10:07:27.480017
Model ind 640 epoch 187 head A head_i_epoch 0 batch 200: avg loss -2.773379 avg loss no lamb -2.773379 time 2019-02-18 10:09:50.720057
last batch sz 160
Model ind 640 epoch 187 head B head_i_epoch 0 batch 0: avg loss -1.487466 avg loss no lamb -1.487466 time 2019-02-18 10:11:35.021281
Model ind 640 epoch 187 head B head_i_epoch 0 batch 100: avg loss -1.346997 avg loss no lamb -1.346997 time 2019-02-18 10:13:58.616470
Model ind 640 epoch 187 head B head_i_epoch 0 batch 200: avg loss -1.570341 avg loss no lamb -1.570341 time 2019-02-18 10:16:20.841067
last batch sz 160
Model ind 640 epoch 187 head B head_i_epoch 1 batch 0: avg loss -1.507544 avg loss no lamb -1.507544 time 2019-02-18 10:18:05.086718
Model ind 640 epoch 187 head B head_i_epoch 1 batch 100: avg loss -1.337855 avg loss no lamb -1.337855 time 2019-02-18 10:20:28.000965
Model ind 640 epoch 187 head B head_i_epoch 1 batch 200: avg loss -1.661288 avg loss no lamb -1.661288 time 2019-02-18 10:22:50.589739
last batch sz 160
Pre: time 2019-02-18 10:24:57.828564: 
 	std: 0.052866224
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47265, 0.58098334, 0.58133334, 0.57993335, 0.47303334]
	train_accs: [0.47265, 0.58098334, 0.58133334, 0.57993335, 0.47303334]
	best_train_sub_head: 2
	worst: 0.47265
	avg: 0.5375867
	best: 0.58133334

Starting e_i: 188
Model ind 640 epoch 188 head A head_i_epoch 0 batch 0: avg loss -2.793897 avg loss no lamb -2.793897 time 2019-02-18 10:24:59.910098
Model ind 640 epoch 188 head A head_i_epoch 0 batch 100: avg loss -2.770098 avg loss no lamb -2.770098 time 2019-02-18 10:27:22.571264
Model ind 640 epoch 188 head A head_i_epoch 0 batch 200: avg loss -2.801014 avg loss no lamb -2.801014 time 2019-02-18 10:29:45.880184
last batch sz 160
Model ind 640 epoch 188 head B head_i_epoch 0 batch 0: avg loss -1.483557 avg loss no lamb -1.483557 time 2019-02-18 10:31:30.372037
Model ind 640 epoch 188 head B head_i_epoch 0 batch 100: avg loss -1.492692 avg loss no lamb -1.492692 time 2019-02-18 10:33:52.384923
Model ind 640 epoch 188 head B head_i_epoch 0 batch 200: avg loss -1.573578 avg loss no lamb -1.573578 time 2019-02-18 10:36:14.380535
last batch sz 160
Model ind 640 epoch 188 head B head_i_epoch 1 batch 0: avg loss -1.463268 avg loss no lamb -1.463268 time 2019-02-18 10:37:57.689197
Model ind 640 epoch 188 head B head_i_epoch 1 batch 100: avg loss -1.365028 avg loss no lamb -1.365028 time 2019-02-18 10:40:19.670515
Model ind 640 epoch 188 head B head_i_epoch 1 batch 200: avg loss -1.547764 avg loss no lamb -1.547764 time 2019-02-18 10:42:41.716090
last batch sz 160
Pre: time 2019-02-18 10:44:49.280515: 
 	std: 0.053650707
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47171667, 0.58068335, 0.5816333, 0.58061665, 0.47121668]
	train_accs: [0.47171667, 0.58068335, 0.5816333, 0.58061665, 0.47121668]
	best_train_sub_head: 2
	worst: 0.47121668
	avg: 0.53717333
	best: 0.5816333

Starting e_i: 189
Model ind 640 epoch 189 head A head_i_epoch 0 batch 0: avg loss -2.817269 avg loss no lamb -2.817269 time 2019-02-18 10:44:51.166289
Model ind 640 epoch 189 head A head_i_epoch 0 batch 100: avg loss -2.725304 avg loss no lamb -2.725304 time 2019-02-18 10:47:14.404216
Model ind 640 epoch 189 head A head_i_epoch 0 batch 200: avg loss -2.763780 avg loss no lamb -2.763780 time 2019-02-18 10:49:36.557063
last batch sz 160
Model ind 640 epoch 189 head B head_i_epoch 0 batch 0: avg loss -1.458594 avg loss no lamb -1.458594 time 2019-02-18 10:51:19.896267
Model ind 640 epoch 189 head B head_i_epoch 0 batch 100: avg loss -1.354352 avg loss no lamb -1.354352 time 2019-02-18 10:53:41.988105
Model ind 640 epoch 189 head B head_i_epoch 0 batch 200: avg loss -1.466365 avg loss no lamb -1.466365 time 2019-02-18 10:56:04.072380
last batch sz 160
Model ind 640 epoch 189 head B head_i_epoch 1 batch 0: avg loss -1.445667 avg loss no lamb -1.445667 time 2019-02-18 10:57:47.449685
Model ind 640 epoch 189 head B head_i_epoch 1 batch 100: avg loss -1.424881 avg loss no lamb -1.424881 time 2019-02-18 11:00:09.395974
Model ind 640 epoch 189 head B head_i_epoch 1 batch 200: avg loss -1.533239 avg loss no lamb -1.533239 time 2019-02-18 11:02:31.857838
last batch sz 160
Pre: time 2019-02-18 11:04:40.774113: 
 	std: 0.050116938
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47933334, 0.58141667, 0.58155, 0.5811833, 0.47883335]
	train_accs: [0.47933334, 0.58141667, 0.58155, 0.5811833, 0.47883335]
	best_train_sub_head: 2
	worst: 0.47883335
	avg: 0.5404633
	best: 0.58155

Starting e_i: 190
Model ind 640 epoch 190 head A head_i_epoch 0 batch 0: avg loss -2.764738 avg loss no lamb -2.764738 time 2019-02-18 11:04:42.959967
Model ind 640 epoch 190 head A head_i_epoch 0 batch 100: avg loss -2.704020 avg loss no lamb -2.704020 time 2019-02-18 11:07:05.706073
Model ind 640 epoch 190 head A head_i_epoch 0 batch 200: avg loss -2.820022 avg loss no lamb -2.820022 time 2019-02-18 11:09:28.756274
last batch sz 160
Model ind 640 epoch 190 head B head_i_epoch 0 batch 0: avg loss -1.561251 avg loss no lamb -1.561251 time 2019-02-18 11:11:12.074600
Model ind 640 epoch 190 head B head_i_epoch 0 batch 100: avg loss -1.403437 avg loss no lamb -1.403437 time 2019-02-18 11:13:35.095900
Model ind 640 epoch 190 head B head_i_epoch 0 batch 200: avg loss -1.501981 avg loss no lamb -1.501981 time 2019-02-18 11:15:58.547064
last batch sz 160
Model ind 640 epoch 190 head B head_i_epoch 1 batch 0: avg loss -1.510812 avg loss no lamb -1.510812 time 2019-02-18 11:17:42.786056
Model ind 640 epoch 190 head B head_i_epoch 1 batch 100: avg loss -1.391854 avg loss no lamb -1.391854 time 2019-02-18 11:20:04.925347
Model ind 640 epoch 190 head B head_i_epoch 1 batch 200: avg loss -1.581609 avg loss no lamb -1.581609 time 2019-02-18 11:22:27.851369
last batch sz 160
Pre: time 2019-02-18 11:24:36.266090: 
 	std: 0.05177042
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47403333, 0.57998335, 0.57926667, 0.57881665, 0.47333333]
	train_accs: [0.47403333, 0.57998335, 0.57926667, 0.57881665, 0.47333333]
	best_train_sub_head: 1
	worst: 0.47333333
	avg: 0.53708667
	best: 0.57998335

Starting e_i: 191
Model ind 640 epoch 191 head A head_i_epoch 0 batch 0: avg loss -2.764812 avg loss no lamb -2.764812 time 2019-02-18 11:24:42.330840
Model ind 640 epoch 191 head A head_i_epoch 0 batch 100: avg loss -2.724354 avg loss no lamb -2.724354 time 2019-02-18 11:27:04.338056
Model ind 640 epoch 191 head A head_i_epoch 0 batch 200: avg loss -2.785891 avg loss no lamb -2.785891 time 2019-02-18 11:29:26.482906
last batch sz 160
Model ind 640 epoch 191 head B head_i_epoch 0 batch 0: avg loss -1.501144 avg loss no lamb -1.501144 time 2019-02-18 11:31:09.973079
Model ind 640 epoch 191 head B head_i_epoch 0 batch 100: avg loss -1.414852 avg loss no lamb -1.414852 time 2019-02-18 11:33:32.467374
Model ind 640 epoch 191 head B head_i_epoch 0 batch 200: avg loss -1.557588 avg loss no lamb -1.557588 time 2019-02-18 11:35:54.872700
last batch sz 160
Model ind 640 epoch 191 head B head_i_epoch 1 batch 0: avg loss -1.520807 avg loss no lamb -1.520807 time 2019-02-18 11:37:37.907467
Model ind 640 epoch 191 head B head_i_epoch 1 batch 100: avg loss -1.472850 avg loss no lamb -1.472850 time 2019-02-18 11:39:59.799778
Model ind 640 epoch 191 head B head_i_epoch 1 batch 200: avg loss -1.537087 avg loss no lamb -1.537087 time 2019-02-18 11:42:21.830021
last batch sz 160
Pre: time 2019-02-18 11:44:29.313681: 
 	std: 0.05334803
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4759, 0.58381665, 0.58496666, 0.5830167, 0.4742]
	train_accs: [0.4759, 0.58381665, 0.58496666, 0.5830167, 0.4742]
	best_train_sub_head: 2
	worst: 0.4742
	avg: 0.54038
	best: 0.58496666

Starting e_i: 192
Model ind 640 epoch 192 head A head_i_epoch 0 batch 0: avg loss -2.834121 avg loss no lamb -2.834121 time 2019-02-18 11:44:31.431904
Model ind 640 epoch 192 head A head_i_epoch 0 batch 100: avg loss -2.806820 avg loss no lamb -2.806820 time 2019-02-18 11:46:53.723239
Model ind 640 epoch 192 head A head_i_epoch 0 batch 200: avg loss -2.896978 avg loss no lamb -2.896978 time 2019-02-18 11:49:16.728915
last batch sz 160
Model ind 640 epoch 192 head B head_i_epoch 0 batch 0: avg loss -1.473414 avg loss no lamb -1.473414 time 2019-02-18 11:51:01.060958
Model ind 640 epoch 192 head B head_i_epoch 0 batch 100: avg loss -1.393044 avg loss no lamb -1.393044 time 2019-02-18 11:53:24.260158
Model ind 640 epoch 192 head B head_i_epoch 0 batch 200: avg loss -1.607780 avg loss no lamb -1.607780 time 2019-02-18 11:55:46.812053
last batch sz 160
Model ind 640 epoch 192 head B head_i_epoch 1 batch 0: avg loss -1.536263 avg loss no lamb -1.536263 time 2019-02-18 11:57:30.296471
Model ind 640 epoch 192 head B head_i_epoch 1 batch 100: avg loss -1.462388 avg loss no lamb -1.462388 time 2019-02-18 11:59:52.448983
Model ind 640 epoch 192 head B head_i_epoch 1 batch 200: avg loss -1.666488 avg loss no lamb -1.666488 time 2019-02-18 12:02:14.956171
last batch sz 160
Pre: time 2019-02-18 12:04:22.607629: 
 	std: 0.05130119
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47706667, 0.58073336, 0.5813, 0.58031666, 0.47508332]
	train_accs: [0.47706667, 0.58073336, 0.5813, 0.58031666, 0.47508332]
	best_train_sub_head: 2
	worst: 0.47508332
	avg: 0.5389
	best: 0.5813

Starting e_i: 193
Model ind 640 epoch 193 head A head_i_epoch 0 batch 0: avg loss -2.770346 avg loss no lamb -2.770346 time 2019-02-18 12:04:24.537026
Model ind 640 epoch 193 head A head_i_epoch 0 batch 100: avg loss -2.735441 avg loss no lamb -2.735441 time 2019-02-18 12:06:46.359232
Model ind 640 epoch 193 head A head_i_epoch 0 batch 200: avg loss -2.775603 avg loss no lamb -2.775603 time 2019-02-18 12:09:09.459791
last batch sz 160
Model ind 640 epoch 193 head B head_i_epoch 0 batch 0: avg loss -1.553820 avg loss no lamb -1.553820 time 2019-02-18 12:10:53.793143
Model ind 640 epoch 193 head B head_i_epoch 0 batch 100: avg loss -1.410662 avg loss no lamb -1.410662 time 2019-02-18 12:13:15.515027
Model ind 640 epoch 193 head B head_i_epoch 0 batch 200: avg loss -1.557267 avg loss no lamb -1.557267 time 2019-02-18 12:15:38.086832
last batch sz 160
Model ind 640 epoch 193 head B head_i_epoch 1 batch 0: avg loss -1.529559 avg loss no lamb -1.529559 time 2019-02-18 12:17:21.493382
Model ind 640 epoch 193 head B head_i_epoch 1 batch 100: avg loss -1.444025 avg loss no lamb -1.444025 time 2019-02-18 12:19:43.364809
Model ind 640 epoch 193 head B head_i_epoch 1 batch 200: avg loss -1.541229 avg loss no lamb -1.541229 time 2019-02-18 12:22:05.288022
last batch sz 160
Pre: time 2019-02-18 12:24:12.895279: 
 	std: 0.05396252
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46876666, 0.57918334, 0.58023334, 0.57876664, 0.46973333]
	train_accs: [0.46876666, 0.57918334, 0.58023334, 0.57876664, 0.46973333]
	best_train_sub_head: 2
	worst: 0.46876666
	avg: 0.5353366
	best: 0.58023334

Starting e_i: 194
Model ind 640 epoch 194 head A head_i_epoch 0 batch 0: avg loss -2.768770 avg loss no lamb -2.768770 time 2019-02-18 12:24:15.071941
Model ind 640 epoch 194 head A head_i_epoch 0 batch 100: avg loss -2.716523 avg loss no lamb -2.716523 time 2019-02-18 12:26:37.238783
Model ind 640 epoch 194 head A head_i_epoch 0 batch 200: avg loss -2.849191 avg loss no lamb -2.849191 time 2019-02-18 12:28:59.491337
last batch sz 160
Model ind 640 epoch 194 head B head_i_epoch 0 batch 0: avg loss -1.473670 avg loss no lamb -1.473670 time 2019-02-18 12:30:42.728979
Model ind 640 epoch 194 head B head_i_epoch 0 batch 100: avg loss -1.434856 avg loss no lamb -1.434856 time 2019-02-18 12:33:05.069780
Model ind 640 epoch 194 head B head_i_epoch 0 batch 200: avg loss -1.588215 avg loss no lamb -1.588215 time 2019-02-18 12:35:28.289217
last batch sz 160
Model ind 640 epoch 194 head B head_i_epoch 1 batch 0: avg loss -1.467181 avg loss no lamb -1.467181 time 2019-02-18 12:37:12.397713
Model ind 640 epoch 194 head B head_i_epoch 1 batch 100: avg loss -1.280712 avg loss no lamb -1.280712 time 2019-02-18 12:39:35.517261
Model ind 640 epoch 194 head B head_i_epoch 1 batch 200: avg loss -1.646077 avg loss no lamb -1.646077 time 2019-02-18 12:41:58.296605
last batch sz 160
Pre: time 2019-02-18 12:44:06.782300: 
 	std: 0.05563453
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47276667, 0.58675, 0.5869833, 0.586, 0.47326666]
	train_accs: [0.47276667, 0.58675, 0.5869833, 0.586, 0.47326666]
	best_train_sub_head: 2
	worst: 0.47276667
	avg: 0.5411533
	best: 0.5869833

Starting e_i: 195
Model ind 640 epoch 195 head A head_i_epoch 0 batch 0: avg loss -2.795612 avg loss no lamb -2.795612 time 2019-02-18 12:44:08.741501
Model ind 640 epoch 195 head A head_i_epoch 0 batch 100: avg loss -2.729112 avg loss no lamb -2.729112 time 2019-02-18 12:46:30.519954
Model ind 640 epoch 195 head A head_i_epoch 0 batch 200: avg loss -2.801254 avg loss no lamb -2.801254 time 2019-02-18 12:48:52.078407
last batch sz 160
Model ind 640 epoch 195 head B head_i_epoch 0 batch 0: avg loss -1.544577 avg loss no lamb -1.544577 time 2019-02-18 12:50:35.944327
Model ind 640 epoch 195 head B head_i_epoch 0 batch 100: avg loss -1.474535 avg loss no lamb -1.474535 time 2019-02-18 12:52:58.330768
Model ind 640 epoch 195 head B head_i_epoch 0 batch 200: avg loss -1.582057 avg loss no lamb -1.582057 time 2019-02-18 12:55:20.859389
last batch sz 160
Model ind 640 epoch 195 head B head_i_epoch 1 batch 0: avg loss -1.518779 avg loss no lamb -1.518779 time 2019-02-18 12:57:04.868948
Model ind 640 epoch 195 head B head_i_epoch 1 batch 100: avg loss -1.454457 avg loss no lamb -1.454457 time 2019-02-18 12:59:26.847658
Model ind 640 epoch 195 head B head_i_epoch 1 batch 200: avg loss -1.477798 avg loss no lamb -1.477798 time 2019-02-18 13:01:48.957681
last batch sz 160
Pre: time 2019-02-18 13:03:56.800822: 
 	std: 0.055253983
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4708, 0.5845, 0.58386666, 0.5832833, 0.4714]
	train_accs: [0.4708, 0.5845, 0.58386666, 0.5832833, 0.4714]
	best_train_sub_head: 1
	worst: 0.4708
	avg: 0.53876996
	best: 0.5845

Starting e_i: 196
Model ind 640 epoch 196 head A head_i_epoch 0 batch 0: avg loss -2.795990 avg loss no lamb -2.795990 time 2019-02-18 13:03:58.956085
Model ind 640 epoch 196 head A head_i_epoch 0 batch 100: avg loss -2.666753 avg loss no lamb -2.666753 time 2019-02-18 13:06:22.378696
Model ind 640 epoch 196 head A head_i_epoch 0 batch 200: avg loss -2.802706 avg loss no lamb -2.802706 time 2019-02-18 13:08:46.223743
last batch sz 160
Model ind 640 epoch 196 head B head_i_epoch 0 batch 0: avg loss -1.460376 avg loss no lamb -1.460376 time 2019-02-18 13:10:30.855861
Model ind 640 epoch 196 head B head_i_epoch 0 batch 100: avg loss -1.379827 avg loss no lamb -1.379827 time 2019-02-18 13:12:54.241169
Model ind 640 epoch 196 head B head_i_epoch 0 batch 200: avg loss -1.591467 avg loss no lamb -1.591467 time 2019-02-18 13:15:18.090532
last batch sz 160
Model ind 640 epoch 196 head B head_i_epoch 1 batch 0: avg loss -1.400895 avg loss no lamb -1.400895 time 2019-02-18 13:17:02.077375
Model ind 640 epoch 196 head B head_i_epoch 1 batch 100: avg loss -1.396882 avg loss no lamb -1.396882 time 2019-02-18 13:19:24.459331
Model ind 640 epoch 196 head B head_i_epoch 1 batch 200: avg loss -1.568340 avg loss no lamb -1.568340 time 2019-02-18 13:21:46.432334
last batch sz 160
Pre: time 2019-02-18 13:23:54.290385: 
 	std: 0.054623637
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47361666, 0.58535, 0.5843833, 0.58458334, 0.47293332]
	train_accs: [0.47361666, 0.58535, 0.5843833, 0.58458334, 0.47293332]
	best_train_sub_head: 1
	worst: 0.47293332
	avg: 0.5401733
	best: 0.58535

Starting e_i: 197
Model ind 640 epoch 197 head A head_i_epoch 0 batch 0: avg loss -2.803015 avg loss no lamb -2.803015 time 2019-02-18 13:23:56.314709
Model ind 640 epoch 197 head A head_i_epoch 0 batch 100: avg loss -2.754091 avg loss no lamb -2.754091 time 2019-02-18 13:26:20.320267
Model ind 640 epoch 197 head A head_i_epoch 0 batch 200: avg loss -2.805785 avg loss no lamb -2.805785 time 2019-02-18 13:28:43.937518
last batch sz 160
Model ind 640 epoch 197 head B head_i_epoch 0 batch 0: avg loss -1.510332 avg loss no lamb -1.510332 time 2019-02-18 13:30:28.375517
Model ind 640 epoch 197 head B head_i_epoch 0 batch 100: avg loss -1.436157 avg loss no lamb -1.436157 time 2019-02-18 13:32:51.832730
Model ind 640 epoch 197 head B head_i_epoch 0 batch 200: avg loss -1.553582 avg loss no lamb -1.553582 time 2019-02-18 13:35:14.998989
last batch sz 160
Model ind 640 epoch 197 head B head_i_epoch 1 batch 0: avg loss -1.517879 avg loss no lamb -1.517879 time 2019-02-18 13:36:58.382335
Model ind 640 epoch 197 head B head_i_epoch 1 batch 100: avg loss -1.472427 avg loss no lamb -1.472427 time 2019-02-18 13:39:21.328484
Model ind 640 epoch 197 head B head_i_epoch 1 batch 200: avg loss -1.541994 avg loss no lamb -1.541994 time 2019-02-18 13:41:44.294362
last batch sz 160
Pre: time 2019-02-18 13:43:53.943138: 
 	std: 0.05492067
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4699, 0.58315, 0.5829, 0.58358335, 0.47233334]
	train_accs: [0.4699, 0.58315, 0.5829, 0.58358335, 0.47233334]
	best_train_sub_head: 3
	worst: 0.4699
	avg: 0.53837335
	best: 0.58358335

Starting e_i: 198
Model ind 640 epoch 198 head A head_i_epoch 0 batch 0: avg loss -2.865898 avg loss no lamb -2.865898 time 2019-02-18 13:43:56.173699
Model ind 640 epoch 198 head A head_i_epoch 0 batch 100: avg loss -2.766442 avg loss no lamb -2.766442 time 2019-02-18 13:46:19.332450
Model ind 640 epoch 198 head A head_i_epoch 0 batch 200: avg loss -2.839663 avg loss no lamb -2.839663 time 2019-02-18 13:48:42.683481
last batch sz 160
Model ind 640 epoch 198 head B head_i_epoch 0 batch 0: avg loss -1.509345 avg loss no lamb -1.509345 time 2019-02-18 13:50:26.983523
Model ind 640 epoch 198 head B head_i_epoch 0 batch 100: avg loss -1.460071 avg loss no lamb -1.460071 time 2019-02-18 13:52:49.899662
Model ind 640 epoch 198 head B head_i_epoch 0 batch 200: avg loss -1.566065 avg loss no lamb -1.566065 time 2019-02-18 13:55:13.131771
last batch sz 160
Model ind 640 epoch 198 head B head_i_epoch 1 batch 0: avg loss -1.512663 avg loss no lamb -1.512663 time 2019-02-18 13:56:57.206315
Model ind 640 epoch 198 head B head_i_epoch 1 batch 100: avg loss -1.333887 avg loss no lamb -1.333887 time 2019-02-18 13:59:20.345750
Model ind 640 epoch 198 head B head_i_epoch 1 batch 200: avg loss -1.564269 avg loss no lamb -1.564269 time 2019-02-18 14:01:43.228809
last batch sz 160
Pre: time 2019-02-18 14:03:51.544783: 
 	std: 0.054088634
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4717, 0.58285, 0.58346665, 0.58288336, 0.47363332]
	train_accs: [0.4717, 0.58285, 0.58346665, 0.58288336, 0.47363332]
	best_train_sub_head: 2
	worst: 0.4717
	avg: 0.5389067
	best: 0.58346665

Starting e_i: 199
Model ind 640 epoch 199 head A head_i_epoch 0 batch 0: avg loss -2.793681 avg loss no lamb -2.793681 time 2019-02-18 14:03:53.500765
Model ind 640 epoch 199 head A head_i_epoch 0 batch 100: avg loss -2.707959 avg loss no lamb -2.707959 time 2019-02-18 14:06:16.553881
Model ind 640 epoch 199 head A head_i_epoch 0 batch 200: avg loss -2.785747 avg loss no lamb -2.785747 time 2019-02-18 14:08:38.740143
last batch sz 160
Model ind 640 epoch 199 head B head_i_epoch 0 batch 0: avg loss -1.427010 avg loss no lamb -1.427010 time 2019-02-18 14:10:22.740762
Model ind 640 epoch 199 head B head_i_epoch 0 batch 100: avg loss -1.461067 avg loss no lamb -1.461067 time 2019-02-18 14:12:44.613178
Model ind 640 epoch 199 head B head_i_epoch 0 batch 200: avg loss -1.482071 avg loss no lamb -1.482071 time 2019-02-18 14:15:06.496492
last batch sz 160
Model ind 640 epoch 199 head B head_i_epoch 1 batch 0: avg loss -1.497437 avg loss no lamb -1.497437 time 2019-02-18 14:16:49.350882
Model ind 640 epoch 199 head B head_i_epoch 1 batch 100: avg loss -1.457526 avg loss no lamb -1.457526 time 2019-02-18 14:19:10.702435
Model ind 640 epoch 199 head B head_i_epoch 1 batch 200: avg loss -1.586141 avg loss no lamb -1.586141 time 2019-02-18 14:21:32.454942
last batch sz 160
Pre: time 2019-02-18 14:23:41.163191: 
 	std: 0.054629408
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46483332, 0.5762333, 0.57708335, 0.5757333, 0.46485]
	train_accs: [0.46483332, 0.5762333, 0.57708335, 0.5757333, 0.46485]
	best_train_sub_head: 2
	worst: 0.46483332
	avg: 0.5317467
	best: 0.57708335

Starting e_i: 200
Model ind 640 epoch 200 head A head_i_epoch 0 batch 0: avg loss -2.688195 avg loss no lamb -2.688195 time 2019-02-18 14:23:43.337047
Model ind 640 epoch 200 head A head_i_epoch 0 batch 100: avg loss -2.687085 avg loss no lamb -2.687085 time 2019-02-18 14:26:07.081698
Model ind 640 epoch 200 head A head_i_epoch 0 batch 200: avg loss -2.804106 avg loss no lamb -2.804106 time 2019-02-18 14:28:30.189270
last batch sz 160
Model ind 640 epoch 200 head B head_i_epoch 0 batch 0: avg loss -1.423578 avg loss no lamb -1.423578 time 2019-02-18 14:30:14.375545
Model ind 640 epoch 200 head B head_i_epoch 0 batch 100: avg loss -1.489110 avg loss no lamb -1.489110 time 2019-02-18 14:32:36.938983
Model ind 640 epoch 200 head B head_i_epoch 0 batch 200: avg loss -1.532412 avg loss no lamb -1.532412 time 2019-02-18 14:34:59.623672
last batch sz 160
Model ind 640 epoch 200 head B head_i_epoch 1 batch 0: avg loss -1.452813 avg loss no lamb -1.452813 time 2019-02-18 14:36:43.521117
Model ind 640 epoch 200 head B head_i_epoch 1 batch 100: avg loss -1.423044 avg loss no lamb -1.423044 time 2019-02-18 14:39:05.623837
Model ind 640 epoch 200 head B head_i_epoch 1 batch 200: avg loss -1.593060 avg loss no lamb -1.593060 time 2019-02-18 14:41:27.767489
last batch sz 160
Pre: time 2019-02-18 14:43:35.747690: 
 	std: 0.05383721
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47981668, 0.58956665, 0.58986664, 0.5895, 0.47968334]
	train_accs: [0.47981668, 0.58956665, 0.58986664, 0.5895, 0.47968334]
	best_train_sub_head: 2
	worst: 0.47968334
	avg: 0.54568666
	best: 0.58986664

Starting e_i: 201
Model ind 640 epoch 201 head A head_i_epoch 0 batch 0: avg loss -2.734893 avg loss no lamb -2.734893 time 2019-02-18 14:43:42.046027
Model ind 640 epoch 201 head A head_i_epoch 0 batch 100: avg loss -2.740659 avg loss no lamb -2.740659 time 2019-02-18 14:46:05.267263
Model ind 640 epoch 201 head A head_i_epoch 0 batch 200: avg loss -2.818633 avg loss no lamb -2.818633 time 2019-02-18 14:48:28.362451
last batch sz 160
Model ind 640 epoch 201 head B head_i_epoch 0 batch 0: avg loss -1.562711 avg loss no lamb -1.562711 time 2019-02-18 14:50:12.384738
Model ind 640 epoch 201 head B head_i_epoch 0 batch 100: avg loss -1.491820 avg loss no lamb -1.491820 time 2019-02-18 14:52:35.571619
Model ind 640 epoch 201 head B head_i_epoch 0 batch 200: avg loss -1.640060 avg loss no lamb -1.640060 time 2019-02-18 14:54:57.718696
last batch sz 160
Model ind 640 epoch 201 head B head_i_epoch 1 batch 0: avg loss -1.569855 avg loss no lamb -1.569855 time 2019-02-18 14:56:41.049511
Model ind 640 epoch 201 head B head_i_epoch 1 batch 100: avg loss -1.405915 avg loss no lamb -1.405915 time 2019-02-18 14:59:02.988712
Model ind 640 epoch 201 head B head_i_epoch 1 batch 200: avg loss -1.586644 avg loss no lamb -1.586644 time 2019-02-18 15:01:25.727234
last batch sz 160
Pre: time 2019-02-18 15:03:33.829346: 
 	std: 0.05238257
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47053334, 0.5772, 0.57766664, 0.5775833, 0.47058332]
	train_accs: [0.47053334, 0.5772, 0.57766664, 0.5775833, 0.47058332]
	best_train_sub_head: 2
	worst: 0.47053334
	avg: 0.5347134
	best: 0.57766664

Starting e_i: 202
Model ind 640 epoch 202 head A head_i_epoch 0 batch 0: avg loss -2.851643 avg loss no lamb -2.851643 time 2019-02-18 15:03:35.976923
Model ind 640 epoch 202 head A head_i_epoch 0 batch 100: avg loss -2.721699 avg loss no lamb -2.721699 time 2019-02-18 15:05:57.503675
Model ind 640 epoch 202 head A head_i_epoch 0 batch 200: avg loss -2.846357 avg loss no lamb -2.846357 time 2019-02-18 15:08:20.005270
last batch sz 160
Model ind 640 epoch 202 head B head_i_epoch 0 batch 0: avg loss -1.539592 avg loss no lamb -1.539592 time 2019-02-18 15:10:03.491589
Model ind 640 epoch 202 head B head_i_epoch 0 batch 100: avg loss -1.477654 avg loss no lamb -1.477654 time 2019-02-18 15:12:25.624860
Model ind 640 epoch 202 head B head_i_epoch 0 batch 200: avg loss -1.600848 avg loss no lamb -1.600848 time 2019-02-18 15:14:47.582302
last batch sz 160
Model ind 640 epoch 202 head B head_i_epoch 1 batch 0: avg loss -1.510513 avg loss no lamb -1.510513 time 2019-02-18 15:16:31.013722
Model ind 640 epoch 202 head B head_i_epoch 1 batch 100: avg loss -1.458242 avg loss no lamb -1.458242 time 2019-02-18 15:18:53.430662
Model ind 640 epoch 202 head B head_i_epoch 1 batch 200: avg loss -1.533700 avg loss no lamb -1.533700 time 2019-02-18 15:21:16.518170
last batch sz 160
Pre: time 2019-02-18 15:23:24.876669: 
 	std: 0.053772278
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47325, 0.5826667, 0.58323336, 0.5826333, 0.47291666]
	train_accs: [0.47325, 0.5826667, 0.58323336, 0.5826333, 0.47291666]
	best_train_sub_head: 2
	worst: 0.47291666
	avg: 0.53894
	best: 0.58323336

Starting e_i: 203
Model ind 640 epoch 203 head A head_i_epoch 0 batch 0: avg loss -2.808455 avg loss no lamb -2.808455 time 2019-02-18 15:23:27.152919
Model ind 640 epoch 203 head A head_i_epoch 0 batch 100: avg loss -2.812368 avg loss no lamb -2.812368 time 2019-02-18 15:25:49.624836
Model ind 640 epoch 203 head A head_i_epoch 0 batch 200: avg loss -2.843837 avg loss no lamb -2.843837 time 2019-02-18 15:28:13.128682
last batch sz 160
Model ind 640 epoch 203 head B head_i_epoch 0 batch 0: avg loss -1.497118 avg loss no lamb -1.497118 time 2019-02-18 15:29:57.385983
Model ind 640 epoch 203 head B head_i_epoch 0 batch 100: avg loss -1.434048 avg loss no lamb -1.434048 time 2019-02-18 15:32:19.754148
Model ind 640 epoch 203 head B head_i_epoch 0 batch 200: avg loss -1.599469 avg loss no lamb -1.599469 time 2019-02-18 15:34:41.990039
last batch sz 160
Model ind 640 epoch 203 head B head_i_epoch 1 batch 0: avg loss -1.517667 avg loss no lamb -1.517667 time 2019-02-18 15:36:26.308823
Model ind 640 epoch 203 head B head_i_epoch 1 batch 100: avg loss -1.348101 avg loss no lamb -1.348101 time 2019-02-18 15:38:49.698759
Model ind 640 epoch 203 head B head_i_epoch 1 batch 200: avg loss -1.580273 avg loss no lamb -1.580273 time 2019-02-18 15:41:12.358178
last batch sz 160
Pre: time 2019-02-18 15:43:20.408414: 
 	std: 0.054083716
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47656667, 0.5869, 0.58708334, 0.5865333, 0.47631666]
	train_accs: [0.47656667, 0.5869, 0.58708334, 0.5865333, 0.47631666]
	best_train_sub_head: 2
	worst: 0.47631666
	avg: 0.54268
	best: 0.58708334

Starting e_i: 204
Model ind 640 epoch 204 head A head_i_epoch 0 batch 0: avg loss -2.844142 avg loss no lamb -2.844142 time 2019-02-18 15:43:22.576839
Model ind 640 epoch 204 head A head_i_epoch 0 batch 100: avg loss -2.785114 avg loss no lamb -2.785114 time 2019-02-18 15:45:45.907168
Model ind 640 epoch 204 head A head_i_epoch 0 batch 200: avg loss -2.788910 avg loss no lamb -2.788910 time 2019-02-18 15:48:08.847419
last batch sz 160
Model ind 640 epoch 204 head B head_i_epoch 0 batch 0: avg loss -1.496813 avg loss no lamb -1.496813 time 2019-02-18 15:49:52.991748
Model ind 640 epoch 204 head B head_i_epoch 0 batch 100: avg loss -1.416740 avg loss no lamb -1.416740 time 2019-02-18 15:52:16.015816
Model ind 640 epoch 204 head B head_i_epoch 0 batch 200: avg loss -1.510270 avg loss no lamb -1.510270 time 2019-02-18 15:54:39.039743
last batch sz 160
Model ind 640 epoch 204 head B head_i_epoch 1 batch 0: avg loss -1.502171 avg loss no lamb -1.502171 time 2019-02-18 15:56:23.264752
Model ind 640 epoch 204 head B head_i_epoch 1 batch 100: avg loss -1.341639 avg loss no lamb -1.341639 time 2019-02-18 15:58:45.763413
Model ind 640 epoch 204 head B head_i_epoch 1 batch 200: avg loss -1.614579 avg loss no lamb -1.614579 time 2019-02-18 16:01:08.747340
last batch sz 160
Pre: time 2019-02-18 16:03:16.950835: 
 	std: 0.055152748
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47483334, 0.5877, 0.58811665, 0.5879667, 0.47586668]
	train_accs: [0.47483334, 0.5877, 0.58811665, 0.5879667, 0.47586668]
	best_train_sub_head: 2
	worst: 0.47483334
	avg: 0.5428966
	best: 0.58811665

Starting e_i: 205
Model ind 640 epoch 205 head A head_i_epoch 0 batch 0: avg loss -2.798299 avg loss no lamb -2.798299 time 2019-02-18 16:03:18.948760
Model ind 640 epoch 205 head A head_i_epoch 0 batch 100: avg loss -2.776264 avg loss no lamb -2.776264 time 2019-02-18 16:05:42.112924
Model ind 640 epoch 205 head A head_i_epoch 0 batch 200: avg loss -2.879545 avg loss no lamb -2.879545 time 2019-02-18 16:08:06.128393
last batch sz 160
Model ind 640 epoch 205 head B head_i_epoch 0 batch 0: avg loss -1.529785 avg loss no lamb -1.529785 time 2019-02-18 16:09:51.106457
Model ind 640 epoch 205 head B head_i_epoch 0 batch 100: avg loss -1.429925 avg loss no lamb -1.429925 time 2019-02-18 16:12:14.101810
Model ind 640 epoch 205 head B head_i_epoch 0 batch 200: avg loss -1.501713 avg loss no lamb -1.501713 time 2019-02-18 16:14:36.508422
last batch sz 160
Model ind 640 epoch 205 head B head_i_epoch 1 batch 0: avg loss -1.501633 avg loss no lamb -1.501633 time 2019-02-18 16:16:20.298695
Model ind 640 epoch 205 head B head_i_epoch 1 batch 100: avg loss -1.350062 avg loss no lamb -1.350062 time 2019-02-18 16:18:42.770462
Model ind 640 epoch 205 head B head_i_epoch 1 batch 200: avg loss -1.532933 avg loss no lamb -1.532933 time 2019-02-18 16:21:05.297149
last batch sz 160
Pre: time 2019-02-18 16:23:13.274475: 
 	std: 0.05310126
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47586668, 0.5846, 0.5851167, 0.58495, 0.47713333]
	train_accs: [0.47586668, 0.5846, 0.5851167, 0.58495, 0.47713333]
	best_train_sub_head: 2
	worst: 0.47586668
	avg: 0.54153335
	best: 0.5851167

Starting e_i: 206
Model ind 640 epoch 206 head A head_i_epoch 0 batch 0: avg loss -2.795441 avg loss no lamb -2.795441 time 2019-02-18 16:23:15.435802
Model ind 640 epoch 206 head A head_i_epoch 0 batch 100: avg loss -2.728042 avg loss no lamb -2.728042 time 2019-02-18 16:25:37.956480
Model ind 640 epoch 206 head A head_i_epoch 0 batch 200: avg loss -2.813003 avg loss no lamb -2.813003 time 2019-02-18 16:28:00.624820
last batch sz 160
Model ind 640 epoch 206 head B head_i_epoch 0 batch 0: avg loss -1.422762 avg loss no lamb -1.422762 time 2019-02-18 16:29:44.394689
Model ind 640 epoch 206 head B head_i_epoch 0 batch 100: avg loss -1.500308 avg loss no lamb -1.500308 time 2019-02-18 16:32:06.440649
Model ind 640 epoch 206 head B head_i_epoch 0 batch 200: avg loss -1.573051 avg loss no lamb -1.573051 time 2019-02-18 16:34:28.466448
last batch sz 160
Model ind 640 epoch 206 head B head_i_epoch 1 batch 0: avg loss -1.453306 avg loss no lamb -1.453306 time 2019-02-18 16:36:11.700125
Model ind 640 epoch 206 head B head_i_epoch 1 batch 100: avg loss -1.493899 avg loss no lamb -1.493899 time 2019-02-18 16:38:33.707201
Model ind 640 epoch 206 head B head_i_epoch 1 batch 200: avg loss -1.615193 avg loss no lamb -1.615193 time 2019-02-18 16:40:56.377391
last batch sz 160
Pre: time 2019-02-18 16:43:04.512153: 
 	std: 0.05348433
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48435, 0.59291667, 0.5937167, 0.59245, 0.48336667]
	train_accs: [0.48435, 0.59291667, 0.5937167, 0.59245, 0.48336667]
	best_train_sub_head: 2
	worst: 0.48336667
	avg: 0.54936004
	best: 0.5937167

Starting e_i: 207
Model ind 640 epoch 207 head A head_i_epoch 0 batch 0: avg loss -2.851310 avg loss no lamb -2.851310 time 2019-02-18 16:43:11.127973
Model ind 640 epoch 207 head A head_i_epoch 0 batch 100: avg loss -2.739628 avg loss no lamb -2.739628 time 2019-02-18 16:45:33.975723
Model ind 640 epoch 207 head A head_i_epoch 0 batch 200: avg loss -2.871070 avg loss no lamb -2.871070 time 2019-02-18 16:47:56.102633
last batch sz 160
Model ind 640 epoch 207 head B head_i_epoch 0 batch 0: avg loss -1.439410 avg loss no lamb -1.439410 time 2019-02-18 16:49:39.537561
Model ind 640 epoch 207 head B head_i_epoch 0 batch 100: avg loss -1.414590 avg loss no lamb -1.414590 time 2019-02-18 16:52:01.707571
Model ind 640 epoch 207 head B head_i_epoch 0 batch 200: avg loss -1.578707 avg loss no lamb -1.578707 time 2019-02-18 16:54:24.369434
last batch sz 160
Model ind 640 epoch 207 head B head_i_epoch 1 batch 0: avg loss -1.504784 avg loss no lamb -1.504784 time 2019-02-18 16:56:07.623051
Model ind 640 epoch 207 head B head_i_epoch 1 batch 100: avg loss -1.440419 avg loss no lamb -1.440419 time 2019-02-18 16:58:29.877552
Model ind 640 epoch 207 head B head_i_epoch 1 batch 200: avg loss -1.482149 avg loss no lamb -1.482149 time 2019-02-18 17:00:52.031512
last batch sz 160
Pre: time 2019-02-18 17:02:59.266752: 
 	std: 0.055133574
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.46578333, 0.5774, 0.57781667, 0.57665, 0.46373335]
	train_accs: [0.46578333, 0.5774, 0.57781667, 0.57665, 0.46373335]
	best_train_sub_head: 2
	worst: 0.46373335
	avg: 0.5322767
	best: 0.57781667

Starting e_i: 208
Model ind 640 epoch 208 head A head_i_epoch 0 batch 0: avg loss -2.815735 avg loss no lamb -2.815735 time 2019-02-18 17:03:01.354911
Model ind 640 epoch 208 head A head_i_epoch 0 batch 100: avg loss -2.693951 avg loss no lamb -2.693951 time 2019-02-18 17:05:23.447681
Model ind 640 epoch 208 head A head_i_epoch 0 batch 200: avg loss -2.817325 avg loss no lamb -2.817325 time 2019-02-18 17:07:47.430060
last batch sz 160
Model ind 640 epoch 208 head B head_i_epoch 0 batch 0: avg loss -1.501606 avg loss no lamb -1.501606 time 2019-02-18 17:09:31.607305
Model ind 640 epoch 208 head B head_i_epoch 0 batch 100: avg loss -1.509432 avg loss no lamb -1.509432 time 2019-02-18 17:11:53.728173
Model ind 640 epoch 208 head B head_i_epoch 0 batch 200: avg loss -1.602755 avg loss no lamb -1.602755 time 2019-02-18 17:14:15.827123
last batch sz 160
Model ind 640 epoch 208 head B head_i_epoch 1 batch 0: avg loss -1.552629 avg loss no lamb -1.552629 time 2019-02-18 17:15:59.217778
Model ind 640 epoch 208 head B head_i_epoch 1 batch 100: avg loss -1.436386 avg loss no lamb -1.436386 time 2019-02-18 17:18:21.345514
Model ind 640 epoch 208 head B head_i_epoch 1 batch 200: avg loss -1.594034 avg loss no lamb -1.594034 time 2019-02-18 17:20:43.238594
last batch sz 160
Pre: time 2019-02-18 17:22:52.504859: 
 	std: 0.052285124
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47973335, 0.5857, 0.58596665, 0.58555, 0.4783]
	train_accs: [0.47973335, 0.5857, 0.58596665, 0.58555, 0.4783]
	best_train_sub_head: 2
	worst: 0.4783
	avg: 0.54305
	best: 0.58596665

Starting e_i: 209
Model ind 640 epoch 209 head A head_i_epoch 0 batch 0: avg loss -2.824242 avg loss no lamb -2.824242 time 2019-02-18 17:22:54.507306
Model ind 640 epoch 209 head A head_i_epoch 0 batch 100: avg loss -2.702259 avg loss no lamb -2.702259 time 2019-02-18 17:25:18.630704
Model ind 640 epoch 209 head A head_i_epoch 0 batch 200: avg loss -2.788113 avg loss no lamb -2.788113 time 2019-02-18 17:27:42.294041
last batch sz 160
Model ind 640 epoch 209 head B head_i_epoch 0 batch 0: avg loss -1.513602 avg loss no lamb -1.513602 time 2019-02-18 17:29:26.396785
Model ind 640 epoch 209 head B head_i_epoch 0 batch 100: avg loss -1.420459 avg loss no lamb -1.420459 time 2019-02-18 17:31:49.109631
Model ind 640 epoch 209 head B head_i_epoch 0 batch 200: avg loss -1.583916 avg loss no lamb -1.583916 time 2019-02-18 17:34:12.202265
last batch sz 160
Model ind 640 epoch 209 head B head_i_epoch 1 batch 0: avg loss -1.502666 avg loss no lamb -1.502666 time 2019-02-18 17:35:56.280687
Model ind 640 epoch 209 head B head_i_epoch 1 batch 100: avg loss -1.443012 avg loss no lamb -1.443012 time 2019-02-18 17:38:18.574725
Model ind 640 epoch 209 head B head_i_epoch 1 batch 200: avg loss -1.550616 avg loss no lamb -1.550616 time 2019-02-18 17:40:41.271623
last batch sz 160
Pre: time 2019-02-18 17:42:49.159155: 
 	std: 0.054542873
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.46425, 0.5756, 0.5756, 0.57485, 0.46378332]
	train_accs: [0.46425, 0.5756, 0.5756, 0.57485, 0.46378332]
	best_train_sub_head: 1
	worst: 0.46378332
	avg: 0.5308167
	best: 0.5756

Starting e_i: 210
Model ind 640 epoch 210 head A head_i_epoch 0 batch 0: avg loss -2.794471 avg loss no lamb -2.794471 time 2019-02-18 17:42:51.361882
Model ind 640 epoch 210 head A head_i_epoch 0 batch 100: avg loss -2.731558 avg loss no lamb -2.731558 time 2019-02-18 17:45:12.982002
Model ind 640 epoch 210 head A head_i_epoch 0 batch 200: avg loss -2.853631 avg loss no lamb -2.853631 time 2019-02-18 17:47:36.576629
last batch sz 160
Model ind 640 epoch 210 head B head_i_epoch 0 batch 0: avg loss -1.472418 avg loss no lamb -1.472418 time 2019-02-18 17:49:21.348519
Model ind 640 epoch 210 head B head_i_epoch 0 batch 100: avg loss -1.461361 avg loss no lamb -1.461361 time 2019-02-18 17:51:45.366491
Model ind 640 epoch 210 head B head_i_epoch 0 batch 200: avg loss -1.516694 avg loss no lamb -1.516694 time 2019-02-18 17:54:08.243854
last batch sz 160
Model ind 640 epoch 210 head B head_i_epoch 1 batch 0: avg loss -1.518982 avg loss no lamb -1.518982 time 2019-02-18 17:55:51.915956
Model ind 640 epoch 210 head B head_i_epoch 1 batch 100: avg loss -1.438490 avg loss no lamb -1.438490 time 2019-02-18 17:58:14.963239
Model ind 640 epoch 210 head B head_i_epoch 1 batch 200: avg loss -1.660121 avg loss no lamb -1.660121 time 2019-02-18 18:00:37.269557
last batch sz 160
Pre: time 2019-02-18 18:02:44.686735: 
 	std: 0.053709645
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47221667, 0.58136666, 0.58095, 0.5812, 0.47086668]
	train_accs: [0.47221667, 0.58136666, 0.58095, 0.5812, 0.47086668]
	best_train_sub_head: 1
	worst: 0.47086668
	avg: 0.53732
	best: 0.58136666

Starting e_i: 211
Model ind 640 epoch 211 head A head_i_epoch 0 batch 0: avg loss -2.819627 avg loss no lamb -2.819627 time 2019-02-18 18:02:50.384162
Model ind 640 epoch 211 head A head_i_epoch 0 batch 100: avg loss -2.791274 avg loss no lamb -2.791274 time 2019-02-18 18:05:13.699958
Model ind 640 epoch 211 head A head_i_epoch 0 batch 200: avg loss -2.905365 avg loss no lamb -2.905365 time 2019-02-18 18:07:36.384658
last batch sz 160
Model ind 640 epoch 211 head B head_i_epoch 0 batch 0: avg loss -1.498462 avg loss no lamb -1.498462 time 2019-02-18 18:09:19.932235
Model ind 640 epoch 211 head B head_i_epoch 0 batch 100: avg loss -1.445762 avg loss no lamb -1.445762 time 2019-02-18 18:11:42.737174
Model ind 640 epoch 211 head B head_i_epoch 0 batch 200: avg loss -1.481325 avg loss no lamb -1.481325 time 2019-02-18 18:14:06.374211
last batch sz 160
Model ind 640 epoch 211 head B head_i_epoch 1 batch 0: avg loss -1.544026 avg loss no lamb -1.544026 time 2019-02-18 18:15:49.863485
Model ind 640 epoch 211 head B head_i_epoch 1 batch 100: avg loss -1.441866 avg loss no lamb -1.441866 time 2019-02-18 18:18:12.529162
Model ind 640 epoch 211 head B head_i_epoch 1 batch 200: avg loss -1.542206 avg loss no lamb -1.542206 time 2019-02-18 18:20:36.365955
last batch sz 160
Pre: time 2019-02-18 18:22:44.842788: 
 	std: 0.052418366
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47095, 0.57745, 0.57825, 0.57781667, 0.47073334]
	train_accs: [0.47095, 0.57745, 0.57825, 0.57781667, 0.47073334]
	best_train_sub_head: 2
	worst: 0.47073334
	avg: 0.53504
	best: 0.57825

Starting e_i: 212
Model ind 640 epoch 212 head A head_i_epoch 0 batch 0: avg loss -2.885441 avg loss no lamb -2.885441 time 2019-02-18 18:22:46.977877
Model ind 640 epoch 212 head A head_i_epoch 0 batch 100: avg loss -2.797402 avg loss no lamb -2.797402 time 2019-02-18 18:25:09.541498
Model ind 640 epoch 212 head A head_i_epoch 0 batch 200: avg loss -2.769035 avg loss no lamb -2.769035 time 2019-02-18 18:27:33.504130
last batch sz 160
Model ind 640 epoch 212 head B head_i_epoch 0 batch 0: avg loss -1.545238 avg loss no lamb -1.545238 time 2019-02-18 18:29:17.899576
Model ind 640 epoch 212 head B head_i_epoch 0 batch 100: avg loss -1.470449 avg loss no lamb -1.470449 time 2019-02-18 18:31:41.317415
Model ind 640 epoch 212 head B head_i_epoch 0 batch 200: avg loss -1.598243 avg loss no lamb -1.598243 time 2019-02-18 18:34:03.479385
last batch sz 160
Model ind 640 epoch 212 head B head_i_epoch 1 batch 0: avg loss -1.518548 avg loss no lamb -1.518548 time 2019-02-18 18:35:47.234698
Model ind 640 epoch 212 head B head_i_epoch 1 batch 100: avg loss -1.408526 avg loss no lamb -1.408526 time 2019-02-18 18:38:10.416589
Model ind 640 epoch 212 head B head_i_epoch 1 batch 200: avg loss -1.552536 avg loss no lamb -1.552536 time 2019-02-18 18:40:34.201506
last batch sz 160
Pre: time 2019-02-18 18:42:42.476942: 
 	std: 0.053657144
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47571668, 0.5844167, 0.5851333, 0.58421665, 0.47441667]
	train_accs: [0.47571668, 0.5844167, 0.5851333, 0.58421665, 0.47441667]
	best_train_sub_head: 2
	worst: 0.47441667
	avg: 0.54078
	best: 0.5851333

Starting e_i: 213
Model ind 640 epoch 213 head A head_i_epoch 0 batch 0: avg loss -2.816580 avg loss no lamb -2.816580 time 2019-02-18 18:42:44.343375
Model ind 640 epoch 213 head A head_i_epoch 0 batch 100: avg loss -2.761839 avg loss no lamb -2.761839 time 2019-02-18 18:45:06.628105
Model ind 640 epoch 213 head A head_i_epoch 0 batch 200: avg loss -2.800008 avg loss no lamb -2.800008 time 2019-02-18 18:47:29.386849
last batch sz 160
Model ind 640 epoch 213 head B head_i_epoch 0 batch 0: avg loss -1.455535 avg loss no lamb -1.455535 time 2019-02-18 18:49:13.225770
Model ind 640 epoch 213 head B head_i_epoch 0 batch 100: avg loss -1.458173 avg loss no lamb -1.458173 time 2019-02-18 18:51:35.916520
Model ind 640 epoch 213 head B head_i_epoch 0 batch 200: avg loss -1.506609 avg loss no lamb -1.506609 time 2019-02-18 18:53:58.410710
last batch sz 160
Model ind 640 epoch 213 head B head_i_epoch 1 batch 0: avg loss -1.445476 avg loss no lamb -1.445476 time 2019-02-18 18:55:41.975187
Model ind 640 epoch 213 head B head_i_epoch 1 batch 100: avg loss -1.499166 avg loss no lamb -1.499166 time 2019-02-18 18:58:04.271592
Model ind 640 epoch 213 head B head_i_epoch 1 batch 200: avg loss -1.605198 avg loss no lamb -1.605198 time 2019-02-18 19:00:26.457414
last batch sz 160
Pre: time 2019-02-18 19:02:34.321331: 
 	std: 0.054292914
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47733334, 0.58891666, 0.5887167, 0.5880833, 0.47816667]
	train_accs: [0.47733334, 0.58891666, 0.5887167, 0.5880833, 0.47816667]
	best_train_sub_head: 1
	worst: 0.47733334
	avg: 0.5442433
	best: 0.58891666

Starting e_i: 214
Model ind 640 epoch 214 head A head_i_epoch 0 batch 0: avg loss -2.788092 avg loss no lamb -2.788092 time 2019-02-18 19:02:36.422256
Model ind 640 epoch 214 head A head_i_epoch 0 batch 100: avg loss -2.697037 avg loss no lamb -2.697037 time 2019-02-18 19:04:58.672989
Model ind 640 epoch 214 head A head_i_epoch 0 batch 200: avg loss -2.859702 avg loss no lamb -2.859702 time 2019-02-18 19:07:21.279794
last batch sz 160
Model ind 640 epoch 214 head B head_i_epoch 0 batch 0: avg loss -1.537075 avg loss no lamb -1.537075 time 2019-02-18 19:09:05.488821
Model ind 640 epoch 214 head B head_i_epoch 0 batch 100: avg loss -1.502884 avg loss no lamb -1.502884 time 2019-02-18 19:11:28.444267
Model ind 640 epoch 214 head B head_i_epoch 0 batch 200: avg loss -1.569360 avg loss no lamb -1.569360 time 2019-02-18 19:13:51.445175
last batch sz 160
Model ind 640 epoch 214 head B head_i_epoch 1 batch 0: avg loss -1.557789 avg loss no lamb -1.557789 time 2019-02-18 19:15:35.018223
Model ind 640 epoch 214 head B head_i_epoch 1 batch 100: avg loss -1.535626 avg loss no lamb -1.535626 time 2019-02-18 19:17:57.313101
Model ind 640 epoch 214 head B head_i_epoch 1 batch 200: avg loss -1.586842 avg loss no lamb -1.586842 time 2019-02-18 19:20:19.494593
last batch sz 160
Pre: time 2019-02-18 19:22:28.505837: 
 	std: 0.053861875
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47486666, 0.58498335, 0.58561665, 0.58555, 0.47601667]
	train_accs: [0.47486666, 0.58498335, 0.58561665, 0.58555, 0.47601667]
	best_train_sub_head: 2
	worst: 0.47486666
	avg: 0.5414067
	best: 0.58561665

Starting e_i: 215
Model ind 640 epoch 215 head A head_i_epoch 0 batch 0: avg loss -2.845219 avg loss no lamb -2.845219 time 2019-02-18 19:22:30.433164
Model ind 640 epoch 215 head A head_i_epoch 0 batch 100: avg loss -2.776976 avg loss no lamb -2.776976 time 2019-02-18 19:24:54.341666
Model ind 640 epoch 215 head A head_i_epoch 0 batch 200: avg loss -2.822975 avg loss no lamb -2.822975 time 2019-02-18 19:27:17.817661
last batch sz 160
Model ind 640 epoch 215 head B head_i_epoch 0 batch 0: avg loss -1.611864 avg loss no lamb -1.611864 time 2019-02-18 19:29:02.074195
Model ind 640 epoch 215 head B head_i_epoch 0 batch 100: avg loss -1.480778 avg loss no lamb -1.480778 time 2019-02-18 19:31:24.258915
Model ind 640 epoch 215 head B head_i_epoch 0 batch 200: avg loss -1.513252 avg loss no lamb -1.513252 time 2019-02-18 19:33:46.109550
last batch sz 160
Model ind 640 epoch 215 head B head_i_epoch 1 batch 0: avg loss -1.505718 avg loss no lamb -1.505718 time 2019-02-18 19:35:28.994372
Model ind 640 epoch 215 head B head_i_epoch 1 batch 100: avg loss -1.391764 avg loss no lamb -1.391764 time 2019-02-18 19:37:50.949494
Model ind 640 epoch 215 head B head_i_epoch 1 batch 200: avg loss -1.588269 avg loss no lamb -1.588269 time 2019-02-18 19:40:13.424097
last batch sz 160
Pre: time 2019-02-18 19:42:20.991433: 
 	std: 0.054396
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4754, 0.5847167, 0.5854333, 0.5840833, 0.47206667]
	train_accs: [0.4754, 0.5847167, 0.5854333, 0.5840833, 0.47206667]
	best_train_sub_head: 2
	worst: 0.47206667
	avg: 0.54034
	best: 0.5854333

Starting e_i: 216
Model ind 640 epoch 216 head A head_i_epoch 0 batch 0: avg loss -2.833978 avg loss no lamb -2.833978 time 2019-02-18 19:42:23.395995
Model ind 640 epoch 216 head A head_i_epoch 0 batch 100: avg loss -2.796804 avg loss no lamb -2.796804 time 2019-02-18 19:44:45.505753
Model ind 640 epoch 216 head A head_i_epoch 0 batch 200: avg loss -2.915253 avg loss no lamb -2.915253 time 2019-02-18 19:47:08.354114
last batch sz 160
Model ind 640 epoch 216 head B head_i_epoch 0 batch 0: avg loss -1.509554 avg loss no lamb -1.509554 time 2019-02-18 19:48:52.964805
Model ind 640 epoch 216 head B head_i_epoch 0 batch 100: avg loss -1.463037 avg loss no lamb -1.463037 time 2019-02-18 19:51:16.527761
Model ind 640 epoch 216 head B head_i_epoch 0 batch 200: avg loss -1.514699 avg loss no lamb -1.514699 time 2019-02-18 19:53:40.063995
last batch sz 160
Model ind 640 epoch 216 head B head_i_epoch 1 batch 0: avg loss -1.451318 avg loss no lamb -1.451318 time 2019-02-18 19:55:24.839660
Model ind 640 epoch 216 head B head_i_epoch 1 batch 100: avg loss -1.430014 avg loss no lamb -1.430014 time 2019-02-18 19:57:48.824237
Model ind 640 epoch 216 head B head_i_epoch 1 batch 200: avg loss -1.538455 avg loss no lamb -1.538455 time 2019-02-18 20:00:12.200082
last batch sz 160
Pre: time 2019-02-18 20:02:20.993823: 
 	std: 0.05394096
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47806665, 0.5872833, 0.5876833, 0.58751667, 0.47671667]
	train_accs: [0.47806665, 0.5872833, 0.5876833, 0.58751667, 0.47671667]
	best_train_sub_head: 2
	worst: 0.47671667
	avg: 0.54345334
	best: 0.5876833

Starting e_i: 217
Model ind 640 epoch 217 head A head_i_epoch 0 batch 0: avg loss -2.804977 avg loss no lamb -2.804977 time 2019-02-18 20:02:22.867926
Model ind 640 epoch 217 head A head_i_epoch 0 batch 100: avg loss -2.787491 avg loss no lamb -2.787491 time 2019-02-18 20:04:45.184862
Model ind 640 epoch 217 head A head_i_epoch 0 batch 200: avg loss -2.841155 avg loss no lamb -2.841155 time 2019-02-18 20:07:07.741257
last batch sz 160
Model ind 640 epoch 217 head B head_i_epoch 0 batch 0: avg loss -1.540757 avg loss no lamb -1.540757 time 2019-02-18 20:08:51.291495
Model ind 640 epoch 217 head B head_i_epoch 0 batch 100: avg loss -1.433651 avg loss no lamb -1.433651 time 2019-02-18 20:11:13.468664
Model ind 640 epoch 217 head B head_i_epoch 0 batch 200: avg loss -1.571346 avg loss no lamb -1.571346 time 2019-02-18 20:13:35.275703
last batch sz 160
Model ind 640 epoch 217 head B head_i_epoch 1 batch 0: avg loss -1.509903 avg loss no lamb -1.509903 time 2019-02-18 20:15:19.003047
Model ind 640 epoch 217 head B head_i_epoch 1 batch 100: avg loss -1.447480 avg loss no lamb -1.447480 time 2019-02-18 20:17:40.928630
Model ind 640 epoch 217 head B head_i_epoch 1 batch 200: avg loss -1.627061 avg loss no lamb -1.627061 time 2019-02-18 20:20:04.490467
last batch sz 160
Pre: time 2019-02-18 20:22:12.288752: 
 	std: 0.05149615
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48313335, 0.58781666, 0.5887333, 0.58786666, 0.48291665]
	train_accs: [0.48313335, 0.58781666, 0.5887333, 0.58786666, 0.48291665]
	best_train_sub_head: 2
	worst: 0.48291665
	avg: 0.54609334
	best: 0.5887333

Starting e_i: 218
Model ind 640 epoch 218 head A head_i_epoch 0 batch 0: avg loss -2.793867 avg loss no lamb -2.793867 time 2019-02-18 20:22:14.492326
Model ind 640 epoch 218 head A head_i_epoch 0 batch 100: avg loss -2.753006 avg loss no lamb -2.753006 time 2019-02-18 20:24:36.479326
Model ind 640 epoch 218 head A head_i_epoch 0 batch 200: avg loss -2.896963 avg loss no lamb -2.896963 time 2019-02-18 20:26:58.412024
last batch sz 160
Model ind 640 epoch 218 head B head_i_epoch 0 batch 0: avg loss -1.588758 avg loss no lamb -1.588758 time 2019-02-18 20:28:42.576638
Model ind 640 epoch 218 head B head_i_epoch 0 batch 100: avg loss -1.419128 avg loss no lamb -1.419128 time 2019-02-18 20:31:05.121852
Model ind 640 epoch 218 head B head_i_epoch 0 batch 200: avg loss -1.563187 avg loss no lamb -1.563187 time 2019-02-18 20:33:27.414596
last batch sz 160
Model ind 640 epoch 218 head B head_i_epoch 1 batch 0: avg loss -1.508436 avg loss no lamb -1.508436 time 2019-02-18 20:35:10.882076
Model ind 640 epoch 218 head B head_i_epoch 1 batch 100: avg loss -1.432759 avg loss no lamb -1.432759 time 2019-02-18 20:37:33.291114
Model ind 640 epoch 218 head B head_i_epoch 1 batch 200: avg loss -1.600113 avg loss no lamb -1.600113 time 2019-02-18 20:39:55.535665
last batch sz 160
Pre: time 2019-02-18 20:42:03.319239: 
 	std: 0.05262809
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48075, 0.58885, 0.59025, 0.58888334, 0.48308334]
	train_accs: [0.48075, 0.58885, 0.59025, 0.58888334, 0.48308334]
	best_train_sub_head: 2
	worst: 0.48075
	avg: 0.5463633
	best: 0.59025

Starting e_i: 219
Model ind 640 epoch 219 head A head_i_epoch 0 batch 0: avg loss -2.883998 avg loss no lamb -2.883998 time 2019-02-18 20:42:05.205763
Model ind 640 epoch 219 head A head_i_epoch 0 batch 100: avg loss -2.802350 avg loss no lamb -2.802350 time 2019-02-18 20:44:27.428023
Model ind 640 epoch 219 head A head_i_epoch 0 batch 200: avg loss -2.825166 avg loss no lamb -2.825166 time 2019-02-18 20:46:49.988458
last batch sz 160
Model ind 640 epoch 219 head B head_i_epoch 0 batch 0: avg loss -1.536666 avg loss no lamb -1.536666 time 2019-02-18 20:48:33.519863
Model ind 640 epoch 219 head B head_i_epoch 0 batch 100: avg loss -1.504200 avg loss no lamb -1.504200 time 2019-02-18 20:50:55.598693
Model ind 640 epoch 219 head B head_i_epoch 0 batch 200: avg loss -1.525892 avg loss no lamb -1.525892 time 2019-02-18 20:53:17.581488
last batch sz 160
Model ind 640 epoch 219 head B head_i_epoch 1 batch 0: avg loss -1.601033 avg loss no lamb -1.601033 time 2019-02-18 20:55:00.882124
Model ind 640 epoch 219 head B head_i_epoch 1 batch 100: avg loss -1.437038 avg loss no lamb -1.437038 time 2019-02-18 20:57:23.150533
Model ind 640 epoch 219 head B head_i_epoch 1 batch 200: avg loss -1.493563 avg loss no lamb -1.493563 time 2019-02-18 20:59:46.687524
last batch sz 160
Pre: time 2019-02-18 21:01:55.779819: 
 	std: 0.052492756
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47546667, 0.5835, 0.5837167, 0.5829667, 0.47703335]
	train_accs: [0.47546667, 0.5835, 0.5837167, 0.5829667, 0.47703335]
	best_train_sub_head: 2
	worst: 0.47546667
	avg: 0.54053664
	best: 0.5837167

Starting e_i: 220
Model ind 640 epoch 220 head A head_i_epoch 0 batch 0: avg loss -2.845513 avg loss no lamb -2.845513 time 2019-02-18 21:01:58.016649
Model ind 640 epoch 220 head A head_i_epoch 0 batch 100: avg loss -2.776719 avg loss no lamb -2.776719 time 2019-02-18 21:04:20.537887
Model ind 640 epoch 220 head A head_i_epoch 0 batch 200: avg loss -2.888723 avg loss no lamb -2.888723 time 2019-02-18 21:06:42.876248
last batch sz 160
Model ind 640 epoch 220 head B head_i_epoch 0 batch 0: avg loss -1.456166 avg loss no lamb -1.456166 time 2019-02-18 21:08:26.667710
Model ind 640 epoch 220 head B head_i_epoch 0 batch 100: avg loss -1.395837 avg loss no lamb -1.395837 time 2019-02-18 21:10:48.839009
Model ind 640 epoch 220 head B head_i_epoch 0 batch 200: avg loss -1.567411 avg loss no lamb -1.567411 time 2019-02-18 21:13:11.247028
last batch sz 160
Model ind 640 epoch 220 head B head_i_epoch 1 batch 0: avg loss -1.536678 avg loss no lamb -1.536678 time 2019-02-18 21:14:54.896760
Model ind 640 epoch 220 head B head_i_epoch 1 batch 100: avg loss -1.413350 avg loss no lamb -1.413350 time 2019-02-18 21:17:18.491348
Model ind 640 epoch 220 head B head_i_epoch 1 batch 200: avg loss -1.581785 avg loss no lamb -1.581785 time 2019-02-18 21:19:41.530507
last batch sz 160
Pre: time 2019-02-18 21:21:48.790025: 
 	std: 0.055978835
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47511667, 0.58986664, 0.5901833, 0.58956665, 0.4761]
	train_accs: [0.47511667, 0.58986664, 0.5901833, 0.58956665, 0.4761]
	best_train_sub_head: 2
	worst: 0.47511667
	avg: 0.5441667
	best: 0.5901833

Starting e_i: 221
Model ind 640 epoch 221 head A head_i_epoch 0 batch 0: avg loss -2.785918 avg loss no lamb -2.785918 time 2019-02-18 21:21:54.798933
Model ind 640 epoch 221 head A head_i_epoch 0 batch 100: avg loss -2.811404 avg loss no lamb -2.811404 time 2019-02-18 21:24:16.914363
Model ind 640 epoch 221 head A head_i_epoch 0 batch 200: avg loss -2.813617 avg loss no lamb -2.813617 time 2019-02-18 21:26:39.113392
last batch sz 160
Model ind 640 epoch 221 head B head_i_epoch 0 batch 0: avg loss -1.496904 avg loss no lamb -1.496904 time 2019-02-18 21:28:22.590023
Model ind 640 epoch 221 head B head_i_epoch 0 batch 100: avg loss -1.415338 avg loss no lamb -1.415338 time 2019-02-18 21:30:44.553014
Model ind 640 epoch 221 head B head_i_epoch 0 batch 200: avg loss -1.597094 avg loss no lamb -1.597094 time 2019-02-18 21:33:06.971931
last batch sz 160
Model ind 640 epoch 221 head B head_i_epoch 1 batch 0: avg loss -1.475757 avg loss no lamb -1.475757 time 2019-02-18 21:34:50.694563
Model ind 640 epoch 221 head B head_i_epoch 1 batch 100: avg loss -1.421471 avg loss no lamb -1.421471 time 2019-02-18 21:37:12.992651
Model ind 640 epoch 221 head B head_i_epoch 1 batch 200: avg loss -1.545352 avg loss no lamb -1.545352 time 2019-02-18 21:39:36.643598
last batch sz 160
Pre: time 2019-02-18 21:41:45.094723: 
 	std: 0.053366445
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48118332, 0.59125, 0.5922167, 0.59043336, 0.48358333]
	train_accs: [0.48118332, 0.59125, 0.5922167, 0.59043336, 0.48358333]
	best_train_sub_head: 2
	worst: 0.48118332
	avg: 0.5477333
	best: 0.5922167

Starting e_i: 222
Model ind 640 epoch 222 head A head_i_epoch 0 batch 0: avg loss -2.901168 avg loss no lamb -2.901168 time 2019-02-18 21:41:47.760994
Model ind 640 epoch 222 head A head_i_epoch 0 batch 100: avg loss -2.677008 avg loss no lamb -2.677008 time 2019-02-18 21:44:11.488560
Model ind 640 epoch 222 head A head_i_epoch 0 batch 200: avg loss -2.881578 avg loss no lamb -2.881578 time 2019-02-18 21:46:35.178557
last batch sz 160
Model ind 640 epoch 222 head B head_i_epoch 0 batch 0: avg loss -1.547591 avg loss no lamb -1.547591 time 2019-02-18 21:48:18.960777
Model ind 640 epoch 222 head B head_i_epoch 0 batch 100: avg loss -1.405972 avg loss no lamb -1.405972 time 2019-02-18 21:50:41.374476
Model ind 640 epoch 222 head B head_i_epoch 0 batch 200: avg loss -1.632167 avg loss no lamb -1.632167 time 2019-02-18 21:53:03.825117
last batch sz 160
Model ind 640 epoch 222 head B head_i_epoch 1 batch 0: avg loss -1.510664 avg loss no lamb -1.510664 time 2019-02-18 21:54:47.636305
Model ind 640 epoch 222 head B head_i_epoch 1 batch 100: avg loss -1.346964 avg loss no lamb -1.346964 time 2019-02-18 21:57:10.886849
Model ind 640 epoch 222 head B head_i_epoch 1 batch 200: avg loss -1.573779 avg loss no lamb -1.573779 time 2019-02-18 21:59:34.096649
last batch sz 160
Pre: time 2019-02-18 22:01:43.104800: 
 	std: 0.053266853
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47845, 0.58806664, 0.59028333, 0.5879167, 0.48166665]
	train_accs: [0.47845, 0.58806664, 0.59028333, 0.5879167, 0.48166665]
	best_train_sub_head: 2
	worst: 0.47845
	avg: 0.54527664
	best: 0.59028333

Starting e_i: 223
Model ind 640 epoch 223 head A head_i_epoch 0 batch 0: avg loss -2.798603 avg loss no lamb -2.798603 time 2019-02-18 22:01:45.216094
Model ind 640 epoch 223 head A head_i_epoch 0 batch 100: avg loss -2.759623 avg loss no lamb -2.759623 time 2019-02-18 22:04:08.457025
Model ind 640 epoch 223 head A head_i_epoch 0 batch 200: avg loss -2.863860 avg loss no lamb -2.863860 time 2019-02-18 22:06:30.899194
last batch sz 160
Model ind 640 epoch 223 head B head_i_epoch 0 batch 0: avg loss -1.517648 avg loss no lamb -1.517648 time 2019-02-18 22:08:14.646486
Model ind 640 epoch 223 head B head_i_epoch 0 batch 100: avg loss -1.449789 avg loss no lamb -1.449789 time 2019-02-18 22:10:37.781459
Model ind 640 epoch 223 head B head_i_epoch 0 batch 200: avg loss -1.536487 avg loss no lamb -1.536487 time 2019-02-18 22:13:00.453175
last batch sz 160
Model ind 640 epoch 223 head B head_i_epoch 1 batch 0: avg loss -1.446843 avg loss no lamb -1.446843 time 2019-02-18 22:14:44.267556
Model ind 640 epoch 223 head B head_i_epoch 1 batch 100: avg loss -1.387762 avg loss no lamb -1.387762 time 2019-02-18 22:17:06.824533
Model ind 640 epoch 223 head B head_i_epoch 1 batch 200: avg loss -1.644672 avg loss no lamb -1.644672 time 2019-02-18 22:19:30.467410
last batch sz 160
Pre: time 2019-02-18 22:21:40.059488: 
 	std: 0.054501664
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47656667, 0.5886, 0.5899, 0.58823335, 0.47878334]
	train_accs: [0.47656667, 0.5886, 0.5899, 0.58823335, 0.47878334]
	best_train_sub_head: 2
	worst: 0.47656667
	avg: 0.54441667
	best: 0.5899

Starting e_i: 224
Model ind 640 epoch 224 head A head_i_epoch 0 batch 0: avg loss -2.853337 avg loss no lamb -2.853337 time 2019-02-18 22:21:42.363244
Model ind 640 epoch 224 head A head_i_epoch 0 batch 100: avg loss -2.770866 avg loss no lamb -2.770866 time 2019-02-18 22:24:04.742267
Model ind 640 epoch 224 head A head_i_epoch 0 batch 200: avg loss -2.906535 avg loss no lamb -2.906535 time 2019-02-18 22:26:28.667495
last batch sz 160
Model ind 640 epoch 224 head B head_i_epoch 0 batch 0: avg loss -1.483182 avg loss no lamb -1.483182 time 2019-02-18 22:28:12.737236
Model ind 640 epoch 224 head B head_i_epoch 0 batch 100: avg loss -1.449209 avg loss no lamb -1.449209 time 2019-02-18 22:30:35.086847
Model ind 640 epoch 224 head B head_i_epoch 0 batch 200: avg loss -1.629184 avg loss no lamb -1.629184 time 2019-02-18 22:32:58.143838
last batch sz 160
Model ind 640 epoch 224 head B head_i_epoch 1 batch 0: avg loss -1.533697 avg loss no lamb -1.533697 time 2019-02-18 22:34:41.748959
Model ind 640 epoch 224 head B head_i_epoch 1 batch 100: avg loss -1.378638 avg loss no lamb -1.378638 time 2019-02-18 22:37:03.841888
Model ind 640 epoch 224 head B head_i_epoch 1 batch 200: avg loss -1.616925 avg loss no lamb -1.616925 time 2019-02-18 22:39:27.968255
last batch sz 160
Pre: time 2019-02-18 22:41:37.254900: 
 	std: 0.052822027
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47705, 0.58525, 0.58575, 0.5847333, 0.4778]
	train_accs: [0.47705, 0.58525, 0.58575, 0.5847333, 0.4778]
	best_train_sub_head: 2
	worst: 0.47705
	avg: 0.54211664
	best: 0.58575

Starting e_i: 225
Model ind 640 epoch 225 head A head_i_epoch 0 batch 0: avg loss -2.843930 avg loss no lamb -2.843930 time 2019-02-18 22:41:39.191079
Model ind 640 epoch 225 head A head_i_epoch 0 batch 100: avg loss -2.762884 avg loss no lamb -2.762884 time 2019-02-18 22:44:01.194576
Model ind 640 epoch 225 head A head_i_epoch 0 batch 200: avg loss -2.849393 avg loss no lamb -2.849393 time 2019-02-18 22:46:23.300143
last batch sz 160
Model ind 640 epoch 225 head B head_i_epoch 0 batch 0: avg loss -1.532429 avg loss no lamb -1.532429 time 2019-02-18 22:48:06.962192
Model ind 640 epoch 225 head B head_i_epoch 0 batch 100: avg loss -1.420153 avg loss no lamb -1.420153 time 2019-02-18 22:50:28.899365
Model ind 640 epoch 225 head B head_i_epoch 0 batch 200: avg loss -1.599777 avg loss no lamb -1.599777 time 2019-02-18 22:52:51.970853
last batch sz 160
Model ind 640 epoch 225 head B head_i_epoch 1 batch 0: avg loss -1.526553 avg loss no lamb -1.526553 time 2019-02-18 22:54:35.100807
Model ind 640 epoch 225 head B head_i_epoch 1 batch 100: avg loss -1.480374 avg loss no lamb -1.480374 time 2019-02-18 22:56:57.534125
Model ind 640 epoch 225 head B head_i_epoch 1 batch 200: avg loss -1.641798 avg loss no lamb -1.641798 time 2019-02-18 22:59:19.591031
last batch sz 160
Pre: time 2019-02-18 23:01:27.002312: 
 	std: 0.05309886
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48131666, 0.58996665, 0.59035, 0.5897667, 0.48196667]
	train_accs: [0.48131666, 0.58996665, 0.59035, 0.5897667, 0.48196667]
	best_train_sub_head: 2
	worst: 0.48131666
	avg: 0.54667336
	best: 0.59035

Starting e_i: 226
Model ind 640 epoch 226 head A head_i_epoch 0 batch 0: avg loss -2.731752 avg loss no lamb -2.731752 time 2019-02-18 23:01:29.205304
Model ind 640 epoch 226 head A head_i_epoch 0 batch 100: avg loss -2.749352 avg loss no lamb -2.749352 time 2019-02-18 23:03:51.251860
Model ind 640 epoch 226 head A head_i_epoch 0 batch 200: avg loss -2.810884 avg loss no lamb -2.810884 time 2019-02-18 23:06:13.285697
last batch sz 160
Model ind 640 epoch 226 head B head_i_epoch 0 batch 0: avg loss -1.434580 avg loss no lamb -1.434580 time 2019-02-18 23:07:56.634625
Model ind 640 epoch 226 head B head_i_epoch 0 batch 100: avg loss -1.446337 avg loss no lamb -1.446337 time 2019-02-18 23:10:18.730204
Model ind 640 epoch 226 head B head_i_epoch 0 batch 200: avg loss -1.658216 avg loss no lamb -1.658216 time 2019-02-18 23:12:40.902914
last batch sz 160
Model ind 640 epoch 226 head B head_i_epoch 1 batch 0: avg loss -1.599552 avg loss no lamb -1.599552 time 2019-02-18 23:14:24.170073
Model ind 640 epoch 226 head B head_i_epoch 1 batch 100: avg loss -1.403932 avg loss no lamb -1.403932 time 2019-02-18 23:16:46.145769
Model ind 640 epoch 226 head B head_i_epoch 1 batch 200: avg loss -1.481728 avg loss no lamb -1.481728 time 2019-02-18 23:19:08.026418
last batch sz 160
Pre: time 2019-02-18 23:21:15.632718: 
 	std: 0.052758224
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47291666, 0.5815, 0.58101666, 0.5804, 0.47365]
	train_accs: [0.47291666, 0.5815, 0.58101666, 0.5804, 0.47365]
	best_train_sub_head: 1
	worst: 0.47291666
	avg: 0.53789663
	best: 0.5815

Starting e_i: 227
Model ind 640 epoch 227 head A head_i_epoch 0 batch 0: avg loss -2.783460 avg loss no lamb -2.783460 time 2019-02-18 23:21:17.522296
Model ind 640 epoch 227 head A head_i_epoch 0 batch 100: avg loss -2.787042 avg loss no lamb -2.787042 time 2019-02-18 23:23:39.803906
Model ind 640 epoch 227 head A head_i_epoch 0 batch 200: avg loss -2.764565 avg loss no lamb -2.764565 time 2019-02-18 23:26:02.067796
last batch sz 160
Model ind 640 epoch 227 head B head_i_epoch 0 batch 0: avg loss -1.521298 avg loss no lamb -1.521298 time 2019-02-18 23:27:46.029227
Model ind 640 epoch 227 head B head_i_epoch 0 batch 100: avg loss -1.401834 avg loss no lamb -1.401834 time 2019-02-18 23:30:08.661049
Model ind 640 epoch 227 head B head_i_epoch 0 batch 200: avg loss -1.578146 avg loss no lamb -1.578146 time 2019-02-18 23:32:32.220331
last batch sz 160
Model ind 640 epoch 227 head B head_i_epoch 1 batch 0: avg loss -1.506865 avg loss no lamb -1.506865 time 2019-02-18 23:34:15.978280
Model ind 640 epoch 227 head B head_i_epoch 1 batch 100: avg loss -1.400441 avg loss no lamb -1.400441 time 2019-02-18 23:36:38.637430
Model ind 640 epoch 227 head B head_i_epoch 1 batch 200: avg loss -1.551309 avg loss no lamb -1.551309 time 2019-02-18 23:39:01.691497
last batch sz 160
Pre: time 2019-02-18 23:41:09.413794: 
 	std: 0.053735834
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47828335, 0.58725, 0.58776665, 0.58786666, 0.4776]
	train_accs: [0.47828335, 0.58725, 0.58776665, 0.58786666, 0.4776]
	best_train_sub_head: 3
	worst: 0.4776
	avg: 0.5437533
	best: 0.58786666

Starting e_i: 228
Model ind 640 epoch 228 head A head_i_epoch 0 batch 0: avg loss -2.838639 avg loss no lamb -2.838639 time 2019-02-18 23:41:11.570172
Model ind 640 epoch 228 head A head_i_epoch 0 batch 100: avg loss -2.775919 avg loss no lamb -2.775919 time 2019-02-18 23:43:34.241196
Model ind 640 epoch 228 head A head_i_epoch 0 batch 200: avg loss -2.830079 avg loss no lamb -2.830079 time 2019-02-18 23:45:57.749753
last batch sz 160
Model ind 640 epoch 228 head B head_i_epoch 0 batch 0: avg loss -1.411162 avg loss no lamb -1.411162 time 2019-02-18 23:47:41.752209
Model ind 640 epoch 228 head B head_i_epoch 0 batch 100: avg loss -1.460334 avg loss no lamb -1.460334 time 2019-02-18 23:50:04.861267
Model ind 640 epoch 228 head B head_i_epoch 0 batch 200: avg loss -1.561585 avg loss no lamb -1.561585 time 2019-02-18 23:52:27.945710
last batch sz 160
Model ind 640 epoch 228 head B head_i_epoch 1 batch 0: avg loss -1.486348 avg loss no lamb -1.486348 time 2019-02-18 23:54:11.446277
Model ind 640 epoch 228 head B head_i_epoch 1 batch 100: avg loss -1.374209 avg loss no lamb -1.374209 time 2019-02-18 23:56:34.229107
Model ind 640 epoch 228 head B head_i_epoch 1 batch 200: avg loss -1.558347 avg loss no lamb -1.558347 time 2019-02-18 23:58:56.885739
last batch sz 160
Pre: time 2019-02-19 00:01:05.682052: 
 	std: 0.052728627
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47345, 0.581, 0.58166665, 0.5813, 0.47393334]
	train_accs: [0.47345, 0.581, 0.58166665, 0.5813, 0.47393334]
	best_train_sub_head: 2
	worst: 0.47345
	avg: 0.53827
	best: 0.58166665

Starting e_i: 229
Model ind 640 epoch 229 head A head_i_epoch 0 batch 0: avg loss -2.866706 avg loss no lamb -2.866706 time 2019-02-19 00:01:07.618154
Model ind 640 epoch 229 head A head_i_epoch 0 batch 100: avg loss -2.759365 avg loss no lamb -2.759365 time 2019-02-19 00:03:31.606452
Model ind 640 epoch 229 head A head_i_epoch 0 batch 200: avg loss -2.832954 avg loss no lamb -2.832954 time 2019-02-19 00:05:55.509089
last batch sz 160
Model ind 640 epoch 229 head B head_i_epoch 0 batch 0: avg loss -1.584723 avg loss no lamb -1.584723 time 2019-02-19 00:07:39.563913
Model ind 640 epoch 229 head B head_i_epoch 0 batch 100: avg loss -1.475481 avg loss no lamb -1.475481 time 2019-02-19 00:10:02.016484
Model ind 640 epoch 229 head B head_i_epoch 0 batch 200: avg loss -1.568538 avg loss no lamb -1.568538 time 2019-02-19 00:12:24.898484
last batch sz 160
Model ind 640 epoch 229 head B head_i_epoch 1 batch 0: avg loss -1.452474 avg loss no lamb -1.452474 time 2019-02-19 00:14:08.655884
Model ind 640 epoch 229 head B head_i_epoch 1 batch 100: avg loss -1.538261 avg loss no lamb -1.538261 time 2019-02-19 00:16:31.233922
Model ind 640 epoch 229 head B head_i_epoch 1 batch 200: avg loss -1.597647 avg loss no lamb -1.597647 time 2019-02-19 00:18:53.811100
last batch sz 160
Pre: time 2019-02-19 00:21:01.641029: 
 	std: 0.054625038
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47753334, 0.5887167, 0.58986664, 0.58896667, 0.47783333]
	train_accs: [0.47753334, 0.5887167, 0.58986664, 0.58896667, 0.47783333]
	best_train_sub_head: 2
	worst: 0.47753334
	avg: 0.5445833
	best: 0.58986664

Starting e_i: 230
Model ind 640 epoch 230 head A head_i_epoch 0 batch 0: avg loss -2.904885 avg loss no lamb -2.904885 time 2019-02-19 00:21:03.802462
Model ind 640 epoch 230 head A head_i_epoch 0 batch 100: avg loss -2.766935 avg loss no lamb -2.766935 time 2019-02-19 00:23:26.467000
Model ind 640 epoch 230 head A head_i_epoch 0 batch 200: avg loss -2.899108 avg loss no lamb -2.899108 time 2019-02-19 00:25:49.265202
last batch sz 160
Model ind 640 epoch 230 head B head_i_epoch 0 batch 0: avg loss -1.582409 avg loss no lamb -1.582409 time 2019-02-19 00:27:33.096381
Model ind 640 epoch 230 head B head_i_epoch 0 batch 100: avg loss -1.501768 avg loss no lamb -1.501768 time 2019-02-19 00:29:54.942911
Model ind 640 epoch 230 head B head_i_epoch 0 batch 200: avg loss -1.649809 avg loss no lamb -1.649809 time 2019-02-19 00:32:17.595901
last batch sz 160
Model ind 640 epoch 230 head B head_i_epoch 1 batch 0: avg loss -1.542241 avg loss no lamb -1.542241 time 2019-02-19 00:34:01.664086
Model ind 640 epoch 230 head B head_i_epoch 1 batch 100: avg loss -1.455295 avg loss no lamb -1.455295 time 2019-02-19 00:36:24.526782
Model ind 640 epoch 230 head B head_i_epoch 1 batch 200: avg loss -1.673840 avg loss no lamb -1.673840 time 2019-02-19 00:38:46.536340
last batch sz 160
Pre: time 2019-02-19 00:40:54.023185: 
 	std: 0.05516839
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48256665, 0.59405, 0.59435, 0.59436667, 0.48073334]
	train_accs: [0.48256665, 0.59405, 0.59435, 0.59436667, 0.48073334]
	best_train_sub_head: 3
	worst: 0.48073334
	avg: 0.5492133
	best: 0.59436667

Starting e_i: 231
Model ind 640 epoch 231 head A head_i_epoch 0 batch 0: avg loss -2.869820 avg loss no lamb -2.869820 time 2019-02-19 00:41:03.932150
Model ind 640 epoch 231 head A head_i_epoch 0 batch 100: avg loss -2.739717 avg loss no lamb -2.739717 time 2019-02-19 00:43:26.049094
Model ind 640 epoch 231 head A head_i_epoch 0 batch 200: avg loss -2.829449 avg loss no lamb -2.829449 time 2019-02-19 00:45:48.537780
last batch sz 160
Model ind 640 epoch 231 head B head_i_epoch 0 batch 0: avg loss -1.563690 avg loss no lamb -1.563690 time 2019-02-19 00:47:32.785778
Model ind 640 epoch 231 head B head_i_epoch 0 batch 100: avg loss -1.403898 avg loss no lamb -1.403898 time 2019-02-19 00:49:56.588604
Model ind 640 epoch 231 head B head_i_epoch 0 batch 200: avg loss -1.651375 avg loss no lamb -1.651375 time 2019-02-19 00:52:20.025257
last batch sz 160
Model ind 640 epoch 231 head B head_i_epoch 1 batch 0: avg loss -1.567984 avg loss no lamb -1.567984 time 2019-02-19 00:54:04.417028
Model ind 640 epoch 231 head B head_i_epoch 1 batch 100: avg loss -1.366565 avg loss no lamb -1.366565 time 2019-02-19 00:56:27.295489
Model ind 640 epoch 231 head B head_i_epoch 1 batch 200: avg loss -1.553648 avg loss no lamb -1.553648 time 2019-02-19 00:58:50.042520
last batch sz 160
Pre: time 2019-02-19 01:00:58.184218: 
 	std: 0.05478674
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47806665, 0.58898336, 0.5883, 0.58898336, 0.4758]
	train_accs: [0.47806665, 0.58898336, 0.5883, 0.58898336, 0.4758]
	best_train_sub_head: 1
	worst: 0.4758
	avg: 0.5440267
	best: 0.58898336

Starting e_i: 232
Model ind 640 epoch 232 head A head_i_epoch 0 batch 0: avg loss -2.888285 avg loss no lamb -2.888285 time 2019-02-19 01:01:00.434578
Model ind 640 epoch 232 head A head_i_epoch 0 batch 100: avg loss -2.805681 avg loss no lamb -2.805681 time 2019-02-19 01:03:23.144905
Model ind 640 epoch 232 head A head_i_epoch 0 batch 200: avg loss -2.819980 avg loss no lamb -2.819980 time 2019-02-19 01:05:47.126089
last batch sz 160
Model ind 640 epoch 232 head B head_i_epoch 0 batch 0: avg loss -1.497779 avg loss no lamb -1.497779 time 2019-02-19 01:07:31.670457
Model ind 640 epoch 232 head B head_i_epoch 0 batch 100: avg loss -1.373279 avg loss no lamb -1.373279 time 2019-02-19 01:09:55.432437
Model ind 640 epoch 232 head B head_i_epoch 0 batch 200: avg loss -1.587482 avg loss no lamb -1.587482 time 2019-02-19 01:12:18.230961
last batch sz 160
Model ind 640 epoch 232 head B head_i_epoch 1 batch 0: avg loss -1.547781 avg loss no lamb -1.547781 time 2019-02-19 01:14:01.380109
Model ind 640 epoch 232 head B head_i_epoch 1 batch 100: avg loss -1.454510 avg loss no lamb -1.454510 time 2019-02-19 01:16:24.446860
Model ind 640 epoch 232 head B head_i_epoch 1 batch 200: avg loss -1.656525 avg loss no lamb -1.656525 time 2019-02-19 01:18:46.705598
last batch sz 160
Pre: time 2019-02-19 01:20:54.109316: 
 	std: 0.055206727
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48046666, 0.59195, 0.5921, 0.5919667, 0.47818333]
	train_accs: [0.48046666, 0.59195, 0.5921, 0.5919667, 0.47818333]
	best_train_sub_head: 2
	worst: 0.47818333
	avg: 0.5469333
	best: 0.5921

Starting e_i: 233
Model ind 640 epoch 233 head A head_i_epoch 0 batch 0: avg loss -2.869014 avg loss no lamb -2.869014 time 2019-02-19 01:20:56.094248
Model ind 640 epoch 233 head A head_i_epoch 0 batch 100: avg loss -2.750307 avg loss no lamb -2.750307 time 2019-02-19 01:23:18.481181
Model ind 640 epoch 233 head A head_i_epoch 0 batch 200: avg loss -2.883054 avg loss no lamb -2.883054 time 2019-02-19 01:25:41.186554
last batch sz 160
Model ind 640 epoch 233 head B head_i_epoch 0 batch 0: avg loss -1.468635 avg loss no lamb -1.468635 time 2019-02-19 01:27:26.074285
Model ind 640 epoch 233 head B head_i_epoch 0 batch 100: avg loss -1.502707 avg loss no lamb -1.502707 time 2019-02-19 01:29:50.052506
Model ind 640 epoch 233 head B head_i_epoch 0 batch 200: avg loss -1.580448 avg loss no lamb -1.580448 time 2019-02-19 01:32:13.979738
last batch sz 160
Model ind 640 epoch 233 head B head_i_epoch 1 batch 0: avg loss -1.495082 avg loss no lamb -1.495082 time 2019-02-19 01:33:57.538158
Model ind 640 epoch 233 head B head_i_epoch 1 batch 100: avg loss -1.434567 avg loss no lamb -1.434567 time 2019-02-19 01:36:20.211365
Model ind 640 epoch 233 head B head_i_epoch 1 batch 200: avg loss -1.598795 avg loss no lamb -1.598795 time 2019-02-19 01:38:43.915162
last batch sz 160
Pre: time 2019-02-19 01:40:51.691567: 
 	std: 0.053627897
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47773334, 0.58671665, 0.58605, 0.5862667, 0.47603333]
	train_accs: [0.47773334, 0.58671665, 0.58605, 0.5862667, 0.47603333]
	best_train_sub_head: 1
	worst: 0.47603333
	avg: 0.54256
	best: 0.58671665

Starting e_i: 234
Model ind 640 epoch 234 head A head_i_epoch 0 batch 0: avg loss -2.830854 avg loss no lamb -2.830854 time 2019-02-19 01:40:53.889079
Model ind 640 epoch 234 head A head_i_epoch 0 batch 100: avg loss -2.730134 avg loss no lamb -2.730134 time 2019-02-19 01:43:17.105825
Model ind 640 epoch 234 head A head_i_epoch 0 batch 200: avg loss -2.807519 avg loss no lamb -2.807519 time 2019-02-19 01:45:40.390654
last batch sz 160
Model ind 640 epoch 234 head B head_i_epoch 0 batch 0: avg loss -1.530954 avg loss no lamb -1.530954 time 2019-02-19 01:47:24.596358
Model ind 640 epoch 234 head B head_i_epoch 0 batch 100: avg loss -1.391162 avg loss no lamb -1.391162 time 2019-02-19 01:49:47.412625
Model ind 640 epoch 234 head B head_i_epoch 0 batch 200: avg loss -1.661306 avg loss no lamb -1.661306 time 2019-02-19 01:52:10.673384
last batch sz 160
Model ind 640 epoch 234 head B head_i_epoch 1 batch 0: avg loss -1.617704 avg loss no lamb -1.617704 time 2019-02-19 01:53:54.758355
Model ind 640 epoch 234 head B head_i_epoch 1 batch 100: avg loss -1.352661 avg loss no lamb -1.352661 time 2019-02-19 01:56:17.805043
Model ind 640 epoch 234 head B head_i_epoch 1 batch 200: avg loss -1.665160 avg loss no lamb -1.665160 time 2019-02-19 01:58:41.198086
last batch sz 160
Pre: time 2019-02-19 02:00:50.073424: 
 	std: 0.05271622
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.47938332, 0.5874, 0.58783334, 0.5877, 0.4807]
	train_accs: [0.47938332, 0.5874, 0.58783334, 0.5877, 0.4807]
	best_train_sub_head: 2
	worst: 0.47938332
	avg: 0.54460335
	best: 0.58783334

Starting e_i: 235
Model ind 640 epoch 235 head A head_i_epoch 0 batch 0: avg loss -2.794900 avg loss no lamb -2.794900 time 2019-02-19 02:00:52.027446
Model ind 640 epoch 235 head A head_i_epoch 0 batch 100: avg loss -2.745806 avg loss no lamb -2.745806 time 2019-02-19 02:03:15.604611
Model ind 640 epoch 235 head A head_i_epoch 0 batch 200: avg loss -2.882698 avg loss no lamb -2.882698 time 2019-02-19 02:05:38.424452
last batch sz 160
Model ind 640 epoch 235 head B head_i_epoch 0 batch 0: avg loss -1.516566 avg loss no lamb -1.516566 time 2019-02-19 02:07:22.022476
Model ind 640 epoch 235 head B head_i_epoch 0 batch 100: avg loss -1.456354 avg loss no lamb -1.456354 time 2019-02-19 02:09:44.447228
Model ind 640 epoch 235 head B head_i_epoch 0 batch 200: avg loss -1.640704 avg loss no lamb -1.640704 time 2019-02-19 02:12:06.819480
last batch sz 160
Model ind 640 epoch 235 head B head_i_epoch 1 batch 0: avg loss -1.421916 avg loss no lamb -1.421916 time 2019-02-19 02:13:50.422900
Model ind 640 epoch 235 head B head_i_epoch 1 batch 100: avg loss -1.417512 avg loss no lamb -1.417512 time 2019-02-19 02:16:13.414255
Model ind 640 epoch 235 head B head_i_epoch 1 batch 200: avg loss -1.547348 avg loss no lamb -1.547348 time 2019-02-19 02:18:37.428339
last batch sz 160
Pre: time 2019-02-19 02:20:46.936852: 
 	std: 0.054139823
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48625, 0.59636664, 0.5964, 0.5962667, 0.48541668]
	train_accs: [0.48625, 0.59636664, 0.5964, 0.5962667, 0.48541668]
	best_train_sub_head: 2
	worst: 0.48541668
	avg: 0.55214
	best: 0.5964

Starting e_i: 236
Model ind 640 epoch 236 head A head_i_epoch 0 batch 0: avg loss -2.825790 avg loss no lamb -2.825790 time 2019-02-19 02:20:53.242382
Model ind 640 epoch 236 head A head_i_epoch 0 batch 100: avg loss -2.745516 avg loss no lamb -2.745516 time 2019-02-19 02:23:16.586518
Model ind 640 epoch 236 head A head_i_epoch 0 batch 200: avg loss -2.868028 avg loss no lamb -2.868028 time 2019-02-19 02:25:39.653068
last batch sz 160
Model ind 640 epoch 236 head B head_i_epoch 0 batch 0: avg loss -1.608883 avg loss no lamb -1.608883 time 2019-02-19 02:27:23.842850
Model ind 640 epoch 236 head B head_i_epoch 0 batch 100: avg loss -1.522340 avg loss no lamb -1.522340 time 2019-02-19 02:29:47.274229
Model ind 640 epoch 236 head B head_i_epoch 0 batch 200: avg loss -1.576239 avg loss no lamb -1.576239 time 2019-02-19 02:32:10.952220
last batch sz 160
Model ind 640 epoch 236 head B head_i_epoch 1 batch 0: avg loss -1.563871 avg loss no lamb -1.563871 time 2019-02-19 02:33:54.577378
Model ind 640 epoch 236 head B head_i_epoch 1 batch 100: avg loss -1.516581 avg loss no lamb -1.516581 time 2019-02-19 02:36:16.889554
Model ind 640 epoch 236 head B head_i_epoch 1 batch 200: avg loss -1.749642 avg loss no lamb -1.749642 time 2019-02-19 02:38:39.852442
last batch sz 160
Pre: time 2019-02-19 02:40:48.684280: 
 	std: 0.05301658
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4821, 0.59035, 0.5901667, 0.5901167, 0.48188335]
	train_accs: [0.4821, 0.59035, 0.5901667, 0.5901167, 0.48188335]
	best_train_sub_head: 1
	worst: 0.48188335
	avg: 0.5469233
	best: 0.59035

Starting e_i: 237
Model ind 640 epoch 237 head A head_i_epoch 0 batch 0: avg loss -2.830165 avg loss no lamb -2.830165 time 2019-02-19 02:40:50.613832
Model ind 640 epoch 237 head A head_i_epoch 0 batch 100: avg loss -2.777546 avg loss no lamb -2.777546 time 2019-02-19 02:43:13.806617
Model ind 640 epoch 237 head A head_i_epoch 0 batch 200: avg loss -2.828494 avg loss no lamb -2.828494 time 2019-02-19 02:45:37.396234
last batch sz 160
Model ind 640 epoch 237 head B head_i_epoch 0 batch 0: avg loss -1.511287 avg loss no lamb -1.511287 time 2019-02-19 02:47:22.105139
Model ind 640 epoch 237 head B head_i_epoch 0 batch 100: avg loss -1.494411 avg loss no lamb -1.494411 time 2019-02-19 02:49:45.346012
Model ind 640 epoch 237 head B head_i_epoch 0 batch 200: avg loss -1.522371 avg loss no lamb -1.522371 time 2019-02-19 02:52:08.751729
last batch sz 160
Model ind 640 epoch 237 head B head_i_epoch 1 batch 0: avg loss -1.452826 avg loss no lamb -1.452826 time 2019-02-19 02:53:52.813084
Model ind 640 epoch 237 head B head_i_epoch 1 batch 100: avg loss -1.491077 avg loss no lamb -1.491077 time 2019-02-19 02:56:16.750745
Model ind 640 epoch 237 head B head_i_epoch 1 batch 200: avg loss -1.651347 avg loss no lamb -1.651347 time 2019-02-19 02:58:40.332304
last batch sz 160
Pre: time 2019-02-19 03:00:48.965330: 
 	std: 0.05433486
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47738335, 0.5890333, 0.5885, 0.5887667, 0.47833332]
	train_accs: [0.47738335, 0.5890333, 0.5885, 0.5887667, 0.47833332]
	best_train_sub_head: 1
	worst: 0.47738335
	avg: 0.5444033
	best: 0.5890333

Starting e_i: 238
Model ind 640 epoch 238 head A head_i_epoch 0 batch 0: avg loss -2.802762 avg loss no lamb -2.802762 time 2019-02-19 03:00:51.492857
Model ind 640 epoch 238 head A head_i_epoch 0 batch 100: avg loss -2.796554 avg loss no lamb -2.796554 time 2019-02-19 03:03:15.829108
Model ind 640 epoch 238 head A head_i_epoch 0 batch 200: avg loss -2.855665 avg loss no lamb -2.855665 time 2019-02-19 03:05:38.937868
last batch sz 160
Model ind 640 epoch 238 head B head_i_epoch 0 batch 0: avg loss -1.528441 avg loss no lamb -1.528441 time 2019-02-19 03:07:23.190067
Model ind 640 epoch 238 head B head_i_epoch 0 batch 100: avg loss -1.449674 avg loss no lamb -1.449674 time 2019-02-19 03:09:45.764720
Model ind 640 epoch 238 head B head_i_epoch 0 batch 200: avg loss -1.635960 avg loss no lamb -1.635960 time 2019-02-19 03:12:09.365793
last batch sz 160
Model ind 640 epoch 238 head B head_i_epoch 1 batch 0: avg loss -1.480492 avg loss no lamb -1.480492 time 2019-02-19 03:13:54.163573
Model ind 640 epoch 238 head B head_i_epoch 1 batch 100: avg loss -1.478328 avg loss no lamb -1.478328 time 2019-02-19 03:16:16.526955
Model ind 640 epoch 238 head B head_i_epoch 1 batch 200: avg loss -1.611619 avg loss no lamb -1.611619 time 2019-02-19 03:18:39.486291
last batch sz 160
Pre: time 2019-02-19 03:20:48.777739: 
 	std: 0.052168995
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48321667, 0.58921665, 0.58883333, 0.5898833, 0.48243332]
	train_accs: [0.48321667, 0.58921665, 0.58883333, 0.5898833, 0.48243332]
	best_train_sub_head: 3
	worst: 0.48243332
	avg: 0.5467167
	best: 0.5898833

Starting e_i: 239
Model ind 640 epoch 239 head A head_i_epoch 0 batch 0: avg loss -2.869167 avg loss no lamb -2.869167 time 2019-02-19 03:20:50.755712
Model ind 640 epoch 239 head A head_i_epoch 0 batch 100: avg loss -2.770160 avg loss no lamb -2.770160 time 2019-02-19 03:23:14.334422
Model ind 640 epoch 239 head A head_i_epoch 0 batch 200: avg loss -2.796935 avg loss no lamb -2.796935 time 2019-02-19 03:25:36.777140
last batch sz 160
Model ind 640 epoch 239 head B head_i_epoch 0 batch 0: avg loss -1.517312 avg loss no lamb -1.517312 time 2019-02-19 03:27:20.295207
Model ind 640 epoch 239 head B head_i_epoch 0 batch 100: avg loss -1.412492 avg loss no lamb -1.412492 time 2019-02-19 03:29:42.635159
Model ind 640 epoch 239 head B head_i_epoch 0 batch 200: avg loss -1.564711 avg loss no lamb -1.564711 time 2019-02-19 03:32:05.219653
last batch sz 160
Model ind 640 epoch 239 head B head_i_epoch 1 batch 0: avg loss -1.475616 avg loss no lamb -1.475616 time 2019-02-19 03:33:48.836023
Model ind 640 epoch 239 head B head_i_epoch 1 batch 100: avg loss -1.486325 avg loss no lamb -1.486325 time 2019-02-19 03:36:11.041654
Model ind 640 epoch 239 head B head_i_epoch 1 batch 200: avg loss -1.628576 avg loss no lamb -1.628576 time 2019-02-19 03:38:34.584969
last batch sz 160
Pre: time 2019-02-19 03:40:43.558641: 
 	std: 0.053612567
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47561666, 0.58535, 0.58488333, 0.58591664, 0.47628334]
	train_accs: [0.47561666, 0.58535, 0.58488333, 0.58591664, 0.47628334]
	best_train_sub_head: 3
	worst: 0.47561666
	avg: 0.54161
	best: 0.58591664

Starting e_i: 240
Model ind 640 epoch 240 head A head_i_epoch 0 batch 0: avg loss -2.764369 avg loss no lamb -2.764369 time 2019-02-19 03:40:45.789886
Model ind 640 epoch 240 head A head_i_epoch 0 batch 100: avg loss -2.744746 avg loss no lamb -2.744746 time 2019-02-19 03:43:09.022651
Model ind 640 epoch 240 head A head_i_epoch 0 batch 200: avg loss -2.865327 avg loss no lamb -2.865327 time 2019-02-19 03:45:32.582557
last batch sz 160
Model ind 640 epoch 240 head B head_i_epoch 0 batch 0: avg loss -1.531751 avg loss no lamb -1.531751 time 2019-02-19 03:47:17.605551
Model ind 640 epoch 240 head B head_i_epoch 0 batch 100: avg loss -1.319074 avg loss no lamb -1.319074 time 2019-02-19 03:49:41.561101
Model ind 640 epoch 240 head B head_i_epoch 0 batch 200: avg loss -1.588405 avg loss no lamb -1.588405 time 2019-02-19 03:52:03.965966
last batch sz 160
Model ind 640 epoch 240 head B head_i_epoch 1 batch 0: avg loss -1.548486 avg loss no lamb -1.548486 time 2019-02-19 03:53:47.475450
Model ind 640 epoch 240 head B head_i_epoch 1 batch 100: avg loss -1.477181 avg loss no lamb -1.477181 time 2019-02-19 03:56:09.917924
Model ind 640 epoch 240 head B head_i_epoch 1 batch 200: avg loss -1.595577 avg loss no lamb -1.595577 time 2019-02-19 03:58:32.553947
last batch sz 160
Pre: time 2019-02-19 04:00:40.555619: 
 	std: 0.052547522
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47653332, 0.58428335, 0.58418334, 0.5847833, 0.47778332]
	train_accs: [0.47653332, 0.58428335, 0.58418334, 0.5847833, 0.47778332]
	best_train_sub_head: 3
	worst: 0.47653332
	avg: 0.5415133
	best: 0.5847833

Starting e_i: 241
Model ind 640 epoch 241 head A head_i_epoch 0 batch 0: avg loss -2.876412 avg loss no lamb -2.876412 time 2019-02-19 04:00:46.265200
Model ind 640 epoch 241 head A head_i_epoch 0 batch 100: avg loss -2.783198 avg loss no lamb -2.783198 time 2019-02-19 04:03:10.289411
Model ind 640 epoch 241 head A head_i_epoch 0 batch 200: avg loss -2.937322 avg loss no lamb -2.937322 time 2019-02-19 04:05:34.082652
last batch sz 160
Model ind 640 epoch 241 head B head_i_epoch 0 batch 0: avg loss -1.538623 avg loss no lamb -1.538623 time 2019-02-19 04:07:18.007255
Model ind 640 epoch 241 head B head_i_epoch 0 batch 100: avg loss -1.492708 avg loss no lamb -1.492708 time 2019-02-19 04:09:40.240596
Model ind 640 epoch 241 head B head_i_epoch 0 batch 200: avg loss -1.593154 avg loss no lamb -1.593154 time 2019-02-19 04:12:03.658176
last batch sz 160
Model ind 640 epoch 241 head B head_i_epoch 1 batch 0: avg loss -1.494974 avg loss no lamb -1.494974 time 2019-02-19 04:13:47.937418
Model ind 640 epoch 241 head B head_i_epoch 1 batch 100: avg loss -1.361325 avg loss no lamb -1.361325 time 2019-02-19 04:16:10.795502
Model ind 640 epoch 241 head B head_i_epoch 1 batch 200: avg loss -1.642260 avg loss no lamb -1.642260 time 2019-02-19 04:18:33.919018
last batch sz 160
Pre: time 2019-02-19 04:20:42.891808: 
 	std: 0.05373988
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48171666, 0.59146667, 0.59201664, 0.59246665, 0.48286667]
	train_accs: [0.48171666, 0.59146667, 0.59201664, 0.59246665, 0.48286667]
	best_train_sub_head: 3
	worst: 0.48171666
	avg: 0.5481067
	best: 0.59246665

Starting e_i: 242
Model ind 640 epoch 242 head A head_i_epoch 0 batch 0: avg loss -2.768255 avg loss no lamb -2.768255 time 2019-02-19 04:20:44.887823
Model ind 640 epoch 242 head A head_i_epoch 0 batch 100: avg loss -2.752074 avg loss no lamb -2.752074 time 2019-02-19 04:23:08.002686
Model ind 640 epoch 242 head A head_i_epoch 0 batch 200: avg loss -2.855486 avg loss no lamb -2.855486 time 2019-02-19 04:25:30.517752
last batch sz 160
Model ind 640 epoch 242 head B head_i_epoch 0 batch 0: avg loss -1.495258 avg loss no lamb -1.495258 time 2019-02-19 04:27:14.454682
Model ind 640 epoch 242 head B head_i_epoch 0 batch 100: avg loss -1.352574 avg loss no lamb -1.352574 time 2019-02-19 04:29:37.975330
Model ind 640 epoch 242 head B head_i_epoch 0 batch 200: avg loss -1.578822 avg loss no lamb -1.578822 time 2019-02-19 04:32:00.987935
last batch sz 160
Model ind 640 epoch 242 head B head_i_epoch 1 batch 0: avg loss -1.441541 avg loss no lamb -1.441541 time 2019-02-19 04:33:45.141864
Model ind 640 epoch 242 head B head_i_epoch 1 batch 100: avg loss -1.413279 avg loss no lamb -1.413279 time 2019-02-19 04:36:07.899171
Model ind 640 epoch 242 head B head_i_epoch 1 batch 200: avg loss -1.629710 avg loss no lamb -1.629710 time 2019-02-19 04:38:30.825324
last batch sz 160
Pre: time 2019-02-19 04:40:38.434362: 
 	std: 0.054106373
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48436666, 0.59493333, 0.5956333, 0.5952333, 0.48528334]
	train_accs: [0.48436666, 0.59493333, 0.5956333, 0.5952333, 0.48528334]
	best_train_sub_head: 2
	worst: 0.48436666
	avg: 0.55108994
	best: 0.5956333

Starting e_i: 243
Model ind 640 epoch 243 head A head_i_epoch 0 batch 0: avg loss -2.869704 avg loss no lamb -2.869704 time 2019-02-19 04:40:40.351021
Model ind 640 epoch 243 head A head_i_epoch 0 batch 100: avg loss -2.781290 avg loss no lamb -2.781290 time 2019-02-19 04:43:03.183076
Model ind 640 epoch 243 head A head_i_epoch 0 batch 200: avg loss -2.816515 avg loss no lamb -2.816515 time 2019-02-19 04:45:26.909184
last batch sz 160
Model ind 640 epoch 243 head B head_i_epoch 0 batch 0: avg loss -1.533966 avg loss no lamb -1.533966 time 2019-02-19 04:47:11.103140
Model ind 640 epoch 243 head B head_i_epoch 0 batch 100: avg loss -1.428756 avg loss no lamb -1.428756 time 2019-02-19 04:49:33.425024
Model ind 640 epoch 243 head B head_i_epoch 0 batch 200: avg loss -1.629105 avg loss no lamb -1.629105 time 2019-02-19 04:51:57.283520
last batch sz 160
Model ind 640 epoch 243 head B head_i_epoch 1 batch 0: avg loss -1.498325 avg loss no lamb -1.498325 time 2019-02-19 04:53:42.118093
Model ind 640 epoch 243 head B head_i_epoch 1 batch 100: avg loss -1.428354 avg loss no lamb -1.428354 time 2019-02-19 04:56:05.918446
Model ind 640 epoch 243 head B head_i_epoch 1 batch 200: avg loss -1.599948 avg loss no lamb -1.599948 time 2019-02-19 04:58:28.351438
last batch sz 160
Pre: time 2019-02-19 05:00:35.706937: 
 	std: 0.053096388
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48011667, 0.5888, 0.5890333, 0.58996665, 0.48166665]
	train_accs: [0.48011667, 0.5888, 0.5890333, 0.58996665, 0.48166665]
	best_train_sub_head: 3
	worst: 0.48011667
	avg: 0.5459167
	best: 0.58996665

Starting e_i: 244
Model ind 640 epoch 244 head A head_i_epoch 0 batch 0: avg loss -2.781096 avg loss no lamb -2.781096 time 2019-02-19 05:00:37.873936
Model ind 640 epoch 244 head A head_i_epoch 0 batch 100: avg loss -2.812039 avg loss no lamb -2.812039 time 2019-02-19 05:03:00.444330
Model ind 640 epoch 244 head A head_i_epoch 0 batch 200: avg loss -2.819129 avg loss no lamb -2.819129 time 2019-02-19 05:05:23.646044
last batch sz 160
Model ind 640 epoch 244 head B head_i_epoch 0 batch 0: avg loss -1.648873 avg loss no lamb -1.648873 time 2019-02-19 05:07:07.265646
Model ind 640 epoch 244 head B head_i_epoch 0 batch 100: avg loss -1.413911 avg loss no lamb -1.413911 time 2019-02-19 05:09:29.399294
Model ind 640 epoch 244 head B head_i_epoch 0 batch 200: avg loss -1.619050 avg loss no lamb -1.619050 time 2019-02-19 05:11:51.888440
last batch sz 160
Model ind 640 epoch 244 head B head_i_epoch 1 batch 0: avg loss -1.482146 avg loss no lamb -1.482146 time 2019-02-19 05:13:36.397333
Model ind 640 epoch 244 head B head_i_epoch 1 batch 100: avg loss -1.406425 avg loss no lamb -1.406425 time 2019-02-19 05:15:59.743538
Model ind 640 epoch 244 head B head_i_epoch 1 batch 200: avg loss -1.560395 avg loss no lamb -1.560395 time 2019-02-19 05:18:22.880529
last batch sz 160
Pre: time 2019-02-19 05:20:31.508243: 
 	std: 0.05172667
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49141666, 0.5971, 0.5969667, 0.5973667, 0.4917]
	train_accs: [0.49141666, 0.5971, 0.5969667, 0.5973667, 0.4917]
	best_train_sub_head: 3
	worst: 0.49141666
	avg: 0.55491006
	best: 0.5973667

Starting e_i: 245
Model ind 640 epoch 245 head A head_i_epoch 0 batch 0: avg loss -2.858860 avg loss no lamb -2.858860 time 2019-02-19 05:20:37.270049
Model ind 640 epoch 245 head A head_i_epoch 0 batch 100: avg loss -2.779708 avg loss no lamb -2.779708 time 2019-02-19 05:23:00.603918
Model ind 640 epoch 245 head A head_i_epoch 0 batch 200: avg loss -2.848895 avg loss no lamb -2.848895 time 2019-02-19 05:25:23.928029
last batch sz 160
Model ind 640 epoch 245 head B head_i_epoch 0 batch 0: avg loss -1.521411 avg loss no lamb -1.521411 time 2019-02-19 05:27:08.346471
Model ind 640 epoch 245 head B head_i_epoch 0 batch 100: avg loss -1.411761 avg loss no lamb -1.411761 time 2019-02-19 05:29:31.772421
Model ind 640 epoch 245 head B head_i_epoch 0 batch 200: avg loss -1.516012 avg loss no lamb -1.516012 time 2019-02-19 05:31:55.170079
last batch sz 160
Model ind 640 epoch 245 head B head_i_epoch 1 batch 0: avg loss -1.605799 avg loss no lamb -1.605799 time 2019-02-19 05:33:39.356516
Model ind 640 epoch 245 head B head_i_epoch 1 batch 100: avg loss -1.393633 avg loss no lamb -1.393633 time 2019-02-19 05:36:02.817998
Model ind 640 epoch 245 head B head_i_epoch 1 batch 200: avg loss -1.606624 avg loss no lamb -1.606624 time 2019-02-19 05:38:25.126351
last batch sz 160
Pre: time 2019-02-19 05:40:33.085874: 
 	std: 0.05240872
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48541668, 0.59253335, 0.5911667, 0.59286666, 0.48501667]
	train_accs: [0.48541668, 0.59253335, 0.5911667, 0.59286666, 0.48501667]
	best_train_sub_head: 3
	worst: 0.48501667
	avg: 0.5494
	best: 0.59286666

Starting e_i: 246
Model ind 640 epoch 246 head A head_i_epoch 0 batch 0: avg loss -2.760704 avg loss no lamb -2.760704 time 2019-02-19 05:40:35.345975
Model ind 640 epoch 246 head A head_i_epoch 0 batch 100: avg loss -2.776286 avg loss no lamb -2.776286 time 2019-02-19 05:42:57.485388
Model ind 640 epoch 246 head A head_i_epoch 0 batch 200: avg loss -2.875050 avg loss no lamb -2.875050 time 2019-02-19 05:45:19.885723
last batch sz 160
Model ind 640 epoch 246 head B head_i_epoch 0 batch 0: avg loss -1.553995 avg loss no lamb -1.553995 time 2019-02-19 05:47:03.514172
Model ind 640 epoch 246 head B head_i_epoch 0 batch 100: avg loss -1.462081 avg loss no lamb -1.462081 time 2019-02-19 05:49:26.255158
Model ind 640 epoch 246 head B head_i_epoch 0 batch 200: avg loss -1.606423 avg loss no lamb -1.606423 time 2019-02-19 05:51:49.545466
last batch sz 160
Model ind 640 epoch 246 head B head_i_epoch 1 batch 0: avg loss -1.512639 avg loss no lamb -1.512639 time 2019-02-19 05:53:32.873330
Model ind 640 epoch 246 head B head_i_epoch 1 batch 100: avg loss -1.429622 avg loss no lamb -1.429622 time 2019-02-19 05:55:54.712002
Model ind 640 epoch 246 head B head_i_epoch 1 batch 200: avg loss -1.603447 avg loss no lamb -1.603447 time 2019-02-19 05:58:16.746532
last batch sz 160
Pre: time 2019-02-19 06:00:24.786619: 
 	std: 0.05415434
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48503333, 0.5951167, 0.59468335, 0.5951167, 0.48383334]
	train_accs: [0.48503333, 0.5951167, 0.59468335, 0.5951167, 0.48383334]
	best_train_sub_head: 1
	worst: 0.48383334
	avg: 0.55075663
	best: 0.5951167

Starting e_i: 247
Model ind 640 epoch 247 head A head_i_epoch 0 batch 0: avg loss -2.863494 avg loss no lamb -2.863494 time 2019-02-19 06:00:26.719236
Model ind 640 epoch 247 head A head_i_epoch 0 batch 100: avg loss -2.851770 avg loss no lamb -2.851770 time 2019-02-19 06:02:49.759531
Model ind 640 epoch 247 head A head_i_epoch 0 batch 200: avg loss -2.952742 avg loss no lamb -2.952742 time 2019-02-19 06:05:12.416265
last batch sz 160
Model ind 640 epoch 247 head B head_i_epoch 0 batch 0: avg loss -1.593800 avg loss no lamb -1.593800 time 2019-02-19 06:06:56.269454
Model ind 640 epoch 247 head B head_i_epoch 0 batch 100: avg loss -1.445751 avg loss no lamb -1.445751 time 2019-02-19 06:09:19.678210
Model ind 640 epoch 247 head B head_i_epoch 0 batch 200: avg loss -1.600595 avg loss no lamb -1.600595 time 2019-02-19 06:11:42.931430
last batch sz 160
Model ind 640 epoch 247 head B head_i_epoch 1 batch 0: avg loss -1.473253 avg loss no lamb -1.473253 time 2019-02-19 06:13:26.418496
Model ind 640 epoch 247 head B head_i_epoch 1 batch 100: avg loss -1.415225 avg loss no lamb -1.415225 time 2019-02-19 06:15:48.260152
Model ind 640 epoch 247 head B head_i_epoch 1 batch 200: avg loss -1.565730 avg loss no lamb -1.565730 time 2019-02-19 06:18:09.886324
last batch sz 160
Pre: time 2019-02-19 06:20:17.672734: 
 	std: 0.05386731
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48223335, 0.59211665, 0.59175, 0.5922, 0.4819]
	train_accs: [0.48223335, 0.59211665, 0.59175, 0.5922, 0.4819]
	best_train_sub_head: 3
	worst: 0.4819
	avg: 0.54804003
	best: 0.5922

Starting e_i: 248
Model ind 640 epoch 248 head A head_i_epoch 0 batch 0: avg loss -2.804078 avg loss no lamb -2.804078 time 2019-02-19 06:20:19.882973
Model ind 640 epoch 248 head A head_i_epoch 0 batch 100: avg loss -2.758424 avg loss no lamb -2.758424 time 2019-02-19 06:22:43.492210
Model ind 640 epoch 248 head A head_i_epoch 0 batch 200: avg loss -2.876249 avg loss no lamb -2.876249 time 2019-02-19 06:25:07.139786
last batch sz 160
Model ind 640 epoch 248 head B head_i_epoch 0 batch 0: avg loss -1.548849 avg loss no lamb -1.548849 time 2019-02-19 06:26:51.411163
Model ind 640 epoch 248 head B head_i_epoch 0 batch 100: avg loss -1.362372 avg loss no lamb -1.362372 time 2019-02-19 06:29:14.678372
Model ind 640 epoch 248 head B head_i_epoch 0 batch 200: avg loss -1.588429 avg loss no lamb -1.588429 time 2019-02-19 06:31:37.119951
last batch sz 160
Model ind 640 epoch 248 head B head_i_epoch 1 batch 0: avg loss -1.496147 avg loss no lamb -1.496147 time 2019-02-19 06:33:21.139532
Model ind 640 epoch 248 head B head_i_epoch 1 batch 100: avg loss -1.365069 avg loss no lamb -1.365069 time 2019-02-19 06:35:44.573590
Model ind 640 epoch 248 head B head_i_epoch 1 batch 200: avg loss -1.592060 avg loss no lamb -1.592060 time 2019-02-19 06:38:07.928658
last batch sz 160
Pre: time 2019-02-19 06:40:16.399165: 
 	std: 0.05474011
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48071668, 0.5927167, 0.5919167, 0.59275, 0.48073334]
	train_accs: [0.48071668, 0.5927167, 0.5919167, 0.59275, 0.48073334]
	best_train_sub_head: 3
	worst: 0.48071668
	avg: 0.5477667
	best: 0.59275

Starting e_i: 249
Model ind 640 epoch 249 head A head_i_epoch 0 batch 0: avg loss -2.875732 avg loss no lamb -2.875732 time 2019-02-19 06:40:18.394008
Model ind 640 epoch 249 head A head_i_epoch 0 batch 100: avg loss -2.752140 avg loss no lamb -2.752140 time 2019-02-19 06:42:41.193064
Model ind 640 epoch 249 head A head_i_epoch 0 batch 200: avg loss -2.899024 avg loss no lamb -2.899024 time 2019-02-19 06:45:03.934211
last batch sz 160
Model ind 640 epoch 249 head B head_i_epoch 0 batch 0: avg loss -1.536962 avg loss no lamb -1.536962 time 2019-02-19 06:46:47.724069
Model ind 640 epoch 249 head B head_i_epoch 0 batch 100: avg loss -1.404800 avg loss no lamb -1.404800 time 2019-02-19 06:49:11.191321
Model ind 640 epoch 249 head B head_i_epoch 0 batch 200: avg loss -1.582741 avg loss no lamb -1.582741 time 2019-02-19 06:51:33.553721
last batch sz 160
Model ind 640 epoch 249 head B head_i_epoch 1 batch 0: avg loss -1.569178 avg loss no lamb -1.569178 time 2019-02-19 06:53:18.191852
Model ind 640 epoch 249 head B head_i_epoch 1 batch 100: avg loss -1.333922 avg loss no lamb -1.333922 time 2019-02-19 06:55:41.679046
Model ind 640 epoch 249 head B head_i_epoch 1 batch 200: avg loss -1.585996 avg loss no lamb -1.585996 time 2019-02-19 06:58:05.113506
last batch sz 160
Pre: time 2019-02-19 07:00:13.260869: 
 	std: 0.05235768
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49055, 0.59793335, 0.5969333, 0.5978, 0.49081665]
	train_accs: [0.49055, 0.59793335, 0.5969333, 0.5978, 0.49081665]
	best_train_sub_head: 1
	worst: 0.49055
	avg: 0.55480665
	best: 0.59793335

Starting e_i: 250
Model ind 640 epoch 250 head A head_i_epoch 0 batch 0: avg loss -2.852646 avg loss no lamb -2.852646 time 2019-02-19 07:00:19.340391
Model ind 640 epoch 250 head A head_i_epoch 0 batch 100: avg loss -2.755908 avg loss no lamb -2.755908 time 2019-02-19 07:02:42.800640
Model ind 640 epoch 250 head A head_i_epoch 0 batch 200: avg loss -2.908076 avg loss no lamb -2.908076 time 2019-02-19 07:05:05.154034
last batch sz 160
Model ind 640 epoch 250 head B head_i_epoch 0 batch 0: avg loss -1.438918 avg loss no lamb -1.438918 time 2019-02-19 07:06:48.665955
Model ind 640 epoch 250 head B head_i_epoch 0 batch 100: avg loss -1.468661 avg loss no lamb -1.468661 time 2019-02-19 07:09:10.795594
Model ind 640 epoch 250 head B head_i_epoch 0 batch 200: avg loss -1.599391 avg loss no lamb -1.599391 time 2019-02-19 07:11:33.888085
last batch sz 160
Model ind 640 epoch 250 head B head_i_epoch 1 batch 0: avg loss -1.576298 avg loss no lamb -1.576298 time 2019-02-19 07:13:18.210742
Model ind 640 epoch 250 head B head_i_epoch 1 batch 100: avg loss -1.467842 avg loss no lamb -1.467842 time 2019-02-19 07:15:40.614099
Model ind 640 epoch 250 head B head_i_epoch 1 batch 200: avg loss -1.647699 avg loss no lamb -1.647699 time 2019-02-19 07:18:04.004227
last batch sz 160
Pre: time 2019-02-19 07:20:13.277072: 
 	std: 0.053060364
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48531666, 0.5944667, 0.59358335, 0.59426665, 0.48628333]
	train_accs: [0.48531666, 0.5944667, 0.59358335, 0.59426665, 0.48628333]
	best_train_sub_head: 1
	worst: 0.48531666
	avg: 0.55078334
	best: 0.5944667

Starting e_i: 251
Model ind 640 epoch 251 head A head_i_epoch 0 batch 0: avg loss -2.907198 avg loss no lamb -2.907198 time 2019-02-19 07:20:18.545686
Model ind 640 epoch 251 head A head_i_epoch 0 batch 100: avg loss -2.789200 avg loss no lamb -2.789200 time 2019-02-19 07:22:41.955632
Model ind 640 epoch 251 head A head_i_epoch 0 batch 200: avg loss -2.831878 avg loss no lamb -2.831878 time 2019-02-19 07:25:05.519721
last batch sz 160
Model ind 640 epoch 251 head B head_i_epoch 0 batch 0: avg loss -1.594453 avg loss no lamb -1.594453 time 2019-02-19 07:26:49.925426
Model ind 640 epoch 251 head B head_i_epoch 0 batch 100: avg loss -1.431935 avg loss no lamb -1.431935 time 2019-02-19 07:29:13.078733
Model ind 640 epoch 251 head B head_i_epoch 0 batch 200: avg loss -1.607829 avg loss no lamb -1.607829 time 2019-02-19 07:31:36.051828
last batch sz 160
Model ind 640 epoch 251 head B head_i_epoch 1 batch 0: avg loss -1.551874 avg loss no lamb -1.551874 time 2019-02-19 07:33:19.461132
Model ind 640 epoch 251 head B head_i_epoch 1 batch 100: avg loss -1.394736 avg loss no lamb -1.394736 time 2019-02-19 07:35:41.846763
Model ind 640 epoch 251 head B head_i_epoch 1 batch 200: avg loss -1.651459 avg loss no lamb -1.651459 time 2019-02-19 07:38:04.431387
last batch sz 160
Pre: time 2019-02-19 07:40:12.513238: 
 	std: 0.05482059
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48575, 0.59845, 0.5977, 0.59866667, 0.487]
	train_accs: [0.48575, 0.59845, 0.5977, 0.59866667, 0.487]
	best_train_sub_head: 3
	worst: 0.48575
	avg: 0.55351335
	best: 0.59866667

Starting e_i: 252
Model ind 640 epoch 252 head A head_i_epoch 0 batch 0: avg loss -2.845413 avg loss no lamb -2.845413 time 2019-02-19 07:40:18.831784
Model ind 640 epoch 252 head A head_i_epoch 0 batch 100: avg loss -2.819001 avg loss no lamb -2.819001 time 2019-02-19 07:42:41.840001
Model ind 640 epoch 252 head A head_i_epoch 0 batch 200: avg loss -2.885872 avg loss no lamb -2.885872 time 2019-02-19 07:45:05.578220
last batch sz 160
Model ind 640 epoch 252 head B head_i_epoch 0 batch 0: avg loss -1.505817 avg loss no lamb -1.505817 time 2019-02-19 07:46:49.161276
Model ind 640 epoch 252 head B head_i_epoch 0 batch 100: avg loss -1.476304 avg loss no lamb -1.476304 time 2019-02-19 07:49:11.957045
Model ind 640 epoch 252 head B head_i_epoch 0 batch 200: avg loss -1.560567 avg loss no lamb -1.560567 time 2019-02-19 07:51:34.482004
last batch sz 160
Model ind 640 epoch 252 head B head_i_epoch 1 batch 0: avg loss -1.571883 avg loss no lamb -1.571883 time 2019-02-19 07:53:18.107809
Model ind 640 epoch 252 head B head_i_epoch 1 batch 100: avg loss -1.484694 avg loss no lamb -1.484694 time 2019-02-19 07:55:40.521338
Model ind 640 epoch 252 head B head_i_epoch 1 batch 200: avg loss -1.650395 avg loss no lamb -1.650395 time 2019-02-19 07:58:03.242125
last batch sz 160
Pre: time 2019-02-19 08:00:11.709057: 
 	std: 0.05374616
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48665, 0.59636664, 0.59618336, 0.5967, 0.48676667]
	train_accs: [0.48665, 0.59636664, 0.59618336, 0.5967, 0.48676667]
	best_train_sub_head: 3
	worst: 0.48665
	avg: 0.55253327
	best: 0.5967

Starting e_i: 253
Model ind 640 epoch 253 head A head_i_epoch 0 batch 0: avg loss -2.931455 avg loss no lamb -2.931455 time 2019-02-19 08:00:13.662481
Model ind 640 epoch 253 head A head_i_epoch 0 batch 100: avg loss -2.754659 avg loss no lamb -2.754659 time 2019-02-19 08:02:37.064158
Model ind 640 epoch 253 head A head_i_epoch 0 batch 200: avg loss -2.952260 avg loss no lamb -2.952260 time 2019-02-19 08:05:00.730670
last batch sz 160
Model ind 640 epoch 253 head B head_i_epoch 0 batch 0: avg loss -1.494117 avg loss no lamb -1.494117 time 2019-02-19 08:06:45.337529
Model ind 640 epoch 253 head B head_i_epoch 0 batch 100: avg loss -1.489058 avg loss no lamb -1.489058 time 2019-02-19 08:09:09.013560
Model ind 640 epoch 253 head B head_i_epoch 0 batch 200: avg loss -1.615930 avg loss no lamb -1.615930 time 2019-02-19 08:11:32.406948
last batch sz 160
Model ind 640 epoch 253 head B head_i_epoch 1 batch 0: avg loss -1.536754 avg loss no lamb -1.536754 time 2019-02-19 08:13:16.638143
Model ind 640 epoch 253 head B head_i_epoch 1 batch 100: avg loss -1.470230 avg loss no lamb -1.470230 time 2019-02-19 08:15:38.649433
Model ind 640 epoch 253 head B head_i_epoch 1 batch 200: avg loss -1.575913 avg loss no lamb -1.575913 time 2019-02-19 08:18:01.544405
last batch sz 160
Pre: time 2019-02-19 08:20:10.516618: 
 	std: 0.05345871
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48853335, 0.5969833, 0.59745, 0.59695, 0.48748332]
	train_accs: [0.48853335, 0.5969833, 0.59745, 0.59695, 0.48748332]
	best_train_sub_head: 2
	worst: 0.48748332
	avg: 0.55348
	best: 0.59745

Starting e_i: 254
Model ind 640 epoch 254 head A head_i_epoch 0 batch 0: avg loss -2.830360 avg loss no lamb -2.830360 time 2019-02-19 08:20:12.793793
Model ind 640 epoch 254 head A head_i_epoch 0 batch 100: avg loss -2.730936 avg loss no lamb -2.730936 time 2019-02-19 08:22:34.864679
Model ind 640 epoch 254 head A head_i_epoch 0 batch 200: avg loss -2.912218 avg loss no lamb -2.912218 time 2019-02-19 08:24:58.454876
last batch sz 160
Model ind 640 epoch 254 head B head_i_epoch 0 batch 0: avg loss -1.528059 avg loss no lamb -1.528059 time 2019-02-19 08:26:42.505011
Model ind 640 epoch 254 head B head_i_epoch 0 batch 100: avg loss -1.472270 avg loss no lamb -1.472270 time 2019-02-19 08:29:05.118161
Model ind 640 epoch 254 head B head_i_epoch 0 batch 200: avg loss -1.759327 avg loss no lamb -1.759327 time 2019-02-19 08:31:27.850986
last batch sz 160
Model ind 640 epoch 254 head B head_i_epoch 1 batch 0: avg loss -1.440466 avg loss no lamb -1.440466 time 2019-02-19 08:33:12.152687
Model ind 640 epoch 254 head B head_i_epoch 1 batch 100: avg loss -1.484619 avg loss no lamb -1.484619 time 2019-02-19 08:35:35.654315
Model ind 640 epoch 254 head B head_i_epoch 1 batch 200: avg loss -1.545034 avg loss no lamb -1.545034 time 2019-02-19 08:37:58.852352
last batch sz 160
Pre: time 2019-02-19 08:40:06.939619: 
 	std: 0.054569986
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.47958332, 0.59108335, 0.5905833, 0.5915, 0.47975]
	train_accs: [0.47958332, 0.59108335, 0.5905833, 0.5915, 0.47975]
	best_train_sub_head: 3
	worst: 0.47958332
	avg: 0.54649997
	best: 0.5915

Starting e_i: 255
Model ind 640 epoch 255 head A head_i_epoch 0 batch 0: avg loss -2.804868 avg loss no lamb -2.804868 time 2019-02-19 08:40:08.911046
Model ind 640 epoch 255 head A head_i_epoch 0 batch 100: avg loss -2.791954 avg loss no lamb -2.791954 time 2019-02-19 08:42:32.464628
Model ind 640 epoch 255 head A head_i_epoch 0 batch 200: avg loss -2.939141 avg loss no lamb -2.939141 time 2019-02-19 08:44:56.395333
last batch sz 160
Model ind 640 epoch 255 head B head_i_epoch 0 batch 0: avg loss -1.566681 avg loss no lamb -1.566681 time 2019-02-19 08:46:40.985029
Model ind 640 epoch 255 head B head_i_epoch 0 batch 100: avg loss -1.484956 avg loss no lamb -1.484956 time 2019-02-19 08:49:04.440450
Model ind 640 epoch 255 head B head_i_epoch 0 batch 200: avg loss -1.610037 avg loss no lamb -1.610037 time 2019-02-19 08:51:28.777743
last batch sz 160
Model ind 640 epoch 255 head B head_i_epoch 1 batch 0: avg loss -1.525215 avg loss no lamb -1.525215 time 2019-02-19 08:53:13.493930
Model ind 640 epoch 255 head B head_i_epoch 1 batch 100: avg loss -1.390917 avg loss no lamb -1.390917 time 2019-02-19 08:55:37.586050
Model ind 640 epoch 255 head B head_i_epoch 1 batch 200: avg loss -1.616327 avg loss no lamb -1.616327 time 2019-02-19 08:58:01.041189
last batch sz 160
Pre: time 2019-02-19 09:00:08.625485: 
 	std: 0.054525897
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4737, 0.58585, 0.5850667, 0.5861667, 0.4751]
	train_accs: [0.4737, 0.58585, 0.5850667, 0.5861667, 0.4751]
	best_train_sub_head: 3
	worst: 0.4737
	avg: 0.5411767
	best: 0.5861667

Starting e_i: 256
Model ind 640 epoch 256 head A head_i_epoch 0 batch 0: avg loss -2.821109 avg loss no lamb -2.821109 time 2019-02-19 09:00:10.825441
Model ind 640 epoch 256 head A head_i_epoch 0 batch 100: avg loss -2.818941 avg loss no lamb -2.818941 time 2019-02-19 09:02:34.216482
Model ind 640 epoch 256 head A head_i_epoch 0 batch 200: avg loss -2.879053 avg loss no lamb -2.879053 time 2019-02-19 09:04:58.433373
last batch sz 160
Model ind 640 epoch 256 head B head_i_epoch 0 batch 0: avg loss -1.518283 avg loss no lamb -1.518283 time 2019-02-19 09:06:42.428686
Model ind 640 epoch 256 head B head_i_epoch 0 batch 100: avg loss -1.441111 avg loss no lamb -1.441111 time 2019-02-19 09:09:05.794292
Model ind 640 epoch 256 head B head_i_epoch 0 batch 200: avg loss -1.679978 avg loss no lamb -1.679978 time 2019-02-19 09:11:28.908680
last batch sz 160
Model ind 640 epoch 256 head B head_i_epoch 1 batch 0: avg loss -1.540014 avg loss no lamb -1.540014 time 2019-02-19 09:13:13.165950
Model ind 640 epoch 256 head B head_i_epoch 1 batch 100: avg loss -1.479382 avg loss no lamb -1.479382 time 2019-02-19 09:15:36.448575
Model ind 640 epoch 256 head B head_i_epoch 1 batch 200: avg loss -1.637989 avg loss no lamb -1.637989 time 2019-02-19 09:17:59.840504
last batch sz 160
Pre: time 2019-02-19 09:20:08.760245: 
 	std: 0.053918913
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4847, 0.5941333, 0.594, 0.59421664, 0.48341668]
	train_accs: [0.4847, 0.5941333, 0.594, 0.59421664, 0.48341668]
	best_train_sub_head: 3
	worst: 0.48341668
	avg: 0.55009335
	best: 0.59421664

Starting e_i: 257
Model ind 640 epoch 257 head A head_i_epoch 0 batch 0: avg loss -2.830245 avg loss no lamb -2.830245 time 2019-02-19 09:20:10.818855
Model ind 640 epoch 257 head A head_i_epoch 0 batch 100: avg loss -2.839411 avg loss no lamb -2.839411 time 2019-02-19 09:22:34.950599
Model ind 640 epoch 257 head A head_i_epoch 0 batch 200: avg loss -2.911266 avg loss no lamb -2.911266 time 2019-02-19 09:24:58.939430
last batch sz 160
Model ind 640 epoch 257 head B head_i_epoch 0 batch 0: avg loss -1.588359 avg loss no lamb -1.588359 time 2019-02-19 09:26:42.937609
Model ind 640 epoch 257 head B head_i_epoch 0 batch 100: avg loss -1.542287 avg loss no lamb -1.542287 time 2019-02-19 09:29:06.195406
Model ind 640 epoch 257 head B head_i_epoch 0 batch 200: avg loss -1.574768 avg loss no lamb -1.574768 time 2019-02-19 09:31:29.743394
last batch sz 160
Model ind 640 epoch 257 head B head_i_epoch 1 batch 0: avg loss -1.543182 avg loss no lamb -1.543182 time 2019-02-19 09:33:14.397725
Model ind 640 epoch 257 head B head_i_epoch 1 batch 100: avg loss -1.522802 avg loss no lamb -1.522802 time 2019-02-19 09:35:37.323806
Model ind 640 epoch 257 head B head_i_epoch 1 batch 200: avg loss -1.534236 avg loss no lamb -1.534236 time 2019-02-19 09:38:00.204687
last batch sz 160
Pre: time 2019-02-19 09:40:10.134381: 
 	std: 0.053630676
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48371667, 0.59311664, 0.59356666, 0.59358335, 0.48418334]
	train_accs: [0.48371667, 0.59311664, 0.59356666, 0.59358335, 0.48418334]
	best_train_sub_head: 3
	worst: 0.48371667
	avg: 0.5496333
	best: 0.59358335

Starting e_i: 258
Model ind 640 epoch 258 head A head_i_epoch 0 batch 0: avg loss -2.832591 avg loss no lamb -2.832591 time 2019-02-19 09:40:12.154625
Model ind 640 epoch 258 head A head_i_epoch 0 batch 100: avg loss -2.864107 avg loss no lamb -2.864107 time 2019-02-19 09:42:36.422568
Model ind 640 epoch 258 head A head_i_epoch 0 batch 200: avg loss -2.901460 avg loss no lamb -2.901460 time 2019-02-19 09:44:59.923475
last batch sz 160
Model ind 640 epoch 258 head B head_i_epoch 0 batch 0: avg loss -1.562741 avg loss no lamb -1.562741 time 2019-02-19 09:46:44.533768
Model ind 640 epoch 258 head B head_i_epoch 0 batch 100: avg loss -1.545807 avg loss no lamb -1.545807 time 2019-02-19 09:49:07.550435
Model ind 640 epoch 258 head B head_i_epoch 0 batch 200: avg loss -1.608895 avg loss no lamb -1.608895 time 2019-02-19 09:51:29.978593
last batch sz 160
Model ind 640 epoch 258 head B head_i_epoch 1 batch 0: avg loss -1.533510 avg loss no lamb -1.533510 time 2019-02-19 09:53:14.186962
Model ind 640 epoch 258 head B head_i_epoch 1 batch 100: avg loss -1.450919 avg loss no lamb -1.450919 time 2019-02-19 09:55:38.227759
Model ind 640 epoch 258 head B head_i_epoch 1 batch 200: avg loss -1.629018 avg loss no lamb -1.629018 time 2019-02-19 09:58:01.258633
last batch sz 160
Pre: time 2019-02-19 10:00:08.951974: 
 	std: 0.052754447
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4811, 0.589, 0.58898336, 0.5893667, 0.48176667]
	train_accs: [0.4811, 0.589, 0.58898336, 0.5893667, 0.48176667]
	best_train_sub_head: 3
	worst: 0.4811
	avg: 0.54604334
	best: 0.5893667

Starting e_i: 259
Model ind 640 epoch 259 head A head_i_epoch 0 batch 0: avg loss -2.800856 avg loss no lamb -2.800856 time 2019-02-19 10:00:10.889033
Model ind 640 epoch 259 head A head_i_epoch 0 batch 100: avg loss -2.810717 avg loss no lamb -2.810717 time 2019-02-19 10:02:33.572226
Model ind 640 epoch 259 head A head_i_epoch 0 batch 200: avg loss -2.913972 avg loss no lamb -2.913972 time 2019-02-19 10:04:56.238883
last batch sz 160
Model ind 640 epoch 259 head B head_i_epoch 0 batch 0: avg loss -1.552060 avg loss no lamb -1.552060 time 2019-02-19 10:06:39.775648
Model ind 640 epoch 259 head B head_i_epoch 0 batch 100: avg loss -1.439398 avg loss no lamb -1.439398 time 2019-02-19 10:09:02.700811
Model ind 640 epoch 259 head B head_i_epoch 0 batch 200: avg loss -1.598292 avg loss no lamb -1.598292 time 2019-02-19 10:11:25.276429
last batch sz 160
Model ind 640 epoch 259 head B head_i_epoch 1 batch 0: avg loss -1.515843 avg loss no lamb -1.515843 time 2019-02-19 10:13:08.902954
Model ind 640 epoch 259 head B head_i_epoch 1 batch 100: avg loss -1.508264 avg loss no lamb -1.508264 time 2019-02-19 10:15:31.131770
Model ind 640 epoch 259 head B head_i_epoch 1 batch 200: avg loss -1.620047 avg loss no lamb -1.620047 time 2019-02-19 10:17:53.167103
last batch sz 160
Pre: time 2019-02-19 10:20:01.157624: 
 	std: 0.052373286
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48443332, 0.5905167, 0.59098333, 0.5901, 0.48283333]
	train_accs: [0.48443332, 0.5905167, 0.59098333, 0.5901, 0.48283333]
	best_train_sub_head: 2
	worst: 0.48283333
	avg: 0.54777336
	best: 0.59098333

Starting e_i: 260
Model ind 640 epoch 260 head A head_i_epoch 0 batch 0: avg loss -2.859101 avg loss no lamb -2.859101 time 2019-02-19 10:20:03.103070
Model ind 640 epoch 260 head A head_i_epoch 0 batch 100: avg loss -2.791825 avg loss no lamb -2.791825 time 2019-02-19 10:22:26.383457
Model ind 640 epoch 260 head A head_i_epoch 0 batch 200: avg loss -2.866123 avg loss no lamb -2.866123 time 2019-02-19 10:24:50.110210
last batch sz 160
Model ind 640 epoch 260 head B head_i_epoch 0 batch 0: avg loss -1.603009 avg loss no lamb -1.603009 time 2019-02-19 10:26:34.429222
Model ind 640 epoch 260 head B head_i_epoch 0 batch 100: avg loss -1.364858 avg loss no lamb -1.364858 time 2019-02-19 10:28:57.429335
Model ind 640 epoch 260 head B head_i_epoch 0 batch 200: avg loss -1.618694 avg loss no lamb -1.618694 time 2019-02-19 10:31:21.470946
last batch sz 160
Model ind 640 epoch 260 head B head_i_epoch 1 batch 0: avg loss -1.448555 avg loss no lamb -1.448555 time 2019-02-19 10:33:06.348004
Model ind 640 epoch 260 head B head_i_epoch 1 batch 100: avg loss -1.485940 avg loss no lamb -1.485940 time 2019-02-19 10:35:29.496526
Model ind 640 epoch 260 head B head_i_epoch 1 batch 200: avg loss -1.562913 avg loss no lamb -1.562913 time 2019-02-19 10:37:51.138669
last batch sz 160
Pre: time 2019-02-19 10:39:58.252693: 
 	std: 0.054631572
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48255, 0.59438336, 0.5934167, 0.5936667, 0.48206666]
	train_accs: [0.48255, 0.59438336, 0.5934167, 0.5936667, 0.48206666]
	best_train_sub_head: 1
	worst: 0.48206666
	avg: 0.5492166
	best: 0.59438336

Starting e_i: 261
Model ind 640 epoch 261 head A head_i_epoch 0 batch 0: avg loss -2.794939 avg loss no lamb -2.794939 time 2019-02-19 10:40:03.910341
Model ind 640 epoch 261 head A head_i_epoch 0 batch 100: avg loss -2.784951 avg loss no lamb -2.784951 time 2019-02-19 10:42:27.309827
Model ind 640 epoch 261 head A head_i_epoch 0 batch 200: avg loss -2.883431 avg loss no lamb -2.883431 time 2019-02-19 10:44:51.280314
last batch sz 160
Model ind 640 epoch 261 head B head_i_epoch 0 batch 0: avg loss -1.547677 avg loss no lamb -1.547677 time 2019-02-19 10:46:34.932059
Model ind 640 epoch 261 head B head_i_epoch 0 batch 100: avg loss -1.432197 avg loss no lamb -1.432197 time 2019-02-19 10:48:57.438230
Model ind 640 epoch 261 head B head_i_epoch 0 batch 200: avg loss -1.525578 avg loss no lamb -1.525578 time 2019-02-19 10:51:20.459077
last batch sz 160
Model ind 640 epoch 261 head B head_i_epoch 1 batch 0: avg loss -1.567505 avg loss no lamb -1.567505 time 2019-02-19 10:53:05.156465
Model ind 640 epoch 261 head B head_i_epoch 1 batch 100: avg loss -1.336437 avg loss no lamb -1.336437 time 2019-02-19 10:55:29.259979
Model ind 640 epoch 261 head B head_i_epoch 1 batch 200: avg loss -1.594121 avg loss no lamb -1.594121 time 2019-02-19 10:57:53.295652
last batch sz 160
Pre: time 2019-02-19 11:00:02.269870: 
 	std: 0.053323593
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48795, 0.59721667, 0.59746665, 0.59705, 0.48885]
	train_accs: [0.48795, 0.59721667, 0.59746665, 0.59705, 0.48885]
	best_train_sub_head: 2
	worst: 0.48795
	avg: 0.55370665
	best: 0.59746665

Starting e_i: 262
Model ind 640 epoch 262 head A head_i_epoch 0 batch 0: avg loss -2.794874 avg loss no lamb -2.794874 time 2019-02-19 11:00:04.251187
Model ind 640 epoch 262 head A head_i_epoch 0 batch 100: avg loss -2.738703 avg loss no lamb -2.738703 time 2019-02-19 11:02:28.131931
Model ind 640 epoch 262 head A head_i_epoch 0 batch 200: avg loss -2.906202 avg loss no lamb -2.906202 time 2019-02-19 11:04:51.009024
last batch sz 160
Model ind 640 epoch 262 head B head_i_epoch 0 batch 0: avg loss -1.619392 avg loss no lamb -1.619392 time 2019-02-19 11:06:34.503825
Model ind 640 epoch 262 head B head_i_epoch 0 batch 100: avg loss -1.479168 avg loss no lamb -1.479168 time 2019-02-19 11:08:57.579359
Model ind 640 epoch 262 head B head_i_epoch 0 batch 200: avg loss -1.558818 avg loss no lamb -1.558818 time 2019-02-19 11:11:19.960267
last batch sz 160
Model ind 640 epoch 262 head B head_i_epoch 1 batch 0: avg loss -1.608436 avg loss no lamb -1.608436 time 2019-02-19 11:13:03.535245
Model ind 640 epoch 262 head B head_i_epoch 1 batch 100: avg loss -1.503663 avg loss no lamb -1.503663 time 2019-02-19 11:15:25.863895
Model ind 640 epoch 262 head B head_i_epoch 1 batch 200: avg loss -1.622110 avg loss no lamb -1.622110 time 2019-02-19 11:17:48.335411
last batch sz 160
Pre: time 2019-02-19 11:19:56.739773: 
 	std: 0.05459458
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48545, 0.59741664, 0.59818333, 0.5978, 0.48728332]
	train_accs: [0.48545, 0.59741664, 0.59818333, 0.5978, 0.48728332]
	best_train_sub_head: 2
	worst: 0.48545
	avg: 0.55322665
	best: 0.59818333

Starting e_i: 263
Model ind 640 epoch 263 head A head_i_epoch 0 batch 0: avg loss -2.868309 avg loss no lamb -2.868309 time 2019-02-19 11:19:58.731704
Model ind 640 epoch 263 head A head_i_epoch 0 batch 100: avg loss -2.855853 avg loss no lamb -2.855853 time 2019-02-19 11:22:21.973466
Model ind 640 epoch 263 head A head_i_epoch 0 batch 200: avg loss -2.847649 avg loss no lamb -2.847649 time 2019-02-19 11:24:45.167408
last batch sz 160
Model ind 640 epoch 263 head B head_i_epoch 0 batch 0: avg loss -1.474416 avg loss no lamb -1.474416 time 2019-02-19 11:26:29.487679
Model ind 640 epoch 263 head B head_i_epoch 0 batch 100: avg loss -1.365687 avg loss no lamb -1.365687 time 2019-02-19 11:28:52.799851
Model ind 640 epoch 263 head B head_i_epoch 0 batch 200: avg loss -1.501037 avg loss no lamb -1.501037 time 2019-02-19 11:31:14.942402
last batch sz 160
Model ind 640 epoch 263 head B head_i_epoch 1 batch 0: avg loss -1.484630 avg loss no lamb -1.484630 time 2019-02-19 11:32:59.895640
Model ind 640 epoch 263 head B head_i_epoch 1 batch 100: avg loss -1.372979 avg loss no lamb -1.372979 time 2019-02-19 11:35:23.891673
Model ind 640 epoch 263 head B head_i_epoch 1 batch 200: avg loss -1.590305 avg loss no lamb -1.590305 time 2019-02-19 11:37:47.101757
last batch sz 160
Pre: time 2019-02-19 11:39:56.582625: 
 	std: 0.052262872
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49111667, 0.5973167, 0.59781665, 0.5986, 0.49135]
	train_accs: [0.49111667, 0.5973167, 0.59781665, 0.5986, 0.49135]
	best_train_sub_head: 3
	worst: 0.49111667
	avg: 0.55524
	best: 0.5986

Starting e_i: 264
Model ind 640 epoch 264 head A head_i_epoch 0 batch 0: avg loss -2.802247 avg loss no lamb -2.802247 time 2019-02-19 11:39:58.570105
Model ind 640 epoch 264 head A head_i_epoch 0 batch 100: avg loss -2.785860 avg loss no lamb -2.785860 time 2019-02-19 11:42:22.743769
Model ind 640 epoch 264 head A head_i_epoch 0 batch 200: avg loss -2.815009 avg loss no lamb -2.815009 time 2019-02-19 11:44:46.417700
last batch sz 160
Model ind 640 epoch 264 head B head_i_epoch 0 batch 0: avg loss -1.578787 avg loss no lamb -1.578787 time 2019-02-19 11:46:30.671672
Model ind 640 epoch 264 head B head_i_epoch 0 batch 100: avg loss -1.378183 avg loss no lamb -1.378183 time 2019-02-19 11:48:54.564240
Model ind 640 epoch 264 head B head_i_epoch 0 batch 200: avg loss -1.621282 avg loss no lamb -1.621282 time 2019-02-19 11:51:17.326472
last batch sz 160
Model ind 640 epoch 264 head B head_i_epoch 1 batch 0: avg loss -1.480181 avg loss no lamb -1.480181 time 2019-02-19 11:53:01.197099
Model ind 640 epoch 264 head B head_i_epoch 1 batch 100: avg loss -1.466322 avg loss no lamb -1.466322 time 2019-02-19 11:55:25.271814
Model ind 640 epoch 264 head B head_i_epoch 1 batch 200: avg loss -1.665874 avg loss no lamb -1.665874 time 2019-02-19 11:57:48.656531
last batch sz 160
Pre: time 2019-02-19 11:59:57.559387: 
 	std: 0.05297252
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48713332, 0.59503335, 0.59505, 0.5959, 0.48726666]
	train_accs: [0.48713332, 0.59503335, 0.59505, 0.5959, 0.48726666]
	best_train_sub_head: 3
	worst: 0.48713332
	avg: 0.55207664
	best: 0.5959

Starting e_i: 265
Model ind 640 epoch 265 head A head_i_epoch 0 batch 0: avg loss -2.881033 avg loss no lamb -2.881033 time 2019-02-19 11:59:59.572602
Model ind 640 epoch 265 head A head_i_epoch 0 batch 100: avg loss -2.738448 avg loss no lamb -2.738448 time 2019-02-19 12:02:23.323509
Model ind 640 epoch 265 head A head_i_epoch 0 batch 200: avg loss -2.933136 avg loss no lamb -2.933136 time 2019-02-19 12:04:46.872971
last batch sz 160
Model ind 640 epoch 265 head B head_i_epoch 0 batch 0: avg loss -1.480783 avg loss no lamb -1.480783 time 2019-02-19 12:06:31.363648
Model ind 640 epoch 265 head B head_i_epoch 0 batch 100: avg loss -1.441939 avg loss no lamb -1.441939 time 2019-02-19 12:08:54.477499
Model ind 640 epoch 265 head B head_i_epoch 0 batch 200: avg loss -1.632770 avg loss no lamb -1.632770 time 2019-02-19 12:11:17.548447
last batch sz 160
Model ind 640 epoch 265 head B head_i_epoch 1 batch 0: avg loss -1.551568 avg loss no lamb -1.551568 time 2019-02-19 12:13:01.740501
Model ind 640 epoch 265 head B head_i_epoch 1 batch 100: avg loss -1.410574 avg loss no lamb -1.410574 time 2019-02-19 12:15:25.260969
Model ind 640 epoch 265 head B head_i_epoch 1 batch 200: avg loss -1.650920 avg loss no lamb -1.650920 time 2019-02-19 12:17:48.789549
last batch sz 160
Pre: time 2019-02-19 12:19:58.626600: 
 	std: 0.05369356
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4866, 0.59611666, 0.5956, 0.59643334, 0.4863]
	train_accs: [0.4866, 0.59611666, 0.5956, 0.59643334, 0.4863]
	best_train_sub_head: 3
	worst: 0.4863
	avg: 0.55221
	best: 0.59643334

Starting e_i: 266
Model ind 640 epoch 266 head A head_i_epoch 0 batch 0: avg loss -2.873306 avg loss no lamb -2.873306 time 2019-02-19 12:20:00.636567
Model ind 640 epoch 266 head A head_i_epoch 0 batch 100: avg loss -2.743970 avg loss no lamb -2.743970 time 2019-02-19 12:22:25.202396
Model ind 640 epoch 266 head A head_i_epoch 0 batch 200: avg loss -2.842606 avg loss no lamb -2.842606 time 2019-02-19 12:24:48.177099
last batch sz 160
Model ind 640 epoch 266 head B head_i_epoch 0 batch 0: avg loss -1.521657 avg loss no lamb -1.521657 time 2019-02-19 12:26:33.079706
Model ind 640 epoch 266 head B head_i_epoch 0 batch 100: avg loss -1.430300 avg loss no lamb -1.430300 time 2019-02-19 12:28:57.530407
Model ind 640 epoch 266 head B head_i_epoch 0 batch 200: avg loss -1.615521 avg loss no lamb -1.615521 time 2019-02-19 12:31:22.156722
last batch sz 160
Model ind 640 epoch 266 head B head_i_epoch 1 batch 0: avg loss -1.631698 avg loss no lamb -1.631698 time 2019-02-19 12:33:07.245050
Model ind 640 epoch 266 head B head_i_epoch 1 batch 100: avg loss -1.424825 avg loss no lamb -1.424825 time 2019-02-19 12:35:31.136888
Model ind 640 epoch 266 head B head_i_epoch 1 batch 200: avg loss -1.673599 avg loss no lamb -1.673599 time 2019-02-19 12:37:53.915864
last batch sz 160
Pre: time 2019-02-19 12:40:02.318543: 
 	std: 0.053931154
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49018332, 0.5999333, 0.6002833, 0.6002667, 0.48996666]
	train_accs: [0.49018332, 0.5999333, 0.6002833, 0.6002667, 0.48996666]
	best_train_sub_head: 2
	worst: 0.48996666
	avg: 0.55612665
	best: 0.6002833

Starting e_i: 267
Model ind 640 epoch 267 head A head_i_epoch 0 batch 0: avg loss -2.856793 avg loss no lamb -2.856793 time 2019-02-19 12:40:07.799537
Model ind 640 epoch 267 head A head_i_epoch 0 batch 100: avg loss -2.882301 avg loss no lamb -2.882301 time 2019-02-19 12:42:29.948195
Model ind 640 epoch 267 head A head_i_epoch 0 batch 200: avg loss -2.899276 avg loss no lamb -2.899276 time 2019-02-19 12:44:52.158510
last batch sz 160
Model ind 640 epoch 267 head B head_i_epoch 0 batch 0: avg loss -1.610309 avg loss no lamb -1.610309 time 2019-02-19 12:46:36.062497
Model ind 640 epoch 267 head B head_i_epoch 0 batch 100: avg loss -1.432994 avg loss no lamb -1.432994 time 2019-02-19 12:48:59.053124
Model ind 640 epoch 267 head B head_i_epoch 0 batch 200: avg loss -1.676824 avg loss no lamb -1.676824 time 2019-02-19 12:51:21.978681
last batch sz 160
Model ind 640 epoch 267 head B head_i_epoch 1 batch 0: avg loss -1.577936 avg loss no lamb -1.577936 time 2019-02-19 12:53:06.008739
Model ind 640 epoch 267 head B head_i_epoch 1 batch 100: avg loss -1.454675 avg loss no lamb -1.454675 time 2019-02-19 12:55:29.467956
Model ind 640 epoch 267 head B head_i_epoch 1 batch 200: avg loss -1.663918 avg loss no lamb -1.663918 time 2019-02-19 12:57:53.662132
last batch sz 160
Pre: time 2019-02-19 13:00:02.914876: 
 	std: 0.053277276
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48368335, 0.59216666, 0.59351665, 0.59345, 0.48491666]
	train_accs: [0.48368335, 0.59216666, 0.59351665, 0.59345, 0.48491666]
	best_train_sub_head: 2
	worst: 0.48368335
	avg: 0.54954666
	best: 0.59351665

Starting e_i: 268
Model ind 640 epoch 268 head A head_i_epoch 0 batch 0: avg loss -2.771105 avg loss no lamb -2.771105 time 2019-02-19 13:00:04.938734
Model ind 640 epoch 268 head A head_i_epoch 0 batch 100: avg loss -2.742822 avg loss no lamb -2.742822 time 2019-02-19 13:02:28.606476
Model ind 640 epoch 268 head A head_i_epoch 0 batch 200: avg loss -2.896152 avg loss no lamb -2.896152 time 2019-02-19 13:04:52.032865
last batch sz 160
Model ind 640 epoch 268 head B head_i_epoch 0 batch 0: avg loss -1.467904 avg loss no lamb -1.467904 time 2019-02-19 13:06:35.914932
Model ind 640 epoch 268 head B head_i_epoch 0 batch 100: avg loss -1.435725 avg loss no lamb -1.435725 time 2019-02-19 13:08:58.037676
Model ind 640 epoch 268 head B head_i_epoch 0 batch 200: avg loss -1.611329 avg loss no lamb -1.611329 time 2019-02-19 13:11:21.631650
last batch sz 160
Model ind 640 epoch 268 head B head_i_epoch 1 batch 0: avg loss -1.491882 avg loss no lamb -1.491882 time 2019-02-19 13:13:05.774842
Model ind 640 epoch 268 head B head_i_epoch 1 batch 100: avg loss -1.432709 avg loss no lamb -1.432709 time 2019-02-19 13:15:29.928458
Model ind 640 epoch 268 head B head_i_epoch 1 batch 200: avg loss -1.622807 avg loss no lamb -1.622807 time 2019-02-19 13:17:53.816972
last batch sz 160
Pre: time 2019-02-19 13:20:03.336758: 
 	std: 0.053341508
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48381665, 0.5921, 0.59165, 0.5919833, 0.48225]
	train_accs: [0.48381665, 0.5921, 0.59165, 0.5919833, 0.48225]
	best_train_sub_head: 1
	worst: 0.48225
	avg: 0.54836
	best: 0.5921

Starting e_i: 269
Model ind 640 epoch 269 head A head_i_epoch 0 batch 0: avg loss -2.902262 avg loss no lamb -2.902262 time 2019-02-19 13:20:05.438041
Model ind 640 epoch 269 head A head_i_epoch 0 batch 100: avg loss -2.744720 avg loss no lamb -2.744720 time 2019-02-19 13:22:29.453531
Model ind 640 epoch 269 head A head_i_epoch 0 batch 200: avg loss -2.870148 avg loss no lamb -2.870148 time 2019-02-19 13:24:53.809517
last batch sz 160
Model ind 640 epoch 269 head B head_i_epoch 0 batch 0: avg loss -1.569815 avg loss no lamb -1.569815 time 2019-02-19 13:26:38.535938
Model ind 640 epoch 269 head B head_i_epoch 0 batch 100: avg loss -1.378387 avg loss no lamb -1.378387 time 2019-02-19 13:29:02.687716
Model ind 640 epoch 269 head B head_i_epoch 0 batch 200: avg loss -1.618507 avg loss no lamb -1.618507 time 2019-02-19 13:31:25.184274
last batch sz 160
Model ind 640 epoch 269 head B head_i_epoch 1 batch 0: avg loss -1.529089 avg loss no lamb -1.529089 time 2019-02-19 13:33:08.333070
Model ind 640 epoch 269 head B head_i_epoch 1 batch 100: avg loss -1.489030 avg loss no lamb -1.489030 time 2019-02-19 13:35:30.256823
Model ind 640 epoch 269 head B head_i_epoch 1 batch 200: avg loss -1.617537 avg loss no lamb -1.617537 time 2019-02-19 13:37:53.662626
last batch sz 160
Pre: time 2019-02-19 13:40:02.537249: 
 	std: 0.053559747
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48738334, 0.59765, 0.59721667, 0.5969833, 0.48853335]
	train_accs: [0.48738334, 0.59765, 0.59721667, 0.5969833, 0.48853335]
	best_train_sub_head: 1
	worst: 0.48738334
	avg: 0.55355334
	best: 0.59765

Starting e_i: 270
Model ind 640 epoch 270 head A head_i_epoch 0 batch 0: avg loss -2.922515 avg loss no lamb -2.922515 time 2019-02-19 13:40:04.514749
Model ind 640 epoch 270 head A head_i_epoch 0 batch 100: avg loss -2.809618 avg loss no lamb -2.809618 time 2019-02-19 13:42:27.483515
Model ind 640 epoch 270 head A head_i_epoch 0 batch 200: avg loss -2.806349 avg loss no lamb -2.806349 time 2019-02-19 13:44:50.181480
last batch sz 160
Model ind 640 epoch 270 head B head_i_epoch 0 batch 0: avg loss -1.489975 avg loss no lamb -1.489975 time 2019-02-19 13:46:34.273623
Model ind 640 epoch 270 head B head_i_epoch 0 batch 100: avg loss -1.516970 avg loss no lamb -1.516970 time 2019-02-19 13:48:57.114197
Model ind 640 epoch 270 head B head_i_epoch 0 batch 200: avg loss -1.644298 avg loss no lamb -1.644298 time 2019-02-19 13:51:20.013338
last batch sz 160
Model ind 640 epoch 270 head B head_i_epoch 1 batch 0: avg loss -1.540590 avg loss no lamb -1.540590 time 2019-02-19 13:53:04.060740
Model ind 640 epoch 270 head B head_i_epoch 1 batch 100: avg loss -1.505542 avg loss no lamb -1.505542 time 2019-02-19 13:55:26.790435
Model ind 640 epoch 270 head B head_i_epoch 1 batch 200: avg loss -1.676701 avg loss no lamb -1.676701 time 2019-02-19 13:57:49.248323
last batch sz 160
Pre: time 2019-02-19 13:59:57.291989: 
 	std: 0.052381303
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48256665, 0.5904667, 0.58985, 0.5897667, 0.48365]
	train_accs: [0.48256665, 0.5904667, 0.58985, 0.5897667, 0.48365]
	best_train_sub_head: 1
	worst: 0.48256665
	avg: 0.54726
	best: 0.5904667

Starting e_i: 271
Model ind 640 epoch 271 head A head_i_epoch 0 batch 0: avg loss -2.865041 avg loss no lamb -2.865041 time 2019-02-19 14:00:02.846459
Model ind 640 epoch 271 head A head_i_epoch 0 batch 100: avg loss -2.761791 avg loss no lamb -2.761791 time 2019-02-19 14:02:24.836958
Model ind 640 epoch 271 head A head_i_epoch 0 batch 200: avg loss -2.879448 avg loss no lamb -2.879448 time 2019-02-19 14:04:48.151039
last batch sz 160
Model ind 640 epoch 271 head B head_i_epoch 0 batch 0: avg loss -1.482046 avg loss no lamb -1.482046 time 2019-02-19 14:06:31.967976
Model ind 640 epoch 271 head B head_i_epoch 0 batch 100: avg loss -1.384747 avg loss no lamb -1.384747 time 2019-02-19 14:08:55.452025
Model ind 640 epoch 271 head B head_i_epoch 0 batch 200: avg loss -1.579657 avg loss no lamb -1.579657 time 2019-02-19 14:11:18.229500
last batch sz 160
Model ind 640 epoch 271 head B head_i_epoch 1 batch 0: avg loss -1.472687 avg loss no lamb -1.472687 time 2019-02-19 14:13:02.117964
Model ind 640 epoch 271 head B head_i_epoch 1 batch 100: avg loss -1.406693 avg loss no lamb -1.406693 time 2019-02-19 14:15:24.773511
Model ind 640 epoch 271 head B head_i_epoch 1 batch 200: avg loss -1.690016 avg loss no lamb -1.690016 time 2019-02-19 14:17:47.167388
last batch sz 160
Pre: time 2019-02-19 14:19:55.093485: 
 	std: 0.052929047
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4831, 0.5923, 0.59135, 0.5916333, 0.48435]
	train_accs: [0.4831, 0.5923, 0.59135, 0.5916333, 0.48435]
	best_train_sub_head: 1
	worst: 0.4831
	avg: 0.5485467
	best: 0.5923

Starting e_i: 272
Model ind 640 epoch 272 head A head_i_epoch 0 batch 0: avg loss -2.847728 avg loss no lamb -2.847728 time 2019-02-19 14:19:57.085159
Model ind 640 epoch 272 head A head_i_epoch 0 batch 100: avg loss -2.774541 avg loss no lamb -2.774541 time 2019-02-19 14:22:20.376458
Model ind 640 epoch 272 head A head_i_epoch 0 batch 200: avg loss -2.859033 avg loss no lamb -2.859033 time 2019-02-19 14:24:42.925873
last batch sz 160
Model ind 640 epoch 272 head B head_i_epoch 0 batch 0: avg loss -1.504364 avg loss no lamb -1.504364 time 2019-02-19 14:26:26.487056
Model ind 640 epoch 272 head B head_i_epoch 0 batch 100: avg loss -1.451073 avg loss no lamb -1.451073 time 2019-02-19 14:28:48.832476
Model ind 640 epoch 272 head B head_i_epoch 0 batch 200: avg loss -1.602686 avg loss no lamb -1.602686 time 2019-02-19 14:31:11.293720
last batch sz 160
Model ind 640 epoch 272 head B head_i_epoch 1 batch 0: avg loss -1.549432 avg loss no lamb -1.549432 time 2019-02-19 14:32:54.834544
Model ind 640 epoch 272 head B head_i_epoch 1 batch 100: avg loss -1.443120 avg loss no lamb -1.443120 time 2019-02-19 14:35:17.665897
Model ind 640 epoch 272 head B head_i_epoch 1 batch 200: avg loss -1.631847 avg loss no lamb -1.631847 time 2019-02-19 14:37:41.077389
last batch sz 160
Pre: time 2019-02-19 14:39:50.009272: 
 	std: 0.053142685
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4898, 0.59915, 0.59891665, 0.5991, 0.49136665]
	train_accs: [0.4898, 0.59915, 0.59891665, 0.5991, 0.49136665]
	best_train_sub_head: 1
	worst: 0.4898
	avg: 0.5556667
	best: 0.59915

Starting e_i: 273
Model ind 640 epoch 273 head A head_i_epoch 0 batch 0: avg loss -2.846596 avg loss no lamb -2.846596 time 2019-02-19 14:39:51.986183
Model ind 640 epoch 273 head A head_i_epoch 0 batch 100: avg loss -2.857744 avg loss no lamb -2.857744 time 2019-02-19 14:42:15.733409
Model ind 640 epoch 273 head A head_i_epoch 0 batch 200: avg loss -2.869778 avg loss no lamb -2.869778 time 2019-02-19 14:44:39.085701
last batch sz 160
Model ind 640 epoch 273 head B head_i_epoch 0 batch 0: avg loss -1.566611 avg loss no lamb -1.566611 time 2019-02-19 14:46:22.843177
Model ind 640 epoch 273 head B head_i_epoch 0 batch 100: avg loss -1.372031 avg loss no lamb -1.372031 time 2019-02-19 14:48:45.465297
Model ind 640 epoch 273 head B head_i_epoch 0 batch 200: avg loss -1.675183 avg loss no lamb -1.675183 time 2019-02-19 14:51:08.586995
last batch sz 160
Model ind 640 epoch 273 head B head_i_epoch 1 batch 0: avg loss -1.621459 avg loss no lamb -1.621459 time 2019-02-19 14:52:53.308152
Model ind 640 epoch 273 head B head_i_epoch 1 batch 100: avg loss -1.389937 avg loss no lamb -1.389937 time 2019-02-19 14:55:16.934726
Model ind 640 epoch 273 head B head_i_epoch 1 batch 200: avg loss -1.695840 avg loss no lamb -1.695840 time 2019-02-19 14:57:41.429189
last batch sz 160
Pre: time 2019-02-19 14:59:52.218552: 
 	std: 0.053923573
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48946667, 0.59936666, 0.59885, 0.59931666, 0.48875]
	train_accs: [0.48946667, 0.59936666, 0.59885, 0.59931666, 0.48875]
	best_train_sub_head: 1
	worst: 0.48875
	avg: 0.55515
	best: 0.59936666

Starting e_i: 274
Model ind 640 epoch 274 head A head_i_epoch 0 batch 0: avg loss -2.865345 avg loss no lamb -2.865345 time 2019-02-19 14:59:54.336875
Model ind 640 epoch 274 head A head_i_epoch 0 batch 100: avg loss -2.865994 avg loss no lamb -2.865994 time 2019-02-19 15:02:18.257076
Model ind 640 epoch 274 head A head_i_epoch 0 batch 200: avg loss -2.845404 avg loss no lamb -2.845404 time 2019-02-19 15:04:43.020058
last batch sz 160
Model ind 640 epoch 274 head B head_i_epoch 0 batch 0: avg loss -1.469539 avg loss no lamb -1.469539 time 2019-02-19 15:06:29.122635
Model ind 640 epoch 274 head B head_i_epoch 0 batch 100: avg loss -1.482518 avg loss no lamb -1.482518 time 2019-02-19 15:08:53.397968
Model ind 640 epoch 274 head B head_i_epoch 0 batch 200: avg loss -1.574023 avg loss no lamb -1.574023 time 2019-02-19 15:11:18.919948
last batch sz 160
Model ind 640 epoch 274 head B head_i_epoch 1 batch 0: avg loss -1.601829 avg loss no lamb -1.601829 time 2019-02-19 15:13:04.150137
Model ind 640 epoch 274 head B head_i_epoch 1 batch 100: avg loss -1.444478 avg loss no lamb -1.444478 time 2019-02-19 15:15:28.366706
Model ind 640 epoch 274 head B head_i_epoch 1 batch 200: avg loss -1.690394 avg loss no lamb -1.690394 time 2019-02-19 15:17:52.060037
last batch sz 160
Pre: time 2019-02-19 15:20:02.098842: 
 	std: 0.05400676
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4869, 0.5977333, 0.59756666, 0.5976167, 0.4879]
	train_accs: [0.4869, 0.5977333, 0.59756666, 0.5976167, 0.4879]
	best_train_sub_head: 1
	worst: 0.4869
	avg: 0.5535433
	best: 0.5977333

Starting e_i: 275
Model ind 640 epoch 275 head A head_i_epoch 0 batch 0: avg loss -2.775606 avg loss no lamb -2.775606 time 2019-02-19 15:20:04.648268
Model ind 640 epoch 275 head A head_i_epoch 0 batch 100: avg loss -2.822389 avg loss no lamb -2.822389 time 2019-02-19 15:22:29.350870
Model ind 640 epoch 275 head A head_i_epoch 0 batch 200: avg loss -2.821808 avg loss no lamb -2.821808 time 2019-02-19 15:24:52.910447
last batch sz 160
Model ind 640 epoch 275 head B head_i_epoch 0 batch 0: avg loss -1.506337 avg loss no lamb -1.506337 time 2019-02-19 15:26:37.177545
Model ind 640 epoch 275 head B head_i_epoch 0 batch 100: avg loss -1.475053 avg loss no lamb -1.475053 time 2019-02-19 15:29:00.737652
Model ind 640 epoch 275 head B head_i_epoch 0 batch 200: avg loss -1.574573 avg loss no lamb -1.574573 time 2019-02-19 15:31:24.211211
last batch sz 160
Model ind 640 epoch 275 head B head_i_epoch 1 batch 0: avg loss -1.543209 avg loss no lamb -1.543209 time 2019-02-19 15:33:08.626425
Model ind 640 epoch 275 head B head_i_epoch 1 batch 100: avg loss -1.426028 avg loss no lamb -1.426028 time 2019-02-19 15:35:32.357786
Model ind 640 epoch 275 head B head_i_epoch 1 batch 200: avg loss -1.598852 avg loss no lamb -1.598852 time 2019-02-19 15:37:55.791950
last batch sz 160
Pre: time 2019-02-19 15:40:04.957995: 
 	std: 0.054032642
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48601666, 0.59706664, 0.5966167, 0.59636664, 0.48676667]
	train_accs: [0.48601666, 0.59706664, 0.5966167, 0.59636664, 0.48676667]
	best_train_sub_head: 1
	worst: 0.48601666
	avg: 0.55256665
	best: 0.59706664

Starting e_i: 276
Model ind 640 epoch 276 head A head_i_epoch 0 batch 0: avg loss -2.852900 avg loss no lamb -2.852900 time 2019-02-19 15:40:06.988963
Model ind 640 epoch 276 head A head_i_epoch 0 batch 100: avg loss -2.758252 avg loss no lamb -2.758252 time 2019-02-19 15:42:31.224725
Model ind 640 epoch 276 head A head_i_epoch 0 batch 200: avg loss -2.932220 avg loss no lamb -2.932220 time 2019-02-19 15:44:55.748230
last batch sz 160
Model ind 640 epoch 276 head B head_i_epoch 0 batch 0: avg loss -1.583317 avg loss no lamb -1.583317 time 2019-02-19 15:46:40.862171
Model ind 640 epoch 276 head B head_i_epoch 0 batch 100: avg loss -1.577957 avg loss no lamb -1.577957 time 2019-02-19 15:49:05.454586
Model ind 640 epoch 276 head B head_i_epoch 0 batch 200: avg loss -1.565633 avg loss no lamb -1.565633 time 2019-02-19 15:51:29.769531
last batch sz 160
Model ind 640 epoch 276 head B head_i_epoch 1 batch 0: avg loss -1.576240 avg loss no lamb -1.576240 time 2019-02-19 15:53:14.479076
Model ind 640 epoch 276 head B head_i_epoch 1 batch 100: avg loss -1.462063 avg loss no lamb -1.462063 time 2019-02-19 15:55:38.517729
Model ind 640 epoch 276 head B head_i_epoch 1 batch 200: avg loss -1.636369 avg loss no lamb -1.636369 time 2019-02-19 15:58:02.380643
last batch sz 160
Pre: time 2019-02-19 16:00:11.650835: 
 	std: 0.05375416
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48013332, 0.58998334, 0.5901, 0.58986664, 0.48038334]
	train_accs: [0.48013332, 0.58998334, 0.5901, 0.58986664, 0.48038334]
	best_train_sub_head: 2
	worst: 0.48013332
	avg: 0.54609334
	best: 0.5901

Starting e_i: 277
Model ind 640 epoch 277 head A head_i_epoch 0 batch 0: avg loss -2.868613 avg loss no lamb -2.868613 time 2019-02-19 16:00:14.071210
Model ind 640 epoch 277 head A head_i_epoch 0 batch 100: avg loss -2.811618 avg loss no lamb -2.811618 time 2019-02-19 16:02:38.528921
Model ind 640 epoch 277 head A head_i_epoch 0 batch 200: avg loss -2.803833 avg loss no lamb -2.803833 time 2019-02-19 16:05:03.633105
last batch sz 160
Model ind 640 epoch 277 head B head_i_epoch 0 batch 0: avg loss -1.508635 avg loss no lamb -1.508635 time 2019-02-19 16:06:48.855514
Model ind 640 epoch 277 head B head_i_epoch 0 batch 100: avg loss -1.466492 avg loss no lamb -1.466492 time 2019-02-19 16:09:12.446051
Model ind 640 epoch 277 head B head_i_epoch 0 batch 200: avg loss -1.613571 avg loss no lamb -1.613571 time 2019-02-19 16:11:36.480278
last batch sz 160
Model ind 640 epoch 277 head B head_i_epoch 1 batch 0: avg loss -1.489946 avg loss no lamb -1.489946 time 2019-02-19 16:13:21.431014
Model ind 640 epoch 277 head B head_i_epoch 1 batch 100: avg loss -1.423161 avg loss no lamb -1.423161 time 2019-02-19 16:15:45.136700
Model ind 640 epoch 277 head B head_i_epoch 1 batch 200: avg loss -1.587660 avg loss no lamb -1.587660 time 2019-02-19 16:18:09.451685
last batch sz 160
Pre: time 2019-02-19 16:20:19.499905: 
 	std: 0.05153818
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.47923332, 0.5850667, 0.58488333, 0.58445, 0.47996667]
	train_accs: [0.47923332, 0.5850667, 0.58488333, 0.58445, 0.47996667]
	best_train_sub_head: 1
	worst: 0.47923332
	avg: 0.54272
	best: 0.5850667

Starting e_i: 278
Model ind 640 epoch 278 head A head_i_epoch 0 batch 0: avg loss -2.883497 avg loss no lamb -2.883497 time 2019-02-19 16:20:21.529100
Model ind 640 epoch 278 head A head_i_epoch 0 batch 100: avg loss -2.832916 avg loss no lamb -2.832916 time 2019-02-19 16:22:45.794156
Model ind 640 epoch 278 head A head_i_epoch 0 batch 200: avg loss -2.950940 avg loss no lamb -2.950940 time 2019-02-19 16:25:09.946506
last batch sz 160
Model ind 640 epoch 278 head B head_i_epoch 0 batch 0: avg loss -1.548717 avg loss no lamb -1.548717 time 2019-02-19 16:26:55.237388
Model ind 640 epoch 278 head B head_i_epoch 0 batch 100: avg loss -1.489501 avg loss no lamb -1.489501 time 2019-02-19 16:29:19.110663
Model ind 640 epoch 278 head B head_i_epoch 0 batch 200: avg loss -1.616167 avg loss no lamb -1.616167 time 2019-02-19 16:31:44.848387
last batch sz 160
Model ind 640 epoch 278 head B head_i_epoch 1 batch 0: avg loss -1.573638 avg loss no lamb -1.573638 time 2019-02-19 16:33:29.736358
Model ind 640 epoch 278 head B head_i_epoch 1 batch 100: avg loss -1.452413 avg loss no lamb -1.452413 time 2019-02-19 16:35:54.046219
Model ind 640 epoch 278 head B head_i_epoch 1 batch 200: avg loss -1.611482 avg loss no lamb -1.611482 time 2019-02-19 16:38:17.923908
last batch sz 160
Pre: time 2019-02-19 16:40:26.755933: 
 	std: 0.0542704
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48713332, 0.5981167, 0.5973833, 0.59798336, 0.48696667]
	train_accs: [0.48713332, 0.5981167, 0.5973833, 0.59798336, 0.48696667]
	best_train_sub_head: 1
	worst: 0.48696667
	avg: 0.5535167
	best: 0.5981167

Starting e_i: 279
Model ind 640 epoch 279 head A head_i_epoch 0 batch 0: avg loss -2.885360 avg loss no lamb -2.885360 time 2019-02-19 16:40:29.283251
Model ind 640 epoch 279 head A head_i_epoch 0 batch 100: avg loss -2.794012 avg loss no lamb -2.794012 time 2019-02-19 16:42:53.772702
Model ind 640 epoch 279 head A head_i_epoch 0 batch 200: avg loss -2.844461 avg loss no lamb -2.844461 time 2019-02-19 16:45:18.329328
last batch sz 160
Model ind 640 epoch 279 head B head_i_epoch 0 batch 0: avg loss -1.557914 avg loss no lamb -1.557914 time 2019-02-19 16:47:03.588172
Model ind 640 epoch 279 head B head_i_epoch 0 batch 100: avg loss -1.508213 avg loss no lamb -1.508213 time 2019-02-19 16:49:27.767026
Model ind 640 epoch 279 head B head_i_epoch 0 batch 200: avg loss -1.603561 avg loss no lamb -1.603561 time 2019-02-19 16:51:52.064939
last batch sz 160
Model ind 640 epoch 279 head B head_i_epoch 1 batch 0: avg loss -1.636278 avg loss no lamb -1.636278 time 2019-02-19 16:53:36.797162
Model ind 640 epoch 279 head B head_i_epoch 1 batch 100: avg loss -1.471596 avg loss no lamb -1.471596 time 2019-02-19 16:56:00.449156
Model ind 640 epoch 279 head B head_i_epoch 1 batch 200: avg loss -1.657131 avg loss no lamb -1.657131 time 2019-02-19 16:58:24.740366
last batch sz 160
Pre: time 2019-02-19 17:00:34.666854: 
 	std: 0.05551004
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48271668, 0.5965667, 0.59608334, 0.5959, 0.48303333]
	train_accs: [0.48271668, 0.5965667, 0.59608334, 0.5959, 0.48303333]
	best_train_sub_head: 1
	worst: 0.48271668
	avg: 0.55086005
	best: 0.5965667

Starting e_i: 280
Model ind 640 epoch 280 head A head_i_epoch 0 batch 0: avg loss -2.875686 avg loss no lamb -2.875686 time 2019-02-19 17:00:36.739205
Model ind 640 epoch 280 head A head_i_epoch 0 batch 100: avg loss -2.783001 avg loss no lamb -2.783001 time 2019-02-19 17:03:01.451557
Model ind 640 epoch 280 head A head_i_epoch 0 batch 200: avg loss -2.897273 avg loss no lamb -2.897273 time 2019-02-19 17:05:26.234866
last batch sz 160
Model ind 640 epoch 280 head B head_i_epoch 0 batch 0: avg loss -1.577470 avg loss no lamb -1.577470 time 2019-02-19 17:07:11.821337
Model ind 640 epoch 280 head B head_i_epoch 0 batch 100: avg loss -1.464677 avg loss no lamb -1.464677 time 2019-02-19 17:09:38.032351
Model ind 640 epoch 280 head B head_i_epoch 0 batch 200: avg loss -1.583648 avg loss no lamb -1.583648 time 2019-02-19 17:12:01.560917
last batch sz 160
Model ind 640 epoch 280 head B head_i_epoch 1 batch 0: avg loss -1.478700 avg loss no lamb -1.478700 time 2019-02-19 17:13:45.927674
Model ind 640 epoch 280 head B head_i_epoch 1 batch 100: avg loss -1.497253 avg loss no lamb -1.497253 time 2019-02-19 17:16:10.056067
Model ind 640 epoch 280 head B head_i_epoch 1 batch 200: avg loss -1.581966 avg loss no lamb -1.581966 time 2019-02-19 17:18:33.623609
last batch sz 160
Pre: time 2019-02-19 17:20:42.823755: 
 	std: 0.054168742
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4806, 0.5917, 0.5918, 0.59113336, 0.48135]
	train_accs: [0.4806, 0.5917, 0.5918, 0.59113336, 0.48135]
	best_train_sub_head: 2
	worst: 0.4806
	avg: 0.5473167
	best: 0.5918

Starting e_i: 281
Model ind 640 epoch 281 head A head_i_epoch 0 batch 0: avg loss -2.932735 avg loss no lamb -2.932735 time 2019-02-19 17:20:49.137015
Model ind 640 epoch 281 head A head_i_epoch 0 batch 100: avg loss -2.807442 avg loss no lamb -2.807442 time 2019-02-19 17:23:13.750942
Model ind 640 epoch 281 head A head_i_epoch 0 batch 200: avg loss -2.845588 avg loss no lamb -2.845588 time 2019-02-19 17:25:37.994573
last batch sz 160
Model ind 640 epoch 281 head B head_i_epoch 0 batch 0: avg loss -1.534907 avg loss no lamb -1.534907 time 2019-02-19 17:27:22.601841
Model ind 640 epoch 281 head B head_i_epoch 0 batch 100: avg loss -1.373387 avg loss no lamb -1.373387 time 2019-02-19 17:29:46.446899
Model ind 640 epoch 281 head B head_i_epoch 0 batch 200: avg loss -1.634489 avg loss no lamb -1.634489 time 2019-02-19 17:32:10.686347
last batch sz 160
Model ind 640 epoch 281 head B head_i_epoch 1 batch 0: avg loss -1.579154 avg loss no lamb -1.579154 time 2019-02-19 17:33:55.669827
Model ind 640 epoch 281 head B head_i_epoch 1 batch 100: avg loss -1.510544 avg loss no lamb -1.510544 time 2019-02-19 17:36:19.385386
Model ind 640 epoch 281 head B head_i_epoch 1 batch 200: avg loss -1.622567 avg loss no lamb -1.622567 time 2019-02-19 17:38:43.131521
last batch sz 160
Pre: time 2019-02-19 17:40:52.727194: 
 	std: 0.05500992
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48215, 0.59433335, 0.5944667, 0.59358335, 0.48153332]
	train_accs: [0.48215, 0.59433335, 0.5944667, 0.59358335, 0.48153332]
	best_train_sub_head: 2
	worst: 0.48153332
	avg: 0.54921335
	best: 0.5944667

Starting e_i: 282
Model ind 640 epoch 282 head A head_i_epoch 0 batch 0: avg loss -2.837420 avg loss no lamb -2.837420 time 2019-02-19 17:40:54.756148
Model ind 640 epoch 282 head A head_i_epoch 0 batch 100: avg loss -2.798094 avg loss no lamb -2.798094 time 2019-02-19 17:43:18.657873
Model ind 640 epoch 282 head A head_i_epoch 0 batch 200: avg loss -2.903477 avg loss no lamb -2.903477 time 2019-02-19 17:45:42.251494
last batch sz 160
Model ind 640 epoch 282 head B head_i_epoch 0 batch 0: avg loss -1.572644 avg loss no lamb -1.572644 time 2019-02-19 17:47:26.529413
Model ind 640 epoch 282 head B head_i_epoch 0 batch 100: avg loss -1.401053 avg loss no lamb -1.401053 time 2019-02-19 17:49:50.183036
Model ind 640 epoch 282 head B head_i_epoch 0 batch 200: avg loss -1.603512 avg loss no lamb -1.603512 time 2019-02-19 17:52:14.452437
last batch sz 160
Model ind 640 epoch 282 head B head_i_epoch 1 batch 0: avg loss -1.510831 avg loss no lamb -1.510831 time 2019-02-19 17:53:59.480044
Model ind 640 epoch 282 head B head_i_epoch 1 batch 100: avg loss -1.554884 avg loss no lamb -1.554884 time 2019-02-19 17:56:24.094858
Model ind 640 epoch 282 head B head_i_epoch 1 batch 200: avg loss -1.713500 avg loss no lamb -1.713500 time 2019-02-19 17:58:50.024466
last batch sz 160
Pre: time 2019-02-19 18:00:59.025395: 
 	std: 0.053434793
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48561665, 0.5944833, 0.5941333, 0.5947, 0.48511666]
	train_accs: [0.48561665, 0.5944833, 0.5941333, 0.5947, 0.48511666]
	best_train_sub_head: 3
	worst: 0.48511666
	avg: 0.55081
	best: 0.5947

Starting e_i: 283
Model ind 640 epoch 283 head A head_i_epoch 0 batch 0: avg loss -2.858813 avg loss no lamb -2.858813 time 2019-02-19 18:01:01.329700
Model ind 640 epoch 283 head A head_i_epoch 0 batch 100: avg loss -2.730945 avg loss no lamb -2.730945 time 2019-02-19 18:03:25.856835
Model ind 640 epoch 283 head A head_i_epoch 0 batch 200: avg loss -2.842972 avg loss no lamb -2.842972 time 2019-02-19 18:05:50.321963
last batch sz 160
Model ind 640 epoch 283 head B head_i_epoch 0 batch 0: avg loss -1.553771 avg loss no lamb -1.553771 time 2019-02-19 18:07:35.631508
Model ind 640 epoch 283 head B head_i_epoch 0 batch 100: avg loss -1.527852 avg loss no lamb -1.527852 time 2019-02-19 18:09:59.622493
Model ind 640 epoch 283 head B head_i_epoch 0 batch 200: avg loss -1.589930 avg loss no lamb -1.589930 time 2019-02-19 18:12:25.246093
last batch sz 160
Model ind 640 epoch 283 head B head_i_epoch 1 batch 0: avg loss -1.481258 avg loss no lamb -1.481258 time 2019-02-19 18:14:10.543358
Model ind 640 epoch 283 head B head_i_epoch 1 batch 100: avg loss -1.431314 avg loss no lamb -1.431314 time 2019-02-19 18:16:34.381553
Model ind 640 epoch 283 head B head_i_epoch 1 batch 200: avg loss -1.672546 avg loss no lamb -1.672546 time 2019-02-19 18:18:58.204557
last batch sz 160
Pre: time 2019-02-19 18:21:06.887811: 
 	std: 0.054919437
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48433334, 0.59636664, 0.5958667, 0.5964, 0.48388332]
	train_accs: [0.48433334, 0.59636664, 0.5958667, 0.5964, 0.48388332]
	best_train_sub_head: 3
	worst: 0.48388332
	avg: 0.55137
	best: 0.5964

Starting e_i: 284
Model ind 640 epoch 284 head A head_i_epoch 0 batch 0: avg loss -2.964229 avg loss no lamb -2.964229 time 2019-02-19 18:21:08.884165
Model ind 640 epoch 284 head A head_i_epoch 0 batch 100: avg loss -2.757243 avg loss no lamb -2.757243 time 2019-02-19 18:23:32.698254
Model ind 640 epoch 284 head A head_i_epoch 0 batch 200: avg loss -2.837419 avg loss no lamb -2.837419 time 2019-02-19 18:25:56.890095
last batch sz 160
Model ind 640 epoch 284 head B head_i_epoch 0 batch 0: avg loss -1.594088 avg loss no lamb -1.594088 time 2019-02-19 18:27:41.834838
Model ind 640 epoch 284 head B head_i_epoch 0 batch 100: avg loss -1.517624 avg loss no lamb -1.517624 time 2019-02-19 18:30:05.759114
Model ind 640 epoch 284 head B head_i_epoch 0 batch 200: avg loss -1.654707 avg loss no lamb -1.654707 time 2019-02-19 18:32:28.566347
last batch sz 160
Model ind 640 epoch 284 head B head_i_epoch 1 batch 0: avg loss -1.547645 avg loss no lamb -1.547645 time 2019-02-19 18:34:13.172039
Model ind 640 epoch 284 head B head_i_epoch 1 batch 100: avg loss -1.482764 avg loss no lamb -1.482764 time 2019-02-19 18:36:37.714521
Model ind 640 epoch 284 head B head_i_epoch 1 batch 200: avg loss -1.654813 avg loss no lamb -1.654813 time 2019-02-19 18:39:01.259056
last batch sz 160
Pre: time 2019-02-19 18:41:10.855320: 
 	std: 0.052569915
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48923334, 0.59776664, 0.59665, 0.5965667, 0.49015]
	train_accs: [0.48923334, 0.59776664, 0.59665, 0.5965667, 0.49015]
	best_train_sub_head: 1
	worst: 0.48923334
	avg: 0.55407333
	best: 0.59776664

Starting e_i: 285
Model ind 640 epoch 285 head A head_i_epoch 0 batch 0: avg loss -2.855446 avg loss no lamb -2.855446 time 2019-02-19 18:41:12.988162
Model ind 640 epoch 285 head A head_i_epoch 0 batch 100: avg loss -2.762291 avg loss no lamb -2.762291 time 2019-02-19 18:43:37.757523
Model ind 640 epoch 285 head A head_i_epoch 0 batch 200: avg loss -2.903336 avg loss no lamb -2.903336 time 2019-02-19 18:46:01.975973
last batch sz 160
Model ind 640 epoch 285 head B head_i_epoch 0 batch 0: avg loss -1.528509 avg loss no lamb -1.528509 time 2019-02-19 18:47:47.887056
Model ind 640 epoch 285 head B head_i_epoch 0 batch 100: avg loss -1.507701 avg loss no lamb -1.507701 time 2019-02-19 18:50:12.629068
Model ind 640 epoch 285 head B head_i_epoch 0 batch 200: avg loss -1.605618 avg loss no lamb -1.605618 time 2019-02-19 18:52:36.839460
last batch sz 160
Model ind 640 epoch 285 head B head_i_epoch 1 batch 0: avg loss -1.613101 avg loss no lamb -1.613101 time 2019-02-19 18:54:21.357204
Model ind 640 epoch 285 head B head_i_epoch 1 batch 100: avg loss -1.502955 avg loss no lamb -1.502955 time 2019-02-19 18:56:44.875977
Model ind 640 epoch 285 head B head_i_epoch 1 batch 200: avg loss -1.605783 avg loss no lamb -1.605783 time 2019-02-19 18:59:08.869198
last batch sz 160
Pre: time 2019-02-19 19:01:18.395541: 
 	std: 0.05507039
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48356667, 0.5962333, 0.59568334, 0.5962667, 0.48373333]
	train_accs: [0.48356667, 0.5962333, 0.59568334, 0.5962667, 0.48373333]
	best_train_sub_head: 3
	worst: 0.48356667
	avg: 0.5510967
	best: 0.5962667

Starting e_i: 286
Model ind 640 epoch 286 head A head_i_epoch 0 batch 0: avg loss -2.883568 avg loss no lamb -2.883568 time 2019-02-19 19:01:20.404720
Model ind 640 epoch 286 head A head_i_epoch 0 batch 100: avg loss -2.834585 avg loss no lamb -2.834585 time 2019-02-19 19:03:44.184124
Model ind 640 epoch 286 head A head_i_epoch 0 batch 200: avg loss -2.948610 avg loss no lamb -2.948610 time 2019-02-19 19:06:08.023316
last batch sz 160
Model ind 640 epoch 286 head B head_i_epoch 0 batch 0: avg loss -1.615801 avg loss no lamb -1.615801 time 2019-02-19 19:07:52.838309
Model ind 640 epoch 286 head B head_i_epoch 0 batch 100: avg loss -1.537938 avg loss no lamb -1.537938 time 2019-02-19 19:10:16.905547
Model ind 640 epoch 286 head B head_i_epoch 0 batch 200: avg loss -1.720598 avg loss no lamb -1.720598 time 2019-02-19 19:12:41.073625
last batch sz 160
Model ind 640 epoch 286 head B head_i_epoch 1 batch 0: avg loss -1.527195 avg loss no lamb -1.527195 time 2019-02-19 19:14:25.445667
Model ind 640 epoch 286 head B head_i_epoch 1 batch 100: avg loss -1.377851 avg loss no lamb -1.377851 time 2019-02-19 19:16:49.372974
Model ind 640 epoch 286 head B head_i_epoch 1 batch 200: avg loss -1.584645 avg loss no lamb -1.584645 time 2019-02-19 19:19:13.085565
last batch sz 160
Pre: time 2019-02-19 19:21:21.524737: 
 	std: 0.05338425
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48651665, 0.59548336, 0.5958, 0.59585, 0.48696667]
	train_accs: [0.48651665, 0.59548336, 0.5958, 0.59585, 0.48696667]
	best_train_sub_head: 3
	worst: 0.48651665
	avg: 0.5521233
	best: 0.59585

Starting e_i: 287
Model ind 640 epoch 287 head A head_i_epoch 0 batch 0: avg loss -2.901254 avg loss no lamb -2.901254 time 2019-02-19 19:21:23.514552
Model ind 640 epoch 287 head A head_i_epoch 0 batch 100: avg loss -2.749509 avg loss no lamb -2.749509 time 2019-02-19 19:23:47.212848
Model ind 640 epoch 287 head A head_i_epoch 0 batch 200: avg loss -2.981756 avg loss no lamb -2.981756 time 2019-02-19 19:26:11.041646
last batch sz 160
Model ind 640 epoch 287 head B head_i_epoch 0 batch 0: avg loss -1.530413 avg loss no lamb -1.530413 time 2019-02-19 19:27:56.225089
Model ind 640 epoch 287 head B head_i_epoch 0 batch 100: avg loss -1.424955 avg loss no lamb -1.424955 time 2019-02-19 19:30:20.261095
Model ind 640 epoch 287 head B head_i_epoch 0 batch 200: avg loss -1.671802 avg loss no lamb -1.671802 time 2019-02-19 19:32:44.467304
last batch sz 160
Model ind 640 epoch 287 head B head_i_epoch 1 batch 0: avg loss -1.596185 avg loss no lamb -1.596185 time 2019-02-19 19:34:29.622035
Model ind 640 epoch 287 head B head_i_epoch 1 batch 100: avg loss -1.440848 avg loss no lamb -1.440848 time 2019-02-19 19:36:53.553601
Model ind 640 epoch 287 head B head_i_epoch 1 batch 200: avg loss -1.615638 avg loss no lamb -1.615638 time 2019-02-19 19:39:17.838990
last batch sz 160
Pre: time 2019-02-19 19:41:27.251506: 
 	std: 0.054201953
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48358333, 0.59471667, 0.59363335, 0.59368336, 0.48316666]
	train_accs: [0.48358333, 0.59471667, 0.59363335, 0.59368336, 0.48316666]
	best_train_sub_head: 1
	worst: 0.48316666
	avg: 0.5497567
	best: 0.59471667

Starting e_i: 288
Model ind 640 epoch 288 head A head_i_epoch 0 batch 0: avg loss -2.872596 avg loss no lamb -2.872596 time 2019-02-19 19:41:29.297354
Model ind 640 epoch 288 head A head_i_epoch 0 batch 100: avg loss -2.832617 avg loss no lamb -2.832617 time 2019-02-19 19:43:53.319839
Model ind 640 epoch 288 head A head_i_epoch 0 batch 200: avg loss -2.794388 avg loss no lamb -2.794388 time 2019-02-19 19:46:17.321670
last batch sz 160
Model ind 640 epoch 288 head B head_i_epoch 0 batch 0: avg loss -1.561293 avg loss no lamb -1.561293 time 2019-02-19 19:48:02.231741
Model ind 640 epoch 288 head B head_i_epoch 0 batch 100: avg loss -1.384274 avg loss no lamb -1.384274 time 2019-02-19 19:50:26.151448
Model ind 640 epoch 288 head B head_i_epoch 0 batch 200: avg loss -1.657660 avg loss no lamb -1.657660 time 2019-02-19 19:52:49.737966
last batch sz 160
Model ind 640 epoch 288 head B head_i_epoch 1 batch 0: avg loss -1.574765 avg loss no lamb -1.574765 time 2019-02-19 19:54:34.330786
Model ind 640 epoch 288 head B head_i_epoch 1 batch 100: avg loss -1.456951 avg loss no lamb -1.456951 time 2019-02-19 19:56:58.333312
Model ind 640 epoch 288 head B head_i_epoch 1 batch 200: avg loss -1.610059 avg loss no lamb -1.610059 time 2019-02-19 19:59:21.554737
last batch sz 160
Pre: time 2019-02-19 20:01:30.995457: 
 	std: 0.05339374
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4833, 0.59313333, 0.59248334, 0.5918667, 0.48371667]
	train_accs: [0.4833, 0.59313333, 0.59248334, 0.5918667, 0.48371667]
	best_train_sub_head: 1
	worst: 0.4833
	avg: 0.5489
	best: 0.59313333

Starting e_i: 289
Model ind 640 epoch 289 head A head_i_epoch 0 batch 0: avg loss -2.908529 avg loss no lamb -2.908529 time 2019-02-19 20:01:33.053727
Model ind 640 epoch 289 head A head_i_epoch 0 batch 100: avg loss -2.828963 avg loss no lamb -2.828963 time 2019-02-19 20:03:57.046155
Model ind 640 epoch 289 head A head_i_epoch 0 batch 200: avg loss -2.832182 avg loss no lamb -2.832182 time 2019-02-19 20:06:21.294566
last batch sz 160
Model ind 640 epoch 289 head B head_i_epoch 0 batch 0: avg loss -1.590968 avg loss no lamb -1.590968 time 2019-02-19 20:08:06.372699
Model ind 640 epoch 289 head B head_i_epoch 0 batch 100: avg loss -1.527974 avg loss no lamb -1.527974 time 2019-02-19 20:10:30.064559
Model ind 640 epoch 289 head B head_i_epoch 0 batch 200: avg loss -1.643709 avg loss no lamb -1.643709 time 2019-02-19 20:12:54.778703
last batch sz 160
Model ind 640 epoch 289 head B head_i_epoch 1 batch 0: avg loss -1.519618 avg loss no lamb -1.519618 time 2019-02-19 20:14:39.485084
Model ind 640 epoch 289 head B head_i_epoch 1 batch 100: avg loss -1.430512 avg loss no lamb -1.430512 time 2019-02-19 20:17:03.637531
Model ind 640 epoch 289 head B head_i_epoch 1 batch 200: avg loss -1.677656 avg loss no lamb -1.677656 time 2019-02-19 20:19:27.636323
last batch sz 160
Pre: time 2019-02-19 20:21:37.014174: 
 	std: 0.05320129
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48786667, 0.59695, 0.59706664, 0.5962167, 0.48843333]
	train_accs: [0.48786667, 0.59695, 0.59706664, 0.5962167, 0.48843333]
	best_train_sub_head: 2
	worst: 0.48786667
	avg: 0.5533067
	best: 0.59706664

Starting e_i: 290
Model ind 640 epoch 290 head A head_i_epoch 0 batch 0: avg loss -2.960153 avg loss no lamb -2.960153 time 2019-02-19 20:21:39.120445
Model ind 640 epoch 290 head A head_i_epoch 0 batch 100: avg loss -2.828354 avg loss no lamb -2.828354 time 2019-02-19 20:24:03.830232
Model ind 640 epoch 290 head A head_i_epoch 0 batch 200: avg loss -2.858307 avg loss no lamb -2.858307 time 2019-02-19 20:26:28.378326
last batch sz 160
Model ind 640 epoch 290 head B head_i_epoch 0 batch 0: avg loss -1.637158 avg loss no lamb -1.637158 time 2019-02-19 20:28:13.369154
Model ind 640 epoch 290 head B head_i_epoch 0 batch 100: avg loss -1.481965 avg loss no lamb -1.481965 time 2019-02-19 20:30:37.285085
Model ind 640 epoch 290 head B head_i_epoch 0 batch 200: avg loss -1.614466 avg loss no lamb -1.614466 time 2019-02-19 20:33:01.131410
last batch sz 160
Model ind 640 epoch 290 head B head_i_epoch 1 batch 0: avg loss -1.564486 avg loss no lamb -1.564486 time 2019-02-19 20:34:46.176370
Model ind 640 epoch 290 head B head_i_epoch 1 batch 100: avg loss -1.480461 avg loss no lamb -1.480461 time 2019-02-19 20:37:09.946091
Model ind 640 epoch 290 head B head_i_epoch 1 batch 200: avg loss -1.706659 avg loss no lamb -1.706659 time 2019-02-19 20:39:34.000808
last batch sz 160
Pre: time 2019-02-19 20:41:43.237327: 
 	std: 0.05323679
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48798335, 0.5967, 0.59685, 0.59585, 0.48761666]
	train_accs: [0.48798335, 0.5967, 0.59685, 0.59585, 0.48761666]
	best_train_sub_head: 2
	worst: 0.48761666
	avg: 0.553
	best: 0.59685

Starting e_i: 291
Model ind 640 epoch 291 head A head_i_epoch 0 batch 0: avg loss -2.854191 avg loss no lamb -2.854191 time 2019-02-19 20:41:48.395761
Model ind 640 epoch 291 head A head_i_epoch 0 batch 100: avg loss -2.792289 avg loss no lamb -2.792289 time 2019-02-19 20:44:12.844093
Model ind 640 epoch 291 head A head_i_epoch 0 batch 200: avg loss -2.926889 avg loss no lamb -2.926889 time 2019-02-19 20:46:37.381474
last batch sz 160
Model ind 640 epoch 291 head B head_i_epoch 0 batch 0: avg loss -1.583848 avg loss no lamb -1.583848 time 2019-02-19 20:48:22.344434
Model ind 640 epoch 291 head B head_i_epoch 0 batch 100: avg loss -1.465892 avg loss no lamb -1.465892 time 2019-02-19 20:50:46.461940
Model ind 640 epoch 291 head B head_i_epoch 0 batch 200: avg loss -1.615158 avg loss no lamb -1.615158 time 2019-02-19 20:53:10.681099
last batch sz 160
Model ind 640 epoch 291 head B head_i_epoch 1 batch 0: avg loss -1.563888 avg loss no lamb -1.563888 time 2019-02-19 20:54:55.665781
Model ind 640 epoch 291 head B head_i_epoch 1 batch 100: avg loss -1.494826 avg loss no lamb -1.494826 time 2019-02-19 20:57:19.779959
Model ind 640 epoch 291 head B head_i_epoch 1 batch 200: avg loss -1.644917 avg loss no lamb -1.644917 time 2019-02-19 20:59:43.952472
last batch sz 160
Pre: time 2019-02-19 21:01:53.400433: 
 	std: 0.055350915
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48636666, 0.6001, 0.59893334, 0.60003334, 0.48705]
	train_accs: [0.48636666, 0.6001, 0.59893334, 0.60003334, 0.48705]
	best_train_sub_head: 1
	worst: 0.48636666
	avg: 0.55449665
	best: 0.6001

Starting e_i: 292
Model ind 640 epoch 292 head A head_i_epoch 0 batch 0: avg loss -2.924793 avg loss no lamb -2.924793 time 2019-02-19 21:01:55.435432
Model ind 640 epoch 292 head A head_i_epoch 0 batch 100: avg loss -2.853657 avg loss no lamb -2.853657 time 2019-02-19 21:04:19.612152
Model ind 640 epoch 292 head A head_i_epoch 0 batch 200: avg loss -2.864940 avg loss no lamb -2.864940 time 2019-02-19 21:06:43.517757
last batch sz 160
Model ind 640 epoch 292 head B head_i_epoch 0 batch 0: avg loss -1.591325 avg loss no lamb -1.591325 time 2019-02-19 21:08:28.219433
Model ind 640 epoch 292 head B head_i_epoch 0 batch 100: avg loss -1.421836 avg loss no lamb -1.421836 time 2019-02-19 21:10:52.558831
Model ind 640 epoch 292 head B head_i_epoch 0 batch 200: avg loss -1.666986 avg loss no lamb -1.666986 time 2019-02-19 21:13:16.560277
last batch sz 160
Model ind 640 epoch 292 head B head_i_epoch 1 batch 0: avg loss -1.577847 avg loss no lamb -1.577847 time 2019-02-19 21:15:01.478563
Model ind 640 epoch 292 head B head_i_epoch 1 batch 100: avg loss -1.544342 avg loss no lamb -1.544342 time 2019-02-19 21:17:26.012699
Model ind 640 epoch 292 head B head_i_epoch 1 batch 200: avg loss -1.645532 avg loss no lamb -1.645532 time 2019-02-19 21:19:50.509443
last batch sz 160
Pre: time 2019-02-19 21:21:59.762453: 
 	std: 0.053716194
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48556668, 0.59503335, 0.59541667, 0.5954667, 0.48575]
	train_accs: [0.48556668, 0.59503335, 0.59541667, 0.5954667, 0.48575]
	best_train_sub_head: 3
	worst: 0.48556668
	avg: 0.5514467
	best: 0.5954667

Starting e_i: 293
Model ind 640 epoch 293 head A head_i_epoch 0 batch 0: avg loss -2.935107 avg loss no lamb -2.935107 time 2019-02-19 21:22:01.798175
Model ind 640 epoch 293 head A head_i_epoch 0 batch 100: avg loss -2.823858 avg loss no lamb -2.823858 time 2019-02-19 21:24:25.626855
Model ind 640 epoch 293 head A head_i_epoch 0 batch 200: avg loss -2.899536 avg loss no lamb -2.899536 time 2019-02-19 21:26:49.360573
last batch sz 160
Model ind 640 epoch 293 head B head_i_epoch 0 batch 0: avg loss -1.606229 avg loss no lamb -1.606229 time 2019-02-19 21:28:34.332012
Model ind 640 epoch 293 head B head_i_epoch 0 batch 100: avg loss -1.482519 avg loss no lamb -1.482519 time 2019-02-19 21:30:58.007759
Model ind 640 epoch 293 head B head_i_epoch 0 batch 200: avg loss -1.624559 avg loss no lamb -1.624559 time 2019-02-19 21:33:21.749537
last batch sz 160
Model ind 640 epoch 293 head B head_i_epoch 1 batch 0: avg loss -1.633641 avg loss no lamb -1.633641 time 2019-02-19 21:35:06.189738
Model ind 640 epoch 293 head B head_i_epoch 1 batch 100: avg loss -1.541761 avg loss no lamb -1.541761 time 2019-02-19 21:37:29.804391
Model ind 640 epoch 293 head B head_i_epoch 1 batch 200: avg loss -1.642528 avg loss no lamb -1.642528 time 2019-02-19 21:39:53.989641
last batch sz 160
Pre: time 2019-02-19 21:42:03.664606: 
 	std: 0.052915934
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48641667, 0.59465, 0.59468335, 0.5944833, 0.48676667]
	train_accs: [0.48641667, 0.59465, 0.59468335, 0.5944833, 0.48676667]
	best_train_sub_head: 2
	worst: 0.48641667
	avg: 0.5514
	best: 0.59468335

Starting e_i: 294
Model ind 640 epoch 294 head A head_i_epoch 0 batch 0: avg loss -2.879634 avg loss no lamb -2.879634 time 2019-02-19 21:42:05.694174
Model ind 640 epoch 294 head A head_i_epoch 0 batch 100: avg loss -2.784657 avg loss no lamb -2.784657 time 2019-02-19 21:44:29.371230
Model ind 640 epoch 294 head A head_i_epoch 0 batch 200: avg loss -2.907923 avg loss no lamb -2.907923 time 2019-02-19 21:46:53.406661
last batch sz 160
Model ind 640 epoch 294 head B head_i_epoch 0 batch 0: avg loss -1.633882 avg loss no lamb -1.633882 time 2019-02-19 21:48:38.299486
Model ind 640 epoch 294 head B head_i_epoch 0 batch 100: avg loss -1.508341 avg loss no lamb -1.508341 time 2019-02-19 21:51:02.465068
Model ind 640 epoch 294 head B head_i_epoch 0 batch 200: avg loss -1.575913 avg loss no lamb -1.575913 time 2019-02-19 21:53:26.642410
last batch sz 160
Model ind 640 epoch 294 head B head_i_epoch 1 batch 0: avg loss -1.531689 avg loss no lamb -1.531689 time 2019-02-19 21:55:11.507606
Model ind 640 epoch 294 head B head_i_epoch 1 batch 100: avg loss -1.464414 avg loss no lamb -1.464414 time 2019-02-19 21:57:35.455777
Model ind 640 epoch 294 head B head_i_epoch 1 batch 200: avg loss -1.705711 avg loss no lamb -1.705711 time 2019-02-19 21:59:59.154363
last batch sz 160
Pre: time 2019-02-19 22:02:08.713508: 
 	std: 0.05471582
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48686665, 0.5991167, 0.5994667, 0.5994167, 0.48843333]
	train_accs: [0.48686665, 0.5991167, 0.5994667, 0.5994167, 0.48843333]
	best_train_sub_head: 2
	worst: 0.48686665
	avg: 0.55465996
	best: 0.5994667

Starting e_i: 295
Model ind 640 epoch 295 head A head_i_epoch 0 batch 0: avg loss -2.884426 avg loss no lamb -2.884426 time 2019-02-19 22:02:10.765041
Model ind 640 epoch 295 head A head_i_epoch 0 batch 100: avg loss -2.850621 avg loss no lamb -2.850621 time 2019-02-19 22:04:35.364302
Model ind 640 epoch 295 head A head_i_epoch 0 batch 200: avg loss -2.888470 avg loss no lamb -2.888470 time 2019-02-19 22:06:59.765234
last batch sz 160
Model ind 640 epoch 295 head B head_i_epoch 0 batch 0: avg loss -1.518475 avg loss no lamb -1.518475 time 2019-02-19 22:08:44.847788
Model ind 640 epoch 295 head B head_i_epoch 0 batch 100: avg loss -1.566638 avg loss no lamb -1.566638 time 2019-02-19 22:11:09.036161
Model ind 640 epoch 295 head B head_i_epoch 0 batch 200: avg loss -1.587840 avg loss no lamb -1.587840 time 2019-02-19 22:13:33.196606
last batch sz 160
Model ind 640 epoch 295 head B head_i_epoch 1 batch 0: avg loss -1.628740 avg loss no lamb -1.628740 time 2019-02-19 22:15:17.872680
Model ind 640 epoch 295 head B head_i_epoch 1 batch 100: avg loss -1.478958 avg loss no lamb -1.478958 time 2019-02-19 22:17:41.878877
Model ind 640 epoch 295 head B head_i_epoch 1 batch 200: avg loss -1.583418 avg loss no lamb -1.583418 time 2019-02-19 22:20:06.234975
last batch sz 160
Pre: time 2019-02-19 22:22:15.964551: 
 	std: 0.053251263
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49185, 0.6012833, 0.6015667, 0.6016, 0.49373335]
	train_accs: [0.49185, 0.6012833, 0.6015667, 0.6016, 0.49373335]
	best_train_sub_head: 3
	worst: 0.49185
	avg: 0.55800664
	best: 0.6016

Starting e_i: 296
Model ind 640 epoch 296 head A head_i_epoch 0 batch 0: avg loss -2.969332 avg loss no lamb -2.969332 time 2019-02-19 22:22:21.477017
Model ind 640 epoch 296 head A head_i_epoch 0 batch 100: avg loss -2.828096 avg loss no lamb -2.828096 time 2019-02-19 22:24:45.704994
Model ind 640 epoch 296 head A head_i_epoch 0 batch 200: avg loss -2.868784 avg loss no lamb -2.868784 time 2019-02-19 22:27:10.475661
last batch sz 160
Model ind 640 epoch 296 head B head_i_epoch 0 batch 0: avg loss -1.559430 avg loss no lamb -1.559430 time 2019-02-19 22:28:55.478840
Model ind 640 epoch 296 head B head_i_epoch 0 batch 100: avg loss -1.456377 avg loss no lamb -1.456377 time 2019-02-19 22:31:19.356005
Model ind 640 epoch 296 head B head_i_epoch 0 batch 200: avg loss -1.604689 avg loss no lamb -1.604689 time 2019-02-19 22:33:43.152090
last batch sz 160
Model ind 640 epoch 296 head B head_i_epoch 1 batch 0: avg loss -1.587830 avg loss no lamb -1.587830 time 2019-02-19 22:35:27.854983
Model ind 640 epoch 296 head B head_i_epoch 1 batch 100: avg loss -1.536601 avg loss no lamb -1.536601 time 2019-02-19 22:37:51.798391
Model ind 640 epoch 296 head B head_i_epoch 1 batch 200: avg loss -1.613442 avg loss no lamb -1.613442 time 2019-02-19 22:40:15.929568
last batch sz 160
Pre: time 2019-02-19 22:42:24.326856: 
 	std: 0.054230068
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48991665, 0.6005, 0.6005833, 0.6014, 0.49035]
	train_accs: [0.48991665, 0.6005, 0.6005833, 0.6014, 0.49035]
	best_train_sub_head: 3
	worst: 0.48991665
	avg: 0.55654997
	best: 0.6014

Starting e_i: 297
Model ind 640 epoch 297 head A head_i_epoch 0 batch 0: avg loss -2.899397 avg loss no lamb -2.899397 time 2019-02-19 22:42:26.334432
Model ind 640 epoch 297 head A head_i_epoch 0 batch 100: avg loss -2.868540 avg loss no lamb -2.868540 time 2019-02-19 22:44:50.100851
Model ind 640 epoch 297 head A head_i_epoch 0 batch 200: avg loss -2.935604 avg loss no lamb -2.935604 time 2019-02-19 22:47:14.113422
last batch sz 160
Model ind 640 epoch 297 head B head_i_epoch 0 batch 0: avg loss -1.502963 avg loss no lamb -1.502963 time 2019-02-19 22:48:58.941250
Model ind 640 epoch 297 head B head_i_epoch 0 batch 100: avg loss -1.583977 avg loss no lamb -1.583977 time 2019-02-19 22:51:22.167806
Model ind 640 epoch 297 head B head_i_epoch 0 batch 200: avg loss -1.678850 avg loss no lamb -1.678850 time 2019-02-19 22:53:46.087639
last batch sz 160
Model ind 640 epoch 297 head B head_i_epoch 1 batch 0: avg loss -1.684888 avg loss no lamb -1.684888 time 2019-02-19 22:55:30.880032
Model ind 640 epoch 297 head B head_i_epoch 1 batch 100: avg loss -1.562286 avg loss no lamb -1.562286 time 2019-02-19 22:57:54.728276
Model ind 640 epoch 297 head B head_i_epoch 1 batch 200: avg loss -1.657460 avg loss no lamb -1.657460 time 2019-02-19 23:00:18.449194
last batch sz 160
Pre: time 2019-02-19 23:02:27.750552: 
 	std: 0.05483416
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48351666, 0.59525, 0.5955667, 0.5961667, 0.48395]
	train_accs: [0.48351666, 0.59525, 0.5955667, 0.5961667, 0.48395]
	best_train_sub_head: 3
	worst: 0.48351666
	avg: 0.55088997
	best: 0.5961667

Starting e_i: 298
Model ind 640 epoch 298 head A head_i_epoch 0 batch 0: avg loss -2.855891 avg loss no lamb -2.855891 time 2019-02-19 23:02:30.644493
Model ind 640 epoch 298 head A head_i_epoch 0 batch 100: avg loss -2.813912 avg loss no lamb -2.813912 time 2019-02-19 23:04:54.768648
Model ind 640 epoch 298 head A head_i_epoch 0 batch 200: avg loss -2.862174 avg loss no lamb -2.862174 time 2019-02-19 23:07:18.571954
last batch sz 160
Model ind 640 epoch 298 head B head_i_epoch 0 batch 0: avg loss -1.601727 avg loss no lamb -1.601727 time 2019-02-19 23:09:03.290142
Model ind 640 epoch 298 head B head_i_epoch 0 batch 100: avg loss -1.529128 avg loss no lamb -1.529128 time 2019-02-19 23:11:27.645237
Model ind 640 epoch 298 head B head_i_epoch 0 batch 200: avg loss -1.636547 avg loss no lamb -1.636547 time 2019-02-19 23:13:52.029567
last batch sz 160
Model ind 640 epoch 298 head B head_i_epoch 1 batch 0: avg loss -1.648761 avg loss no lamb -1.648761 time 2019-02-19 23:15:36.720527
Model ind 640 epoch 298 head B head_i_epoch 1 batch 100: avg loss -1.405546 avg loss no lamb -1.405546 time 2019-02-19 23:18:00.210733
Model ind 640 epoch 298 head B head_i_epoch 1 batch 200: avg loss -1.614602 avg loss no lamb -1.614602 time 2019-02-19 23:20:23.741369
last batch sz 160
Pre: time 2019-02-19 23:22:32.608338: 
 	std: 0.05269933
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48148334, 0.5901167, 0.59145, 0.5908167, 0.48501667]
	train_accs: [0.48148334, 0.5901167, 0.59145, 0.5908167, 0.48501667]
	best_train_sub_head: 2
	worst: 0.48148334
	avg: 0.54777664
	best: 0.59145

Starting e_i: 299
Model ind 640 epoch 299 head A head_i_epoch 0 batch 0: avg loss -2.942007 avg loss no lamb -2.942007 time 2019-02-19 23:22:34.638538
Model ind 640 epoch 299 head A head_i_epoch 0 batch 100: avg loss -2.805573 avg loss no lamb -2.805573 time 2019-02-19 23:24:58.434648
Model ind 640 epoch 299 head A head_i_epoch 0 batch 200: avg loss -2.884354 avg loss no lamb -2.884354 time 2019-02-19 23:27:22.782649
last batch sz 160
Model ind 640 epoch 299 head B head_i_epoch 0 batch 0: avg loss -1.554797 avg loss no lamb -1.554797 time 2019-02-19 23:29:07.603089
Model ind 640 epoch 299 head B head_i_epoch 0 batch 100: avg loss -1.451487 avg loss no lamb -1.451487 time 2019-02-19 23:31:31.487554
Model ind 640 epoch 299 head B head_i_epoch 0 batch 200: avg loss -1.632595 avg loss no lamb -1.632595 time 2019-02-19 23:33:55.204898
last batch sz 160
Model ind 640 epoch 299 head B head_i_epoch 1 batch 0: avg loss -1.606459 avg loss no lamb -1.606459 time 2019-02-19 23:35:39.752731
Model ind 640 epoch 299 head B head_i_epoch 1 batch 100: avg loss -1.513072 avg loss no lamb -1.513072 time 2019-02-19 23:38:03.762234
Model ind 640 epoch 299 head B head_i_epoch 1 batch 200: avg loss -1.582063 avg loss no lamb -1.582063 time 2019-02-19 23:40:27.693469
last batch sz 160
Pre: time 2019-02-19 23:42:36.804528: 
 	std: 0.05329831
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48838332, 0.5986, 0.59851664, 0.59845, 0.4911]
	train_accs: [0.48838332, 0.5986, 0.59851664, 0.59845, 0.4911]
	best_train_sub_head: 1
	worst: 0.48838332
	avg: 0.55500996
	best: 0.5986

Starting e_i: 300
Model ind 640 epoch 300 head A head_i_epoch 0 batch 0: avg loss -2.858990 avg loss no lamb -2.858990 time 2019-02-19 23:42:39.157960
Model ind 640 epoch 300 head A head_i_epoch 0 batch 100: avg loss -2.822710 avg loss no lamb -2.822710 time 2019-02-19 23:45:02.537236
Model ind 640 epoch 300 head A head_i_epoch 0 batch 200: avg loss -3.023771 avg loss no lamb -3.023771 time 2019-02-19 23:47:26.168892
last batch sz 160
Model ind 640 epoch 300 head B head_i_epoch 0 batch 0: avg loss -1.580579 avg loss no lamb -1.580579 time 2019-02-19 23:49:10.927192
Model ind 640 epoch 300 head B head_i_epoch 0 batch 100: avg loss -1.556219 avg loss no lamb -1.556219 time 2019-02-19 23:51:34.992329
Model ind 640 epoch 300 head B head_i_epoch 0 batch 200: avg loss -1.591095 avg loss no lamb -1.591095 time 2019-02-19 23:53:58.482972
last batch sz 160
Model ind 640 epoch 300 head B head_i_epoch 1 batch 0: avg loss -1.615201 avg loss no lamb -1.615201 time 2019-02-19 23:55:43.242143
Model ind 640 epoch 300 head B head_i_epoch 1 batch 100: avg loss -1.524407 avg loss no lamb -1.524407 time 2019-02-19 23:58:07.182082
Model ind 640 epoch 300 head B head_i_epoch 1 batch 200: avg loss -1.580745 avg loss no lamb -1.580745 time 2019-02-20 00:00:31.065005
last batch sz 160
Pre: time 2019-02-20 00:02:41.147401: 
 	std: 0.054613244
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48586667, 0.5977833, 0.59735, 0.59775, 0.48643333]
	train_accs: [0.48586667, 0.5977833, 0.59735, 0.59775, 0.48643333]
	best_train_sub_head: 1
	worst: 0.48586667
	avg: 0.55303663
	best: 0.5977833

Starting e_i: 301
Model ind 640 epoch 301 head A head_i_epoch 0 batch 0: avg loss -2.929669 avg loss no lamb -2.929669 time 2019-02-20 00:02:46.659044
Model ind 640 epoch 301 head A head_i_epoch 0 batch 100: avg loss -2.777698 avg loss no lamb -2.777698 time 2019-02-20 00:05:11.031747
Model ind 640 epoch 301 head A head_i_epoch 0 batch 200: avg loss -2.949959 avg loss no lamb -2.949959 time 2019-02-20 00:07:35.436705
last batch sz 160
Model ind 640 epoch 301 head B head_i_epoch 0 batch 0: avg loss -1.540479 avg loss no lamb -1.540479 time 2019-02-20 00:09:20.384992
Model ind 640 epoch 301 head B head_i_epoch 0 batch 100: avg loss -1.518556 avg loss no lamb -1.518556 time 2019-02-20 00:11:44.324963
Model ind 640 epoch 301 head B head_i_epoch 0 batch 200: avg loss -1.639650 avg loss no lamb -1.639650 time 2019-02-20 00:14:08.239262
last batch sz 160
Model ind 640 epoch 301 head B head_i_epoch 1 batch 0: avg loss -1.504434 avg loss no lamb -1.504434 time 2019-02-20 00:15:53.031059
Model ind 640 epoch 301 head B head_i_epoch 1 batch 100: avg loss -1.551610 avg loss no lamb -1.551610 time 2019-02-20 00:18:16.474568
Model ind 640 epoch 301 head B head_i_epoch 1 batch 200: avg loss -1.631802 avg loss no lamb -1.631802 time 2019-02-20 00:20:40.198637
last batch sz 160
Pre: time 2019-02-20 00:22:49.394272: 
 	std: 0.053316336
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49058333, 0.5997, 0.599, 0.59931666, 0.49043334]
	train_accs: [0.49058333, 0.5997, 0.599, 0.59931666, 0.49043334]
	best_train_sub_head: 1
	worst: 0.49043334
	avg: 0.55580664
	best: 0.5997

Starting e_i: 302
Model ind 640 epoch 302 head A head_i_epoch 0 batch 0: avg loss -2.924197 avg loss no lamb -2.924197 time 2019-02-20 00:22:51.479357
Model ind 640 epoch 302 head A head_i_epoch 0 batch 100: avg loss -2.851362 avg loss no lamb -2.851362 time 2019-02-20 00:25:15.720372
Model ind 640 epoch 302 head A head_i_epoch 0 batch 200: avg loss -2.896837 avg loss no lamb -2.896837 time 2019-02-20 00:27:40.435698
last batch sz 160
Model ind 640 epoch 302 head B head_i_epoch 0 batch 0: avg loss -1.722644 avg loss no lamb -1.722644 time 2019-02-20 00:29:25.262612
Model ind 640 epoch 302 head B head_i_epoch 0 batch 100: avg loss -1.535097 avg loss no lamb -1.535097 time 2019-02-20 00:31:48.868488
Model ind 640 epoch 302 head B head_i_epoch 0 batch 200: avg loss -1.689670 avg loss no lamb -1.689670 time 2019-02-20 00:34:12.777387
last batch sz 160
Model ind 640 epoch 302 head B head_i_epoch 1 batch 0: avg loss -1.593440 avg loss no lamb -1.593440 time 2019-02-20 00:35:57.539713
Model ind 640 epoch 302 head B head_i_epoch 1 batch 100: avg loss -1.478312 avg loss no lamb -1.478312 time 2019-02-20 00:38:21.667544
Model ind 640 epoch 302 head B head_i_epoch 1 batch 200: avg loss -1.602996 avg loss no lamb -1.602996 time 2019-02-20 00:40:45.532144
last batch sz 160
Pre: time 2019-02-20 00:42:54.510621: 
 	std: 0.053756814
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49133334, 0.6012, 0.60106665, 0.6012, 0.49151668]
	train_accs: [0.49133334, 0.6012, 0.60106665, 0.6012, 0.49151668]
	best_train_sub_head: 1
	worst: 0.49133334
	avg: 0.55726326
	best: 0.6012

Starting e_i: 303
Model ind 640 epoch 303 head A head_i_epoch 0 batch 0: avg loss -2.855295 avg loss no lamb -2.855295 time 2019-02-20 00:42:56.548818
Model ind 640 epoch 303 head A head_i_epoch 0 batch 100: avg loss -2.838073 avg loss no lamb -2.838073 time 2019-02-20 00:45:20.404556
Model ind 640 epoch 303 head A head_i_epoch 0 batch 200: avg loss -2.929597 avg loss no lamb -2.929597 time 2019-02-20 00:47:44.489494
last batch sz 160
Model ind 640 epoch 303 head B head_i_epoch 0 batch 0: avg loss -1.638716 avg loss no lamb -1.638716 time 2019-02-20 00:49:29.259545
Model ind 640 epoch 303 head B head_i_epoch 0 batch 100: avg loss -1.458194 avg loss no lamb -1.458194 time 2019-02-20 00:51:52.814933
Model ind 640 epoch 303 head B head_i_epoch 0 batch 200: avg loss -1.715227 avg loss no lamb -1.715227 time 2019-02-20 00:54:16.690501
last batch sz 160
Model ind 640 epoch 303 head B head_i_epoch 1 batch 0: avg loss -1.588135 avg loss no lamb -1.588135 time 2019-02-20 00:56:01.298798
Model ind 640 epoch 303 head B head_i_epoch 1 batch 100: avg loss -1.493997 avg loss no lamb -1.493997 time 2019-02-20 00:58:25.137551
Model ind 640 epoch 303 head B head_i_epoch 1 batch 200: avg loss -1.683990 avg loss no lamb -1.683990 time 2019-02-20 01:00:49.174545
last batch sz 160
Pre: time 2019-02-20 01:02:58.089026: 
 	std: 0.0541095
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48833334, 0.59888333, 0.59973335, 0.59925, 0.48935]
	train_accs: [0.48833334, 0.59888333, 0.59973335, 0.59925, 0.48935]
	best_train_sub_head: 2
	worst: 0.48833334
	avg: 0.55511004
	best: 0.59973335

Starting e_i: 304
Model ind 640 epoch 304 head A head_i_epoch 0 batch 0: avg loss -2.966844 avg loss no lamb -2.966844 time 2019-02-20 01:03:00.166950
Model ind 640 epoch 304 head A head_i_epoch 0 batch 100: avg loss -2.799445 avg loss no lamb -2.799445 time 2019-02-20 01:05:23.957080
Model ind 640 epoch 304 head A head_i_epoch 0 batch 200: avg loss -2.943645 avg loss no lamb -2.943645 time 2019-02-20 01:07:47.789845
last batch sz 160
Model ind 640 epoch 304 head B head_i_epoch 0 batch 0: avg loss -1.575421 avg loss no lamb -1.575421 time 2019-02-20 01:09:32.807840
Model ind 640 epoch 304 head B head_i_epoch 0 batch 100: avg loss -1.468862 avg loss no lamb -1.468862 time 2019-02-20 01:11:56.921437
Model ind 640 epoch 304 head B head_i_epoch 0 batch 200: avg loss -1.614348 avg loss no lamb -1.614348 time 2019-02-20 01:14:21.108291
last batch sz 160
Model ind 640 epoch 304 head B head_i_epoch 1 batch 0: avg loss -1.486438 avg loss no lamb -1.486438 time 2019-02-20 01:16:06.022073
Model ind 640 epoch 304 head B head_i_epoch 1 batch 100: avg loss -1.475917 avg loss no lamb -1.475917 time 2019-02-20 01:18:29.926719
Model ind 640 epoch 304 head B head_i_epoch 1 batch 200: avg loss -1.719978 avg loss no lamb -1.719978 time 2019-02-20 01:20:53.438813
last batch sz 160
Pre: time 2019-02-20 01:23:02.816685: 
 	std: 0.055045456
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.489, 0.6012333, 0.60213333, 0.60188335, 0.48978335]
	train_accs: [0.489, 0.6012333, 0.60213333, 0.60188335, 0.48978335]
	best_train_sub_head: 2
	worst: 0.489
	avg: 0.5568067
	best: 0.60213333

Starting e_i: 305
Model ind 640 epoch 305 head A head_i_epoch 0 batch 0: avg loss -2.876513 avg loss no lamb -2.876513 time 2019-02-20 01:23:08.908250
Model ind 640 epoch 305 head A head_i_epoch 0 batch 100: avg loss -2.857854 avg loss no lamb -2.857854 time 2019-02-20 01:25:33.011350
Model ind 640 epoch 305 head A head_i_epoch 0 batch 200: avg loss -2.940602 avg loss no lamb -2.940602 time 2019-02-20 01:27:57.100495
last batch sz 160
Model ind 640 epoch 305 head B head_i_epoch 0 batch 0: avg loss -1.588354 avg loss no lamb -1.588354 time 2019-02-20 01:29:41.813598
Model ind 640 epoch 305 head B head_i_epoch 0 batch 100: avg loss -1.445242 avg loss no lamb -1.445242 time 2019-02-20 01:32:05.917297
Model ind 640 epoch 305 head B head_i_epoch 0 batch 200: avg loss -1.633759 avg loss no lamb -1.633759 time 2019-02-20 01:34:29.684467
last batch sz 160
Model ind 640 epoch 305 head B head_i_epoch 1 batch 0: avg loss -1.603520 avg loss no lamb -1.603520 time 2019-02-20 01:36:14.805311
Model ind 640 epoch 305 head B head_i_epoch 1 batch 100: avg loss -1.473414 avg loss no lamb -1.473414 time 2019-02-20 01:38:40.035039
Model ind 640 epoch 305 head B head_i_epoch 1 batch 200: avg loss -1.696747 avg loss no lamb -1.696747 time 2019-02-20 01:41:04.474114
last batch sz 160
Pre: time 2019-02-20 01:43:14.091608: 
 	std: 0.052552983
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48658332, 0.5941333, 0.5949, 0.5938, 0.48743334]
	train_accs: [0.48658332, 0.5941333, 0.5949, 0.5938, 0.48743334]
	best_train_sub_head: 2
	worst: 0.48658332
	avg: 0.55137
	best: 0.5949

Starting e_i: 306
Model ind 640 epoch 306 head A head_i_epoch 0 batch 0: avg loss -2.962336 avg loss no lamb -2.962336 time 2019-02-20 01:43:16.212525
Model ind 640 epoch 306 head A head_i_epoch 0 batch 100: avg loss -2.888685 avg loss no lamb -2.888685 time 2019-02-20 01:45:40.202252
Model ind 640 epoch 306 head A head_i_epoch 0 batch 200: avg loss -3.003564 avg loss no lamb -3.003564 time 2019-02-20 01:48:04.532525
last batch sz 160
Model ind 640 epoch 306 head B head_i_epoch 0 batch 0: avg loss -1.550851 avg loss no lamb -1.550851 time 2019-02-20 01:49:49.430915
Model ind 640 epoch 306 head B head_i_epoch 0 batch 100: avg loss -1.503897 avg loss no lamb -1.503897 time 2019-02-20 01:52:13.167151
Model ind 640 epoch 306 head B head_i_epoch 0 batch 200: avg loss -1.785963 avg loss no lamb -1.785963 time 2019-02-20 01:54:37.399790
last batch sz 160
Model ind 640 epoch 306 head B head_i_epoch 1 batch 0: avg loss -1.686631 avg loss no lamb -1.686631 time 2019-02-20 01:56:22.403574
Model ind 640 epoch 306 head B head_i_epoch 1 batch 100: avg loss -1.479545 avg loss no lamb -1.479545 time 2019-02-20 01:58:47.106929
Model ind 640 epoch 306 head B head_i_epoch 1 batch 200: avg loss -1.671274 avg loss no lamb -1.671274 time 2019-02-20 02:01:11.386235
last batch sz 160
Pre: time 2019-02-20 02:03:21.048085: 
 	std: 0.053424776
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49148333, 0.6005833, 0.60043335, 0.60041666, 0.49136665]
	train_accs: [0.49148333, 0.6005833, 0.60043335, 0.60041666, 0.49136665]
	best_train_sub_head: 1
	worst: 0.49136665
	avg: 0.5568567
	best: 0.6005833

Starting e_i: 307
Model ind 640 epoch 307 head A head_i_epoch 0 batch 0: avg loss -2.864104 avg loss no lamb -2.864104 time 2019-02-20 02:03:23.162818
Model ind 640 epoch 307 head A head_i_epoch 0 batch 100: avg loss -2.780034 avg loss no lamb -2.780034 time 2019-02-20 02:05:47.265111
Model ind 640 epoch 307 head A head_i_epoch 0 batch 200: avg loss -2.928989 avg loss no lamb -2.928989 time 2019-02-20 02:08:11.761243
last batch sz 160
Model ind 640 epoch 307 head B head_i_epoch 0 batch 0: avg loss -1.593776 avg loss no lamb -1.593776 time 2019-02-20 02:09:56.935535
Model ind 640 epoch 307 head B head_i_epoch 0 batch 100: avg loss -1.478920 avg loss no lamb -1.478920 time 2019-02-20 02:12:21.147734
Model ind 640 epoch 307 head B head_i_epoch 0 batch 200: avg loss -1.653576 avg loss no lamb -1.653576 time 2019-02-20 02:14:45.540681
last batch sz 160
Model ind 640 epoch 307 head B head_i_epoch 1 batch 0: avg loss -1.542787 avg loss no lamb -1.542787 time 2019-02-20 02:16:30.364436
Model ind 640 epoch 307 head B head_i_epoch 1 batch 100: avg loss -1.515579 avg loss no lamb -1.515579 time 2019-02-20 02:18:54.405863
Model ind 640 epoch 307 head B head_i_epoch 1 batch 200: avg loss -1.713962 avg loss no lamb -1.713962 time 2019-02-20 02:21:18.106536
last batch sz 160
Pre: time 2019-02-20 02:23:27.547467: 
 	std: 0.055578843
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48203334, 0.59566665, 0.59635, 0.5965667, 0.48346666]
	train_accs: [0.48203334, 0.59566665, 0.59635, 0.5965667, 0.48346666]
	best_train_sub_head: 3
	worst: 0.48203334
	avg: 0.55081666
	best: 0.5965667

Starting e_i: 308
Model ind 640 epoch 308 head A head_i_epoch 0 batch 0: avg loss -2.932862 avg loss no lamb -2.932862 time 2019-02-20 02:23:29.650699
Model ind 640 epoch 308 head A head_i_epoch 0 batch 100: avg loss -2.827866 avg loss no lamb -2.827866 time 2019-02-20 02:25:54.013587
Model ind 640 epoch 308 head A head_i_epoch 0 batch 200: avg loss -2.948062 avg loss no lamb -2.948062 time 2019-02-20 02:28:18.240787
last batch sz 160
Model ind 640 epoch 308 head B head_i_epoch 0 batch 0: avg loss -1.549315 avg loss no lamb -1.549315 time 2019-02-20 02:30:03.473714
Model ind 640 epoch 308 head B head_i_epoch 0 batch 100: avg loss -1.489354 avg loss no lamb -1.489354 time 2019-02-20 02:32:27.475459
Model ind 640 epoch 308 head B head_i_epoch 0 batch 200: avg loss -1.626596 avg loss no lamb -1.626596 time 2019-02-20 02:34:52.569250
last batch sz 160
Model ind 640 epoch 308 head B head_i_epoch 1 batch 0: avg loss -1.549238 avg loss no lamb -1.549238 time 2019-02-20 02:36:37.952021
Model ind 640 epoch 308 head B head_i_epoch 1 batch 100: avg loss -1.427922 avg loss no lamb -1.427922 time 2019-02-20 02:39:02.280462
Model ind 640 epoch 308 head B head_i_epoch 1 batch 200: avg loss -1.696032 avg loss no lamb -1.696032 time 2019-02-20 02:41:26.721981
last batch sz 160
Pre: time 2019-02-20 02:43:36.620771: 
 	std: 0.05514225
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48885, 0.60216665, 0.6016167, 0.60213333, 0.48998332]
	train_accs: [0.48885, 0.60216665, 0.6016167, 0.60213333, 0.48998332]
	best_train_sub_head: 1
	worst: 0.48885
	avg: 0.55695
	best: 0.60216665

Starting e_i: 309
Model ind 640 epoch 309 head A head_i_epoch 0 batch 0: avg loss -2.963488 avg loss no lamb -2.963488 time 2019-02-20 02:43:43.648716
Model ind 640 epoch 309 head A head_i_epoch 0 batch 100: avg loss -2.809518 avg loss no lamb -2.809518 time 2019-02-20 02:46:08.366393
Model ind 640 epoch 309 head A head_i_epoch 0 batch 200: avg loss -2.918118 avg loss no lamb -2.918118 time 2019-02-20 02:48:32.663199
last batch sz 160
Model ind 640 epoch 309 head B head_i_epoch 0 batch 0: avg loss -1.618014 avg loss no lamb -1.618014 time 2019-02-20 02:50:17.527807
Model ind 640 epoch 309 head B head_i_epoch 0 batch 100: avg loss -1.464903 avg loss no lamb -1.464903 time 2019-02-20 02:52:41.890093
Model ind 640 epoch 309 head B head_i_epoch 0 batch 200: avg loss -1.679176 avg loss no lamb -1.679176 time 2019-02-20 02:55:05.927309
last batch sz 160
Model ind 640 epoch 309 head B head_i_epoch 1 batch 0: avg loss -1.583051 avg loss no lamb -1.583051 time 2019-02-20 02:56:50.917381
Model ind 640 epoch 309 head B head_i_epoch 1 batch 100: avg loss -1.483588 avg loss no lamb -1.483588 time 2019-02-20 02:59:15.590071
Model ind 640 epoch 309 head B head_i_epoch 1 batch 200: avg loss -1.713199 avg loss no lamb -1.713199 time 2019-02-20 03:01:39.627285
last batch sz 160
Pre: time 2019-02-20 03:03:49.011453: 
 	std: 0.052718528
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49196666, 0.60055, 0.6008667, 0.60031664, 0.49398333]
	train_accs: [0.49196666, 0.60055, 0.6008667, 0.60031664, 0.49398333]
	best_train_sub_head: 2
	worst: 0.49196666
	avg: 0.55753666
	best: 0.6008667

Starting e_i: 310
Model ind 640 epoch 310 head A head_i_epoch 0 batch 0: avg loss -2.884228 avg loss no lamb -2.884228 time 2019-02-20 03:03:51.107077
Model ind 640 epoch 310 head A head_i_epoch 0 batch 100: avg loss -2.814255 avg loss no lamb -2.814255 time 2019-02-20 03:06:16.104287
Model ind 640 epoch 310 head A head_i_epoch 0 batch 200: avg loss -2.920297 avg loss no lamb -2.920297 time 2019-02-20 03:08:40.581787
last batch sz 160
Model ind 640 epoch 310 head B head_i_epoch 0 batch 0: avg loss -1.639587 avg loss no lamb -1.639587 time 2019-02-20 03:10:25.235550
Model ind 640 epoch 310 head B head_i_epoch 0 batch 100: avg loss -1.508904 avg loss no lamb -1.508904 time 2019-02-20 03:12:49.700161
Model ind 640 epoch 310 head B head_i_epoch 0 batch 200: avg loss -1.686982 avg loss no lamb -1.686982 time 2019-02-20 03:15:13.930972
last batch sz 160
Model ind 640 epoch 310 head B head_i_epoch 1 batch 0: avg loss -1.539246 avg loss no lamb -1.539246 time 2019-02-20 03:16:58.900050
Model ind 640 epoch 310 head B head_i_epoch 1 batch 100: avg loss -1.576227 avg loss no lamb -1.576227 time 2019-02-20 03:19:23.316835
Model ind 640 epoch 310 head B head_i_epoch 1 batch 200: avg loss -1.658380 avg loss no lamb -1.658380 time 2019-02-20 03:21:47.201588
last batch sz 160
Pre: time 2019-02-20 03:23:56.953128: 
 	std: 0.05402495
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48943335, 0.6009, 0.60113335, 0.60113335, 0.49215]
	train_accs: [0.48943335, 0.6009, 0.60113335, 0.60113335, 0.49215]
	best_train_sub_head: 2
	worst: 0.48943335
	avg: 0.55695003
	best: 0.60113335

Starting e_i: 311
Model ind 640 epoch 311 head A head_i_epoch 0 batch 0: avg loss -2.938589 avg loss no lamb -2.938589 time 2019-02-20 03:24:02.680114
Model ind 640 epoch 311 head A head_i_epoch 0 batch 100: avg loss -2.822775 avg loss no lamb -2.822775 time 2019-02-20 03:26:27.244853
Model ind 640 epoch 311 head A head_i_epoch 0 batch 200: avg loss -2.934410 avg loss no lamb -2.934410 time 2019-02-20 03:28:51.809668
last batch sz 160
Model ind 640 epoch 311 head B head_i_epoch 0 batch 0: avg loss -1.628802 avg loss no lamb -1.628802 time 2019-02-20 03:30:36.960351
Model ind 640 epoch 311 head B head_i_epoch 0 batch 100: avg loss -1.527296 avg loss no lamb -1.527296 time 2019-02-20 03:33:01.050036
Model ind 640 epoch 311 head B head_i_epoch 0 batch 200: avg loss -1.595860 avg loss no lamb -1.595860 time 2019-02-20 03:35:24.483216
last batch sz 160
Model ind 640 epoch 311 head B head_i_epoch 1 batch 0: avg loss -1.618714 avg loss no lamb -1.618714 time 2019-02-20 03:37:08.625021
Model ind 640 epoch 311 head B head_i_epoch 1 batch 100: avg loss -1.477162 avg loss no lamb -1.477162 time 2019-02-20 03:39:32.228971
Model ind 640 epoch 311 head B head_i_epoch 1 batch 200: avg loss -1.683057 avg loss no lamb -1.683057 time 2019-02-20 03:41:55.268851
last batch sz 160
Pre: time 2019-02-20 03:44:04.181478: 
 	std: 0.055138793
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48836666, 0.60185, 0.6016667, 0.60145, 0.48985]
	train_accs: [0.48836666, 0.60185, 0.6016667, 0.60145, 0.48985]
	best_train_sub_head: 1
	worst: 0.48836666
	avg: 0.5566367
	best: 0.60185

Starting e_i: 312
Model ind 640 epoch 312 head A head_i_epoch 0 batch 0: avg loss -2.909171 avg loss no lamb -2.909171 time 2019-02-20 03:44:06.270689
Model ind 640 epoch 312 head A head_i_epoch 0 batch 100: avg loss -2.789700 avg loss no lamb -2.789700 time 2019-02-20 03:46:30.581478
Model ind 640 epoch 312 head A head_i_epoch 0 batch 200: avg loss -2.924912 avg loss no lamb -2.924912 time 2019-02-20 03:48:54.578541
last batch sz 160
Model ind 640 epoch 312 head B head_i_epoch 0 batch 0: avg loss -1.617125 avg loss no lamb -1.617125 time 2019-02-20 03:50:39.454712
Model ind 640 epoch 312 head B head_i_epoch 0 batch 100: avg loss -1.507481 avg loss no lamb -1.507481 time 2019-02-20 03:53:03.031579
Model ind 640 epoch 312 head B head_i_epoch 0 batch 200: avg loss -1.674032 avg loss no lamb -1.674032 time 2019-02-20 03:55:27.069813
last batch sz 160
Model ind 640 epoch 312 head B head_i_epoch 1 batch 0: avg loss -1.593385 avg loss no lamb -1.593385 time 2019-02-20 03:57:11.966346
Model ind 640 epoch 312 head B head_i_epoch 1 batch 100: avg loss -1.509338 avg loss no lamb -1.509338 time 2019-02-20 03:59:35.927804
Model ind 640 epoch 312 head B head_i_epoch 1 batch 200: avg loss -1.618247 avg loss no lamb -1.618247 time 2019-02-20 04:01:59.401895
last batch sz 160
Pre: time 2019-02-20 04:04:09.033978: 
 	std: 0.054624755
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48861668, 0.60183334, 0.60111666, 0.6012167, 0.49118334]
	train_accs: [0.48861668, 0.60183334, 0.60111666, 0.6012167, 0.49118334]
	best_train_sub_head: 1
	worst: 0.48861668
	avg: 0.55679333
	best: 0.60183334

Starting e_i: 313
Model ind 640 epoch 313 head A head_i_epoch 0 batch 0: avg loss -2.926046 avg loss no lamb -2.926046 time 2019-02-20 04:04:11.099242
Model ind 640 epoch 313 head A head_i_epoch 0 batch 100: avg loss -2.853477 avg loss no lamb -2.853477 time 2019-02-20 04:06:34.521612
Model ind 640 epoch 313 head A head_i_epoch 0 batch 200: avg loss -2.957013 avg loss no lamb -2.957013 time 2019-02-20 04:08:57.875647
last batch sz 160
Model ind 640 epoch 313 head B head_i_epoch 0 batch 0: avg loss -1.660046 avg loss no lamb -1.660046 time 2019-02-20 04:10:42.576578
Model ind 640 epoch 313 head B head_i_epoch 0 batch 100: avg loss -1.509781 avg loss no lamb -1.509781 time 2019-02-20 04:13:06.234984
Model ind 640 epoch 313 head B head_i_epoch 0 batch 200: avg loss -1.652417 avg loss no lamb -1.652417 time 2019-02-20 04:15:30.588504
last batch sz 160
Model ind 640 epoch 313 head B head_i_epoch 1 batch 0: avg loss -1.525628 avg loss no lamb -1.525628 time 2019-02-20 04:17:15.564435
Model ind 640 epoch 313 head B head_i_epoch 1 batch 100: avg loss -1.502175 avg loss no lamb -1.502175 time 2019-02-20 04:19:39.610414
Model ind 640 epoch 313 head B head_i_epoch 1 batch 200: avg loss -1.606439 avg loss no lamb -1.606439 time 2019-02-20 04:22:04.042639
last batch sz 160
Pre: time 2019-02-20 04:24:13.002087: 
 	std: 0.055558622
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48458335, 0.59863335, 0.5987167, 0.59851664, 0.48585]
	train_accs: [0.48458335, 0.59863335, 0.5987167, 0.59851664, 0.48585]
	best_train_sub_head: 2
	worst: 0.48458335
	avg: 0.55326
	best: 0.5987167

Starting e_i: 314
Model ind 640 epoch 314 head A head_i_epoch 0 batch 0: avg loss -2.874098 avg loss no lamb -2.874098 time 2019-02-20 04:24:15.065990
Model ind 640 epoch 314 head A head_i_epoch 0 batch 100: avg loss -2.851063 avg loss no lamb -2.851063 time 2019-02-20 04:26:39.004546
Model ind 640 epoch 314 head A head_i_epoch 0 batch 200: avg loss -3.026182 avg loss no lamb -3.026182 time 2019-02-20 04:29:03.613806
last batch sz 160
Model ind 640 epoch 314 head B head_i_epoch 0 batch 0: avg loss -1.620936 avg loss no lamb -1.620936 time 2019-02-20 04:30:48.482688
Model ind 640 epoch 314 head B head_i_epoch 0 batch 100: avg loss -1.495862 avg loss no lamb -1.495862 time 2019-02-20 04:33:11.931126
Model ind 640 epoch 314 head B head_i_epoch 0 batch 200: avg loss -1.602322 avg loss no lamb -1.602322 time 2019-02-20 04:35:36.333193
last batch sz 160
Model ind 640 epoch 314 head B head_i_epoch 1 batch 0: avg loss -1.559612 avg loss no lamb -1.559612 time 2019-02-20 04:37:20.990047
Model ind 640 epoch 314 head B head_i_epoch 1 batch 100: avg loss -1.561736 avg loss no lamb -1.561736 time 2019-02-20 04:39:44.050033
Model ind 640 epoch 314 head B head_i_epoch 1 batch 200: avg loss -1.708813 avg loss no lamb -1.708813 time 2019-02-20 04:42:08.874935
last batch sz 160
Pre: time 2019-02-20 04:44:18.427261: 
 	std: 0.054459438
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48676667, 0.5994833, 0.5991333, 0.5988167, 0.48921666]
	train_accs: [0.48676667, 0.5994833, 0.5991333, 0.5988167, 0.48921666]
	best_train_sub_head: 1
	worst: 0.48676667
	avg: 0.5546833
	best: 0.5994833

Starting e_i: 315
Model ind 640 epoch 315 head A head_i_epoch 0 batch 0: avg loss -2.836872 avg loss no lamb -2.836872 time 2019-02-20 04:44:20.467974
Model ind 640 epoch 315 head A head_i_epoch 0 batch 100: avg loss -2.836314 avg loss no lamb -2.836314 time 2019-02-20 04:46:44.733520
Model ind 640 epoch 315 head A head_i_epoch 0 batch 200: avg loss -3.006792 avg loss no lamb -3.006792 time 2019-02-20 04:49:09.217901
last batch sz 160
Model ind 640 epoch 315 head B head_i_epoch 0 batch 0: avg loss -1.529468 avg loss no lamb -1.529468 time 2019-02-20 04:50:54.011907
Model ind 640 epoch 315 head B head_i_epoch 0 batch 100: avg loss -1.504842 avg loss no lamb -1.504842 time 2019-02-20 04:53:18.059762
Model ind 640 epoch 315 head B head_i_epoch 0 batch 200: avg loss -1.652223 avg loss no lamb -1.652223 time 2019-02-20 04:55:43.952624
last batch sz 160
Model ind 640 epoch 315 head B head_i_epoch 1 batch 0: avg loss -1.539677 avg loss no lamb -1.539677 time 2019-02-20 04:57:30.310802
Model ind 640 epoch 315 head B head_i_epoch 1 batch 100: avg loss -1.478504 avg loss no lamb -1.478504 time 2019-02-20 04:59:54.043657
Model ind 640 epoch 315 head B head_i_epoch 1 batch 200: avg loss -1.623611 avg loss no lamb -1.623611 time 2019-02-20 05:02:18.319824
last batch sz 160
Pre: time 2019-02-20 05:04:27.334438: 
 	std: 0.05279534
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48863333, 0.5972, 0.5977, 0.5966333, 0.4902]
	train_accs: [0.48863333, 0.5972, 0.5977, 0.5966333, 0.4902]
	best_train_sub_head: 2
	worst: 0.48863333
	avg: 0.55407333
	best: 0.5977

Starting e_i: 316
Model ind 640 epoch 316 head A head_i_epoch 0 batch 0: avg loss -2.840749 avg loss no lamb -2.840749 time 2019-02-20 05:04:29.427890
Model ind 640 epoch 316 head A head_i_epoch 0 batch 100: avg loss -2.831639 avg loss no lamb -2.831639 time 2019-02-20 05:06:53.576631
Model ind 640 epoch 316 head A head_i_epoch 0 batch 200: avg loss -2.973469 avg loss no lamb -2.973469 time 2019-02-20 05:09:17.820967
last batch sz 160
Model ind 640 epoch 316 head B head_i_epoch 0 batch 0: avg loss -1.557808 avg loss no lamb -1.557808 time 2019-02-20 05:11:02.875225
Model ind 640 epoch 316 head B head_i_epoch 0 batch 100: avg loss -1.547946 avg loss no lamb -1.547946 time 2019-02-20 05:13:27.180700
Model ind 640 epoch 316 head B head_i_epoch 0 batch 200: avg loss -1.669819 avg loss no lamb -1.669819 time 2019-02-20 05:15:51.135485
last batch sz 160
Model ind 640 epoch 316 head B head_i_epoch 1 batch 0: avg loss -1.544128 avg loss no lamb -1.544128 time 2019-02-20 05:17:35.697118
Model ind 640 epoch 316 head B head_i_epoch 1 batch 100: avg loss -1.519796 avg loss no lamb -1.519796 time 2019-02-20 05:19:59.616859
Model ind 640 epoch 316 head B head_i_epoch 1 batch 200: avg loss -1.702547 avg loss no lamb -1.702547 time 2019-02-20 05:22:23.617661
last batch sz 160
Pre: time 2019-02-20 05:24:32.533091: 
 	std: 0.05403222
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48866665, 0.5994667, 0.59888333, 0.59875, 0.48881668]
	train_accs: [0.48866665, 0.5994667, 0.59888333, 0.59875, 0.48881668]
	best_train_sub_head: 1
	worst: 0.48866665
	avg: 0.5549167
	best: 0.5994667

Starting e_i: 317
Model ind 640 epoch 317 head A head_i_epoch 0 batch 0: avg loss -2.905252 avg loss no lamb -2.905252 time 2019-02-20 05:24:34.947914
Model ind 640 epoch 317 head A head_i_epoch 0 batch 100: avg loss -2.875172 avg loss no lamb -2.875172 time 2019-02-20 05:27:00.353399
Model ind 640 epoch 317 head A head_i_epoch 0 batch 200: avg loss -2.941033 avg loss no lamb -2.941033 time 2019-02-20 05:29:25.001801
last batch sz 160
Model ind 640 epoch 317 head B head_i_epoch 0 batch 0: avg loss -1.575156 avg loss no lamb -1.575156 time 2019-02-20 05:31:09.877048
Model ind 640 epoch 317 head B head_i_epoch 0 batch 100: avg loss -1.519443 avg loss no lamb -1.519443 time 2019-02-20 05:33:33.497730
Model ind 640 epoch 317 head B head_i_epoch 0 batch 200: avg loss -1.716143 avg loss no lamb -1.716143 time 2019-02-20 05:35:57.216123
last batch sz 160
Model ind 640 epoch 317 head B head_i_epoch 1 batch 0: avg loss -1.619489 avg loss no lamb -1.619489 time 2019-02-20 05:37:41.932493
Model ind 640 epoch 317 head B head_i_epoch 1 batch 100: avg loss -1.457150 avg loss no lamb -1.457150 time 2019-02-20 05:40:05.686504
Model ind 640 epoch 317 head B head_i_epoch 1 batch 200: avg loss -1.677602 avg loss no lamb -1.677602 time 2019-02-20 05:42:30.627752
last batch sz 160
Pre: time 2019-02-20 05:44:39.971282: 
 	std: 0.05313765
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49035, 0.59915, 0.5983833, 0.5980833, 0.4898]
	train_accs: [0.49035, 0.59915, 0.5983833, 0.5980833, 0.4898]
	best_train_sub_head: 1
	worst: 0.4898
	avg: 0.5551533
	best: 0.59915

Starting e_i: 318
Model ind 640 epoch 318 head A head_i_epoch 0 batch 0: avg loss -2.955752 avg loss no lamb -2.955752 time 2019-02-20 05:44:42.114395
Model ind 640 epoch 318 head A head_i_epoch 0 batch 100: avg loss -2.898292 avg loss no lamb -2.898292 time 2019-02-20 05:47:06.021946
Model ind 640 epoch 318 head A head_i_epoch 0 batch 200: avg loss -3.048064 avg loss no lamb -3.048064 time 2019-02-20 05:49:30.611858
last batch sz 160
Model ind 640 epoch 318 head B head_i_epoch 0 batch 0: avg loss -1.595205 avg loss no lamb -1.595205 time 2019-02-20 05:51:15.522958
Model ind 640 epoch 318 head B head_i_epoch 0 batch 100: avg loss -1.426843 avg loss no lamb -1.426843 time 2019-02-20 05:53:39.691145
Model ind 640 epoch 318 head B head_i_epoch 0 batch 200: avg loss -1.641073 avg loss no lamb -1.641073 time 2019-02-20 05:56:02.963395
last batch sz 160
Model ind 640 epoch 318 head B head_i_epoch 1 batch 0: avg loss -1.611819 avg loss no lamb -1.611819 time 2019-02-20 05:57:47.899149
Model ind 640 epoch 318 head B head_i_epoch 1 batch 100: avg loss -1.448618 avg loss no lamb -1.448618 time 2019-02-20 06:00:12.164422
Model ind 640 epoch 318 head B head_i_epoch 1 batch 200: avg loss -1.647531 avg loss no lamb -1.647531 time 2019-02-20 06:02:36.696334
last batch sz 160
Pre: time 2019-02-20 06:04:45.640991: 
 	std: 0.05478522
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48711666, 0.59915, 0.5987833, 0.59776664, 0.48636666]
	train_accs: [0.48711666, 0.59915, 0.5987833, 0.59776664, 0.48636666]
	best_train_sub_head: 1
	worst: 0.48636666
	avg: 0.5538367
	best: 0.59915

Starting e_i: 319
Model ind 640 epoch 319 head A head_i_epoch 0 batch 0: avg loss -2.912578 avg loss no lamb -2.912578 time 2019-02-20 06:04:48.046808
Model ind 640 epoch 319 head A head_i_epoch 0 batch 100: avg loss -2.790590 avg loss no lamb -2.790590 time 2019-02-20 06:07:11.862033
Model ind 640 epoch 319 head A head_i_epoch 0 batch 200: avg loss -2.961405 avg loss no lamb -2.961405 time 2019-02-20 06:09:36.113069
last batch sz 160
Model ind 640 epoch 319 head B head_i_epoch 0 batch 0: avg loss -1.589026 avg loss no lamb -1.589026 time 2019-02-20 06:11:21.342661
Model ind 640 epoch 319 head B head_i_epoch 0 batch 100: avg loss -1.600760 avg loss no lamb -1.600760 time 2019-02-20 06:13:45.256116
Model ind 640 epoch 319 head B head_i_epoch 0 batch 200: avg loss -1.696870 avg loss no lamb -1.696870 time 2019-02-20 06:16:09.463534
last batch sz 160
Model ind 640 epoch 319 head B head_i_epoch 1 batch 0: avg loss -1.626358 avg loss no lamb -1.626358 time 2019-02-20 06:17:54.353459
Model ind 640 epoch 319 head B head_i_epoch 1 batch 100: avg loss -1.498101 avg loss no lamb -1.498101 time 2019-02-20 06:20:18.243430
Model ind 640 epoch 319 head B head_i_epoch 1 batch 200: avg loss -1.736282 avg loss no lamb -1.736282 time 2019-02-20 06:22:41.927214
last batch sz 160
Pre: time 2019-02-20 06:24:50.987075: 
 	std: 0.054532964
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4889, 0.5995333, 0.59926665, 0.60008335, 0.48773333]
	train_accs: [0.4889, 0.5995333, 0.59926665, 0.60008335, 0.48773333]
	best_train_sub_head: 3
	worst: 0.48773333
	avg: 0.5551033
	best: 0.60008335

Starting e_i: 320
Model ind 640 epoch 320 head A head_i_epoch 0 batch 0: avg loss -2.863200 avg loss no lamb -2.863200 time 2019-02-20 06:24:53.053899
Model ind 640 epoch 320 head A head_i_epoch 0 batch 100: avg loss -2.724030 avg loss no lamb -2.724030 time 2019-02-20 06:27:17.164002
Model ind 640 epoch 320 head A head_i_epoch 0 batch 200: avg loss -3.006458 avg loss no lamb -3.006458 time 2019-02-20 06:29:41.168860
last batch sz 160
Model ind 640 epoch 320 head B head_i_epoch 0 batch 0: avg loss -1.582096 avg loss no lamb -1.582096 time 2019-02-20 06:31:25.795088
Model ind 640 epoch 320 head B head_i_epoch 0 batch 100: avg loss -1.447779 avg loss no lamb -1.447779 time 2019-02-20 06:33:49.656820
Model ind 640 epoch 320 head B head_i_epoch 0 batch 200: avg loss -1.753029 avg loss no lamb -1.753029 time 2019-02-20 06:36:13.464903
last batch sz 160
Model ind 640 epoch 320 head B head_i_epoch 1 batch 0: avg loss -1.625467 avg loss no lamb -1.625467 time 2019-02-20 06:37:58.008225
Model ind 640 epoch 320 head B head_i_epoch 1 batch 100: avg loss -1.529041 avg loss no lamb -1.529041 time 2019-02-20 06:40:21.507639
Model ind 640 epoch 320 head B head_i_epoch 1 batch 200: avg loss -1.678351 avg loss no lamb -1.678351 time 2019-02-20 06:42:45.222335
last batch sz 160
Pre: time 2019-02-20 06:44:54.381852: 
 	std: 0.052601874
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49566665, 0.6033667, 0.6021, 0.60286665, 0.49515]
	train_accs: [0.49566665, 0.6033667, 0.6021, 0.60286665, 0.49515]
	best_train_sub_head: 1
	worst: 0.49515
	avg: 0.55983
	best: 0.6033667

Starting e_i: 321
Model ind 640 epoch 321 head A head_i_epoch 0 batch 0: avg loss -2.769804 avg loss no lamb -2.769804 time 2019-02-20 06:45:03.775545
Model ind 640 epoch 321 head A head_i_epoch 0 batch 100: avg loss -2.858321 avg loss no lamb -2.858321 time 2019-02-20 06:47:27.879560
Model ind 640 epoch 321 head A head_i_epoch 0 batch 200: avg loss -2.942039 avg loss no lamb -2.942039 time 2019-02-20 06:49:51.898239
last batch sz 160
Model ind 640 epoch 321 head B head_i_epoch 0 batch 0: avg loss -1.489916 avg loss no lamb -1.489916 time 2019-02-20 06:51:36.674615
Model ind 640 epoch 321 head B head_i_epoch 0 batch 100: avg loss -1.488406 avg loss no lamb -1.488406 time 2019-02-20 06:54:00.602621
Model ind 640 epoch 321 head B head_i_epoch 0 batch 200: avg loss -1.649785 avg loss no lamb -1.649785 time 2019-02-20 06:56:24.657869
last batch sz 160
Model ind 640 epoch 321 head B head_i_epoch 1 batch 0: avg loss -1.622987 avg loss no lamb -1.622987 time 2019-02-20 06:58:09.589270
Model ind 640 epoch 321 head B head_i_epoch 1 batch 100: avg loss -1.479870 avg loss no lamb -1.479870 time 2019-02-20 07:00:33.764797
Model ind 640 epoch 321 head B head_i_epoch 1 batch 200: avg loss -1.599852 avg loss no lamb -1.599852 time 2019-02-20 07:02:57.738061
last batch sz 160
Pre: time 2019-02-20 07:05:07.648795: 
 	std: 0.053471964
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49051666, 0.59868336, 0.59861666, 0.5987, 0.48853335]
	train_accs: [0.49051666, 0.59868336, 0.59861666, 0.5987, 0.48853335]
	best_train_sub_head: 3
	worst: 0.48853335
	avg: 0.55500996
	best: 0.5987

Starting e_i: 322
Model ind 640 epoch 322 head A head_i_epoch 0 batch 0: avg loss -2.861164 avg loss no lamb -2.861164 time 2019-02-20 07:05:09.736621
Model ind 640 epoch 322 head A head_i_epoch 0 batch 100: avg loss -2.861620 avg loss no lamb -2.861620 time 2019-02-20 07:07:33.950626
Model ind 640 epoch 322 head A head_i_epoch 0 batch 200: avg loss -2.975090 avg loss no lamb -2.975090 time 2019-02-20 07:09:58.063056
last batch sz 160
Model ind 640 epoch 322 head B head_i_epoch 0 batch 0: avg loss -1.683669 avg loss no lamb -1.683669 time 2019-02-20 07:11:43.083000
Model ind 640 epoch 322 head B head_i_epoch 0 batch 100: avg loss -1.432811 avg loss no lamb -1.432811 time 2019-02-20 07:14:07.204364
Model ind 640 epoch 322 head B head_i_epoch 0 batch 200: avg loss -1.703298 avg loss no lamb -1.703298 time 2019-02-20 07:16:30.999449
last batch sz 160
Model ind 640 epoch 322 head B head_i_epoch 1 batch 0: avg loss -1.595604 avg loss no lamb -1.595604 time 2019-02-20 07:18:15.531853
Model ind 640 epoch 322 head B head_i_epoch 1 batch 100: avg loss -1.546617 avg loss no lamb -1.546617 time 2019-02-20 07:20:39.120190
Model ind 640 epoch 322 head B head_i_epoch 1 batch 200: avg loss -1.733660 avg loss no lamb -1.733660 time 2019-02-20 07:23:02.680277
last batch sz 160
Pre: time 2019-02-20 07:25:11.780933: 
 	std: 0.054323748
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49096668, 0.60245, 0.6017333, 0.6023, 0.49158335]
	train_accs: [0.49096668, 0.60245, 0.6017333, 0.6023, 0.49158335]
	best_train_sub_head: 1
	worst: 0.49096668
	avg: 0.5578067
	best: 0.60245

Starting e_i: 323
Model ind 640 epoch 323 head A head_i_epoch 0 batch 0: avg loss -2.883971 avg loss no lamb -2.883971 time 2019-02-20 07:25:13.866956
Model ind 640 epoch 323 head A head_i_epoch 0 batch 100: avg loss -2.858129 avg loss no lamb -2.858129 time 2019-02-20 07:27:37.950701
Model ind 640 epoch 323 head A head_i_epoch 0 batch 200: avg loss -2.910202 avg loss no lamb -2.910202 time 2019-02-20 07:30:02.583044
last batch sz 160
Model ind 640 epoch 323 head B head_i_epoch 0 batch 0: avg loss -1.536405 avg loss no lamb -1.536405 time 2019-02-20 07:31:47.802937
Model ind 640 epoch 323 head B head_i_epoch 0 batch 100: avg loss -1.522761 avg loss no lamb -1.522761 time 2019-02-20 07:34:11.857802
Model ind 640 epoch 323 head B head_i_epoch 0 batch 200: avg loss -1.651920 avg loss no lamb -1.651920 time 2019-02-20 07:36:35.113372
last batch sz 160
Model ind 640 epoch 323 head B head_i_epoch 1 batch 0: avg loss -1.589919 avg loss no lamb -1.589919 time 2019-02-20 07:38:19.979518
Model ind 640 epoch 323 head B head_i_epoch 1 batch 100: avg loss -1.526233 avg loss no lamb -1.526233 time 2019-02-20 07:40:44.458938
Model ind 640 epoch 323 head B head_i_epoch 1 batch 200: avg loss -1.596958 avg loss no lamb -1.596958 time 2019-02-20 07:43:08.546514
last batch sz 160
Pre: time 2019-02-20 07:45:18.303490: 
 	std: 0.052761815
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49783334, 0.6053333, 0.6048667, 0.6049167, 0.49685]
	train_accs: [0.49783334, 0.6053333, 0.6048667, 0.6049167, 0.49685]
	best_train_sub_head: 1
	worst: 0.49685
	avg: 0.56196004
	best: 0.6053333

Starting e_i: 324
Model ind 640 epoch 324 head A head_i_epoch 0 batch 0: avg loss -2.928101 avg loss no lamb -2.928101 time 2019-02-20 07:45:24.256997
Model ind 640 epoch 324 head A head_i_epoch 0 batch 100: avg loss -2.948556 avg loss no lamb -2.948556 time 2019-02-20 07:47:48.769758
Model ind 640 epoch 324 head A head_i_epoch 0 batch 200: avg loss -2.919794 avg loss no lamb -2.919794 time 2019-02-20 07:50:13.163247
last batch sz 160
Model ind 640 epoch 324 head B head_i_epoch 0 batch 0: avg loss -1.574025 avg loss no lamb -1.574025 time 2019-02-20 07:51:58.255747
Model ind 640 epoch 324 head B head_i_epoch 0 batch 100: avg loss -1.512677 avg loss no lamb -1.512677 time 2019-02-20 07:54:22.230024
Model ind 640 epoch 324 head B head_i_epoch 0 batch 200: avg loss -1.686924 avg loss no lamb -1.686924 time 2019-02-20 07:56:46.289174
last batch sz 160
Model ind 640 epoch 324 head B head_i_epoch 1 batch 0: avg loss -1.552379 avg loss no lamb -1.552379 time 2019-02-20 07:58:31.318502
Model ind 640 epoch 324 head B head_i_epoch 1 batch 100: avg loss -1.441354 avg loss no lamb -1.441354 time 2019-02-20 08:00:55.269695
Model ind 640 epoch 324 head B head_i_epoch 1 batch 200: avg loss -1.737872 avg loss no lamb -1.737872 time 2019-02-20 08:03:19.896785
last batch sz 160
Pre: time 2019-02-20 08:05:29.464551: 
 	std: 0.054794546
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4857, 0.59758335, 0.59706664, 0.5969667, 0.48501667]
	train_accs: [0.4857, 0.59758335, 0.59706664, 0.5969667, 0.48501667]
	best_train_sub_head: 1
	worst: 0.48501667
	avg: 0.5524667
	best: 0.59758335

Starting e_i: 325
Model ind 640 epoch 325 head A head_i_epoch 0 batch 0: avg loss -2.977279 avg loss no lamb -2.977279 time 2019-02-20 08:05:31.877981
Model ind 640 epoch 325 head A head_i_epoch 0 batch 100: avg loss -2.847357 avg loss no lamb -2.847357 time 2019-02-20 08:07:55.817827
Model ind 640 epoch 325 head A head_i_epoch 0 batch 200: avg loss -2.952762 avg loss no lamb -2.952762 time 2019-02-20 08:10:20.503687
last batch sz 160
Model ind 640 epoch 325 head B head_i_epoch 0 batch 0: avg loss -1.568565 avg loss no lamb -1.568565 time 2019-02-20 08:12:05.672820
Model ind 640 epoch 325 head B head_i_epoch 0 batch 100: avg loss -1.482022 avg loss no lamb -1.482022 time 2019-02-20 08:14:30.204731
Model ind 640 epoch 325 head B head_i_epoch 0 batch 200: avg loss -1.724568 avg loss no lamb -1.724568 time 2019-02-20 08:16:55.329805
last batch sz 160
Model ind 640 epoch 325 head B head_i_epoch 1 batch 0: avg loss -1.604836 avg loss no lamb -1.604836 time 2019-02-20 08:18:40.544065
Model ind 640 epoch 325 head B head_i_epoch 1 batch 100: avg loss -1.497851 avg loss no lamb -1.497851 time 2019-02-20 08:21:04.593988
Model ind 640 epoch 325 head B head_i_epoch 1 batch 200: avg loss -1.667397 avg loss no lamb -1.667397 time 2019-02-20 08:23:28.607541
last batch sz 160
Pre: time 2019-02-20 08:25:37.862807: 
 	std: 0.05288972
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.48696667, 0.59571666, 0.59498334, 0.59475, 0.48741665]
	train_accs: [0.48696667, 0.59571666, 0.59498334, 0.59475, 0.48741665]
	best_train_sub_head: 1
	worst: 0.48696667
	avg: 0.55196667
	best: 0.59571666

Starting e_i: 326
Model ind 640 epoch 326 head A head_i_epoch 0 batch 0: avg loss -2.905691 avg loss no lamb -2.905691 time 2019-02-20 08:25:40.105867
Model ind 640 epoch 326 head A head_i_epoch 0 batch 100: avg loss -2.820329 avg loss no lamb -2.820329 time 2019-02-20 08:28:04.252166
Model ind 640 epoch 326 head A head_i_epoch 0 batch 200: avg loss -2.954688 avg loss no lamb -2.954688 time 2019-02-20 08:30:28.964492
last batch sz 160
Model ind 640 epoch 326 head B head_i_epoch 0 batch 0: avg loss -1.581701 avg loss no lamb -1.581701 time 2019-02-20 08:32:14.341078
Model ind 640 epoch 326 head B head_i_epoch 0 batch 100: avg loss -1.462072 avg loss no lamb -1.462072 time 2019-02-20 08:34:38.613897
Model ind 640 epoch 326 head B head_i_epoch 0 batch 200: avg loss -1.631047 avg loss no lamb -1.631047 time 2019-02-20 08:37:02.959192
last batch sz 160
Model ind 640 epoch 326 head B head_i_epoch 1 batch 0: avg loss -1.576221 avg loss no lamb -1.576221 time 2019-02-20 08:38:47.780455
Model ind 640 epoch 326 head B head_i_epoch 1 batch 100: avg loss -1.520310 avg loss no lamb -1.520310 time 2019-02-20 08:41:11.680097
Model ind 640 epoch 326 head B head_i_epoch 1 batch 200: avg loss -1.604131 avg loss no lamb -1.604131 time 2019-02-20 08:43:35.193004
last batch sz 160
Pre: time 2019-02-20 08:45:44.751470: 
 	std: 0.0539712
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4957, 0.60611665, 0.60545, 0.60538334, 0.49526668]
	train_accs: [0.4957, 0.60611665, 0.60545, 0.60538334, 0.49526668]
	best_train_sub_head: 1
	worst: 0.49526668
	avg: 0.56158334
	best: 0.60611665

Starting e_i: 327
Model ind 640 epoch 327 head A head_i_epoch 0 batch 0: avg loss -2.936761 avg loss no lamb -2.936761 time 2019-02-20 08:45:51.145350
Model ind 640 epoch 327 head A head_i_epoch 0 batch 100: avg loss -2.835632 avg loss no lamb -2.835632 time 2019-02-20 08:48:15.094549
Model ind 640 epoch 327 head A head_i_epoch 0 batch 200: avg loss -2.902628 avg loss no lamb -2.902628 time 2019-02-20 08:50:39.039073
last batch sz 160
Model ind 640 epoch 327 head B head_i_epoch 0 batch 0: avg loss -1.615193 avg loss no lamb -1.615193 time 2019-02-20 08:52:24.515249
Model ind 640 epoch 327 head B head_i_epoch 0 batch 100: avg loss -1.430128 avg loss no lamb -1.430128 time 2019-02-20 08:54:49.616587
Model ind 640 epoch 327 head B head_i_epoch 0 batch 200: avg loss -1.624694 avg loss no lamb -1.624694 time 2019-02-20 08:57:13.467482
last batch sz 160
Model ind 640 epoch 327 head B head_i_epoch 1 batch 0: avg loss -1.587241 avg loss no lamb -1.587241 time 2019-02-20 08:58:58.712523
Model ind 640 epoch 327 head B head_i_epoch 1 batch 100: avg loss -1.448405 avg loss no lamb -1.448405 time 2019-02-20 09:01:23.412608
Model ind 640 epoch 327 head B head_i_epoch 1 batch 200: avg loss -1.698127 avg loss no lamb -1.698127 time 2019-02-20 09:03:47.703639
last batch sz 160
Pre: time 2019-02-20 09:05:57.758967: 
 	std: 0.052905604
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49226665, 0.6012833, 0.59995, 0.6002333, 0.49273333]
	train_accs: [0.49226665, 0.6012833, 0.59995, 0.6002333, 0.49273333]
	best_train_sub_head: 1
	worst: 0.49226665
	avg: 0.5572933
	best: 0.6012833

Starting e_i: 328
Model ind 640 epoch 328 head A head_i_epoch 0 batch 0: avg loss -2.965023 avg loss no lamb -2.965023 time 2019-02-20 09:05:59.863556
Model ind 640 epoch 328 head A head_i_epoch 0 batch 100: avg loss -2.814858 avg loss no lamb -2.814858 time 2019-02-20 09:08:26.001268
Model ind 640 epoch 328 head A head_i_epoch 0 batch 200: avg loss -2.941896 avg loss no lamb -2.941896 time 2019-02-20 09:10:50.206003
last batch sz 160
Model ind 640 epoch 328 head B head_i_epoch 0 batch 0: avg loss -1.584304 avg loss no lamb -1.584304 time 2019-02-20 09:12:35.421971
Model ind 640 epoch 328 head B head_i_epoch 0 batch 100: avg loss -1.469698 avg loss no lamb -1.469698 time 2019-02-20 09:14:59.770062
Model ind 640 epoch 328 head B head_i_epoch 0 batch 200: avg loss -1.706282 avg loss no lamb -1.706282 time 2019-02-20 09:17:24.086925
last batch sz 160
Model ind 640 epoch 328 head B head_i_epoch 1 batch 0: avg loss -1.502631 avg loss no lamb -1.502631 time 2019-02-20 09:19:09.002486
Model ind 640 epoch 328 head B head_i_epoch 1 batch 100: avg loss -1.494002 avg loss no lamb -1.494002 time 2019-02-20 09:21:33.299872
Model ind 640 epoch 328 head B head_i_epoch 1 batch 200: avg loss -1.734412 avg loss no lamb -1.734412 time 2019-02-20 09:23:57.691047
last batch sz 160
Pre: time 2019-02-20 09:26:07.474349: 
 	std: 0.05260059
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49471667, 0.60248333, 0.6023333, 0.60181665, 0.49496666]
	train_accs: [0.49471667, 0.60248333, 0.6023333, 0.60181665, 0.49496666]
	best_train_sub_head: 1
	worst: 0.49471667
	avg: 0.55926335
	best: 0.60248333

Starting e_i: 329
Model ind 640 epoch 329 head A head_i_epoch 0 batch 0: avg loss -2.970761 avg loss no lamb -2.970761 time 2019-02-20 09:26:09.826283
Model ind 640 epoch 329 head A head_i_epoch 0 batch 100: avg loss -2.922022 avg loss no lamb -2.922022 time 2019-02-20 09:28:34.170749
Model ind 640 epoch 329 head A head_i_epoch 0 batch 200: avg loss -2.955432 avg loss no lamb -2.955432 time 2019-02-20 09:30:59.035863
last batch sz 160
Model ind 640 epoch 329 head B head_i_epoch 0 batch 0: avg loss -1.565842 avg loss no lamb -1.565842 time 2019-02-20 09:32:44.807398
Model ind 640 epoch 329 head B head_i_epoch 0 batch 100: avg loss -1.560032 avg loss no lamb -1.560032 time 2019-02-20 09:35:09.230379
Model ind 640 epoch 329 head B head_i_epoch 0 batch 200: avg loss -1.661581 avg loss no lamb -1.661581 time 2019-02-20 09:37:33.386744
last batch sz 160
Model ind 640 epoch 329 head B head_i_epoch 1 batch 0: avg loss -1.602882 avg loss no lamb -1.602882 time 2019-02-20 09:39:18.433513
Model ind 640 epoch 329 head B head_i_epoch 1 batch 100: avg loss -1.548321 avg loss no lamb -1.548321 time 2019-02-20 09:41:43.308153
Model ind 640 epoch 329 head B head_i_epoch 1 batch 200: avg loss -1.674675 avg loss no lamb -1.674675 time 2019-02-20 09:44:08.471993
last batch sz 160
Pre: time 2019-02-20 09:46:18.799530: 
 	std: 0.054041114
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4932, 0.60375, 0.6036, 0.6027, 0.49288332]
	train_accs: [0.4932, 0.60375, 0.6036, 0.6027, 0.49288332]
	best_train_sub_head: 1
	worst: 0.49288332
	avg: 0.5592267
	best: 0.60375

Starting e_i: 330
Model ind 640 epoch 330 head A head_i_epoch 0 batch 0: avg loss -2.857893 avg loss no lamb -2.857893 time 2019-02-20 09:46:21.193218
Model ind 640 epoch 330 head A head_i_epoch 0 batch 100: avg loss -2.976985 avg loss no lamb -2.976985 time 2019-02-20 09:48:45.742651
Model ind 640 epoch 330 head A head_i_epoch 0 batch 200: avg loss -3.069040 avg loss no lamb -3.069040 time 2019-02-20 09:51:09.774031
last batch sz 160
Model ind 640 epoch 330 head B head_i_epoch 0 batch 0: avg loss -1.588572 avg loss no lamb -1.588572 time 2019-02-20 09:52:54.420355
Model ind 640 epoch 330 head B head_i_epoch 0 batch 100: avg loss -1.536065 avg loss no lamb -1.536065 time 2019-02-20 09:55:18.087663
Model ind 640 epoch 330 head B head_i_epoch 0 batch 200: avg loss -1.691834 avg loss no lamb -1.691834 time 2019-02-20 09:57:42.154719
last batch sz 160
Model ind 640 epoch 330 head B head_i_epoch 1 batch 0: avg loss -1.541713 avg loss no lamb -1.541713 time 2019-02-20 09:59:27.106437
Model ind 640 epoch 330 head B head_i_epoch 1 batch 100: avg loss -1.598556 avg loss no lamb -1.598556 time 2019-02-20 10:01:50.759610
Model ind 640 epoch 330 head B head_i_epoch 1 batch 200: avg loss -1.720030 avg loss no lamb -1.720030 time 2019-02-20 10:04:14.676303
last batch sz 160
Pre: time 2019-02-20 10:06:23.672080: 
 	std: 0.054395575
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4871, 0.59791666, 0.59781665, 0.5970333, 0.48601666]
	train_accs: [0.4871, 0.59791666, 0.59781665, 0.5970333, 0.48601666]
	best_train_sub_head: 1
	worst: 0.48601666
	avg: 0.5531767
	best: 0.59791666

Starting e_i: 331
Model ind 640 epoch 331 head A head_i_epoch 0 batch 0: avg loss -2.902804 avg loss no lamb -2.902804 time 2019-02-20 10:06:30.351240
Model ind 640 epoch 331 head A head_i_epoch 0 batch 100: avg loss -2.824620 avg loss no lamb -2.824620 time 2019-02-20 10:08:54.836123
Model ind 640 epoch 331 head A head_i_epoch 0 batch 200: avg loss -2.939311 avg loss no lamb -2.939311 time 2019-02-20 10:11:20.075715
last batch sz 160
Model ind 640 epoch 331 head B head_i_epoch 0 batch 0: avg loss -1.606757 avg loss no lamb -1.606757 time 2019-02-20 10:13:05.858984
Model ind 640 epoch 331 head B head_i_epoch 0 batch 100: avg loss -1.404202 avg loss no lamb -1.404202 time 2019-02-20 10:15:30.091358
Model ind 640 epoch 331 head B head_i_epoch 0 batch 200: avg loss -1.607949 avg loss no lamb -1.607949 time 2019-02-20 10:17:54.517632
last batch sz 160
Model ind 640 epoch 331 head B head_i_epoch 1 batch 0: avg loss -1.567710 avg loss no lamb -1.567710 time 2019-02-20 10:19:39.655871
Model ind 640 epoch 331 head B head_i_epoch 1 batch 100: avg loss -1.555734 avg loss no lamb -1.555734 time 2019-02-20 10:22:03.985826
Model ind 640 epoch 331 head B head_i_epoch 1 batch 200: avg loss -1.679432 avg loss no lamb -1.679432 time 2019-02-20 10:24:28.581191
last batch sz 160
Pre: time 2019-02-20 10:26:38.750617: 
 	std: 0.052748572
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49106666, 0.59893334, 0.59901667, 0.5987667, 0.4914]
	train_accs: [0.49106666, 0.59893334, 0.59901667, 0.5987667, 0.4914]
	best_train_sub_head: 2
	worst: 0.49106666
	avg: 0.5558367
	best: 0.59901667

Starting e_i: 332
Model ind 640 epoch 332 head A head_i_epoch 0 batch 0: avg loss -2.943997 avg loss no lamb -2.943997 time 2019-02-20 10:26:40.826262
Model ind 640 epoch 332 head A head_i_epoch 0 batch 100: avg loss -2.803621 avg loss no lamb -2.803621 time 2019-02-20 10:29:05.330625
Model ind 640 epoch 332 head A head_i_epoch 0 batch 200: avg loss -2.953878 avg loss no lamb -2.953878 time 2019-02-20 10:31:30.297838
last batch sz 160
Model ind 640 epoch 332 head B head_i_epoch 0 batch 0: avg loss -1.571408 avg loss no lamb -1.571408 time 2019-02-20 10:33:15.270429
Model ind 640 epoch 332 head B head_i_epoch 0 batch 100: avg loss -1.494969 avg loss no lamb -1.494969 time 2019-02-20 10:35:39.733785
Model ind 640 epoch 332 head B head_i_epoch 0 batch 200: avg loss -1.681661 avg loss no lamb -1.681661 time 2019-02-20 10:38:04.421418
last batch sz 160
Model ind 640 epoch 332 head B head_i_epoch 1 batch 0: avg loss -1.580873 avg loss no lamb -1.580873 time 2019-02-20 10:39:49.955806
Model ind 640 epoch 332 head B head_i_epoch 1 batch 100: avg loss -1.591662 avg loss no lamb -1.591662 time 2019-02-20 10:42:14.964696
Model ind 640 epoch 332 head B head_i_epoch 1 batch 200: avg loss -1.717920 avg loss no lamb -1.717920 time 2019-02-20 10:44:40.126908
last batch sz 160
Pre: time 2019-02-20 10:46:49.644074: 
 	std: 0.053746056
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4937, 0.60331666, 0.6037, 0.60338336, 0.49381667]
	train_accs: [0.4937, 0.60331666, 0.6037, 0.60338336, 0.49381667]
	best_train_sub_head: 2
	worst: 0.4937
	avg: 0.5595833
	best: 0.6037

Starting e_i: 333
Model ind 640 epoch 333 head A head_i_epoch 0 batch 0: avg loss -2.884219 avg loss no lamb -2.884219 time 2019-02-20 10:46:51.736205
Model ind 640 epoch 333 head A head_i_epoch 0 batch 100: avg loss -2.871722 avg loss no lamb -2.871722 time 2019-02-20 10:49:16.399859
Model ind 640 epoch 333 head A head_i_epoch 0 batch 200: avg loss -2.911101 avg loss no lamb -2.911101 time 2019-02-20 10:51:41.248454
last batch sz 160
Model ind 640 epoch 333 head B head_i_epoch 0 batch 0: avg loss -1.643645 avg loss no lamb -1.643645 time 2019-02-20 10:53:26.016542
Model ind 640 epoch 333 head B head_i_epoch 0 batch 100: avg loss -1.527657 avg loss no lamb -1.527657 time 2019-02-20 10:55:50.328164
Model ind 640 epoch 333 head B head_i_epoch 0 batch 200: avg loss -1.724313 avg loss no lamb -1.724313 time 2019-02-20 10:58:14.697649
last batch sz 160
Model ind 640 epoch 333 head B head_i_epoch 1 batch 0: avg loss -1.632632 avg loss no lamb -1.632632 time 2019-02-20 10:59:59.726069
Model ind 640 epoch 333 head B head_i_epoch 1 batch 100: avg loss -1.442678 avg loss no lamb -1.442678 time 2019-02-20 11:02:24.270696
Model ind 640 epoch 333 head B head_i_epoch 1 batch 200: avg loss -1.686511 avg loss no lamb -1.686511 time 2019-02-20 11:04:48.204306
last batch sz 160
Pre: time 2019-02-20 11:06:57.738661: 
 	std: 0.052991703
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4864, 0.5948, 0.59388334, 0.59421664, 0.48586667]
	train_accs: [0.4864, 0.5948, 0.59388334, 0.59421664, 0.48586667]
	best_train_sub_head: 1
	worst: 0.48586667
	avg: 0.5510333
	best: 0.5948

Starting e_i: 334
Model ind 640 epoch 334 head A head_i_epoch 0 batch 0: avg loss -2.904523 avg loss no lamb -2.904523 time 2019-02-20 11:06:59.771545
Model ind 640 epoch 334 head A head_i_epoch 0 batch 100: avg loss -2.800792 avg loss no lamb -2.800792 time 2019-02-20 11:09:23.214823
Model ind 640 epoch 334 head A head_i_epoch 0 batch 200: avg loss -2.958431 avg loss no lamb -2.958431 time 2019-02-20 11:11:46.694947
last batch sz 160
Model ind 640 epoch 334 head B head_i_epoch 0 batch 0: avg loss -1.627377 avg loss no lamb -1.627377 time 2019-02-20 11:13:31.569774
Model ind 640 epoch 334 head B head_i_epoch 0 batch 100: avg loss -1.501874 avg loss no lamb -1.501874 time 2019-02-20 11:15:56.066405
Model ind 640 epoch 334 head B head_i_epoch 0 batch 200: avg loss -1.627587 avg loss no lamb -1.627587 time 2019-02-20 11:18:20.686507
last batch sz 160
Model ind 640 epoch 334 head B head_i_epoch 1 batch 0: avg loss -1.615311 avg loss no lamb -1.615311 time 2019-02-20 11:20:05.498172
Model ind 640 epoch 334 head B head_i_epoch 1 batch 100: avg loss -1.575389 avg loss no lamb -1.575389 time 2019-02-20 11:22:29.574917
Model ind 640 epoch 334 head B head_i_epoch 1 batch 200: avg loss -1.711830 avg loss no lamb -1.711830 time 2019-02-20 11:24:53.710390
last batch sz 160
Pre: time 2019-02-20 11:27:03.322927: 
 	std: 0.05361001
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49101666, 0.60066664, 0.6002667, 0.6006333, 0.49116668]
	train_accs: [0.49101666, 0.60066664, 0.6002667, 0.6006333, 0.49116668]
	best_train_sub_head: 1
	worst: 0.49101666
	avg: 0.55674994
	best: 0.60066664

Starting e_i: 335
Model ind 640 epoch 335 head A head_i_epoch 0 batch 0: avg loss -2.931540 avg loss no lamb -2.931540 time 2019-02-20 11:27:05.458576
Model ind 640 epoch 335 head A head_i_epoch 0 batch 100: avg loss -2.843476 avg loss no lamb -2.843476 time 2019-02-20 11:29:30.290998
Model ind 640 epoch 335 head A head_i_epoch 0 batch 200: avg loss -2.913146 avg loss no lamb -2.913146 time 2019-02-20 11:31:54.912482
last batch sz 160
Model ind 640 epoch 335 head B head_i_epoch 0 batch 0: avg loss -1.556462 avg loss no lamb -1.556462 time 2019-02-20 11:33:40.790474
Model ind 640 epoch 335 head B head_i_epoch 0 batch 100: avg loss -1.523373 avg loss no lamb -1.523373 time 2019-02-20 11:36:06.046799
Model ind 640 epoch 335 head B head_i_epoch 0 batch 200: avg loss -1.562978 avg loss no lamb -1.562978 time 2019-02-20 11:38:30.564946
last batch sz 160
Model ind 640 epoch 335 head B head_i_epoch 1 batch 0: avg loss -1.595119 avg loss no lamb -1.595119 time 2019-02-20 11:40:15.574090
Model ind 640 epoch 335 head B head_i_epoch 1 batch 100: avg loss -1.577688 avg loss no lamb -1.577688 time 2019-02-20 11:42:40.152801
Model ind 640 epoch 335 head B head_i_epoch 1 batch 200: avg loss -1.711301 avg loss no lamb -1.711301 time 2019-02-20 11:45:04.292777
last batch sz 160
Pre: time 2019-02-20 11:47:14.139840: 
 	std: 0.053953167
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49098334, 0.60116667, 0.59978336, 0.60135, 0.4903]
	train_accs: [0.49098334, 0.60116667, 0.59978336, 0.60135, 0.4903]
	best_train_sub_head: 3
	worst: 0.4903
	avg: 0.5567167
	best: 0.60135

Starting e_i: 336
Model ind 640 epoch 336 head A head_i_epoch 0 batch 0: avg loss -2.925299 avg loss no lamb -2.925299 time 2019-02-20 11:47:16.638683
Model ind 640 epoch 336 head A head_i_epoch 0 batch 100: avg loss -2.835074 avg loss no lamb -2.835074 time 2019-02-20 11:49:40.946860
Model ind 640 epoch 336 head A head_i_epoch 0 batch 200: avg loss -2.917852 avg loss no lamb -2.917852 time 2019-02-20 11:52:05.703421
last batch sz 160
Model ind 640 epoch 336 head B head_i_epoch 0 batch 0: avg loss -1.666145 avg loss no lamb -1.666145 time 2019-02-20 11:53:51.494917
Model ind 640 epoch 336 head B head_i_epoch 0 batch 100: avg loss -1.521932 avg loss no lamb -1.521932 time 2019-02-20 11:56:16.313139
Model ind 640 epoch 336 head B head_i_epoch 0 batch 200: avg loss -1.558353 avg loss no lamb -1.558353 time 2019-02-20 11:58:39.888404
last batch sz 160
Model ind 640 epoch 336 head B head_i_epoch 1 batch 0: avg loss -1.592853 avg loss no lamb -1.592853 time 2019-02-20 12:00:24.381188
Model ind 640 epoch 336 head B head_i_epoch 1 batch 100: avg loss -1.657025 avg loss no lamb -1.657025 time 2019-02-20 12:02:48.819049
Model ind 640 epoch 336 head B head_i_epoch 1 batch 200: avg loss -1.693512 avg loss no lamb -1.693512 time 2019-02-20 12:05:12.858136
last batch sz 160
Pre: time 2019-02-20 12:07:22.757116: 
 	std: 0.05478527
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48945, 0.601, 0.60005, 0.6013, 0.48846668]
	train_accs: [0.48945, 0.601, 0.60005, 0.6013, 0.48846668]
	best_train_sub_head: 3
	worst: 0.48846668
	avg: 0.55605334
	best: 0.6013

Starting e_i: 337
Model ind 640 epoch 337 head A head_i_epoch 0 batch 0: avg loss -2.920349 avg loss no lamb -2.920349 time 2019-02-20 12:07:24.898140
Model ind 640 epoch 337 head A head_i_epoch 0 batch 100: avg loss -2.830293 avg loss no lamb -2.830293 time 2019-02-20 12:09:49.218731
Model ind 640 epoch 337 head A head_i_epoch 0 batch 200: avg loss -2.956012 avg loss no lamb -2.956012 time 2019-02-20 12:12:14.020675
last batch sz 160
Model ind 640 epoch 337 head B head_i_epoch 0 batch 0: avg loss -1.657336 avg loss no lamb -1.657336 time 2019-02-20 12:13:59.436761
Model ind 640 epoch 337 head B head_i_epoch 0 batch 100: avg loss -1.500034 avg loss no lamb -1.500034 time 2019-02-20 12:16:24.235501
Model ind 640 epoch 337 head B head_i_epoch 0 batch 200: avg loss -1.735412 avg loss no lamb -1.735412 time 2019-02-20 12:18:48.808554
last batch sz 160
Model ind 640 epoch 337 head B head_i_epoch 1 batch 0: avg loss -1.633748 avg loss no lamb -1.633748 time 2019-02-20 12:20:33.926531
Model ind 640 epoch 337 head B head_i_epoch 1 batch 100: avg loss -1.541305 avg loss no lamb -1.541305 time 2019-02-20 12:22:59.093002
Model ind 640 epoch 337 head B head_i_epoch 1 batch 200: avg loss -1.774656 avg loss no lamb -1.774656 time 2019-02-20 12:25:23.334502
last batch sz 160
Pre: time 2019-02-20 12:27:33.032059: 
 	std: 0.05517253
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49075, 0.60296667, 0.603, 0.60351664, 0.49033332]
	train_accs: [0.49075, 0.60296667, 0.603, 0.60351664, 0.49033332]
	best_train_sub_head: 3
	worst: 0.49033332
	avg: 0.5581133
	best: 0.60351664

Starting e_i: 338
Model ind 640 epoch 338 head A head_i_epoch 0 batch 0: avg loss -2.955426 avg loss no lamb -2.955426 time 2019-02-20 12:27:35.606899
Model ind 640 epoch 338 head A head_i_epoch 0 batch 100: avg loss -2.908587 avg loss no lamb -2.908587 time 2019-02-20 12:30:00.415308
Model ind 640 epoch 338 head A head_i_epoch 0 batch 200: avg loss -2.891852 avg loss no lamb -2.891852 time 2019-02-20 12:32:25.121031
last batch sz 160
Model ind 640 epoch 338 head B head_i_epoch 0 batch 0: avg loss -1.568124 avg loss no lamb -1.568124 time 2019-02-20 12:34:10.036961
Model ind 640 epoch 338 head B head_i_epoch 0 batch 100: avg loss -1.453316 avg loss no lamb -1.453316 time 2019-02-20 12:36:34.537654
Model ind 640 epoch 338 head B head_i_epoch 0 batch 200: avg loss -1.668804 avg loss no lamb -1.668804 time 2019-02-20 12:38:59.278188
last batch sz 160
Model ind 640 epoch 338 head B head_i_epoch 1 batch 0: avg loss -1.616436 avg loss no lamb -1.616436 time 2019-02-20 12:40:44.070237
Model ind 640 epoch 338 head B head_i_epoch 1 batch 100: avg loss -1.536690 avg loss no lamb -1.536690 time 2019-02-20 12:43:08.006425
Model ind 640 epoch 338 head B head_i_epoch 1 batch 200: avg loss -1.635565 avg loss no lamb -1.635565 time 2019-02-20 12:45:31.938984
last batch sz 160
Pre: time 2019-02-20 12:47:40.759735: 
 	std: 0.05386023
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49163333, 0.60105, 0.60073334, 0.6013833, 0.4906]
	train_accs: [0.49163333, 0.60105, 0.60073334, 0.6013833, 0.4906]
	best_train_sub_head: 3
	worst: 0.4906
	avg: 0.55708
	best: 0.6013833

Starting e_i: 339
Model ind 640 epoch 339 head A head_i_epoch 0 batch 0: avg loss -2.912712 avg loss no lamb -2.912712 time 2019-02-20 12:47:42.859465
Model ind 640 epoch 339 head A head_i_epoch 0 batch 100: avg loss -2.817881 avg loss no lamb -2.817881 time 2019-02-20 12:50:07.068692
Model ind 640 epoch 339 head A head_i_epoch 0 batch 200: avg loss -2.945139 avg loss no lamb -2.945139 time 2019-02-20 12:52:31.449195
last batch sz 160
Model ind 640 epoch 339 head B head_i_epoch 0 batch 0: avg loss -1.662501 avg loss no lamb -1.662501 time 2019-02-20 12:54:16.454181
Model ind 640 epoch 339 head B head_i_epoch 0 batch 100: avg loss -1.499843 avg loss no lamb -1.499843 time 2019-02-20 12:56:40.762804
Model ind 640 epoch 339 head B head_i_epoch 0 batch 200: avg loss -1.696343 avg loss no lamb -1.696343 time 2019-02-20 12:59:05.019282
last batch sz 160
Model ind 640 epoch 339 head B head_i_epoch 1 batch 0: avg loss -1.650408 avg loss no lamb -1.650408 time 2019-02-20 13:00:49.992306
Model ind 640 epoch 339 head B head_i_epoch 1 batch 100: avg loss -1.452493 avg loss no lamb -1.452493 time 2019-02-20 13:03:14.405235
Model ind 640 epoch 339 head B head_i_epoch 1 batch 200: avg loss -1.675173 avg loss no lamb -1.675173 time 2019-02-20 13:05:38.679965
last batch sz 160
Pre: time 2019-02-20 13:07:48.566706: 
 	std: 0.054051798
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4908, 0.6002667, 0.60015, 0.60066664, 0.48926666]
	train_accs: [0.4908, 0.6002667, 0.60015, 0.60066664, 0.48926666]
	best_train_sub_head: 3
	worst: 0.48926666
	avg: 0.55623
	best: 0.60066664

Starting e_i: 340
Model ind 640 epoch 340 head A head_i_epoch 0 batch 0: avg loss -2.936071 avg loss no lamb -2.936071 time 2019-02-20 13:07:50.687318
Model ind 640 epoch 340 head A head_i_epoch 0 batch 100: avg loss -2.845012 avg loss no lamb -2.845012 time 2019-02-20 13:10:15.191516
Model ind 640 epoch 340 head A head_i_epoch 0 batch 200: avg loss -2.896193 avg loss no lamb -2.896193 time 2019-02-20 13:12:39.486999
last batch sz 160
Model ind 640 epoch 340 head B head_i_epoch 0 batch 0: avg loss -1.578919 avg loss no lamb -1.578919 time 2019-02-20 13:14:24.699563
Model ind 640 epoch 340 head B head_i_epoch 0 batch 100: avg loss -1.521197 avg loss no lamb -1.521197 time 2019-02-20 13:16:49.415239
Model ind 640 epoch 340 head B head_i_epoch 0 batch 200: avg loss -1.642986 avg loss no lamb -1.642986 time 2019-02-20 13:19:13.591270
last batch sz 160
Model ind 640 epoch 340 head B head_i_epoch 1 batch 0: avg loss -1.587546 avg loss no lamb -1.587546 time 2019-02-20 13:20:58.598490
Model ind 640 epoch 340 head B head_i_epoch 1 batch 100: avg loss -1.633768 avg loss no lamb -1.633768 time 2019-02-20 13:23:22.424505
Model ind 640 epoch 340 head B head_i_epoch 1 batch 200: avg loss -1.744652 avg loss no lamb -1.744652 time 2019-02-20 13:25:46.101857
last batch sz 160
Pre: time 2019-02-20 13:27:55.815868: 
 	std: 0.055062354
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49305, 0.6045, 0.60473335, 0.60531664, 0.49186668]
	train_accs: [0.49305, 0.6045, 0.60473335, 0.60531664, 0.49186668]
	best_train_sub_head: 3
	worst: 0.49186668
	avg: 0.5598933
	best: 0.60531664

Starting e_i: 341
Model ind 640 epoch 341 head A head_i_epoch 0 batch 0: avg loss -2.882006 avg loss no lamb -2.882006 time 2019-02-20 13:28:01.440402
Model ind 640 epoch 341 head A head_i_epoch 0 batch 100: avg loss -2.870492 avg loss no lamb -2.870492 time 2019-02-20 13:30:25.797991
Model ind 640 epoch 341 head A head_i_epoch 0 batch 200: avg loss -2.959638 avg loss no lamb -2.959638 time 2019-02-20 13:32:49.661305
last batch sz 160
Model ind 640 epoch 341 head B head_i_epoch 0 batch 0: avg loss -1.687538 avg loss no lamb -1.687538 time 2019-02-20 13:34:34.476638
Model ind 640 epoch 341 head B head_i_epoch 0 batch 100: avg loss -1.500328 avg loss no lamb -1.500328 time 2019-02-20 13:36:59.021375
Model ind 640 epoch 341 head B head_i_epoch 0 batch 200: avg loss -1.621928 avg loss no lamb -1.621928 time 2019-02-20 13:39:23.279963
last batch sz 160
Model ind 640 epoch 341 head B head_i_epoch 1 batch 0: avg loss -1.565101 avg loss no lamb -1.565101 time 2019-02-20 13:41:08.185389
Model ind 640 epoch 341 head B head_i_epoch 1 batch 100: avg loss -1.570263 avg loss no lamb -1.570263 time 2019-02-20 13:43:32.044486
Model ind 640 epoch 341 head B head_i_epoch 1 batch 200: avg loss -1.724472 avg loss no lamb -1.724472 time 2019-02-20 13:45:55.876957
last batch sz 160
Pre: time 2019-02-20 13:48:06.365907: 
 	std: 0.05523939
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49306667, 0.60616666, 0.6064, 0.6061, 0.49386665]
	train_accs: [0.49306667, 0.60616666, 0.6064, 0.6061, 0.49386665]
	best_train_sub_head: 2
	worst: 0.49306667
	avg: 0.56112003
	best: 0.6064

Starting e_i: 342
Model ind 640 epoch 342 head A head_i_epoch 0 batch 0: avg loss -2.840050 avg loss no lamb -2.840050 time 2019-02-20 13:48:12.373695
Model ind 640 epoch 342 head A head_i_epoch 0 batch 100: avg loss -2.803112 avg loss no lamb -2.803112 time 2019-02-20 13:50:37.334402
Model ind 640 epoch 342 head A head_i_epoch 0 batch 200: avg loss -2.967844 avg loss no lamb -2.967844 time 2019-02-20 13:53:01.967368
last batch sz 160
Model ind 640 epoch 342 head B head_i_epoch 0 batch 0: avg loss -1.610314 avg loss no lamb -1.610314 time 2019-02-20 13:54:47.298067
Model ind 640 epoch 342 head B head_i_epoch 0 batch 100: avg loss -1.501575 avg loss no lamb -1.501575 time 2019-02-20 13:57:11.434338
Model ind 640 epoch 342 head B head_i_epoch 0 batch 200: avg loss -1.660891 avg loss no lamb -1.660891 time 2019-02-20 13:59:35.836801
last batch sz 160
Model ind 640 epoch 342 head B head_i_epoch 1 batch 0: avg loss -1.553848 avg loss no lamb -1.553848 time 2019-02-20 14:01:20.812515
Model ind 640 epoch 342 head B head_i_epoch 1 batch 100: avg loss -1.471209 avg loss no lamb -1.471209 time 2019-02-20 14:03:45.084248
Model ind 640 epoch 342 head B head_i_epoch 1 batch 200: avg loss -1.718803 avg loss no lamb -1.718803 time 2019-02-20 14:06:08.966117
last batch sz 160
Pre: time 2019-02-20 14:08:18.691718: 
 	std: 0.055003162
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49495, 0.60651666, 0.60613334, 0.60658336, 0.49333334]
	train_accs: [0.49495, 0.60651666, 0.60613334, 0.60658336, 0.49333334]
	best_train_sub_head: 3
	worst: 0.49333334
	avg: 0.5615033
	best: 0.60658336

Starting e_i: 343
Model ind 640 epoch 343 head A head_i_epoch 0 batch 0: avg loss -2.854794 avg loss no lamb -2.854794 time 2019-02-20 14:08:24.969120
Model ind 640 epoch 343 head A head_i_epoch 0 batch 100: avg loss -2.895260 avg loss no lamb -2.895260 time 2019-02-20 14:10:49.038153
Model ind 640 epoch 343 head A head_i_epoch 0 batch 200: avg loss -2.979636 avg loss no lamb -2.979636 time 2019-02-20 14:13:13.358971
last batch sz 160
Model ind 640 epoch 343 head B head_i_epoch 0 batch 0: avg loss -1.736820 avg loss no lamb -1.736820 time 2019-02-20 14:14:58.588251
Model ind 640 epoch 343 head B head_i_epoch 0 batch 100: avg loss -1.472454 avg loss no lamb -1.472454 time 2019-02-20 14:17:23.156238
Model ind 640 epoch 343 head B head_i_epoch 0 batch 200: avg loss -1.739664 avg loss no lamb -1.739664 time 2019-02-20 14:19:47.752301
last batch sz 160
Model ind 640 epoch 343 head B head_i_epoch 1 batch 0: avg loss -1.604736 avg loss no lamb -1.604736 time 2019-02-20 14:21:32.618399
Model ind 640 epoch 343 head B head_i_epoch 1 batch 100: avg loss -1.563228 avg loss no lamb -1.563228 time 2019-02-20 14:23:56.248727
Model ind 640 epoch 343 head B head_i_epoch 1 batch 200: avg loss -1.692685 avg loss no lamb -1.692685 time 2019-02-20 14:26:19.756918
last batch sz 160
Pre: time 2019-02-20 14:28:29.435527: 
 	std: 0.053178046
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5025667, 0.61155, 0.6115, 0.6117667, 0.50355]
	train_accs: [0.5025667, 0.61155, 0.6115, 0.6117667, 0.50355]
	best_train_sub_head: 3
	worst: 0.5025667
	avg: 0.56818664
	best: 0.6117667

Starting e_i: 344
Model ind 640 epoch 344 head A head_i_epoch 0 batch 0: avg loss -2.919892 avg loss no lamb -2.919892 time 2019-02-20 14:28:36.283248
Model ind 640 epoch 344 head A head_i_epoch 0 batch 100: avg loss -2.798273 avg loss no lamb -2.798273 time 2019-02-20 14:31:00.216803
Model ind 640 epoch 344 head A head_i_epoch 0 batch 200: avg loss -2.959557 avg loss no lamb -2.959557 time 2019-02-20 14:33:24.621986
last batch sz 160
Model ind 640 epoch 344 head B head_i_epoch 0 batch 0: avg loss -1.598105 avg loss no lamb -1.598105 time 2019-02-20 14:35:09.933805
Model ind 640 epoch 344 head B head_i_epoch 0 batch 100: avg loss -1.559881 avg loss no lamb -1.559881 time 2019-02-20 14:37:34.005292
Model ind 640 epoch 344 head B head_i_epoch 0 batch 200: avg loss -1.642440 avg loss no lamb -1.642440 time 2019-02-20 14:39:58.304405
last batch sz 160
Model ind 640 epoch 344 head B head_i_epoch 1 batch 0: avg loss -1.570798 avg loss no lamb -1.570798 time 2019-02-20 14:41:43.923531
Model ind 640 epoch 344 head B head_i_epoch 1 batch 100: avg loss -1.537113 avg loss no lamb -1.537113 time 2019-02-20 14:44:08.311361
Model ind 640 epoch 344 head B head_i_epoch 1 batch 200: avg loss -1.761602 avg loss no lamb -1.761602 time 2019-02-20 14:46:32.030816
last batch sz 160
Pre: time 2019-02-20 14:48:41.652116: 
 	std: 0.0552603
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4927, 0.6049333, 0.60506666, 0.6048167, 0.49158335]
	train_accs: [0.4927, 0.6049333, 0.60506666, 0.6048167, 0.49158335]
	best_train_sub_head: 2
	worst: 0.49158335
	avg: 0.55982
	best: 0.60506666

Starting e_i: 345
Model ind 640 epoch 345 head A head_i_epoch 0 batch 0: avg loss -2.900004 avg loss no lamb -2.900004 time 2019-02-20 14:48:43.792855
Model ind 640 epoch 345 head A head_i_epoch 0 batch 100: avg loss -2.846307 avg loss no lamb -2.846307 time 2019-02-20 14:51:07.711195
Model ind 640 epoch 345 head A head_i_epoch 0 batch 200: avg loss -2.939786 avg loss no lamb -2.939786 time 2019-02-20 14:53:32.284238
last batch sz 160
Model ind 640 epoch 345 head B head_i_epoch 0 batch 0: avg loss -1.584020 avg loss no lamb -1.584020 time 2019-02-20 14:55:17.488309
Model ind 640 epoch 345 head B head_i_epoch 0 batch 100: avg loss -1.452014 avg loss no lamb -1.452014 time 2019-02-20 14:57:41.575287
Model ind 640 epoch 345 head B head_i_epoch 0 batch 200: avg loss -1.692647 avg loss no lamb -1.692647 time 2019-02-20 15:00:05.316899
last batch sz 160
Model ind 640 epoch 345 head B head_i_epoch 1 batch 0: avg loss -1.626299 avg loss no lamb -1.626299 time 2019-02-20 15:01:50.236275
Model ind 640 epoch 345 head B head_i_epoch 1 batch 100: avg loss -1.503197 avg loss no lamb -1.503197 time 2019-02-20 15:04:14.588038
Model ind 640 epoch 345 head B head_i_epoch 1 batch 200: avg loss -1.702147 avg loss no lamb -1.702147 time 2019-02-20 15:06:38.933346
last batch sz 160
Pre: time 2019-02-20 15:08:48.666738: 
 	std: 0.05299482
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49378332, 0.6013833, 0.6014, 0.6012333, 0.49255]
	train_accs: [0.49378332, 0.6013833, 0.6014, 0.6012333, 0.49255]
	best_train_sub_head: 2
	worst: 0.49255
	avg: 0.55807
	best: 0.6014

Starting e_i: 346
Model ind 640 epoch 346 head A head_i_epoch 0 batch 0: avg loss -2.988442 avg loss no lamb -2.988442 time 2019-02-20 15:08:50.802420
Model ind 640 epoch 346 head A head_i_epoch 0 batch 100: avg loss -2.862737 avg loss no lamb -2.862737 time 2019-02-20 15:11:15.183297
Model ind 640 epoch 346 head A head_i_epoch 0 batch 200: avg loss -3.044877 avg loss no lamb -3.044877 time 2019-02-20 15:13:39.850804
last batch sz 160
Model ind 640 epoch 346 head B head_i_epoch 0 batch 0: avg loss -1.570999 avg loss no lamb -1.570999 time 2019-02-20 15:15:25.006764
Model ind 640 epoch 346 head B head_i_epoch 0 batch 100: avg loss -1.368179 avg loss no lamb -1.368179 time 2019-02-20 15:17:49.103327
Model ind 640 epoch 346 head B head_i_epoch 0 batch 200: avg loss -1.729184 avg loss no lamb -1.729184 time 2019-02-20 15:20:13.728539
last batch sz 160
Model ind 640 epoch 346 head B head_i_epoch 1 batch 0: avg loss -1.624338 avg loss no lamb -1.624338 time 2019-02-20 15:21:58.862856
Model ind 640 epoch 346 head B head_i_epoch 1 batch 100: avg loss -1.494607 avg loss no lamb -1.494607 time 2019-02-20 15:24:23.926304
Model ind 640 epoch 346 head B head_i_epoch 1 batch 200: avg loss -1.623051 avg loss no lamb -1.623051 time 2019-02-20 15:26:48.154699
last batch sz 160
Pre: time 2019-02-20 15:28:58.024064: 
 	std: 0.05441255
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49285, 0.6030667, 0.60293335, 0.6026833, 0.49081665]
	train_accs: [0.49285, 0.6030667, 0.60293335, 0.6026833, 0.49081665]
	best_train_sub_head: 1
	worst: 0.49081665
	avg: 0.55847
	best: 0.6030667

Starting e_i: 347
Model ind 640 epoch 347 head A head_i_epoch 0 batch 0: avg loss -2.898585 avg loss no lamb -2.898585 time 2019-02-20 15:29:00.129074
Model ind 640 epoch 347 head A head_i_epoch 0 batch 100: avg loss -2.977463 avg loss no lamb -2.977463 time 2019-02-20 15:31:24.453443
Model ind 640 epoch 347 head A head_i_epoch 0 batch 200: avg loss -2.968169 avg loss no lamb -2.968169 time 2019-02-20 15:33:49.477773
last batch sz 160
Model ind 640 epoch 347 head B head_i_epoch 0 batch 0: avg loss -1.545358 avg loss no lamb -1.545358 time 2019-02-20 15:35:34.747616
Model ind 640 epoch 347 head B head_i_epoch 0 batch 100: avg loss -1.505512 avg loss no lamb -1.505512 time 2019-02-20 15:37:59.341055
Model ind 640 epoch 347 head B head_i_epoch 0 batch 200: avg loss -1.721697 avg loss no lamb -1.721697 time 2019-02-20 15:40:23.790462
last batch sz 160
Model ind 640 epoch 347 head B head_i_epoch 1 batch 0: avg loss -1.569413 avg loss no lamb -1.569413 time 2019-02-20 15:42:09.001640
Model ind 640 epoch 347 head B head_i_epoch 1 batch 100: avg loss -1.477871 avg loss no lamb -1.477871 time 2019-02-20 15:44:33.737091
Model ind 640 epoch 347 head B head_i_epoch 1 batch 200: avg loss -1.627713 avg loss no lamb -1.627713 time 2019-02-20 15:46:58.084485
last batch sz 160
Pre: time 2019-02-20 15:49:07.422866: 
 	std: 0.051253844
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50186664, 0.60548335, 0.60518336, 0.6049833, 0.49935]
	train_accs: [0.50186664, 0.60548335, 0.60518336, 0.6049833, 0.49935]
	best_train_sub_head: 1
	worst: 0.49935
	avg: 0.5633734
	best: 0.60548335

Starting e_i: 348
Model ind 640 epoch 348 head A head_i_epoch 0 batch 0: avg loss -2.979830 avg loss no lamb -2.979830 time 2019-02-20 15:49:09.536036
Model ind 640 epoch 348 head A head_i_epoch 0 batch 100: avg loss -2.854968 avg loss no lamb -2.854968 time 2019-02-20 15:51:33.727177
Model ind 640 epoch 348 head A head_i_epoch 0 batch 200: avg loss -2.926923 avg loss no lamb -2.926923 time 2019-02-20 15:53:58.075805
last batch sz 160
Model ind 640 epoch 348 head B head_i_epoch 0 batch 0: avg loss -1.568395 avg loss no lamb -1.568395 time 2019-02-20 15:55:43.436428
Model ind 640 epoch 348 head B head_i_epoch 0 batch 100: avg loss -1.488079 avg loss no lamb -1.488079 time 2019-02-20 15:58:07.025231
Model ind 640 epoch 348 head B head_i_epoch 0 batch 200: avg loss -1.840612 avg loss no lamb -1.840612 time 2019-02-20 16:00:31.485803
last batch sz 160
Model ind 640 epoch 348 head B head_i_epoch 1 batch 0: avg loss -1.656785 avg loss no lamb -1.656785 time 2019-02-20 16:02:16.546977
Model ind 640 epoch 348 head B head_i_epoch 1 batch 100: avg loss -1.515924 avg loss no lamb -1.515924 time 2019-02-20 16:04:40.950207
Model ind 640 epoch 348 head B head_i_epoch 1 batch 200: avg loss -1.705701 avg loss no lamb -1.705701 time 2019-02-20 16:07:05.541619
last batch sz 160
Pre: time 2019-02-20 16:09:14.873683: 
 	std: 0.0530906
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49995, 0.60753334, 0.60715, 0.607, 0.49778333]
	train_accs: [0.49995, 0.60753334, 0.60715, 0.607, 0.49778333]
	best_train_sub_head: 1
	worst: 0.49778333
	avg: 0.56388336
	best: 0.60753334

Starting e_i: 349
Model ind 640 epoch 349 head A head_i_epoch 0 batch 0: avg loss -2.948419 avg loss no lamb -2.948419 time 2019-02-20 16:09:16.945252
Model ind 640 epoch 349 head A head_i_epoch 0 batch 100: avg loss -2.868824 avg loss no lamb -2.868824 time 2019-02-20 16:11:40.192211
Model ind 640 epoch 349 head A head_i_epoch 0 batch 200: avg loss -3.005584 avg loss no lamb -3.005584 time 2019-02-20 16:14:04.798971
last batch sz 160
Model ind 640 epoch 349 head B head_i_epoch 0 batch 0: avg loss -1.629196 avg loss no lamb -1.629196 time 2019-02-20 16:15:49.664081
Model ind 640 epoch 349 head B head_i_epoch 0 batch 100: avg loss -1.549176 avg loss no lamb -1.549176 time 2019-02-20 16:18:13.677295
Model ind 640 epoch 349 head B head_i_epoch 0 batch 200: avg loss -1.736266 avg loss no lamb -1.736266 time 2019-02-20 16:20:37.125931
last batch sz 160
Model ind 640 epoch 349 head B head_i_epoch 1 batch 0: avg loss -1.499796 avg loss no lamb -1.499796 time 2019-02-20 16:22:21.684933
Model ind 640 epoch 349 head B head_i_epoch 1 batch 100: avg loss -1.538852 avg loss no lamb -1.538852 time 2019-02-20 16:24:45.695394
Model ind 640 epoch 349 head B head_i_epoch 1 batch 200: avg loss -1.611956 avg loss no lamb -1.611956 time 2019-02-20 16:27:09.795139
last batch sz 160
Pre: time 2019-02-20 16:29:19.271660: 
 	std: 0.054921016
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49068335, 0.6026667, 0.6023167, 0.60223335, 0.48991665]
	train_accs: [0.49068335, 0.6026667, 0.6023167, 0.60223335, 0.48991665]
	best_train_sub_head: 1
	worst: 0.48991665
	avg: 0.55756336
	best: 0.6026667

Starting e_i: 350
Model ind 640 epoch 350 head A head_i_epoch 0 batch 0: avg loss -2.947754 avg loss no lamb -2.947754 time 2019-02-20 16:29:21.459884
Model ind 640 epoch 350 head A head_i_epoch 0 batch 100: avg loss -2.836675 avg loss no lamb -2.836675 time 2019-02-20 16:31:45.785406
Model ind 640 epoch 350 head A head_i_epoch 0 batch 200: avg loss -2.934348 avg loss no lamb -2.934348 time 2019-02-20 16:34:09.947178
last batch sz 160
Model ind 640 epoch 350 head B head_i_epoch 0 batch 0: avg loss -1.656212 avg loss no lamb -1.656212 time 2019-02-20 16:35:54.923480
Model ind 640 epoch 350 head B head_i_epoch 0 batch 100: avg loss -1.522391 avg loss no lamb -1.522391 time 2019-02-20 16:38:18.995269
Model ind 640 epoch 350 head B head_i_epoch 0 batch 200: avg loss -1.707770 avg loss no lamb -1.707770 time 2019-02-20 16:40:43.872815
last batch sz 160
Model ind 640 epoch 350 head B head_i_epoch 1 batch 0: avg loss -1.573007 avg loss no lamb -1.573007 time 2019-02-20 16:42:29.230870
Model ind 640 epoch 350 head B head_i_epoch 1 batch 100: avg loss -1.544263 avg loss no lamb -1.544263 time 2019-02-20 16:44:53.573464
Model ind 640 epoch 350 head B head_i_epoch 1 batch 200: avg loss -1.630537 avg loss no lamb -1.630537 time 2019-02-20 16:47:18.160792
last batch sz 160
Pre: time 2019-02-20 16:49:27.377973: 
 	std: 0.05299286
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49515, 0.60395, 0.60351664, 0.6030667, 0.49553335]
	train_accs: [0.49515, 0.60395, 0.60351664, 0.6030667, 0.49553335]
	best_train_sub_head: 1
	worst: 0.49515
	avg: 0.5602433
	best: 0.60395

Starting e_i: 351
Model ind 640 epoch 351 head A head_i_epoch 0 batch 0: avg loss -2.903384 avg loss no lamb -2.903384 time 2019-02-20 16:49:33.114433
Model ind 640 epoch 351 head A head_i_epoch 0 batch 100: avg loss -2.778031 avg loss no lamb -2.778031 time 2019-02-20 16:51:57.911149
Model ind 640 epoch 351 head A head_i_epoch 0 batch 200: avg loss -2.956385 avg loss no lamb -2.956385 time 2019-02-20 16:54:22.240478
last batch sz 160
Model ind 640 epoch 351 head B head_i_epoch 0 batch 0: avg loss -1.630953 avg loss no lamb -1.630953 time 2019-02-20 16:56:07.137742
Model ind 640 epoch 351 head B head_i_epoch 0 batch 100: avg loss -1.467445 avg loss no lamb -1.467445 time 2019-02-20 16:58:32.634700
Model ind 640 epoch 351 head B head_i_epoch 0 batch 200: avg loss -1.687287 avg loss no lamb -1.687287 time 2019-02-20 17:00:56.366864
last batch sz 160
Model ind 640 epoch 351 head B head_i_epoch 1 batch 0: avg loss -1.606565 avg loss no lamb -1.606565 time 2019-02-20 17:02:40.938400
Model ind 640 epoch 351 head B head_i_epoch 1 batch 100: avg loss -1.562033 avg loss no lamb -1.562033 time 2019-02-20 17:05:05.453060
Model ind 640 epoch 351 head B head_i_epoch 1 batch 200: avg loss -1.694582 avg loss no lamb -1.694582 time 2019-02-20 17:07:29.563602
last batch sz 160
Pre: time 2019-02-20 17:09:40.087923: 
 	std: 0.054917745
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48888335, 0.60106665, 0.6012667, 0.6012167, 0.48928332]
	train_accs: [0.48888335, 0.60106665, 0.6012667, 0.6012167, 0.48928332]
	best_train_sub_head: 2
	worst: 0.48888335
	avg: 0.5563433
	best: 0.6012667

Starting e_i: 352
Model ind 640 epoch 352 head A head_i_epoch 0 batch 0: avg loss -2.853031 avg loss no lamb -2.853031 time 2019-02-20 17:09:42.239083
Model ind 640 epoch 352 head A head_i_epoch 0 batch 100: avg loss -2.889189 avg loss no lamb -2.889189 time 2019-02-20 17:12:07.488907
Model ind 640 epoch 352 head A head_i_epoch 0 batch 200: avg loss -2.940521 avg loss no lamb -2.940521 time 2019-02-20 17:14:31.845572
last batch sz 160
Model ind 640 epoch 352 head B head_i_epoch 0 batch 0: avg loss -1.646919 avg loss no lamb -1.646919 time 2019-02-20 17:16:17.102182
Model ind 640 epoch 352 head B head_i_epoch 0 batch 100: avg loss -1.544388 avg loss no lamb -1.544388 time 2019-02-20 17:18:41.398421
Model ind 640 epoch 352 head B head_i_epoch 0 batch 200: avg loss -1.691508 avg loss no lamb -1.691508 time 2019-02-20 17:21:06.304867
last batch sz 160
Model ind 640 epoch 352 head B head_i_epoch 1 batch 0: avg loss -1.552119 avg loss no lamb -1.552119 time 2019-02-20 17:22:51.319882
Model ind 640 epoch 352 head B head_i_epoch 1 batch 100: avg loss -1.524509 avg loss no lamb -1.524509 time 2019-02-20 17:25:16.106283
Model ind 640 epoch 352 head B head_i_epoch 1 batch 200: avg loss -1.656872 avg loss no lamb -1.656872 time 2019-02-20 17:27:40.441824
last batch sz 160
Pre: time 2019-02-20 17:29:49.730098: 
 	std: 0.051816642
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4945, 0.59966666, 0.59998333, 0.59935, 0.4933]
	train_accs: [0.4945, 0.59966666, 0.59998333, 0.59935, 0.4933]
	best_train_sub_head: 2
	worst: 0.4933
	avg: 0.55736
	best: 0.59998333

Starting e_i: 353
Model ind 640 epoch 353 head A head_i_epoch 0 batch 0: avg loss -2.911581 avg loss no lamb -2.911581 time 2019-02-20 17:29:52.214571
Model ind 640 epoch 353 head A head_i_epoch 0 batch 100: avg loss -2.806372 avg loss no lamb -2.806372 time 2019-02-20 17:32:16.418058
Model ind 640 epoch 353 head A head_i_epoch 0 batch 200: avg loss -3.008595 avg loss no lamb -3.008595 time 2019-02-20 17:34:40.972607
last batch sz 160
Model ind 640 epoch 353 head B head_i_epoch 0 batch 0: avg loss -1.599718 avg loss no lamb -1.599718 time 2019-02-20 17:36:26.263720
Model ind 640 epoch 353 head B head_i_epoch 0 batch 100: avg loss -1.500500 avg loss no lamb -1.500500 time 2019-02-20 17:38:50.229134
Model ind 640 epoch 353 head B head_i_epoch 0 batch 200: avg loss -1.624030 avg loss no lamb -1.624030 time 2019-02-20 17:41:15.028001
last batch sz 160
Model ind 640 epoch 353 head B head_i_epoch 1 batch 0: avg loss -1.595978 avg loss no lamb -1.595978 time 2019-02-20 17:43:00.212900
Model ind 640 epoch 353 head B head_i_epoch 1 batch 100: avg loss -1.484563 avg loss no lamb -1.484563 time 2019-02-20 17:45:24.693430
Model ind 640 epoch 353 head B head_i_epoch 1 batch 200: avg loss -1.771168 avg loss no lamb -1.771168 time 2019-02-20 17:47:48.459395
last batch sz 160
Pre: time 2019-02-20 17:49:58.279657: 
 	std: 0.053099845
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49148333, 0.59915, 0.5998667, 0.5990667, 0.49046665]
	train_accs: [0.49148333, 0.59915, 0.5998667, 0.5990667, 0.49046665]
	best_train_sub_head: 2
	worst: 0.49046665
	avg: 0.55600667
	best: 0.5998667

Starting e_i: 354
Model ind 640 epoch 354 head A head_i_epoch 0 batch 0: avg loss -2.987098 avg loss no lamb -2.987098 time 2019-02-20 17:50:00.425908
Model ind 640 epoch 354 head A head_i_epoch 0 batch 100: avg loss -2.849868 avg loss no lamb -2.849868 time 2019-02-20 17:52:25.438528
Model ind 640 epoch 354 head A head_i_epoch 0 batch 200: avg loss -2.929498 avg loss no lamb -2.929498 time 2019-02-20 17:54:50.449918
last batch sz 160
Model ind 640 epoch 354 head B head_i_epoch 0 batch 0: avg loss -1.630093 avg loss no lamb -1.630093 time 2019-02-20 17:56:35.760540
Model ind 640 epoch 354 head B head_i_epoch 0 batch 100: avg loss -1.453537 avg loss no lamb -1.453537 time 2019-02-20 17:58:59.850999
Model ind 640 epoch 354 head B head_i_epoch 0 batch 200: avg loss -1.711469 avg loss no lamb -1.711469 time 2019-02-20 18:01:24.353553
last batch sz 160
Model ind 640 epoch 354 head B head_i_epoch 1 batch 0: avg loss -1.529773 avg loss no lamb -1.529773 time 2019-02-20 18:03:09.478356
Model ind 640 epoch 354 head B head_i_epoch 1 batch 100: avg loss -1.474402 avg loss no lamb -1.474402 time 2019-02-20 18:05:33.373160
Model ind 640 epoch 354 head B head_i_epoch 1 batch 200: avg loss -1.688637 avg loss no lamb -1.688637 time 2019-02-20 18:07:57.386867
last batch sz 160
Pre: time 2019-02-20 18:10:06.470252: 
 	std: 0.053819533
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49883333, 0.6075, 0.60751665, 0.60735, 0.49638334]
	train_accs: [0.49883333, 0.6075, 0.60751665, 0.60735, 0.49638334]
	best_train_sub_head: 2
	worst: 0.49638334
	avg: 0.5635167
	best: 0.60751665

Starting e_i: 355
Model ind 640 epoch 355 head A head_i_epoch 0 batch 0: avg loss -2.947189 avg loss no lamb -2.947189 time 2019-02-20 18:10:08.581631
Model ind 640 epoch 355 head A head_i_epoch 0 batch 100: avg loss -2.808627 avg loss no lamb -2.808627 time 2019-02-20 18:12:33.172300
Model ind 640 epoch 355 head A head_i_epoch 0 batch 200: avg loss -2.984496 avg loss no lamb -2.984496 time 2019-02-20 18:14:58.192944
last batch sz 160
Model ind 640 epoch 355 head B head_i_epoch 0 batch 0: avg loss -1.473810 avg loss no lamb -1.473810 time 2019-02-20 18:16:43.942876
Model ind 640 epoch 355 head B head_i_epoch 0 batch 100: avg loss -1.519863 avg loss no lamb -1.519863 time 2019-02-20 18:19:08.316702
Model ind 640 epoch 355 head B head_i_epoch 0 batch 200: avg loss -1.754158 avg loss no lamb -1.754158 time 2019-02-20 18:21:32.546801
last batch sz 160
Model ind 640 epoch 355 head B head_i_epoch 1 batch 0: avg loss -1.655823 avg loss no lamb -1.655823 time 2019-02-20 18:23:17.129437
Model ind 640 epoch 355 head B head_i_epoch 1 batch 100: avg loss -1.410831 avg loss no lamb -1.410831 time 2019-02-20 18:25:41.282568
Model ind 640 epoch 355 head B head_i_epoch 1 batch 200: avg loss -1.676050 avg loss no lamb -1.676050 time 2019-02-20 18:28:05.190882
last batch sz 160
Pre: time 2019-02-20 18:30:14.532827: 
 	std: 0.05547982
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49233332, 0.60338336, 0.60425, 0.60335, 0.48855]
	train_accs: [0.49233332, 0.60338336, 0.60425, 0.60335, 0.48855]
	best_train_sub_head: 2
	worst: 0.48855
	avg: 0.55837333
	best: 0.60425

Starting e_i: 356
Model ind 640 epoch 356 head A head_i_epoch 0 batch 0: avg loss -2.932129 avg loss no lamb -2.932129 time 2019-02-20 18:30:16.670081
Model ind 640 epoch 356 head A head_i_epoch 0 batch 100: avg loss -2.861114 avg loss no lamb -2.861114 time 2019-02-20 18:32:41.020790
Model ind 640 epoch 356 head A head_i_epoch 0 batch 200: avg loss -2.844007 avg loss no lamb -2.844007 time 2019-02-20 18:35:04.847863
last batch sz 160
Model ind 640 epoch 356 head B head_i_epoch 0 batch 0: avg loss -1.581072 avg loss no lamb -1.581072 time 2019-02-20 18:36:49.534280
Model ind 640 epoch 356 head B head_i_epoch 0 batch 100: avg loss -1.506649 avg loss no lamb -1.506649 time 2019-02-20 18:39:13.634003
Model ind 640 epoch 356 head B head_i_epoch 0 batch 200: avg loss -1.597177 avg loss no lamb -1.597177 time 2019-02-20 18:41:37.832038
last batch sz 160
Model ind 640 epoch 356 head B head_i_epoch 1 batch 0: avg loss -1.605237 avg loss no lamb -1.605237 time 2019-02-20 18:43:22.433736
Model ind 640 epoch 356 head B head_i_epoch 1 batch 100: avg loss -1.528532 avg loss no lamb -1.528532 time 2019-02-20 18:45:46.663541
Model ind 640 epoch 356 head B head_i_epoch 1 batch 200: avg loss -1.713973 avg loss no lamb -1.713973 time 2019-02-20 18:48:10.572678
last batch sz 160
Pre: time 2019-02-20 18:50:19.947440: 
 	std: 0.055298723
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49581668, 0.60791665, 0.60823333, 0.60805, 0.49456668]
	train_accs: [0.49581668, 0.60791665, 0.60823333, 0.60805, 0.49456668]
	best_train_sub_head: 2
	worst: 0.49456668
	avg: 0.56291664
	best: 0.60823333

Starting e_i: 357
Model ind 640 epoch 357 head A head_i_epoch 0 batch 0: avg loss -2.892620 avg loss no lamb -2.892620 time 2019-02-20 18:50:22.077273
Model ind 640 epoch 357 head A head_i_epoch 0 batch 100: avg loss -2.818034 avg loss no lamb -2.818034 time 2019-02-20 18:52:46.042017
Model ind 640 epoch 357 head A head_i_epoch 0 batch 200: avg loss -2.892060 avg loss no lamb -2.892060 time 2019-02-20 18:55:10.777724
last batch sz 160
Model ind 640 epoch 357 head B head_i_epoch 0 batch 0: avg loss -1.625379 avg loss no lamb -1.625379 time 2019-02-20 18:56:55.984073
Model ind 640 epoch 357 head B head_i_epoch 0 batch 100: avg loss -1.551435 avg loss no lamb -1.551435 time 2019-02-20 18:59:20.104937
Model ind 640 epoch 357 head B head_i_epoch 0 batch 200: avg loss -1.676088 avg loss no lamb -1.676088 time 2019-02-20 19:01:44.350491
last batch sz 160
Model ind 640 epoch 357 head B head_i_epoch 1 batch 0: avg loss -1.577191 avg loss no lamb -1.577191 time 2019-02-20 19:03:29.689329
Model ind 640 epoch 357 head B head_i_epoch 1 batch 100: avg loss -1.578488 avg loss no lamb -1.578488 time 2019-02-20 19:05:54.491380
Model ind 640 epoch 357 head B head_i_epoch 1 batch 200: avg loss -1.643608 avg loss no lamb -1.643608 time 2019-02-20 19:08:18.957798
last batch sz 160
Pre: time 2019-02-20 19:10:28.535638: 
 	std: 0.05418885
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4917, 0.6008, 0.6015, 0.6013833, 0.48955]
	train_accs: [0.4917, 0.6008, 0.6015, 0.6013833, 0.48955]
	best_train_sub_head: 2
	worst: 0.48955
	avg: 0.5569867
	best: 0.6015

Starting e_i: 358
Model ind 640 epoch 358 head A head_i_epoch 0 batch 0: avg loss -2.972142 avg loss no lamb -2.972142 time 2019-02-20 19:10:30.626722
Model ind 640 epoch 358 head A head_i_epoch 0 batch 100: avg loss -2.858433 avg loss no lamb -2.858433 time 2019-02-20 19:12:54.380847
Model ind 640 epoch 358 head A head_i_epoch 0 batch 200: avg loss -2.922350 avg loss no lamb -2.922350 time 2019-02-20 19:15:19.382725
last batch sz 160
Model ind 640 epoch 358 head B head_i_epoch 0 batch 0: avg loss -1.550341 avg loss no lamb -1.550341 time 2019-02-20 19:17:04.586907
Model ind 640 epoch 358 head B head_i_epoch 0 batch 100: avg loss -1.593365 avg loss no lamb -1.593365 time 2019-02-20 19:19:28.640712
Model ind 640 epoch 358 head B head_i_epoch 0 batch 200: avg loss -1.770007 avg loss no lamb -1.770007 time 2019-02-20 19:21:52.399121
last batch sz 160
Model ind 640 epoch 358 head B head_i_epoch 1 batch 0: avg loss -1.605083 avg loss no lamb -1.605083 time 2019-02-20 19:23:37.484157
Model ind 640 epoch 358 head B head_i_epoch 1 batch 100: avg loss -1.559976 avg loss no lamb -1.559976 time 2019-02-20 19:26:01.776943
Model ind 640 epoch 358 head B head_i_epoch 1 batch 200: avg loss -1.724320 avg loss no lamb -1.724320 time 2019-02-20 19:28:25.903151
last batch sz 160
Pre: time 2019-02-20 19:30:35.413573: 
 	std: 0.05370836
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49503332, 0.60323334, 0.60363334, 0.6039, 0.4929]
	train_accs: [0.49503332, 0.60323334, 0.60363334, 0.6039, 0.4929]
	best_train_sub_head: 3
	worst: 0.4929
	avg: 0.55973995
	best: 0.6039

Starting e_i: 359
Model ind 640 epoch 359 head A head_i_epoch 0 batch 0: avg loss -2.891441 avg loss no lamb -2.891441 time 2019-02-20 19:30:37.552985
Model ind 640 epoch 359 head A head_i_epoch 0 batch 100: avg loss -2.818608 avg loss no lamb -2.818608 time 2019-02-20 19:33:01.940578
Model ind 640 epoch 359 head A head_i_epoch 0 batch 200: avg loss -2.971061 avg loss no lamb -2.971061 time 2019-02-20 19:35:26.655133
last batch sz 160
Model ind 640 epoch 359 head B head_i_epoch 0 batch 0: avg loss -1.643689 avg loss no lamb -1.643689 time 2019-02-20 19:37:11.755772
Model ind 640 epoch 359 head B head_i_epoch 0 batch 100: avg loss -1.457862 avg loss no lamb -1.457862 time 2019-02-20 19:39:36.318362
Model ind 640 epoch 359 head B head_i_epoch 0 batch 200: avg loss -1.730457 avg loss no lamb -1.730457 time 2019-02-20 19:42:01.683334
last batch sz 160
Model ind 640 epoch 359 head B head_i_epoch 1 batch 0: avg loss -1.603733 avg loss no lamb -1.603733 time 2019-02-20 19:43:47.328123
Model ind 640 epoch 359 head B head_i_epoch 1 batch 100: avg loss -1.512267 avg loss no lamb -1.512267 time 2019-02-20 19:46:12.986545
Model ind 640 epoch 359 head B head_i_epoch 1 batch 200: avg loss -1.620533 avg loss no lamb -1.620533 time 2019-02-20 19:48:37.672261
last batch sz 160
Pre: time 2019-02-20 19:50:47.814743: 
 	std: 0.054005124
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49498335, 0.604, 0.60438335, 0.6044833, 0.49313334]
	train_accs: [0.49498335, 0.604, 0.60438335, 0.6044833, 0.49313334]
	best_train_sub_head: 3
	worst: 0.49313334
	avg: 0.56019664
	best: 0.6044833

Starting e_i: 360
Model ind 640 epoch 360 head A head_i_epoch 0 batch 0: avg loss -3.011881 avg loss no lamb -3.011881 time 2019-02-20 19:50:49.993127
Model ind 640 epoch 360 head A head_i_epoch 0 batch 100: avg loss -2.876738 avg loss no lamb -2.876738 time 2019-02-20 19:53:13.924809
Model ind 640 epoch 360 head A head_i_epoch 0 batch 200: avg loss -2.926282 avg loss no lamb -2.926282 time 2019-02-20 19:55:38.234334
last batch sz 160
Model ind 640 epoch 360 head B head_i_epoch 0 batch 0: avg loss -1.656426 avg loss no lamb -1.656426 time 2019-02-20 19:57:22.891741
Model ind 640 epoch 360 head B head_i_epoch 0 batch 100: avg loss -1.554829 avg loss no lamb -1.554829 time 2019-02-20 19:59:46.339944
Model ind 640 epoch 360 head B head_i_epoch 0 batch 200: avg loss -1.688444 avg loss no lamb -1.688444 time 2019-02-20 20:02:10.348412
last batch sz 160
Model ind 640 epoch 360 head B head_i_epoch 1 batch 0: avg loss -1.606491 avg loss no lamb -1.606491 time 2019-02-20 20:03:55.152723
Model ind 640 epoch 360 head B head_i_epoch 1 batch 100: avg loss -1.566763 avg loss no lamb -1.566763 time 2019-02-20 20:06:19.940698
Model ind 640 epoch 360 head B head_i_epoch 1 batch 200: avg loss -1.707141 avg loss no lamb -1.707141 time 2019-02-20 20:08:44.681352
last batch sz 160
Pre: time 2019-02-20 20:10:54.684714: 
 	std: 0.054453637
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49733335, 0.6086, 0.60875, 0.60858333, 0.49765]
	train_accs: [0.49733335, 0.6086, 0.60875, 0.60858333, 0.49765]
	best_train_sub_head: 2
	worst: 0.49733335
	avg: 0.56418335
	best: 0.60875

Starting e_i: 361
Model ind 640 epoch 361 head A head_i_epoch 0 batch 0: avg loss -2.963285 avg loss no lamb -2.963285 time 2019-02-20 20:11:00.222269
Model ind 640 epoch 361 head A head_i_epoch 0 batch 100: avg loss -2.873870 avg loss no lamb -2.873870 time 2019-02-20 20:13:24.609932
Model ind 640 epoch 361 head A head_i_epoch 0 batch 200: avg loss -2.945600 avg loss no lamb -2.945600 time 2019-02-20 20:15:49.897180
last batch sz 160
Model ind 640 epoch 361 head B head_i_epoch 0 batch 0: avg loss -1.613207 avg loss no lamb -1.613207 time 2019-02-20 20:17:35.446965
Model ind 640 epoch 361 head B head_i_epoch 0 batch 100: avg loss -1.503143 avg loss no lamb -1.503143 time 2019-02-20 20:20:01.125512
Model ind 640 epoch 361 head B head_i_epoch 0 batch 200: avg loss -1.693098 avg loss no lamb -1.693098 time 2019-02-20 20:22:26.915040
last batch sz 160
Model ind 640 epoch 361 head B head_i_epoch 1 batch 0: avg loss -1.631772 avg loss no lamb -1.631772 time 2019-02-20 20:24:12.032220
Model ind 640 epoch 361 head B head_i_epoch 1 batch 100: avg loss -1.573429 avg loss no lamb -1.573429 time 2019-02-20 20:26:35.563890
Model ind 640 epoch 361 head B head_i_epoch 1 batch 200: avg loss -1.705018 avg loss no lamb -1.705018 time 2019-02-20 20:28:59.837604
last batch sz 160
Pre: time 2019-02-20 20:31:09.736989: 
 	std: 0.053720415
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49835, 0.60711664, 0.6073667, 0.6076, 0.49706668]
	train_accs: [0.49835, 0.60711664, 0.6073667, 0.6076, 0.49706668]
	best_train_sub_head: 3
	worst: 0.49706668
	avg: 0.5635
	best: 0.6076

Starting e_i: 362
Model ind 640 epoch 362 head A head_i_epoch 0 batch 0: avg loss -2.956637 avg loss no lamb -2.956637 time 2019-02-20 20:31:12.303926
Model ind 640 epoch 362 head A head_i_epoch 0 batch 100: avg loss -2.861721 avg loss no lamb -2.861721 time 2019-02-20 20:33:36.637116
Model ind 640 epoch 362 head A head_i_epoch 0 batch 200: avg loss -2.963378 avg loss no lamb -2.963378 time 2019-02-20 20:36:01.208838
last batch sz 160
Model ind 640 epoch 362 head B head_i_epoch 0 batch 0: avg loss -1.608752 avg loss no lamb -1.608752 time 2019-02-20 20:37:46.203304
Model ind 640 epoch 362 head B head_i_epoch 0 batch 100: avg loss -1.558593 avg loss no lamb -1.558593 time 2019-02-20 20:40:10.956445
Model ind 640 epoch 362 head B head_i_epoch 0 batch 200: avg loss -1.717686 avg loss no lamb -1.717686 time 2019-02-20 20:42:35.095728
last batch sz 160
Model ind 640 epoch 362 head B head_i_epoch 1 batch 0: avg loss -1.646746 avg loss no lamb -1.646746 time 2019-02-20 20:44:20.023122
Model ind 640 epoch 362 head B head_i_epoch 1 batch 100: avg loss -1.556161 avg loss no lamb -1.556161 time 2019-02-20 20:46:45.262225
Model ind 640 epoch 362 head B head_i_epoch 1 batch 200: avg loss -1.729467 avg loss no lamb -1.729467 time 2019-02-20 20:49:10.207434
last batch sz 160
Pre: time 2019-02-20 20:51:19.956080: 
 	std: 0.0549062
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49065, 0.60256666, 0.60275, 0.6034333, 0.49103335]
	train_accs: [0.49065, 0.60256666, 0.60275, 0.6034333, 0.49103335]
	best_train_sub_head: 3
	worst: 0.49065
	avg: 0.5580867
	best: 0.6034333

Starting e_i: 363
Model ind 640 epoch 363 head A head_i_epoch 0 batch 0: avg loss -2.970655 avg loss no lamb -2.970655 time 2019-02-20 20:51:22.153024
Model ind 640 epoch 363 head A head_i_epoch 0 batch 100: avg loss -2.812599 avg loss no lamb -2.812599 time 2019-02-20 20:53:46.224644
Model ind 640 epoch 363 head A head_i_epoch 0 batch 200: avg loss -2.928478 avg loss no lamb -2.928478 time 2019-02-20 20:56:10.866248
last batch sz 160
Model ind 640 epoch 363 head B head_i_epoch 0 batch 0: avg loss -1.649315 avg loss no lamb -1.649315 time 2019-02-20 20:57:55.636955
Model ind 640 epoch 363 head B head_i_epoch 0 batch 100: avg loss -1.535337 avg loss no lamb -1.535337 time 2019-02-20 21:00:20.458420
Model ind 640 epoch 363 head B head_i_epoch 0 batch 200: avg loss -1.677728 avg loss no lamb -1.677728 time 2019-02-20 21:02:45.905017
last batch sz 160
Model ind 640 epoch 363 head B head_i_epoch 1 batch 0: avg loss -1.617321 avg loss no lamb -1.617321 time 2019-02-20 21:04:30.765408
Model ind 640 epoch 363 head B head_i_epoch 1 batch 100: avg loss -1.584146 avg loss no lamb -1.584146 time 2019-02-20 21:06:55.782649
Model ind 640 epoch 363 head B head_i_epoch 1 batch 200: avg loss -1.804211 avg loss no lamb -1.804211 time 2019-02-20 21:09:20.274943
last batch sz 160
Pre: time 2019-02-20 21:11:30.202177: 
 	std: 0.053618733
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49296665, 0.60298336, 0.60291666, 0.6034333, 0.49436668]
	train_accs: [0.49296665, 0.60298336, 0.60291666, 0.6034333, 0.49436668]
	best_train_sub_head: 3
	worst: 0.49296665
	avg: 0.5593333
	best: 0.6034333

Starting e_i: 364
Model ind 640 epoch 364 head A head_i_epoch 0 batch 0: avg loss -2.914347 avg loss no lamb -2.914347 time 2019-02-20 21:11:32.404645
Model ind 640 epoch 364 head A head_i_epoch 0 batch 100: avg loss -2.824755 avg loss no lamb -2.824755 time 2019-02-20 21:13:56.809830
Model ind 640 epoch 364 head A head_i_epoch 0 batch 200: avg loss -2.968177 avg loss no lamb -2.968177 time 2019-02-20 21:16:21.801958
last batch sz 160
Model ind 640 epoch 364 head B head_i_epoch 0 batch 0: avg loss -1.667518 avg loss no lamb -1.667518 time 2019-02-20 21:18:06.742045
Model ind 640 epoch 364 head B head_i_epoch 0 batch 100: avg loss -1.557697 avg loss no lamb -1.557697 time 2019-02-20 21:20:32.277100
Model ind 640 epoch 364 head B head_i_epoch 0 batch 200: avg loss -1.689537 avg loss no lamb -1.689537 time 2019-02-20 21:22:57.503863
last batch sz 160
Model ind 640 epoch 364 head B head_i_epoch 1 batch 0: avg loss -1.553054 avg loss no lamb -1.553054 time 2019-02-20 21:24:43.811534
Model ind 640 epoch 364 head B head_i_epoch 1 batch 100: avg loss -1.510472 avg loss no lamb -1.510472 time 2019-02-20 21:27:09.094299
Model ind 640 epoch 364 head B head_i_epoch 1 batch 200: avg loss -1.670135 avg loss no lamb -1.670135 time 2019-02-20 21:29:33.695749
last batch sz 160
Pre: time 2019-02-20 21:31:43.178242: 
 	std: 0.05380462
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49238333, 0.6033, 0.60321665, 0.60331666, 0.49453333]
	train_accs: [0.49238333, 0.6033, 0.60321665, 0.60331666, 0.49453333]
	best_train_sub_head: 3
	worst: 0.49238333
	avg: 0.55934995
	best: 0.60331666

Starting e_i: 365
Model ind 640 epoch 365 head A head_i_epoch 0 batch 0: avg loss -3.032271 avg loss no lamb -3.032271 time 2019-02-20 21:31:45.353125
Model ind 640 epoch 365 head A head_i_epoch 0 batch 100: avg loss -2.864062 avg loss no lamb -2.864062 time 2019-02-20 21:34:12.025723
Model ind 640 epoch 365 head A head_i_epoch 0 batch 200: avg loss -3.021483 avg loss no lamb -3.021483 time 2019-02-20 21:36:36.758317
last batch sz 160
Model ind 640 epoch 365 head B head_i_epoch 0 batch 0: avg loss -1.625001 avg loss no lamb -1.625001 time 2019-02-20 21:38:21.689202
Model ind 640 epoch 365 head B head_i_epoch 0 batch 100: avg loss -1.604784 avg loss no lamb -1.604784 time 2019-02-20 21:40:46.025924
Model ind 640 epoch 365 head B head_i_epoch 0 batch 200: avg loss -1.776432 avg loss no lamb -1.776432 time 2019-02-20 21:43:09.617142
last batch sz 160
Model ind 640 epoch 365 head B head_i_epoch 1 batch 0: avg loss -1.647145 avg loss no lamb -1.647145 time 2019-02-20 21:44:53.691826
Model ind 640 epoch 365 head B head_i_epoch 1 batch 100: avg loss -1.464062 avg loss no lamb -1.464062 time 2019-02-20 21:47:18.441834
Model ind 640 epoch 365 head B head_i_epoch 1 batch 200: avg loss -1.626849 avg loss no lamb -1.626849 time 2019-02-20 21:49:43.101271
last batch sz 160
Pre: time 2019-02-20 21:51:52.317133: 
 	std: 0.05487906
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.48611668, 0.59968334, 0.60048336, 0.60003334, 0.49003333]
	train_accs: [0.48611668, 0.59968334, 0.60048336, 0.60003334, 0.49003333]
	best_train_sub_head: 2
	worst: 0.48611668
	avg: 0.55527
	best: 0.60048336

Starting e_i: 366
Model ind 640 epoch 366 head A head_i_epoch 0 batch 0: avg loss -2.997340 avg loss no lamb -2.997340 time 2019-02-20 21:51:54.466250
Model ind 640 epoch 366 head A head_i_epoch 0 batch 100: avg loss -2.829152 avg loss no lamb -2.829152 time 2019-02-20 21:54:18.873169
Model ind 640 epoch 366 head A head_i_epoch 0 batch 200: avg loss -2.937895 avg loss no lamb -2.937895 time 2019-02-20 21:56:43.929736
last batch sz 160
Model ind 640 epoch 366 head B head_i_epoch 0 batch 0: avg loss -1.637513 avg loss no lamb -1.637513 time 2019-02-20 21:58:29.631808
Model ind 640 epoch 366 head B head_i_epoch 0 batch 100: avg loss -1.481724 avg loss no lamb -1.481724 time 2019-02-20 22:00:53.814187
Model ind 640 epoch 366 head B head_i_epoch 0 batch 200: avg loss -1.809688 avg loss no lamb -1.809688 time 2019-02-20 22:03:18.144002
last batch sz 160
Model ind 640 epoch 366 head B head_i_epoch 1 batch 0: avg loss -1.660729 avg loss no lamb -1.660729 time 2019-02-20 22:05:03.224768
Model ind 640 epoch 366 head B head_i_epoch 1 batch 100: avg loss -1.494097 avg loss no lamb -1.494097 time 2019-02-20 22:07:27.599531
Model ind 640 epoch 366 head B head_i_epoch 1 batch 200: avg loss -1.716291 avg loss no lamb -1.716291 time 2019-02-20 22:09:52.660153
last batch sz 160
Pre: time 2019-02-20 22:12:03.013071: 
 	std: 0.054980077
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48823333, 0.60108334, 0.60136664, 0.60141665, 0.4899]
	train_accs: [0.48823333, 0.60108334, 0.60136664, 0.60141665, 0.4899]
	best_train_sub_head: 3
	worst: 0.48823333
	avg: 0.5564
	best: 0.60141665

Starting e_i: 367
Model ind 640 epoch 367 head A head_i_epoch 0 batch 0: avg loss -2.921864 avg loss no lamb -2.921864 time 2019-02-20 22:12:05.177472
Model ind 640 epoch 367 head A head_i_epoch 0 batch 100: avg loss -2.793627 avg loss no lamb -2.793627 time 2019-02-20 22:14:29.617523
Model ind 640 epoch 367 head A head_i_epoch 0 batch 200: avg loss -2.990692 avg loss no lamb -2.990692 time 2019-02-20 22:16:54.912026
last batch sz 160
Model ind 640 epoch 367 head B head_i_epoch 0 batch 0: avg loss -1.656224 avg loss no lamb -1.656224 time 2019-02-20 22:18:39.762084
Model ind 640 epoch 367 head B head_i_epoch 0 batch 100: avg loss -1.549713 avg loss no lamb -1.549713 time 2019-02-20 22:21:03.965525
Model ind 640 epoch 367 head B head_i_epoch 0 batch 200: avg loss -1.711477 avg loss no lamb -1.711477 time 2019-02-20 22:23:28.362450
last batch sz 160
Model ind 640 epoch 367 head B head_i_epoch 1 batch 0: avg loss -1.613775 avg loss no lamb -1.613775 time 2019-02-20 22:25:13.417567
Model ind 640 epoch 367 head B head_i_epoch 1 batch 100: avg loss -1.490357 avg loss no lamb -1.490357 time 2019-02-20 22:27:37.530006
Model ind 640 epoch 367 head B head_i_epoch 1 batch 200: avg loss -1.774285 avg loss no lamb -1.774285 time 2019-02-20 22:30:01.489489
last batch sz 160
Pre: time 2019-02-20 22:32:11.461321: 
 	std: 0.054128923
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49268332, 0.60215, 0.6034833, 0.60315, 0.4922]
	train_accs: [0.49268332, 0.60215, 0.6034833, 0.60315, 0.4922]
	best_train_sub_head: 2
	worst: 0.4922
	avg: 0.55873334
	best: 0.6034833

Starting e_i: 368
Model ind 640 epoch 368 head A head_i_epoch 0 batch 0: avg loss -2.892376 avg loss no lamb -2.892376 time 2019-02-20 22:32:13.607834
Model ind 640 epoch 368 head A head_i_epoch 0 batch 100: avg loss -2.826679 avg loss no lamb -2.826679 time 2019-02-20 22:34:38.741673
Model ind 640 epoch 368 head A head_i_epoch 0 batch 200: avg loss -2.937420 avg loss no lamb -2.937420 time 2019-02-20 22:37:03.610153
last batch sz 160
Model ind 640 epoch 368 head B head_i_epoch 0 batch 0: avg loss -1.680840 avg loss no lamb -1.680840 time 2019-02-20 22:38:48.750871
Model ind 640 epoch 368 head B head_i_epoch 0 batch 100: avg loss -1.430893 avg loss no lamb -1.430893 time 2019-02-20 22:41:13.511833
Model ind 640 epoch 368 head B head_i_epoch 0 batch 200: avg loss -1.708635 avg loss no lamb -1.708635 time 2019-02-20 22:43:38.022261
last batch sz 160
Model ind 640 epoch 368 head B head_i_epoch 1 batch 0: avg loss -1.616367 avg loss no lamb -1.616367 time 2019-02-20 22:45:23.112788
Model ind 640 epoch 368 head B head_i_epoch 1 batch 100: avg loss -1.532472 avg loss no lamb -1.532472 time 2019-02-20 22:47:47.665277
Model ind 640 epoch 368 head B head_i_epoch 1 batch 200: avg loss -1.805519 avg loss no lamb -1.805519 time 2019-02-20 22:50:11.681039
last batch sz 160
Pre: time 2019-02-20 22:52:20.663855: 
 	std: 0.051920105
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49628332, 0.60188335, 0.6025, 0.6024333, 0.4963]
	train_accs: [0.49628332, 0.60188335, 0.6025, 0.6024333, 0.4963]
	best_train_sub_head: 2
	worst: 0.49628332
	avg: 0.55988
	best: 0.6025

Starting e_i: 369
Model ind 640 epoch 369 head A head_i_epoch 0 batch 0: avg loss -2.923293 avg loss no lamb -2.923293 time 2019-02-20 22:52:22.748487
Model ind 640 epoch 369 head A head_i_epoch 0 batch 100: avg loss -2.882856 avg loss no lamb -2.882856 time 2019-02-20 22:54:47.413907
Model ind 640 epoch 369 head A head_i_epoch 0 batch 200: avg loss -2.939061 avg loss no lamb -2.939061 time 2019-02-20 22:57:12.067291
last batch sz 160
Model ind 640 epoch 369 head B head_i_epoch 0 batch 0: avg loss -1.686296 avg loss no lamb -1.686296 time 2019-02-20 22:58:57.427314
Model ind 640 epoch 369 head B head_i_epoch 0 batch 100: avg loss -1.557949 avg loss no lamb -1.557949 time 2019-02-20 23:01:21.493364
Model ind 640 epoch 369 head B head_i_epoch 0 batch 200: avg loss -1.700284 avg loss no lamb -1.700284 time 2019-02-20 23:03:45.509617
last batch sz 160
Model ind 640 epoch 369 head B head_i_epoch 1 batch 0: avg loss -1.649876 avg loss no lamb -1.649876 time 2019-02-20 23:05:30.057618
Model ind 640 epoch 369 head B head_i_epoch 1 batch 100: avg loss -1.622097 avg loss no lamb -1.622097 time 2019-02-20 23:07:54.748372
Model ind 640 epoch 369 head B head_i_epoch 1 batch 200: avg loss -1.684608 avg loss no lamb -1.684608 time 2019-02-20 23:10:18.829614
last batch sz 160
Pre: time 2019-02-20 23:12:28.269441: 
 	std: 0.05351564
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49208334, 0.60073334, 0.60155, 0.60175, 0.49213332]
	train_accs: [0.49208334, 0.60073334, 0.60155, 0.60175, 0.49213332]
	best_train_sub_head: 3
	worst: 0.49208334
	avg: 0.55765
	best: 0.60175

Starting e_i: 370
Model ind 640 epoch 370 head A head_i_epoch 0 batch 0: avg loss -2.925504 avg loss no lamb -2.925504 time 2019-02-20 23:12:30.410891
Model ind 640 epoch 370 head A head_i_epoch 0 batch 100: avg loss -2.860698 avg loss no lamb -2.860698 time 2019-02-20 23:14:54.511498
Model ind 640 epoch 370 head A head_i_epoch 0 batch 200: avg loss -2.989960 avg loss no lamb -2.989960 time 2019-02-20 23:17:18.706775
last batch sz 160
Model ind 640 epoch 370 head B head_i_epoch 0 batch 0: avg loss -1.556555 avg loss no lamb -1.556555 time 2019-02-20 23:19:03.684517
Model ind 640 epoch 370 head B head_i_epoch 0 batch 100: avg loss -1.496341 avg loss no lamb -1.496341 time 2019-02-20 23:21:28.501155
Model ind 640 epoch 370 head B head_i_epoch 0 batch 200: avg loss -1.704074 avg loss no lamb -1.704074 time 2019-02-20 23:23:53.798143
last batch sz 160
Model ind 640 epoch 370 head B head_i_epoch 1 batch 0: avg loss -1.674232 avg loss no lamb -1.674232 time 2019-02-20 23:25:39.523484
Model ind 640 epoch 370 head B head_i_epoch 1 batch 100: avg loss -1.503863 avg loss no lamb -1.503863 time 2019-02-20 23:28:04.019354
Model ind 640 epoch 370 head B head_i_epoch 1 batch 200: avg loss -1.656454 avg loss no lamb -1.656454 time 2019-02-20 23:30:28.332749
last batch sz 160
Pre: time 2019-02-20 23:32:38.114048: 
 	std: 0.054155122
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.48793334, 0.59815, 0.59868336, 0.5991167, 0.48828334]
	train_accs: [0.48793334, 0.59815, 0.59868336, 0.5991167, 0.48828334]
	best_train_sub_head: 3
	worst: 0.48793334
	avg: 0.5544334
	best: 0.5991167

Starting e_i: 371
Model ind 640 epoch 371 head A head_i_epoch 0 batch 0: avg loss -2.970527 avg loss no lamb -2.970527 time 2019-02-20 23:32:44.582082
Model ind 640 epoch 371 head A head_i_epoch 0 batch 100: avg loss -2.834063 avg loss no lamb -2.834063 time 2019-02-20 23:35:08.864234
Model ind 640 epoch 371 head A head_i_epoch 0 batch 200: avg loss -2.982899 avg loss no lamb -2.982899 time 2019-02-20 23:37:34.126940
last batch sz 160
Model ind 640 epoch 371 head B head_i_epoch 0 batch 0: avg loss -1.589370 avg loss no lamb -1.589370 time 2019-02-20 23:39:20.443221
Model ind 640 epoch 371 head B head_i_epoch 0 batch 100: avg loss -1.666847 avg loss no lamb -1.666847 time 2019-02-20 23:41:44.219504
Model ind 640 epoch 371 head B head_i_epoch 0 batch 200: avg loss -1.668512 avg loss no lamb -1.668512 time 2019-02-20 23:44:08.285807
last batch sz 160
Model ind 640 epoch 371 head B head_i_epoch 1 batch 0: avg loss -1.630173 avg loss no lamb -1.630173 time 2019-02-20 23:45:53.036092
Model ind 640 epoch 371 head B head_i_epoch 1 batch 100: avg loss -1.522691 avg loss no lamb -1.522691 time 2019-02-20 23:48:17.154754
Model ind 640 epoch 371 head B head_i_epoch 1 batch 200: avg loss -1.733314 avg loss no lamb -1.733314 time 2019-02-20 23:50:41.115150
last batch sz 160
Pre: time 2019-02-20 23:52:50.789157: 
 	std: 0.05299625
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49528334, 0.60265, 0.60431665, 0.60305, 0.49505]
	train_accs: [0.49528334, 0.60265, 0.60431665, 0.60305, 0.49505]
	best_train_sub_head: 2
	worst: 0.49505
	avg: 0.56007
	best: 0.60431665

Starting e_i: 372
Model ind 640 epoch 372 head A head_i_epoch 0 batch 0: avg loss -2.964737 avg loss no lamb -2.964737 time 2019-02-20 23:52:52.941992
Model ind 640 epoch 372 head A head_i_epoch 0 batch 100: avg loss -2.875940 avg loss no lamb -2.875940 time 2019-02-20 23:55:17.853806
Model ind 640 epoch 372 head A head_i_epoch 0 batch 200: avg loss -3.015445 avg loss no lamb -3.015445 time 2019-02-20 23:57:42.749862
last batch sz 160
Model ind 640 epoch 372 head B head_i_epoch 0 batch 0: avg loss -1.606749 avg loss no lamb -1.606749 time 2019-02-20 23:59:28.038073
Model ind 640 epoch 372 head B head_i_epoch 0 batch 100: avg loss -1.463667 avg loss no lamb -1.463667 time 2019-02-21 00:01:52.407416
Model ind 640 epoch 372 head B head_i_epoch 0 batch 200: avg loss -1.724080 avg loss no lamb -1.724080 time 2019-02-21 00:04:16.644990
last batch sz 160
Model ind 640 epoch 372 head B head_i_epoch 1 batch 0: avg loss -1.523485 avg loss no lamb -1.523485 time 2019-02-21 00:06:01.834635
Model ind 640 epoch 372 head B head_i_epoch 1 batch 100: avg loss -1.467874 avg loss no lamb -1.467874 time 2019-02-21 00:08:25.919372
Model ind 640 epoch 372 head B head_i_epoch 1 batch 200: avg loss -1.737822 avg loss no lamb -1.737822 time 2019-02-21 00:10:49.682884
last batch sz 160
Pre: time 2019-02-21 00:12:59.052838: 
 	std: 0.052683566
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49528334, 0.60265, 0.6032, 0.60316664, 0.49565]
	train_accs: [0.49528334, 0.60265, 0.6032, 0.60316664, 0.49565]
	best_train_sub_head: 2
	worst: 0.49528334
	avg: 0.55999
	best: 0.6032

Starting e_i: 373
Model ind 640 epoch 373 head A head_i_epoch 0 batch 0: avg loss -2.991490 avg loss no lamb -2.991490 time 2019-02-21 00:13:01.219315
Model ind 640 epoch 373 head A head_i_epoch 0 batch 100: avg loss -2.917191 avg loss no lamb -2.917191 time 2019-02-21 00:15:25.436265
Model ind 640 epoch 373 head A head_i_epoch 0 batch 200: avg loss -3.006255 avg loss no lamb -3.006255 time 2019-02-21 00:17:49.856319
last batch sz 160
Model ind 640 epoch 373 head B head_i_epoch 0 batch 0: avg loss -1.631957 avg loss no lamb -1.631957 time 2019-02-21 00:19:35.679692
Model ind 640 epoch 373 head B head_i_epoch 0 batch 100: avg loss -1.518630 avg loss no lamb -1.518630 time 2019-02-21 00:22:00.211745
Model ind 640 epoch 373 head B head_i_epoch 0 batch 200: avg loss -1.701797 avg loss no lamb -1.701797 time 2019-02-21 00:24:24.585068
last batch sz 160
Model ind 640 epoch 373 head B head_i_epoch 1 batch 0: avg loss -1.704509 avg loss no lamb -1.704509 time 2019-02-21 00:26:09.476126
Model ind 640 epoch 373 head B head_i_epoch 1 batch 100: avg loss -1.599442 avg loss no lamb -1.599442 time 2019-02-21 00:28:33.827857
Model ind 640 epoch 373 head B head_i_epoch 1 batch 200: avg loss -1.779088 avg loss no lamb -1.779088 time 2019-02-21 00:30:57.609731
last batch sz 160
Pre: time 2019-02-21 00:33:07.507694: 
 	std: 0.052405648
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49646667, 0.60373336, 0.60435, 0.6045167, 0.498]
	train_accs: [0.49646667, 0.60373336, 0.60435, 0.6045167, 0.498]
	best_train_sub_head: 3
	worst: 0.49646667
	avg: 0.56141335
	best: 0.6045167

Starting e_i: 374
Model ind 640 epoch 374 head A head_i_epoch 0 batch 0: avg loss -2.952443 avg loss no lamb -2.952443 time 2019-02-21 00:33:09.671589
Model ind 640 epoch 374 head A head_i_epoch 0 batch 100: avg loss -2.864529 avg loss no lamb -2.864529 time 2019-02-21 00:35:33.706031
Model ind 640 epoch 374 head A head_i_epoch 0 batch 200: avg loss -3.010035 avg loss no lamb -3.010035 time 2019-02-21 00:37:58.521647
last batch sz 160
Model ind 640 epoch 374 head B head_i_epoch 0 batch 0: avg loss -1.599338 avg loss no lamb -1.599338 time 2019-02-21 00:39:43.130761
Model ind 640 epoch 374 head B head_i_epoch 0 batch 100: avg loss -1.517963 avg loss no lamb -1.517963 time 2019-02-21 00:42:06.413420
Model ind 640 epoch 374 head B head_i_epoch 0 batch 200: avg loss -1.757389 avg loss no lamb -1.757389 time 2019-02-21 00:44:30.241607
last batch sz 160
Model ind 640 epoch 374 head B head_i_epoch 1 batch 0: avg loss -1.633028 avg loss no lamb -1.633028 time 2019-02-21 00:46:14.866985
Model ind 640 epoch 374 head B head_i_epoch 1 batch 100: avg loss -1.597045 avg loss no lamb -1.597045 time 2019-02-21 00:48:39.351369
Model ind 640 epoch 374 head B head_i_epoch 1 batch 200: avg loss -1.715097 avg loss no lamb -1.715097 time 2019-02-21 00:51:03.585342
last batch sz 160
Pre: time 2019-02-21 00:53:12.130207: 
 	std: 0.0527043
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49646667, 0.60406667, 0.60441667, 0.60468334, 0.49715]
	train_accs: [0.49646667, 0.60406667, 0.60441667, 0.60468334, 0.49715]
	best_train_sub_head: 3
	worst: 0.49646667
	avg: 0.56135666
	best: 0.60468334

Starting e_i: 375
Model ind 640 epoch 375 head A head_i_epoch 0 batch 0: avg loss -3.037510 avg loss no lamb -3.037510 time 2019-02-21 00:53:14.285094
Model ind 640 epoch 375 head A head_i_epoch 0 batch 100: avg loss -2.884634 avg loss no lamb -2.884634 time 2019-02-21 00:55:38.882869
Model ind 640 epoch 375 head A head_i_epoch 0 batch 200: avg loss -2.946031 avg loss no lamb -2.946031 time 2019-02-21 00:58:03.464609
last batch sz 160
Model ind 640 epoch 375 head B head_i_epoch 0 batch 0: avg loss -1.689908 avg loss no lamb -1.689908 time 2019-02-21 00:59:48.543737
Model ind 640 epoch 375 head B head_i_epoch 0 batch 100: avg loss -1.530147 avg loss no lamb -1.530147 time 2019-02-21 01:02:12.908045
Model ind 640 epoch 375 head B head_i_epoch 0 batch 200: avg loss -1.709910 avg loss no lamb -1.709910 time 2019-02-21 01:04:37.088732
last batch sz 160
Model ind 640 epoch 375 head B head_i_epoch 1 batch 0: avg loss -1.589898 avg loss no lamb -1.589898 time 2019-02-21 01:06:21.995844
Model ind 640 epoch 375 head B head_i_epoch 1 batch 100: avg loss -1.529117 avg loss no lamb -1.529117 time 2019-02-21 01:08:46.157508
Model ind 640 epoch 375 head B head_i_epoch 1 batch 200: avg loss -1.711859 avg loss no lamb -1.711859 time 2019-02-21 01:11:09.829760
last batch sz 160
Pre: time 2019-02-21 01:13:19.973684: 
 	std: 0.053395003
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49741668, 0.60543334, 0.60618335, 0.6064, 0.49661666]
	train_accs: [0.49741668, 0.60543334, 0.60618335, 0.6064, 0.49661666]
	best_train_sub_head: 3
	worst: 0.49661666
	avg: 0.56241
	best: 0.6064

Starting e_i: 376
Model ind 640 epoch 376 head A head_i_epoch 0 batch 0: avg loss -3.007851 avg loss no lamb -3.007851 time 2019-02-21 01:13:22.159324
Model ind 640 epoch 376 head A head_i_epoch 0 batch 100: avg loss -2.899940 avg loss no lamb -2.899940 time 2019-02-21 01:15:46.777859
Model ind 640 epoch 376 head A head_i_epoch 0 batch 200: avg loss -3.018673 avg loss no lamb -3.018673 time 2019-02-21 01:18:11.493474
last batch sz 160
Model ind 640 epoch 376 head B head_i_epoch 0 batch 0: avg loss -1.608458 avg loss no lamb -1.608458 time 2019-02-21 01:19:56.464118
Model ind 640 epoch 376 head B head_i_epoch 0 batch 100: avg loss -1.534808 avg loss no lamb -1.534808 time 2019-02-21 01:22:20.737020
Model ind 640 epoch 376 head B head_i_epoch 0 batch 200: avg loss -1.746667 avg loss no lamb -1.746667 time 2019-02-21 01:24:45.236278
last batch sz 160
Model ind 640 epoch 376 head B head_i_epoch 1 batch 0: avg loss -1.500217 avg loss no lamb -1.500217 time 2019-02-21 01:26:30.680066
Model ind 640 epoch 376 head B head_i_epoch 1 batch 100: avg loss -1.516768 avg loss no lamb -1.516768 time 2019-02-21 01:28:54.769089
Model ind 640 epoch 376 head B head_i_epoch 1 batch 200: avg loss -1.702167 avg loss no lamb -1.702167 time 2019-02-21 01:31:18.771511
last batch sz 160
Pre: time 2019-02-21 01:33:28.603663: 
 	std: 0.052798547
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49731666, 0.6056167, 0.60548335, 0.60576665, 0.49838334]
	train_accs: [0.49731666, 0.6056167, 0.60548335, 0.60576665, 0.49838334]
	best_train_sub_head: 3
	worst: 0.49731666
	avg: 0.5625133
	best: 0.60576665

Starting e_i: 377
Model ind 640 epoch 377 head A head_i_epoch 0 batch 0: avg loss -2.982185 avg loss no lamb -2.982185 time 2019-02-21 01:33:30.763260
Model ind 640 epoch 377 head A head_i_epoch 0 batch 100: avg loss -2.803553 avg loss no lamb -2.803553 time 2019-02-21 01:35:55.197783
Model ind 640 epoch 377 head A head_i_epoch 0 batch 200: avg loss -2.963379 avg loss no lamb -2.963379 time 2019-02-21 01:38:19.809002
last batch sz 160
Model ind 640 epoch 377 head B head_i_epoch 0 batch 0: avg loss -1.707786 avg loss no lamb -1.707786 time 2019-02-21 01:40:05.025828
Model ind 640 epoch 377 head B head_i_epoch 0 batch 100: avg loss -1.531919 avg loss no lamb -1.531919 time 2019-02-21 01:42:29.111516
Model ind 640 epoch 377 head B head_i_epoch 0 batch 200: avg loss -1.691359 avg loss no lamb -1.691359 time 2019-02-21 01:44:53.717956
last batch sz 160
Model ind 640 epoch 377 head B head_i_epoch 1 batch 0: avg loss -1.562956 avg loss no lamb -1.562956 time 2019-02-21 01:46:38.801640
Model ind 640 epoch 377 head B head_i_epoch 1 batch 100: avg loss -1.551489 avg loss no lamb -1.551489 time 2019-02-21 01:49:02.868971
Model ind 640 epoch 377 head B head_i_epoch 1 batch 200: avg loss -1.696195 avg loss no lamb -1.696195 time 2019-02-21 01:51:26.804696
last batch sz 160
Pre: time 2019-02-21 01:53:36.643426: 
 	std: 0.0532688
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49588335, 0.60578334, 0.6056167, 0.60575, 0.4981]
	train_accs: [0.49588335, 0.60578334, 0.6056167, 0.60575, 0.4981]
	best_train_sub_head: 1
	worst: 0.49588335
	avg: 0.56222665
	best: 0.60578334

Starting e_i: 378
Model ind 640 epoch 378 head A head_i_epoch 0 batch 0: avg loss -3.012062 avg loss no lamb -3.012062 time 2019-02-21 01:53:38.811413
Model ind 640 epoch 378 head A head_i_epoch 0 batch 100: avg loss -2.854461 avg loss no lamb -2.854461 time 2019-02-21 01:56:03.417051
Model ind 640 epoch 378 head A head_i_epoch 0 batch 200: avg loss -2.961947 avg loss no lamb -2.961947 time 2019-02-21 01:58:28.283234
last batch sz 160
Model ind 640 epoch 378 head B head_i_epoch 0 batch 0: avg loss -1.549216 avg loss no lamb -1.549216 time 2019-02-21 02:00:13.267963
Model ind 640 epoch 378 head B head_i_epoch 0 batch 100: avg loss -1.483076 avg loss no lamb -1.483076 time 2019-02-21 02:02:37.772847
Model ind 640 epoch 378 head B head_i_epoch 0 batch 200: avg loss -1.666909 avg loss no lamb -1.666909 time 2019-02-21 02:05:01.622357
last batch sz 160
Model ind 640 epoch 378 head B head_i_epoch 1 batch 0: avg loss -1.603578 avg loss no lamb -1.603578 time 2019-02-21 02:06:46.141869
Model ind 640 epoch 378 head B head_i_epoch 1 batch 100: avg loss -1.531183 avg loss no lamb -1.531183 time 2019-02-21 02:09:09.723865
Model ind 640 epoch 378 head B head_i_epoch 1 batch 200: avg loss -1.765453 avg loss no lamb -1.765453 time 2019-02-21 02:11:33.838315
last batch sz 160
Pre: time 2019-02-21 02:13:43.723098: 
 	std: 0.051063303
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5014167, 0.60611665, 0.6063, 0.6059, 0.50233334]
	train_accs: [0.5014167, 0.60611665, 0.6063, 0.6059, 0.50233334]
	best_train_sub_head: 2
	worst: 0.5014167
	avg: 0.56441337
	best: 0.6063

Starting e_i: 379
Model ind 640 epoch 379 head A head_i_epoch 0 batch 0: avg loss -2.984124 avg loss no lamb -2.984124 time 2019-02-21 02:13:45.854245
Model ind 640 epoch 379 head A head_i_epoch 0 batch 100: avg loss -2.980821 avg loss no lamb -2.980821 time 2019-02-21 02:16:09.232816
Model ind 640 epoch 379 head A head_i_epoch 0 batch 200: avg loss -2.988842 avg loss no lamb -2.988842 time 2019-02-21 02:18:33.203928
last batch sz 160
Model ind 640 epoch 379 head B head_i_epoch 0 batch 0: avg loss -1.686553 avg loss no lamb -1.686553 time 2019-02-21 02:20:17.992604
Model ind 640 epoch 379 head B head_i_epoch 0 batch 100: avg loss -1.541225 avg loss no lamb -1.541225 time 2019-02-21 02:22:42.136101
Model ind 640 epoch 379 head B head_i_epoch 0 batch 200: avg loss -1.694221 avg loss no lamb -1.694221 time 2019-02-21 02:25:06.676362
last batch sz 160
Model ind 640 epoch 379 head B head_i_epoch 1 batch 0: avg loss -1.527161 avg loss no lamb -1.527161 time 2019-02-21 02:26:51.252872
Model ind 640 epoch 379 head B head_i_epoch 1 batch 100: avg loss -1.526282 avg loss no lamb -1.526282 time 2019-02-21 02:29:14.931425
Model ind 640 epoch 379 head B head_i_epoch 1 batch 200: avg loss -1.742078 avg loss no lamb -1.742078 time 2019-02-21 02:31:38.817091
last batch sz 160
Pre: time 2019-02-21 02:33:47.883854: 
 	std: 0.05349983
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49351665, 0.6033, 0.60403335, 0.6038333, 0.49553335]
	train_accs: [0.49351665, 0.6033, 0.60403335, 0.6038333, 0.49553335]
	best_train_sub_head: 2
	worst: 0.49351665
	avg: 0.56004333
	best: 0.60403335

Starting e_i: 380
Model ind 640 epoch 380 head A head_i_epoch 0 batch 0: avg loss -2.929972 avg loss no lamb -2.929972 time 2019-02-21 02:33:50.493980
Model ind 640 epoch 380 head A head_i_epoch 0 batch 100: avg loss -2.883698 avg loss no lamb -2.883698 time 2019-02-21 02:36:14.846942
Model ind 640 epoch 380 head A head_i_epoch 0 batch 200: avg loss -2.980283 avg loss no lamb -2.980283 time 2019-02-21 02:38:39.136223
last batch sz 160
Model ind 640 epoch 380 head B head_i_epoch 0 batch 0: avg loss -1.683081 avg loss no lamb -1.683081 time 2019-02-21 02:40:24.005948
Model ind 640 epoch 380 head B head_i_epoch 0 batch 100: avg loss -1.517143 avg loss no lamb -1.517143 time 2019-02-21 02:42:47.969754
Model ind 640 epoch 380 head B head_i_epoch 0 batch 200: avg loss -1.663981 avg loss no lamb -1.663981 time 2019-02-21 02:45:12.344213
last batch sz 160
Model ind 640 epoch 380 head B head_i_epoch 1 batch 0: avg loss -1.612362 avg loss no lamb -1.612362 time 2019-02-21 02:46:57.644493
Model ind 640 epoch 380 head B head_i_epoch 1 batch 100: avg loss -1.582552 avg loss no lamb -1.582552 time 2019-02-21 02:49:22.327529
Model ind 640 epoch 380 head B head_i_epoch 1 batch 200: avg loss -1.811342 avg loss no lamb -1.811342 time 2019-02-21 02:51:46.431048
last batch sz 160
Pre: time 2019-02-21 02:53:56.115012: 
 	std: 0.052365907
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5025167, 0.6098667, 0.61048335, 0.61065, 0.5043833]
	train_accs: [0.5025167, 0.6098667, 0.61048335, 0.61065, 0.5043833]
	best_train_sub_head: 3
	worst: 0.5025167
	avg: 0.56758004
	best: 0.61065

Starting e_i: 381
Model ind 640 epoch 381 head A head_i_epoch 0 batch 0: avg loss -3.039893 avg loss no lamb -3.039893 time 2019-02-21 02:54:01.572803
Model ind 640 epoch 381 head A head_i_epoch 0 batch 100: avg loss -3.011134 avg loss no lamb -3.011134 time 2019-02-21 02:56:25.692995
Model ind 640 epoch 381 head A head_i_epoch 0 batch 200: avg loss -3.018032 avg loss no lamb -3.018032 time 2019-02-21 02:58:50.573984
last batch sz 160
Model ind 640 epoch 381 head B head_i_epoch 0 batch 0: avg loss -1.666569 avg loss no lamb -1.666569 time 2019-02-21 03:00:35.842291
Model ind 640 epoch 381 head B head_i_epoch 0 batch 100: avg loss -1.525998 avg loss no lamb -1.525998 time 2019-02-21 03:02:59.765079
Model ind 640 epoch 381 head B head_i_epoch 0 batch 200: avg loss -1.655818 avg loss no lamb -1.655818 time 2019-02-21 03:05:23.746028
last batch sz 160
Model ind 640 epoch 381 head B head_i_epoch 1 batch 0: avg loss -1.638737 avg loss no lamb -1.638737 time 2019-02-21 03:07:08.827035
Model ind 640 epoch 381 head B head_i_epoch 1 batch 100: avg loss -1.579093 avg loss no lamb -1.579093 time 2019-02-21 03:09:33.614328
Model ind 640 epoch 381 head B head_i_epoch 1 batch 200: avg loss -1.695604 avg loss no lamb -1.695604 time 2019-02-21 03:11:58.151190
last batch sz 160
Pre: time 2019-02-21 03:14:07.930804: 
 	std: 0.054113954
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49388334, 0.60573334, 0.60571665, 0.6063667, 0.49711666]
	train_accs: [0.49388334, 0.60573334, 0.60571665, 0.6063667, 0.49711666]
	best_train_sub_head: 3
	worst: 0.49388334
	avg: 0.5617633
	best: 0.6063667

Starting e_i: 382
Model ind 640 epoch 382 head A head_i_epoch 0 batch 0: avg loss -2.911864 avg loss no lamb -2.911864 time 2019-02-21 03:14:10.594120
Model ind 640 epoch 382 head A head_i_epoch 0 batch 100: avg loss -2.924847 avg loss no lamb -2.924847 time 2019-02-21 03:16:35.438409
Model ind 640 epoch 382 head A head_i_epoch 0 batch 200: avg loss -2.980199 avg loss no lamb -2.980199 time 2019-02-21 03:19:00.359289
last batch sz 160
Model ind 640 epoch 382 head B head_i_epoch 0 batch 0: avg loss -1.596059 avg loss no lamb -1.596059 time 2019-02-21 03:20:45.225108
Model ind 640 epoch 382 head B head_i_epoch 0 batch 100: avg loss -1.563901 avg loss no lamb -1.563901 time 2019-02-21 03:23:09.872323
Model ind 640 epoch 382 head B head_i_epoch 0 batch 200: avg loss -1.657676 avg loss no lamb -1.657676 time 2019-02-21 03:25:33.800137
last batch sz 160
Model ind 640 epoch 382 head B head_i_epoch 1 batch 0: avg loss -1.632774 avg loss no lamb -1.632774 time 2019-02-21 03:27:18.614234
Model ind 640 epoch 382 head B head_i_epoch 1 batch 100: avg loss -1.513962 avg loss no lamb -1.513962 time 2019-02-21 03:29:43.356260
Model ind 640 epoch 382 head B head_i_epoch 1 batch 200: avg loss -1.671153 avg loss no lamb -1.671153 time 2019-02-21 03:32:07.779122
last batch sz 160
Pre: time 2019-02-21 03:34:18.471780: 
 	std: 0.053149704
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4973, 0.60728335, 0.60755, 0.60718334, 0.5004333]
	train_accs: [0.4973, 0.60728335, 0.60755, 0.60718334, 0.5004333]
	best_train_sub_head: 2
	worst: 0.4973
	avg: 0.56395006
	best: 0.60755

Starting e_i: 383
Model ind 640 epoch 383 head A head_i_epoch 0 batch 0: avg loss -2.918124 avg loss no lamb -2.918124 time 2019-02-21 03:34:20.768142
Model ind 640 epoch 383 head A head_i_epoch 0 batch 100: avg loss -2.857698 avg loss no lamb -2.857698 time 2019-02-21 03:36:45.462796
Model ind 640 epoch 383 head A head_i_epoch 0 batch 200: avg loss -2.994487 avg loss no lamb -2.994487 time 2019-02-21 03:39:09.773110
last batch sz 160
Model ind 640 epoch 383 head B head_i_epoch 0 batch 0: avg loss -1.644940 avg loss no lamb -1.644940 time 2019-02-21 03:40:54.795508
Model ind 640 epoch 383 head B head_i_epoch 0 batch 100: avg loss -1.561828 avg loss no lamb -1.561828 time 2019-02-21 03:43:19.370807
Model ind 640 epoch 383 head B head_i_epoch 0 batch 200: avg loss -1.722549 avg loss no lamb -1.722549 time 2019-02-21 03:45:44.028877
last batch sz 160
Model ind 640 epoch 383 head B head_i_epoch 1 batch 0: avg loss -1.670022 avg loss no lamb -1.670022 time 2019-02-21 03:47:28.603711
Model ind 640 epoch 383 head B head_i_epoch 1 batch 100: avg loss -1.597916 avg loss no lamb -1.597916 time 2019-02-21 03:49:52.718643
Model ind 640 epoch 383 head B head_i_epoch 1 batch 200: avg loss -1.728030 avg loss no lamb -1.728030 time 2019-02-21 03:52:16.832976
last batch sz 160
Pre: time 2019-02-21 03:54:26.820648: 
 	std: 0.05240692
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49178332, 0.5999, 0.6001, 0.6002333, 0.49445]
	train_accs: [0.49178332, 0.5999, 0.6001, 0.6002333, 0.49445]
	best_train_sub_head: 3
	worst: 0.49178332
	avg: 0.55729336
	best: 0.6002333

Starting e_i: 384
Model ind 640 epoch 384 head A head_i_epoch 0 batch 0: avg loss -2.976805 avg loss no lamb -2.976805 time 2019-02-21 03:54:29.054273
Model ind 640 epoch 384 head A head_i_epoch 0 batch 100: avg loss -2.876091 avg loss no lamb -2.876091 time 2019-02-21 03:56:53.852121
Model ind 640 epoch 384 head A head_i_epoch 0 batch 200: avg loss -2.917322 avg loss no lamb -2.917322 time 2019-02-21 03:59:18.416477
last batch sz 160
Model ind 640 epoch 384 head B head_i_epoch 0 batch 0: avg loss -1.591815 avg loss no lamb -1.591815 time 2019-02-21 04:01:03.374850
Model ind 640 epoch 384 head B head_i_epoch 0 batch 100: avg loss -1.520261 avg loss no lamb -1.520261 time 2019-02-21 04:03:27.264546
Model ind 640 epoch 384 head B head_i_epoch 0 batch 200: avg loss -1.657877 avg loss no lamb -1.657877 time 2019-02-21 04:05:52.224537
last batch sz 160
Model ind 640 epoch 384 head B head_i_epoch 1 batch 0: avg loss -1.663616 avg loss no lamb -1.663616 time 2019-02-21 04:07:37.510755
Model ind 640 epoch 384 head B head_i_epoch 1 batch 100: avg loss -1.629774 avg loss no lamb -1.629774 time 2019-02-21 04:10:02.460616
Model ind 640 epoch 384 head B head_i_epoch 1 batch 200: avg loss -1.671925 avg loss no lamb -1.671925 time 2019-02-21 04:12:26.556043
last batch sz 160
Pre: time 2019-02-21 04:14:36.676159: 
 	std: 0.053501505
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49415, 0.60441667, 0.60476667, 0.60548335, 0.49725]
	train_accs: [0.49415, 0.60441667, 0.60476667, 0.60548335, 0.49725]
	best_train_sub_head: 3
	worst: 0.49415
	avg: 0.5612134
	best: 0.60548335

Starting e_i: 385
Model ind 640 epoch 385 head A head_i_epoch 0 batch 0: avg loss -2.962757 avg loss no lamb -2.962757 time 2019-02-21 04:14:39.074551
Model ind 640 epoch 385 head A head_i_epoch 0 batch 100: avg loss -2.923870 avg loss no lamb -2.923870 time 2019-02-21 04:17:04.290173
Model ind 640 epoch 385 head A head_i_epoch 0 batch 200: avg loss -2.940616 avg loss no lamb -2.940616 time 2019-02-21 04:19:28.588944
last batch sz 160
Model ind 640 epoch 385 head B head_i_epoch 0 batch 0: avg loss -1.695436 avg loss no lamb -1.695436 time 2019-02-21 04:21:13.080364
Model ind 640 epoch 385 head B head_i_epoch 0 batch 100: avg loss -1.583035 avg loss no lamb -1.583035 time 2019-02-21 04:23:36.793029
Model ind 640 epoch 385 head B head_i_epoch 0 batch 200: avg loss -1.734775 avg loss no lamb -1.734775 time 2019-02-21 04:26:01.247043
last batch sz 160
Model ind 640 epoch 385 head B head_i_epoch 1 batch 0: avg loss -1.594789 avg loss no lamb -1.594789 time 2019-02-21 04:27:45.926366
Model ind 640 epoch 385 head B head_i_epoch 1 batch 100: avg loss -1.480704 avg loss no lamb -1.480704 time 2019-02-21 04:30:10.073713
Model ind 640 epoch 385 head B head_i_epoch 1 batch 200: avg loss -1.783366 avg loss no lamb -1.783366 time 2019-02-21 04:32:34.265223
last batch sz 160
Pre: time 2019-02-21 04:34:44.493808: 
 	std: 0.052174855
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49516666, 0.60293335, 0.6038833, 0.6037, 0.4989]
	train_accs: [0.49516666, 0.60293335, 0.6038833, 0.6037, 0.4989]
	best_train_sub_head: 2
	worst: 0.49516666
	avg: 0.5609166
	best: 0.6038833

Starting e_i: 386
Model ind 640 epoch 386 head A head_i_epoch 0 batch 0: avg loss -3.067931 avg loss no lamb -3.067931 time 2019-02-21 04:34:46.737385
Model ind 640 epoch 386 head A head_i_epoch 0 batch 100: avg loss -2.939259 avg loss no lamb -2.939259 time 2019-02-21 04:37:10.897767
Model ind 640 epoch 386 head A head_i_epoch 0 batch 200: avg loss -2.979825 avg loss no lamb -2.979825 time 2019-02-21 04:39:34.886531
last batch sz 160
Model ind 640 epoch 386 head B head_i_epoch 0 batch 0: avg loss -1.665279 avg loss no lamb -1.665279 time 2019-02-21 04:41:19.924148
Model ind 640 epoch 386 head B head_i_epoch 0 batch 100: avg loss -1.520019 avg loss no lamb -1.520019 time 2019-02-21 04:43:44.544461
Model ind 640 epoch 386 head B head_i_epoch 0 batch 200: avg loss -1.789063 avg loss no lamb -1.789063 time 2019-02-21 04:46:08.286478
last batch sz 160
Model ind 640 epoch 386 head B head_i_epoch 1 batch 0: avg loss -1.710871 avg loss no lamb -1.710871 time 2019-02-21 04:47:53.010264
Model ind 640 epoch 386 head B head_i_epoch 1 batch 100: avg loss -1.477953 avg loss no lamb -1.477953 time 2019-02-21 04:50:17.110776
Model ind 640 epoch 386 head B head_i_epoch 1 batch 200: avg loss -1.757668 avg loss no lamb -1.757668 time 2019-02-21 04:52:41.022506
last batch sz 160
Pre: time 2019-02-21 04:54:51.420379: 
 	std: 0.051569015
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49916667, 0.6051667, 0.60585, 0.60576665, 0.50151664]
	train_accs: [0.49916667, 0.6051667, 0.60585, 0.60576665, 0.50151664]
	best_train_sub_head: 2
	worst: 0.49916667
	avg: 0.5634933
	best: 0.60585

Starting e_i: 387
Model ind 640 epoch 387 head A head_i_epoch 0 batch 0: avg loss -2.933300 avg loss no lamb -2.933300 time 2019-02-21 04:54:53.668357
Model ind 640 epoch 387 head A head_i_epoch 0 batch 100: avg loss -2.850593 avg loss no lamb -2.850593 time 2019-02-21 04:57:17.987350
Model ind 640 epoch 387 head A head_i_epoch 0 batch 200: avg loss -2.994184 avg loss no lamb -2.994184 time 2019-02-21 04:59:42.582996
last batch sz 160
Model ind 640 epoch 387 head B head_i_epoch 0 batch 0: avg loss -1.616085 avg loss no lamb -1.616085 time 2019-02-21 05:01:27.900003
Model ind 640 epoch 387 head B head_i_epoch 0 batch 100: avg loss -1.496955 avg loss no lamb -1.496955 time 2019-02-21 05:03:52.625093
Model ind 640 epoch 387 head B head_i_epoch 0 batch 200: avg loss -1.707361 avg loss no lamb -1.707361 time 2019-02-21 05:06:17.125152
last batch sz 160
Model ind 640 epoch 387 head B head_i_epoch 1 batch 0: avg loss -1.621740 avg loss no lamb -1.621740 time 2019-02-21 05:08:01.986970
Model ind 640 epoch 387 head B head_i_epoch 1 batch 100: avg loss -1.572093 avg loss no lamb -1.572093 time 2019-02-21 05:10:26.175567
Model ind 640 epoch 387 head B head_i_epoch 1 batch 200: avg loss -1.757954 avg loss no lamb -1.757954 time 2019-02-21 05:12:50.690787
last batch sz 160
Pre: time 2019-02-21 05:15:00.488083: 
 	std: 0.054583985
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49075, 0.60288334, 0.60395, 0.60333335, 0.49321666]
	train_accs: [0.49075, 0.60288334, 0.60395, 0.60333335, 0.49321666]
	best_train_sub_head: 2
	worst: 0.49075
	avg: 0.5588267
	best: 0.60395

Starting e_i: 388
Model ind 640 epoch 388 head A head_i_epoch 0 batch 0: avg loss -2.973098 avg loss no lamb -2.973098 time 2019-02-21 05:15:02.740233
Model ind 640 epoch 388 head A head_i_epoch 0 batch 100: avg loss -2.816760 avg loss no lamb -2.816760 time 2019-02-21 05:17:26.645831
Model ind 640 epoch 388 head A head_i_epoch 0 batch 200: avg loss -2.956125 avg loss no lamb -2.956125 time 2019-02-21 05:19:51.558826
last batch sz 160
Model ind 640 epoch 388 head B head_i_epoch 0 batch 0: avg loss -1.721490 avg loss no lamb -1.721490 time 2019-02-21 05:21:37.004937
Model ind 640 epoch 388 head B head_i_epoch 0 batch 100: avg loss -1.505333 avg loss no lamb -1.505333 time 2019-02-21 05:24:01.312792
Model ind 640 epoch 388 head B head_i_epoch 0 batch 200: avg loss -1.748479 avg loss no lamb -1.748479 time 2019-02-21 05:26:26.094839
last batch sz 160
Model ind 640 epoch 388 head B head_i_epoch 1 batch 0: avg loss -1.616595 avg loss no lamb -1.616595 time 2019-02-21 05:28:11.044407
Model ind 640 epoch 388 head B head_i_epoch 1 batch 100: avg loss -1.552565 avg loss no lamb -1.552565 time 2019-02-21 05:30:35.544691
Model ind 640 epoch 388 head B head_i_epoch 1 batch 200: avg loss -1.666726 avg loss no lamb -1.666726 time 2019-02-21 05:33:00.467136
last batch sz 160
Pre: time 2019-02-21 05:35:10.364065: 
 	std: 0.052607495
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49588335, 0.6034667, 0.60445, 0.6042167, 0.49745]
	train_accs: [0.49588335, 0.6034667, 0.60445, 0.6042167, 0.49745]
	best_train_sub_head: 2
	worst: 0.49588335
	avg: 0.56109333
	best: 0.60445

Starting e_i: 389
Model ind 640 epoch 389 head A head_i_epoch 0 batch 0: avg loss -2.963848 avg loss no lamb -2.963848 time 2019-02-21 05:35:12.962848
Model ind 640 epoch 389 head A head_i_epoch 0 batch 100: avg loss -2.965086 avg loss no lamb -2.965086 time 2019-02-21 05:37:37.531900
Model ind 640 epoch 389 head A head_i_epoch 0 batch 200: avg loss -2.962430 avg loss no lamb -2.962430 time 2019-02-21 05:40:02.138577
last batch sz 160
Model ind 640 epoch 389 head B head_i_epoch 0 batch 0: avg loss -1.637819 avg loss no lamb -1.637819 time 2019-02-21 05:41:48.011253
Model ind 640 epoch 389 head B head_i_epoch 0 batch 100: avg loss -1.598967 avg loss no lamb -1.598967 time 2019-02-21 05:44:13.777197
Model ind 640 epoch 389 head B head_i_epoch 0 batch 200: avg loss -1.677949 avg loss no lamb -1.677949 time 2019-02-21 05:46:38.208821
last batch sz 160
Model ind 640 epoch 389 head B head_i_epoch 1 batch 0: avg loss -1.552074 avg loss no lamb -1.552074 time 2019-02-21 05:48:23.088906
Model ind 640 epoch 389 head B head_i_epoch 1 batch 100: avg loss -1.507337 avg loss no lamb -1.507337 time 2019-02-21 05:50:47.314978
Model ind 640 epoch 389 head B head_i_epoch 1 batch 200: avg loss -1.629731 avg loss no lamb -1.629731 time 2019-02-21 05:53:11.188457
last batch sz 160
Pre: time 2019-02-21 05:55:20.887265: 
 	std: 0.053223502
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49471667, 0.6031833, 0.6035, 0.60356665, 0.49483332]
	train_accs: [0.49471667, 0.6031833, 0.6035, 0.60356665, 0.49483332]
	best_train_sub_head: 3
	worst: 0.49471667
	avg: 0.55996
	best: 0.60356665

Starting e_i: 390
Model ind 640 epoch 390 head A head_i_epoch 0 batch 0: avg loss -2.955580 avg loss no lamb -2.955580 time 2019-02-21 05:55:23.324293
Model ind 640 epoch 390 head A head_i_epoch 0 batch 100: avg loss -2.798677 avg loss no lamb -2.798677 time 2019-02-21 05:57:48.043508
Model ind 640 epoch 390 head A head_i_epoch 0 batch 200: avg loss -2.997942 avg loss no lamb -2.997942 time 2019-02-21 06:00:12.358081
last batch sz 160
Model ind 640 epoch 390 head B head_i_epoch 0 batch 0: avg loss -1.630485 avg loss no lamb -1.630485 time 2019-02-21 06:01:57.339450
Model ind 640 epoch 390 head B head_i_epoch 0 batch 100: avg loss -1.548094 avg loss no lamb -1.548094 time 2019-02-21 06:04:21.417361
Model ind 640 epoch 390 head B head_i_epoch 0 batch 200: avg loss -1.723005 avg loss no lamb -1.723005 time 2019-02-21 06:06:46.599263
last batch sz 160
Model ind 640 epoch 390 head B head_i_epoch 1 batch 0: avg loss -1.617419 avg loss no lamb -1.617419 time 2019-02-21 06:08:31.322139
Model ind 640 epoch 390 head B head_i_epoch 1 batch 100: avg loss -1.563416 avg loss no lamb -1.563416 time 2019-02-21 06:10:55.286615
Model ind 640 epoch 390 head B head_i_epoch 1 batch 200: avg loss -1.760392 avg loss no lamb -1.760392 time 2019-02-21 06:13:19.240147
last batch sz 160
Pre: time 2019-02-21 06:15:28.405696: 
 	std: 0.052075237
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49888334, 0.6066667, 0.60655, 0.6063833, 0.50161666]
	train_accs: [0.49888334, 0.6066667, 0.60655, 0.6063833, 0.50161666]
	best_train_sub_head: 1
	worst: 0.49888334
	avg: 0.56402004
	best: 0.6066667

Starting e_i: 391
Model ind 640 epoch 391 head A head_i_epoch 0 batch 0: avg loss -3.033015 avg loss no lamb -3.033015 time 2019-02-21 06:15:33.797388
Model ind 640 epoch 391 head A head_i_epoch 0 batch 100: avg loss -2.834969 avg loss no lamb -2.834969 time 2019-02-21 06:17:58.180192
Model ind 640 epoch 391 head A head_i_epoch 0 batch 200: avg loss -2.991989 avg loss no lamb -2.991989 time 2019-02-21 06:20:23.745158
last batch sz 160
Model ind 640 epoch 391 head B head_i_epoch 0 batch 0: avg loss -1.602052 avg loss no lamb -1.602052 time 2019-02-21 06:22:09.187039
Model ind 640 epoch 391 head B head_i_epoch 0 batch 100: avg loss -1.580744 avg loss no lamb -1.580744 time 2019-02-21 06:24:33.310853
Model ind 640 epoch 391 head B head_i_epoch 0 batch 200: avg loss -1.648785 avg loss no lamb -1.648785 time 2019-02-21 06:26:56.861882
last batch sz 160
Model ind 640 epoch 391 head B head_i_epoch 1 batch 0: avg loss -1.702353 avg loss no lamb -1.702353 time 2019-02-21 06:28:42.108207
Model ind 640 epoch 391 head B head_i_epoch 1 batch 100: avg loss -1.547444 avg loss no lamb -1.547444 time 2019-02-21 06:31:06.127881
Model ind 640 epoch 391 head B head_i_epoch 1 batch 200: avg loss -1.712708 avg loss no lamb -1.712708 time 2019-02-21 06:33:31.159311
last batch sz 160
Pre: time 2019-02-21 06:35:40.730830: 
 	std: 0.05209665
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4945, 0.6022, 0.60246664, 0.60218334, 0.49741668]
	train_accs: [0.4945, 0.6022, 0.60246664, 0.60218334, 0.49741668]
	best_train_sub_head: 2
	worst: 0.4945
	avg: 0.5597533
	best: 0.60246664

Starting e_i: 392
Model ind 640 epoch 392 head A head_i_epoch 0 batch 0: avg loss -2.953760 avg loss no lamb -2.953760 time 2019-02-21 06:35:42.933915
Model ind 640 epoch 392 head A head_i_epoch 0 batch 100: avg loss -2.862478 avg loss no lamb -2.862478 time 2019-02-21 06:38:07.078784
Model ind 640 epoch 392 head A head_i_epoch 0 batch 200: avg loss -2.983353 avg loss no lamb -2.983353 time 2019-02-21 06:40:31.198256
last batch sz 160
Model ind 640 epoch 392 head B head_i_epoch 0 batch 0: avg loss -1.714854 avg loss no lamb -1.714854 time 2019-02-21 06:42:16.706620
Model ind 640 epoch 392 head B head_i_epoch 0 batch 100: avg loss -1.634446 avg loss no lamb -1.634446 time 2019-02-21 06:44:40.807236
Model ind 640 epoch 392 head B head_i_epoch 0 batch 200: avg loss -1.745126 avg loss no lamb -1.745126 time 2019-02-21 06:47:04.789933
last batch sz 160
Model ind 640 epoch 392 head B head_i_epoch 1 batch 0: avg loss -1.637075 avg loss no lamb -1.637075 time 2019-02-21 06:48:49.819459
Model ind 640 epoch 392 head B head_i_epoch 1 batch 100: avg loss -1.557432 avg loss no lamb -1.557432 time 2019-02-21 06:51:14.054864
Model ind 640 epoch 392 head B head_i_epoch 1 batch 200: avg loss -1.689891 avg loss no lamb -1.689891 time 2019-02-21 06:53:38.089424
last batch sz 160
Pre: time 2019-02-21 06:55:48.179348: 
 	std: 0.052141447
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49593332, 0.60326666, 0.6032, 0.6031167, 0.4976]
	train_accs: [0.49593332, 0.60326666, 0.6032, 0.6031167, 0.4976]
	best_train_sub_head: 1
	worst: 0.49593332
	avg: 0.56062335
	best: 0.60326666

Starting e_i: 393
Model ind 640 epoch 393 head A head_i_epoch 0 batch 0: avg loss -2.954931 avg loss no lamb -2.954931 time 2019-02-21 06:55:50.440843
Model ind 640 epoch 393 head A head_i_epoch 0 batch 100: avg loss -2.946682 avg loss no lamb -2.946682 time 2019-02-21 06:58:14.721055
Model ind 640 epoch 393 head A head_i_epoch 0 batch 200: avg loss -2.996275 avg loss no lamb -2.996275 time 2019-02-21 07:00:38.844539
last batch sz 160
Model ind 640 epoch 393 head B head_i_epoch 0 batch 0: avg loss -1.606936 avg loss no lamb -1.606936 time 2019-02-21 07:02:23.785469
Model ind 640 epoch 393 head B head_i_epoch 0 batch 100: avg loss -1.547185 avg loss no lamb -1.547185 time 2019-02-21 07:04:47.981197
Model ind 640 epoch 393 head B head_i_epoch 0 batch 200: avg loss -1.798365 avg loss no lamb -1.798365 time 2019-02-21 07:07:12.264034
last batch sz 160
Model ind 640 epoch 393 head B head_i_epoch 1 batch 0: avg loss -1.580632 avg loss no lamb -1.580632 time 2019-02-21 07:08:57.167063
Model ind 640 epoch 393 head B head_i_epoch 1 batch 100: avg loss -1.560808 avg loss no lamb -1.560808 time 2019-02-21 07:11:21.535565
Model ind 640 epoch 393 head B head_i_epoch 1 batch 200: avg loss -1.647470 avg loss no lamb -1.647470 time 2019-02-21 07:13:45.554030
last batch sz 160
Pre: time 2019-02-21 07:15:56.054737: 
 	std: 0.050679862
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5065, 0.6116167, 0.61146665, 0.6113333, 0.50958335]
	train_accs: [0.5065, 0.6116167, 0.61146665, 0.6113333, 0.50958335]
	best_train_sub_head: 1
	worst: 0.5065
	avg: 0.5701
	best: 0.6116167

Starting e_i: 394
Model ind 640 epoch 394 head A head_i_epoch 0 batch 0: avg loss -2.934232 avg loss no lamb -2.934232 time 2019-02-21 07:15:58.247155
Model ind 640 epoch 394 head A head_i_epoch 0 batch 100: avg loss -2.845343 avg loss no lamb -2.845343 time 2019-02-21 07:18:22.964281
Model ind 640 epoch 394 head A head_i_epoch 0 batch 200: avg loss -3.025455 avg loss no lamb -3.025455 time 2019-02-21 07:20:48.332188
last batch sz 160
Model ind 640 epoch 394 head B head_i_epoch 0 batch 0: avg loss -1.669214 avg loss no lamb -1.669214 time 2019-02-21 07:22:33.405951
Model ind 640 epoch 394 head B head_i_epoch 0 batch 100: avg loss -1.585779 avg loss no lamb -1.585779 time 2019-02-21 07:24:57.518530
Model ind 640 epoch 394 head B head_i_epoch 0 batch 200: avg loss -1.717182 avg loss no lamb -1.717182 time 2019-02-21 07:27:22.391167
last batch sz 160
Model ind 640 epoch 394 head B head_i_epoch 1 batch 0: avg loss -1.674428 avg loss no lamb -1.674428 time 2019-02-21 07:29:07.404293
Model ind 640 epoch 394 head B head_i_epoch 1 batch 100: avg loss -1.496332 avg loss no lamb -1.496332 time 2019-02-21 07:31:31.581645
Model ind 640 epoch 394 head B head_i_epoch 1 batch 200: avg loss -1.782027 avg loss no lamb -1.782027 time 2019-02-21 07:33:55.878211
last batch sz 160
Pre: time 2019-02-21 07:36:05.094396: 
 	std: 0.05209399
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49378332, 0.6012167, 0.6017333, 0.6013167, 0.49641666]
	train_accs: [0.49378332, 0.6012167, 0.6017333, 0.6013167, 0.49641666]
	best_train_sub_head: 2
	worst: 0.49378332
	avg: 0.5588933
	best: 0.6017333

Starting e_i: 395
Model ind 640 epoch 395 head A head_i_epoch 0 batch 0: avg loss -2.902539 avg loss no lamb -2.902539 time 2019-02-21 07:36:07.297398
Model ind 640 epoch 395 head A head_i_epoch 0 batch 100: avg loss -2.872557 avg loss no lamb -2.872557 time 2019-02-21 07:38:31.433946
Model ind 640 epoch 395 head A head_i_epoch 0 batch 200: avg loss -3.013551 avg loss no lamb -3.013551 time 2019-02-21 07:40:56.645276
last batch sz 160
Model ind 640 epoch 395 head B head_i_epoch 0 batch 0: avg loss -1.690004 avg loss no lamb -1.690004 time 2019-02-21 07:42:42.078764
Model ind 640 epoch 395 head B head_i_epoch 0 batch 100: avg loss -1.557777 avg loss no lamb -1.557777 time 2019-02-21 07:45:05.737877
Model ind 640 epoch 395 head B head_i_epoch 0 batch 200: avg loss -1.833483 avg loss no lamb -1.833483 time 2019-02-21 07:47:29.183134
last batch sz 160
Model ind 640 epoch 395 head B head_i_epoch 1 batch 0: avg loss -1.631088 avg loss no lamb -1.631088 time 2019-02-21 07:49:14.864819
Model ind 640 epoch 395 head B head_i_epoch 1 batch 100: avg loss -1.510354 avg loss no lamb -1.510354 time 2019-02-21 07:51:39.824265
Model ind 640 epoch 395 head B head_i_epoch 1 batch 200: avg loss -1.701850 avg loss no lamb -1.701850 time 2019-02-21 07:54:04.959796
last batch sz 160
Pre: time 2019-02-21 07:56:15.108774: 
 	std: 0.052230682
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50125, 0.60861665, 0.6088667, 0.60891664, 0.50313336]
	train_accs: [0.50125, 0.60861665, 0.6088667, 0.60891664, 0.50313336]
	best_train_sub_head: 3
	worst: 0.50125
	avg: 0.5661567
	best: 0.60891664

Starting e_i: 396
Model ind 640 epoch 396 head A head_i_epoch 0 batch 0: avg loss -2.963690 avg loss no lamb -2.963690 time 2019-02-21 07:56:17.285076
Model ind 640 epoch 396 head A head_i_epoch 0 batch 100: avg loss -2.931570 avg loss no lamb -2.931570 time 2019-02-21 07:58:41.894676
Model ind 640 epoch 396 head A head_i_epoch 0 batch 200: avg loss -3.005858 avg loss no lamb -3.005858 time 2019-02-21 08:01:06.539613
last batch sz 160
Model ind 640 epoch 396 head B head_i_epoch 0 batch 0: avg loss -1.654572 avg loss no lamb -1.654572 time 2019-02-21 08:02:51.252240
Model ind 640 epoch 396 head B head_i_epoch 0 batch 100: avg loss -1.614390 avg loss no lamb -1.614390 time 2019-02-21 08:05:15.377091
Model ind 640 epoch 396 head B head_i_epoch 0 batch 200: avg loss -1.798596 avg loss no lamb -1.798596 time 2019-02-21 08:07:39.524807
last batch sz 160
Model ind 640 epoch 396 head B head_i_epoch 1 batch 0: avg loss -1.614385 avg loss no lamb -1.614385 time 2019-02-21 08:09:24.651730
Model ind 640 epoch 396 head B head_i_epoch 1 batch 100: avg loss -1.600303 avg loss no lamb -1.600303 time 2019-02-21 08:11:48.899412
Model ind 640 epoch 396 head B head_i_epoch 1 batch 200: avg loss -1.649648 avg loss no lamb -1.649648 time 2019-02-21 08:14:13.548434
last batch sz 160
Pre: time 2019-02-21 08:16:23.964514: 
 	std: 0.053959623
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49138334, 0.6020667, 0.60251665, 0.60248333, 0.49305]
	train_accs: [0.49138334, 0.6020667, 0.60251665, 0.60248333, 0.49305]
	best_train_sub_head: 2
	worst: 0.49138334
	avg: 0.5583
	best: 0.60251665

Starting e_i: 397
Model ind 640 epoch 397 head A head_i_epoch 0 batch 0: avg loss -3.004865 avg loss no lamb -3.004865 time 2019-02-21 08:16:26.190188
Model ind 640 epoch 397 head A head_i_epoch 0 batch 100: avg loss -2.912683 avg loss no lamb -2.912683 time 2019-02-21 08:18:51.837960
Model ind 640 epoch 397 head A head_i_epoch 0 batch 200: avg loss -3.050949 avg loss no lamb -3.050949 time 2019-02-21 08:21:16.529744
last batch sz 160
Model ind 640 epoch 397 head B head_i_epoch 0 batch 0: avg loss -1.685862 avg loss no lamb -1.685862 time 2019-02-21 08:23:01.980498
Model ind 640 epoch 397 head B head_i_epoch 0 batch 100: avg loss -1.555951 avg loss no lamb -1.555951 time 2019-02-21 08:25:26.512326
Model ind 640 epoch 397 head B head_i_epoch 0 batch 200: avg loss -1.691240 avg loss no lamb -1.691240 time 2019-02-21 08:27:50.959526
last batch sz 160
Model ind 640 epoch 397 head B head_i_epoch 1 batch 0: avg loss -1.586606 avg loss no lamb -1.586606 time 2019-02-21 08:29:36.142415
Model ind 640 epoch 397 head B head_i_epoch 1 batch 100: avg loss -1.492193 avg loss no lamb -1.492193 time 2019-02-21 08:32:00.422155
Model ind 640 epoch 397 head B head_i_epoch 1 batch 200: avg loss -1.724491 avg loss no lamb -1.724491 time 2019-02-21 08:34:24.435745
last batch sz 160
Pre: time 2019-02-21 08:36:34.023872: 
 	std: 0.053075153
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49553335, 0.6048833, 0.60546666, 0.60468334, 0.49783334]
	train_accs: [0.49553335, 0.6048833, 0.60546666, 0.60468334, 0.49783334]
	best_train_sub_head: 2
	worst: 0.49553335
	avg: 0.56167996
	best: 0.60546666

Starting e_i: 398
Model ind 640 epoch 398 head A head_i_epoch 0 batch 0: avg loss -3.021171 avg loss no lamb -3.021171 time 2019-02-21 08:36:36.633543
Model ind 640 epoch 398 head A head_i_epoch 0 batch 100: avg loss -2.874888 avg loss no lamb -2.874888 time 2019-02-21 08:39:01.420823
Model ind 640 epoch 398 head A head_i_epoch 0 batch 200: avg loss -2.961539 avg loss no lamb -2.961539 time 2019-02-21 08:41:25.728893
last batch sz 160
Model ind 640 epoch 398 head B head_i_epoch 0 batch 0: avg loss -1.684504 avg loss no lamb -1.684504 time 2019-02-21 08:43:10.724409
Model ind 640 epoch 398 head B head_i_epoch 0 batch 100: avg loss -1.598008 avg loss no lamb -1.598008 time 2019-02-21 08:45:35.489542
Model ind 640 epoch 398 head B head_i_epoch 0 batch 200: avg loss -1.718865 avg loss no lamb -1.718865 time 2019-02-21 08:48:00.044718
last batch sz 160
Model ind 640 epoch 398 head B head_i_epoch 1 batch 0: avg loss -1.624116 avg loss no lamb -1.624116 time 2019-02-21 08:49:45.243006
Model ind 640 epoch 398 head B head_i_epoch 1 batch 100: avg loss -1.492348 avg loss no lamb -1.492348 time 2019-02-21 08:52:09.580393
Model ind 640 epoch 398 head B head_i_epoch 1 batch 200: avg loss -1.787434 avg loss no lamb -1.787434 time 2019-02-21 08:54:33.239036
last batch sz 160
Pre: time 2019-02-21 08:56:42.288138: 
 	std: 0.052705
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50018334, 0.60805, 0.60908335, 0.60805, 0.50145]
	train_accs: [0.50018334, 0.60805, 0.60908335, 0.60805, 0.50145]
	best_train_sub_head: 2
	worst: 0.50018334
	avg: 0.5653633
	best: 0.60908335

Starting e_i: 399
Model ind 640 epoch 399 head A head_i_epoch 0 batch 0: avg loss -2.925713 avg loss no lamb -2.925713 time 2019-02-21 08:56:44.458557
Model ind 640 epoch 399 head A head_i_epoch 0 batch 100: avg loss -2.844210 avg loss no lamb -2.844210 time 2019-02-21 08:59:08.308031
Model ind 640 epoch 399 head A head_i_epoch 0 batch 200: avg loss -3.021173 avg loss no lamb -3.021173 time 2019-02-21 09:01:32.499198
last batch sz 160
Model ind 640 epoch 399 head B head_i_epoch 0 batch 0: avg loss -1.643017 avg loss no lamb -1.643017 time 2019-02-21 09:03:17.584028
Model ind 640 epoch 399 head B head_i_epoch 0 batch 100: avg loss -1.530255 avg loss no lamb -1.530255 time 2019-02-21 09:05:41.719706
Model ind 640 epoch 399 head B head_i_epoch 0 batch 200: avg loss -1.758017 avg loss no lamb -1.758017 time 2019-02-21 09:08:05.672656
last batch sz 160
Model ind 640 epoch 399 head B head_i_epoch 1 batch 0: avg loss -1.662617 avg loss no lamb -1.662617 time 2019-02-21 09:09:50.028389
Model ind 640 epoch 399 head B head_i_epoch 1 batch 100: avg loss -1.648273 avg loss no lamb -1.648273 time 2019-02-21 09:12:13.817362
Model ind 640 epoch 399 head B head_i_epoch 1 batch 200: avg loss -1.648072 avg loss no lamb -1.648072 time 2019-02-21 09:14:37.424264
last batch sz 160
Pre: time 2019-02-21 09:16:46.669502: 
 	std: 0.05268068
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49848333, 0.6057, 0.60615, 0.60575, 0.49818334]
	train_accs: [0.49848333, 0.6057, 0.60615, 0.60575, 0.49818334]
	best_train_sub_head: 2
	worst: 0.49818334
	avg: 0.56285334
	best: 0.60615

Starting e_i: 400
Model ind 640 epoch 400 head A head_i_epoch 0 batch 0: avg loss -2.992735 avg loss no lamb -2.992735 time 2019-02-21 09:16:48.832135
Model ind 640 epoch 400 head A head_i_epoch 0 batch 100: avg loss -2.831097 avg loss no lamb -2.831097 time 2019-02-21 09:19:12.860172
Model ind 640 epoch 400 head A head_i_epoch 0 batch 200: avg loss -3.030589 avg loss no lamb -3.030589 time 2019-02-21 09:21:37.178923
last batch sz 160
Model ind 640 epoch 400 head B head_i_epoch 0 batch 0: avg loss -1.648168 avg loss no lamb -1.648168 time 2019-02-21 09:23:22.366519
Model ind 640 epoch 400 head B head_i_epoch 0 batch 100: avg loss -1.571252 avg loss no lamb -1.571252 time 2019-02-21 09:25:47.000499
Model ind 640 epoch 400 head B head_i_epoch 0 batch 200: avg loss -1.755365 avg loss no lamb -1.755365 time 2019-02-21 09:28:11.451491
last batch sz 160
Model ind 640 epoch 400 head B head_i_epoch 1 batch 0: avg loss -1.717880 avg loss no lamb -1.717880 time 2019-02-21 09:29:57.361315
Model ind 640 epoch 400 head B head_i_epoch 1 batch 100: avg loss -1.522066 avg loss no lamb -1.522066 time 2019-02-21 09:32:21.915242
Model ind 640 epoch 400 head B head_i_epoch 1 batch 200: avg loss -1.703531 avg loss no lamb -1.703531 time 2019-02-21 09:34:46.249791
last batch sz 160
Pre: time 2019-02-21 09:36:55.903447: 
 	std: 0.05431348
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49278334, 0.60408336, 0.60443336, 0.60425, 0.494]
	train_accs: [0.49278334, 0.60408336, 0.60443336, 0.60425, 0.494]
	best_train_sub_head: 2
	worst: 0.49278334
	avg: 0.55991
	best: 0.60443336

Starting e_i: 401
Model ind 640 epoch 401 head A head_i_epoch 0 batch 0: avg loss -2.965411 avg loss no lamb -2.965411 time 2019-02-21 09:37:01.479872
Model ind 640 epoch 401 head A head_i_epoch 0 batch 100: avg loss -2.862726 avg loss no lamb -2.862726 time 2019-02-21 09:39:25.613690
Model ind 640 epoch 401 head A head_i_epoch 0 batch 200: avg loss -2.978662 avg loss no lamb -2.978662 time 2019-02-21 09:41:49.910381
last batch sz 160
Model ind 640 epoch 401 head B head_i_epoch 0 batch 0: avg loss -1.633212 avg loss no lamb -1.633212 time 2019-02-21 09:43:34.891068
Model ind 640 epoch 401 head B head_i_epoch 0 batch 100: avg loss -1.516473 avg loss no lamb -1.516473 time 2019-02-21 09:45:59.694163
Model ind 640 epoch 401 head B head_i_epoch 0 batch 200: avg loss -1.754312 avg loss no lamb -1.754312 time 2019-02-21 09:48:23.767997
last batch sz 160
Model ind 640 epoch 401 head B head_i_epoch 1 batch 0: avg loss -1.563761 avg loss no lamb -1.563761 time 2019-02-21 09:50:08.561387
Model ind 640 epoch 401 head B head_i_epoch 1 batch 100: avg loss -1.579776 avg loss no lamb -1.579776 time 2019-02-21 09:52:32.826131
Model ind 640 epoch 401 head B head_i_epoch 1 batch 200: avg loss -1.778513 avg loss no lamb -1.778513 time 2019-02-21 09:54:57.005746
last batch sz 160
Pre: time 2019-02-21 09:57:06.887081: 
 	std: 0.053980783
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49383333, 0.60391665, 0.60466665, 0.60435, 0.49441665]
	train_accs: [0.49383333, 0.60391665, 0.60466665, 0.60435, 0.49441665]
	best_train_sub_head: 2
	worst: 0.49383333
	avg: 0.56023663
	best: 0.60466665

Starting e_i: 402
Model ind 640 epoch 402 head A head_i_epoch 0 batch 0: avg loss -2.996813 avg loss no lamb -2.996813 time 2019-02-21 09:57:09.122323
Model ind 640 epoch 402 head A head_i_epoch 0 batch 100: avg loss -2.874510 avg loss no lamb -2.874510 time 2019-02-21 09:59:34.371120
Model ind 640 epoch 402 head A head_i_epoch 0 batch 200: avg loss -3.082848 avg loss no lamb -3.082848 time 2019-02-21 10:01:59.234856
last batch sz 160
Model ind 640 epoch 402 head B head_i_epoch 0 batch 0: avg loss -1.716202 avg loss no lamb -1.716202 time 2019-02-21 10:03:44.474642
Model ind 640 epoch 402 head B head_i_epoch 0 batch 100: avg loss -1.487244 avg loss no lamb -1.487244 time 2019-02-21 10:06:08.307704
Model ind 640 epoch 402 head B head_i_epoch 0 batch 200: avg loss -1.741927 avg loss no lamb -1.741927 time 2019-02-21 10:08:32.674248
last batch sz 160
Model ind 640 epoch 402 head B head_i_epoch 1 batch 0: avg loss -1.676330 avg loss no lamb -1.676330 time 2019-02-21 10:10:18.246385
Model ind 640 epoch 402 head B head_i_epoch 1 batch 100: avg loss -1.625973 avg loss no lamb -1.625973 time 2019-02-21 10:12:42.789917
Model ind 640 epoch 402 head B head_i_epoch 1 batch 200: avg loss -1.780276 avg loss no lamb -1.780276 time 2019-02-21 10:15:06.850787
last batch sz 160
Pre: time 2019-02-21 10:17:16.725434: 
 	std: 0.05421298
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49536666, 0.60655, 0.60691667, 0.60653335, 0.49665]
	train_accs: [0.49536666, 0.60655, 0.60691667, 0.60653335, 0.49665]
	best_train_sub_head: 2
	worst: 0.49536666
	avg: 0.5624033
	best: 0.60691667

Starting e_i: 403
Model ind 640 epoch 403 head A head_i_epoch 0 batch 0: avg loss -3.024827 avg loss no lamb -3.024827 time 2019-02-21 10:17:19.475787
Model ind 640 epoch 403 head A head_i_epoch 0 batch 100: avg loss -2.895936 avg loss no lamb -2.895936 time 2019-02-21 10:19:44.022682
Model ind 640 epoch 403 head A head_i_epoch 0 batch 200: avg loss -2.970010 avg loss no lamb -2.970010 time 2019-02-21 10:22:08.149615
last batch sz 160
Model ind 640 epoch 403 head B head_i_epoch 0 batch 0: avg loss -1.671926 avg loss no lamb -1.671926 time 2019-02-21 10:23:53.141857
Model ind 640 epoch 403 head B head_i_epoch 0 batch 100: avg loss -1.530917 avg loss no lamb -1.530917 time 2019-02-21 10:26:17.073384
Model ind 640 epoch 403 head B head_i_epoch 0 batch 200: avg loss -1.716841 avg loss no lamb -1.716841 time 2019-02-21 10:28:41.189489
last batch sz 160
Model ind 640 epoch 403 head B head_i_epoch 1 batch 0: avg loss -1.631911 avg loss no lamb -1.631911 time 2019-02-21 10:30:25.799825
Model ind 640 epoch 403 head B head_i_epoch 1 batch 100: avg loss -1.502608 avg loss no lamb -1.502608 time 2019-02-21 10:32:49.713650
Model ind 640 epoch 403 head B head_i_epoch 1 batch 200: avg loss -1.788759 avg loss no lamb -1.788759 time 2019-02-21 10:35:13.846382
last batch sz 160
Pre: time 2019-02-21 10:37:23.565727: 
 	std: 0.05295063
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49703333, 0.60553336, 0.60573334, 0.60538334, 0.4979]
	train_accs: [0.49703333, 0.60553336, 0.60573334, 0.60538334, 0.4979]
	best_train_sub_head: 2
	worst: 0.49703333
	avg: 0.5623167
	best: 0.60573334

Starting e_i: 404
Model ind 640 epoch 404 head A head_i_epoch 0 batch 0: avg loss -2.971104 avg loss no lamb -2.971104 time 2019-02-21 10:37:25.907546
Model ind 640 epoch 404 head A head_i_epoch 0 batch 100: avg loss -2.843859 avg loss no lamb -2.843859 time 2019-02-21 10:39:51.766170
Model ind 640 epoch 404 head A head_i_epoch 0 batch 200: avg loss -2.989624 avg loss no lamb -2.989624 time 2019-02-21 10:42:17.214990
last batch sz 160
Model ind 640 epoch 404 head B head_i_epoch 0 batch 0: avg loss -1.726311 avg loss no lamb -1.726311 time 2019-02-21 10:44:02.536656
Model ind 640 epoch 404 head B head_i_epoch 0 batch 100: avg loss -1.552885 avg loss no lamb -1.552885 time 2019-02-21 10:46:26.589836
Model ind 640 epoch 404 head B head_i_epoch 0 batch 200: avg loss -1.791880 avg loss no lamb -1.791880 time 2019-02-21 10:48:51.037062
last batch sz 160
Model ind 640 epoch 404 head B head_i_epoch 1 batch 0: avg loss -1.690943 avg loss no lamb -1.690943 time 2019-02-21 10:50:35.709658
Model ind 640 epoch 404 head B head_i_epoch 1 batch 100: avg loss -1.536175 avg loss no lamb -1.536175 time 2019-02-21 10:53:00.083478
Model ind 640 epoch 404 head B head_i_epoch 1 batch 200: avg loss -1.735356 avg loss no lamb -1.735356 time 2019-02-21 10:55:24.990831
last batch sz 160
Pre: time 2019-02-21 10:57:34.874010: 
 	std: 0.052372426
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49973333, 0.60613334, 0.60651666, 0.6059833, 0.49888334]
	train_accs: [0.49973333, 0.60613334, 0.60651666, 0.6059833, 0.49888334]
	best_train_sub_head: 2
	worst: 0.49888334
	avg: 0.56345
	best: 0.60651666

Starting e_i: 405
Model ind 640 epoch 405 head A head_i_epoch 0 batch 0: avg loss -2.940740 avg loss no lamb -2.940740 time 2019-02-21 10:57:37.492921
Model ind 640 epoch 405 head A head_i_epoch 0 batch 100: avg loss -2.879078 avg loss no lamb -2.879078 time 2019-02-21 11:00:01.839787
Model ind 640 epoch 405 head A head_i_epoch 0 batch 200: avg loss -2.930684 avg loss no lamb -2.930684 time 2019-02-21 11:02:26.678366
last batch sz 160
Model ind 640 epoch 405 head B head_i_epoch 0 batch 0: avg loss -1.633443 avg loss no lamb -1.633443 time 2019-02-21 11:04:12.139482
Model ind 640 epoch 405 head B head_i_epoch 0 batch 100: avg loss -1.587942 avg loss no lamb -1.587942 time 2019-02-21 11:06:36.667659
Model ind 640 epoch 405 head B head_i_epoch 0 batch 200: avg loss -1.650506 avg loss no lamb -1.650506 time 2019-02-21 11:09:01.152226
last batch sz 160
Model ind 640 epoch 405 head B head_i_epoch 1 batch 0: avg loss -1.715917 avg loss no lamb -1.715917 time 2019-02-21 11:10:46.177819
Model ind 640 epoch 405 head B head_i_epoch 1 batch 100: avg loss -1.539152 avg loss no lamb -1.539152 time 2019-02-21 11:13:10.611658
Model ind 640 epoch 405 head B head_i_epoch 1 batch 200: avg loss -1.630297 avg loss no lamb -1.630297 time 2019-02-21 11:15:34.883721
last batch sz 160
Pre: time 2019-02-21 11:17:44.878861: 
 	std: 0.053085953
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49731666, 0.60553336, 0.6055833, 0.60576665, 0.49721667]
	train_accs: [0.49731666, 0.60553336, 0.6055833, 0.60576665, 0.49721667]
	best_train_sub_head: 3
	worst: 0.49721667
	avg: 0.56228334
	best: 0.60576665

Starting e_i: 406
Model ind 640 epoch 406 head A head_i_epoch 0 batch 0: avg loss -3.046587 avg loss no lamb -3.046587 time 2019-02-21 11:17:47.173929
Model ind 640 epoch 406 head A head_i_epoch 0 batch 100: avg loss -2.906127 avg loss no lamb -2.906127 time 2019-02-21 11:20:11.869410
Model ind 640 epoch 406 head A head_i_epoch 0 batch 200: avg loss -2.967510 avg loss no lamb -2.967510 time 2019-02-21 11:22:36.668136
last batch sz 160
Model ind 640 epoch 406 head B head_i_epoch 0 batch 0: avg loss -1.696998 avg loss no lamb -1.696998 time 2019-02-21 11:24:21.689135
Model ind 640 epoch 406 head B head_i_epoch 0 batch 100: avg loss -1.619530 avg loss no lamb -1.619530 time 2019-02-21 11:26:45.861904
Model ind 640 epoch 406 head B head_i_epoch 0 batch 200: avg loss -1.771228 avg loss no lamb -1.771228 time 2019-02-21 11:29:09.940033
last batch sz 160
Model ind 640 epoch 406 head B head_i_epoch 1 batch 0: avg loss -1.665166 avg loss no lamb -1.665166 time 2019-02-21 11:30:54.982718
Model ind 640 epoch 406 head B head_i_epoch 1 batch 100: avg loss -1.614563 avg loss no lamb -1.614563 time 2019-02-21 11:33:19.632492
Model ind 640 epoch 406 head B head_i_epoch 1 batch 200: avg loss -1.722531 avg loss no lamb -1.722531 time 2019-02-21 11:35:44.304979
last batch sz 160
Pre: time 2019-02-21 11:37:54.110119: 
 	std: 0.053504482
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49673334, 0.6056, 0.60535, 0.60611665, 0.49621665]
	train_accs: [0.49673334, 0.6056, 0.60535, 0.60611665, 0.49621665]
	best_train_sub_head: 3
	worst: 0.49621665
	avg: 0.5620033
	best: 0.60611665

Starting e_i: 407
Model ind 640 epoch 407 head A head_i_epoch 0 batch 0: avg loss -2.992439 avg loss no lamb -2.992439 time 2019-02-21 11:37:56.867921
Model ind 640 epoch 407 head A head_i_epoch 0 batch 100: avg loss -2.924797 avg loss no lamb -2.924797 time 2019-02-21 11:40:21.954056
Model ind 640 epoch 407 head A head_i_epoch 0 batch 200: avg loss -3.019011 avg loss no lamb -3.019011 time 2019-02-21 11:42:46.932379
last batch sz 160
Model ind 640 epoch 407 head B head_i_epoch 0 batch 0: avg loss -1.729274 avg loss no lamb -1.729274 time 2019-02-21 11:44:32.119379
Model ind 640 epoch 407 head B head_i_epoch 0 batch 100: avg loss -1.550561 avg loss no lamb -1.550561 time 2019-02-21 11:46:56.059434
Model ind 640 epoch 407 head B head_i_epoch 0 batch 200: avg loss -1.718824 avg loss no lamb -1.718824 time 2019-02-21 11:49:20.220267
last batch sz 160
Model ind 640 epoch 407 head B head_i_epoch 1 batch 0: avg loss -1.680230 avg loss no lamb -1.680230 time 2019-02-21 11:51:05.127763
Model ind 640 epoch 407 head B head_i_epoch 1 batch 100: avg loss -1.564246 avg loss no lamb -1.564246 time 2019-02-21 11:53:28.981261
Model ind 640 epoch 407 head B head_i_epoch 1 batch 200: avg loss -1.772578 avg loss no lamb -1.772578 time 2019-02-21 11:55:53.427643
last batch sz 160
Pre: time 2019-02-21 11:58:03.556920: 
 	std: 0.054178767
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4931, 0.60358334, 0.6034167, 0.6037, 0.49285]
	train_accs: [0.4931, 0.60358334, 0.6034167, 0.6037, 0.49285]
	best_train_sub_head: 3
	worst: 0.49285
	avg: 0.55933
	best: 0.6037

Starting e_i: 408
Model ind 640 epoch 408 head A head_i_epoch 0 batch 0: avg loss -3.001929 avg loss no lamb -3.001929 time 2019-02-21 11:58:05.774792
Model ind 640 epoch 408 head A head_i_epoch 0 batch 100: avg loss -2.866710 avg loss no lamb -2.866710 time 2019-02-21 12:00:29.752006
Model ind 640 epoch 408 head A head_i_epoch 0 batch 200: avg loss -3.033115 avg loss no lamb -3.033115 time 2019-02-21 12:02:53.939782
last batch sz 160
Model ind 640 epoch 408 head B head_i_epoch 0 batch 0: avg loss -1.721158 avg loss no lamb -1.721158 time 2019-02-21 12:04:38.963510
Model ind 640 epoch 408 head B head_i_epoch 0 batch 100: avg loss -1.627561 avg loss no lamb -1.627561 time 2019-02-21 12:07:03.131927
Model ind 640 epoch 408 head B head_i_epoch 0 batch 200: avg loss -1.729414 avg loss no lamb -1.729414 time 2019-02-21 12:09:27.463678
last batch sz 160
Model ind 640 epoch 408 head B head_i_epoch 1 batch 0: avg loss -1.607993 avg loss no lamb -1.607993 time 2019-02-21 12:11:12.276363
Model ind 640 epoch 408 head B head_i_epoch 1 batch 100: avg loss -1.527513 avg loss no lamb -1.527513 time 2019-02-21 12:13:36.750914
Model ind 640 epoch 408 head B head_i_epoch 1 batch 200: avg loss -1.825364 avg loss no lamb -1.825364 time 2019-02-21 12:16:01.410930
last batch sz 160
Pre: time 2019-02-21 12:18:11.397221: 
 	std: 0.053869165
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49858335, 0.6085167, 0.6078333, 0.60845, 0.49803334]
	train_accs: [0.49858335, 0.6085167, 0.6078333, 0.60845, 0.49803334]
	best_train_sub_head: 1
	worst: 0.49803334
	avg: 0.5642833
	best: 0.6085167

Starting e_i: 409
Model ind 640 epoch 409 head A head_i_epoch 0 batch 0: avg loss -2.990670 avg loss no lamb -2.990670 time 2019-02-21 12:18:13.687923
Model ind 640 epoch 409 head A head_i_epoch 0 batch 100: avg loss -2.909178 avg loss no lamb -2.909178 time 2019-02-21 12:20:38.686197
Model ind 640 epoch 409 head A head_i_epoch 0 batch 200: avg loss -3.080457 avg loss no lamb -3.080457 time 2019-02-21 12:23:03.631558
last batch sz 160
Model ind 640 epoch 409 head B head_i_epoch 0 batch 0: avg loss -1.693320 avg loss no lamb -1.693320 time 2019-02-21 12:24:49.100437
Model ind 640 epoch 409 head B head_i_epoch 0 batch 100: avg loss -1.476782 avg loss no lamb -1.476782 time 2019-02-21 12:27:13.161397
Model ind 640 epoch 409 head B head_i_epoch 0 batch 200: avg loss -1.787910 avg loss no lamb -1.787910 time 2019-02-21 12:29:37.913085
last batch sz 160
Model ind 640 epoch 409 head B head_i_epoch 1 batch 0: avg loss -1.571345 avg loss no lamb -1.571345 time 2019-02-21 12:31:23.081084
Model ind 640 epoch 409 head B head_i_epoch 1 batch 100: avg loss -1.636965 avg loss no lamb -1.636965 time 2019-02-21 12:33:48.084295
Model ind 640 epoch 409 head B head_i_epoch 1 batch 200: avg loss -1.688461 avg loss no lamb -1.688461 time 2019-02-21 12:36:12.920952
last batch sz 160
Pre: time 2019-02-21 12:38:22.830677: 
 	std: 0.053780526
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49686667, 0.6066833, 0.6060333, 0.6066167, 0.49646667]
	train_accs: [0.49686667, 0.6066833, 0.6060333, 0.6066167, 0.49646667]
	best_train_sub_head: 1
	worst: 0.49646667
	avg: 0.5625333
	best: 0.6066833

Starting e_i: 410
Model ind 640 epoch 410 head A head_i_epoch 0 batch 0: avg loss -2.960247 avg loss no lamb -2.960247 time 2019-02-21 12:38:25.015633
Model ind 640 epoch 410 head A head_i_epoch 0 batch 100: avg loss -2.872204 avg loss no lamb -2.872204 time 2019-02-21 12:40:48.803369
Model ind 640 epoch 410 head A head_i_epoch 0 batch 200: avg loss -2.966611 avg loss no lamb -2.966611 time 2019-02-21 12:43:13.329352
last batch sz 160
Model ind 640 epoch 410 head B head_i_epoch 0 batch 0: avg loss -1.659787 avg loss no lamb -1.659787 time 2019-02-21 12:44:58.103783
Model ind 640 epoch 410 head B head_i_epoch 0 batch 100: avg loss -1.655540 avg loss no lamb -1.655540 time 2019-02-21 12:47:22.333573
Model ind 640 epoch 410 head B head_i_epoch 0 batch 200: avg loss -1.720356 avg loss no lamb -1.720356 time 2019-02-21 12:49:46.631705
last batch sz 160
Model ind 640 epoch 410 head B head_i_epoch 1 batch 0: avg loss -1.621023 avg loss no lamb -1.621023 time 2019-02-21 12:51:31.676710
Model ind 640 epoch 410 head B head_i_epoch 1 batch 100: avg loss -1.553686 avg loss no lamb -1.553686 time 2019-02-21 12:53:55.853532
Model ind 640 epoch 410 head B head_i_epoch 1 batch 200: avg loss -1.786579 avg loss no lamb -1.786579 time 2019-02-21 12:56:19.945515
last batch sz 160
Pre: time 2019-02-21 12:58:30.161467: 
 	std: 0.052018505
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50226665, 0.6087, 0.6078333, 0.60858333, 0.5021167]
	train_accs: [0.50226665, 0.6087, 0.6078333, 0.60858333, 0.5021167]
	best_train_sub_head: 1
	worst: 0.5021167
	avg: 0.5659
	best: 0.6087

Starting e_i: 411
Model ind 640 epoch 411 head A head_i_epoch 0 batch 0: avg loss -3.002393 avg loss no lamb -3.002393 time 2019-02-21 12:58:35.793670
Model ind 640 epoch 411 head A head_i_epoch 0 batch 100: avg loss -2.948675 avg loss no lamb -2.948675 time 2019-02-21 13:01:01.065255
Model ind 640 epoch 411 head A head_i_epoch 0 batch 200: avg loss -3.014977 avg loss no lamb -3.014977 time 2019-02-21 13:03:25.590816
last batch sz 160
Model ind 640 epoch 411 head B head_i_epoch 0 batch 0: avg loss -1.657941 avg loss no lamb -1.657941 time 2019-02-21 13:05:10.685390
Model ind 640 epoch 411 head B head_i_epoch 0 batch 100: avg loss -1.534453 avg loss no lamb -1.534453 time 2019-02-21 13:07:35.003460
Model ind 640 epoch 411 head B head_i_epoch 0 batch 200: avg loss -1.787265 avg loss no lamb -1.787265 time 2019-02-21 13:09:59.260014
last batch sz 160
Model ind 640 epoch 411 head B head_i_epoch 1 batch 0: avg loss -1.705867 avg loss no lamb -1.705867 time 2019-02-21 13:11:44.218457
Model ind 640 epoch 411 head B head_i_epoch 1 batch 100: avg loss -1.565236 avg loss no lamb -1.565236 time 2019-02-21 13:14:08.544136
Model ind 640 epoch 411 head B head_i_epoch 1 batch 200: avg loss -1.762845 avg loss no lamb -1.762845 time 2019-02-21 13:16:33.295504
last batch sz 160
Pre: time 2019-02-21 13:18:43.720054: 
 	std: 0.053736217
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5003333, 0.61035, 0.6093, 0.61018336, 0.50018334]
	train_accs: [0.5003333, 0.61035, 0.6093, 0.61018336, 0.50018334]
	best_train_sub_head: 1
	worst: 0.50018334
	avg: 0.56607
	best: 0.61035

Starting e_i: 412
Model ind 640 epoch 412 head A head_i_epoch 0 batch 0: avg loss -2.964013 avg loss no lamb -2.964013 time 2019-02-21 13:18:45.831675
Model ind 640 epoch 412 head A head_i_epoch 0 batch 100: avg loss -2.930394 avg loss no lamb -2.930394 time 2019-02-21 13:21:10.985117
Model ind 640 epoch 412 head A head_i_epoch 0 batch 200: avg loss -3.030643 avg loss no lamb -3.030643 time 2019-02-21 13:23:35.753863
last batch sz 160
Model ind 640 epoch 412 head B head_i_epoch 0 batch 0: avg loss -1.665715 avg loss no lamb -1.665715 time 2019-02-21 13:25:20.795051
Model ind 640 epoch 412 head B head_i_epoch 0 batch 100: avg loss -1.607154 avg loss no lamb -1.607154 time 2019-02-21 13:27:44.844871
Model ind 640 epoch 412 head B head_i_epoch 0 batch 200: avg loss -1.693268 avg loss no lamb -1.693268 time 2019-02-21 13:30:08.500359
last batch sz 160
Model ind 640 epoch 412 head B head_i_epoch 1 batch 0: avg loss -1.594019 avg loss no lamb -1.594019 time 2019-02-21 13:31:53.123836
Model ind 640 epoch 412 head B head_i_epoch 1 batch 100: avg loss -1.605917 avg loss no lamb -1.605917 time 2019-02-21 13:34:17.457153
Model ind 640 epoch 412 head B head_i_epoch 1 batch 200: avg loss -1.745960 avg loss no lamb -1.745960 time 2019-02-21 13:36:41.517921
last batch sz 160
Pre: time 2019-02-21 13:38:51.776182: 
 	std: 0.053187914
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49395, 0.6032, 0.6026, 0.6032, 0.49491668]
	train_accs: [0.49395, 0.6032, 0.6026, 0.6032, 0.49491668]
	best_train_sub_head: 1
	worst: 0.49395
	avg: 0.55957335
	best: 0.6032

Starting e_i: 413
Model ind 640 epoch 413 head A head_i_epoch 0 batch 0: avg loss -2.956354 avg loss no lamb -2.956354 time 2019-02-21 13:38:53.991723
Model ind 640 epoch 413 head A head_i_epoch 0 batch 100: avg loss -2.961530 avg loss no lamb -2.961530 time 2019-02-21 13:41:19.141771
Model ind 640 epoch 413 head A head_i_epoch 0 batch 200: avg loss -3.036671 avg loss no lamb -3.036671 time 2019-02-21 13:43:43.768723
last batch sz 160
Model ind 640 epoch 413 head B head_i_epoch 0 batch 0: avg loss -1.657049 avg loss no lamb -1.657049 time 2019-02-21 13:45:29.604492
Model ind 640 epoch 413 head B head_i_epoch 0 batch 100: avg loss -1.588062 avg loss no lamb -1.588062 time 2019-02-21 13:47:56.230663
Model ind 640 epoch 413 head B head_i_epoch 0 batch 200: avg loss -1.756705 avg loss no lamb -1.756705 time 2019-02-21 13:50:21.219287
last batch sz 160
Model ind 640 epoch 413 head B head_i_epoch 1 batch 0: avg loss -1.663631 avg loss no lamb -1.663631 time 2019-02-21 13:52:06.555766
Model ind 640 epoch 413 head B head_i_epoch 1 batch 100: avg loss -1.580332 avg loss no lamb -1.580332 time 2019-02-21 13:54:30.737632
Model ind 640 epoch 413 head B head_i_epoch 1 batch 200: avg loss -1.739024 avg loss no lamb -1.739024 time 2019-02-21 13:56:55.548971
last batch sz 160
Pre: time 2019-02-21 13:59:05.351816: 
 	std: 0.052790917
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4967, 0.60445, 0.60436666, 0.6048833, 0.49691668]
	train_accs: [0.4967, 0.60445, 0.60436666, 0.6048833, 0.49691668]
	best_train_sub_head: 3
	worst: 0.4967
	avg: 0.56146336
	best: 0.6048833

Starting e_i: 414
Model ind 640 epoch 414 head A head_i_epoch 0 batch 0: avg loss -2.934230 avg loss no lamb -2.934230 time 2019-02-21 13:59:07.900602
Model ind 640 epoch 414 head A head_i_epoch 0 batch 100: avg loss -2.892059 avg loss no lamb -2.892059 time 2019-02-21 14:01:32.123447
Model ind 640 epoch 414 head A head_i_epoch 0 batch 200: avg loss -2.974924 avg loss no lamb -2.974924 time 2019-02-21 14:03:57.558394
last batch sz 160
Model ind 640 epoch 414 head B head_i_epoch 0 batch 0: avg loss -1.593807 avg loss no lamb -1.593807 time 2019-02-21 14:05:42.860491
Model ind 640 epoch 414 head B head_i_epoch 0 batch 100: avg loss -1.534659 avg loss no lamb -1.534659 time 2019-02-21 14:08:07.762381
Model ind 640 epoch 414 head B head_i_epoch 0 batch 200: avg loss -1.835637 avg loss no lamb -1.835637 time 2019-02-21 14:10:32.436662
last batch sz 160
Model ind 640 epoch 414 head B head_i_epoch 1 batch 0: avg loss -1.565093 avg loss no lamb -1.565093 time 2019-02-21 14:12:18.058252
Model ind 640 epoch 414 head B head_i_epoch 1 batch 100: avg loss -1.550843 avg loss no lamb -1.550843 time 2019-02-21 14:14:42.981367
Model ind 640 epoch 414 head B head_i_epoch 1 batch 200: avg loss -1.731219 avg loss no lamb -1.731219 time 2019-02-21 14:17:06.994678
last batch sz 160
Pre: time 2019-02-21 14:19:17.175517: 
 	std: 0.05374238
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4957, 0.60496664, 0.60515, 0.6056833, 0.49543333]
	train_accs: [0.4957, 0.60496664, 0.60515, 0.6056833, 0.49543333]
	best_train_sub_head: 3
	worst: 0.49543333
	avg: 0.5613867
	best: 0.6056833

Starting e_i: 415
Model ind 640 epoch 415 head A head_i_epoch 0 batch 0: avg loss -3.030088 avg loss no lamb -3.030088 time 2019-02-21 14:19:19.468133
Model ind 640 epoch 415 head A head_i_epoch 0 batch 100: avg loss -3.003878 avg loss no lamb -3.003878 time 2019-02-21 14:21:44.768951
Model ind 640 epoch 415 head A head_i_epoch 0 batch 200: avg loss -3.011403 avg loss no lamb -3.011403 time 2019-02-21 14:24:09.121992
last batch sz 160
Model ind 640 epoch 415 head B head_i_epoch 0 batch 0: avg loss -1.658049 avg loss no lamb -1.658049 time 2019-02-21 14:25:54.385097
Model ind 640 epoch 415 head B head_i_epoch 0 batch 100: avg loss -1.606293 avg loss no lamb -1.606293 time 2019-02-21 14:28:19.023357
Model ind 640 epoch 415 head B head_i_epoch 0 batch 200: avg loss -1.766628 avg loss no lamb -1.766628 time 2019-02-21 14:30:43.655946
last batch sz 160
Model ind 640 epoch 415 head B head_i_epoch 1 batch 0: avg loss -1.639949 avg loss no lamb -1.639949 time 2019-02-21 14:32:28.818514
Model ind 640 epoch 415 head B head_i_epoch 1 batch 100: avg loss -1.538879 avg loss no lamb -1.538879 time 2019-02-21 14:34:51.660374
Model ind 640 epoch 415 head B head_i_epoch 1 batch 200: avg loss -1.767916 avg loss no lamb -1.767916 time 2019-02-21 14:37:15.621708
last batch sz 160
Pre: time 2019-02-21 14:39:25.149718: 
 	std: 0.051200453
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50305, 0.60755, 0.60693336, 0.60745, 0.50255]
	train_accs: [0.50305, 0.60755, 0.60693336, 0.60745, 0.50255]
	best_train_sub_head: 1
	worst: 0.50255
	avg: 0.56550664
	best: 0.60755

Starting e_i: 416
Model ind 640 epoch 416 head A head_i_epoch 0 batch 0: avg loss -2.992499 avg loss no lamb -2.992499 time 2019-02-21 14:39:27.435631
Model ind 640 epoch 416 head A head_i_epoch 0 batch 100: avg loss -2.851337 avg loss no lamb -2.851337 time 2019-02-21 14:41:52.070267
Model ind 640 epoch 416 head A head_i_epoch 0 batch 200: avg loss -3.041404 avg loss no lamb -3.041404 time 2019-02-21 14:44:16.814940
last batch sz 160
Model ind 640 epoch 416 head B head_i_epoch 0 batch 0: avg loss -1.591258 avg loss no lamb -1.591258 time 2019-02-21 14:46:02.133911
Model ind 640 epoch 416 head B head_i_epoch 0 batch 100: avg loss -1.509307 avg loss no lamb -1.509307 time 2019-02-21 14:48:28.119920
Model ind 640 epoch 416 head B head_i_epoch 0 batch 200: avg loss -1.734224 avg loss no lamb -1.734224 time 2019-02-21 14:50:53.292842
last batch sz 160
Model ind 640 epoch 416 head B head_i_epoch 1 batch 0: avg loss -1.731650 avg loss no lamb -1.731650 time 2019-02-21 14:52:38.239653
Model ind 640 epoch 416 head B head_i_epoch 1 batch 100: avg loss -1.606517 avg loss no lamb -1.606517 time 2019-02-21 14:55:02.615760
Model ind 640 epoch 416 head B head_i_epoch 1 batch 200: avg loss -1.731690 avg loss no lamb -1.731690 time 2019-02-21 14:57:27.337813
last batch sz 160
Pre: time 2019-02-21 14:59:37.251263: 
 	std: 0.054306876
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49768335, 0.60845, 0.6081, 0.60833335, 0.4972]
	train_accs: [0.49768335, 0.60845, 0.6081, 0.60833335, 0.4972]
	best_train_sub_head: 1
	worst: 0.4972
	avg: 0.56395334
	best: 0.60845

Starting e_i: 417
Model ind 640 epoch 417 head A head_i_epoch 0 batch 0: avg loss -3.015720 avg loss no lamb -3.015720 time 2019-02-21 14:59:39.628016
Model ind 640 epoch 417 head A head_i_epoch 0 batch 100: avg loss -2.919730 avg loss no lamb -2.919730 time 2019-02-21 15:02:04.206677
Model ind 640 epoch 417 head A head_i_epoch 0 batch 200: avg loss -3.004345 avg loss no lamb -3.004345 time 2019-02-21 15:04:28.817477
last batch sz 160
Model ind 640 epoch 417 head B head_i_epoch 0 batch 0: avg loss -1.697194 avg loss no lamb -1.697194 time 2019-02-21 15:06:14.003296
Model ind 640 epoch 417 head B head_i_epoch 0 batch 100: avg loss -1.653777 avg loss no lamb -1.653777 time 2019-02-21 15:08:38.202805
Model ind 640 epoch 417 head B head_i_epoch 0 batch 200: avg loss -1.706182 avg loss no lamb -1.706182 time 2019-02-21 15:11:02.372324
last batch sz 160
Model ind 640 epoch 417 head B head_i_epoch 1 batch 0: avg loss -1.701851 avg loss no lamb -1.701851 time 2019-02-21 15:12:47.432809
Model ind 640 epoch 417 head B head_i_epoch 1 batch 100: avg loss -1.570238 avg loss no lamb -1.570238 time 2019-02-21 15:15:11.839396
Model ind 640 epoch 417 head B head_i_epoch 1 batch 200: avg loss -1.869283 avg loss no lamb -1.869283 time 2019-02-21 15:17:35.880084
last batch sz 160
Pre: time 2019-02-21 15:19:45.523295: 
 	std: 0.052084647
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50268334, 0.6087667, 0.60861665, 0.60906667, 0.50231665]
	train_accs: [0.50268334, 0.6087667, 0.60861665, 0.60906667, 0.50231665]
	best_train_sub_head: 3
	worst: 0.50231665
	avg: 0.56629
	best: 0.60906667

Starting e_i: 418
Model ind 640 epoch 418 head A head_i_epoch 0 batch 0: avg loss -2.960050 avg loss no lamb -2.960050 time 2019-02-21 15:19:47.982202
Model ind 640 epoch 418 head A head_i_epoch 0 batch 100: avg loss -2.943218 avg loss no lamb -2.943218 time 2019-02-21 15:22:12.228461
Model ind 640 epoch 418 head A head_i_epoch 0 batch 200: avg loss -3.069595 avg loss no lamb -3.069595 time 2019-02-21 15:24:36.700880
last batch sz 160
Model ind 640 epoch 418 head B head_i_epoch 0 batch 0: avg loss -1.636759 avg loss no lamb -1.636759 time 2019-02-21 15:26:22.026314
Model ind 640 epoch 418 head B head_i_epoch 0 batch 100: avg loss -1.639196 avg loss no lamb -1.639196 time 2019-02-21 15:28:46.147533
Model ind 640 epoch 418 head B head_i_epoch 0 batch 200: avg loss -1.761127 avg loss no lamb -1.761127 time 2019-02-21 15:31:10.767114
last batch sz 160
Model ind 640 epoch 418 head B head_i_epoch 1 batch 0: avg loss -1.654624 avg loss no lamb -1.654624 time 2019-02-21 15:32:55.588979
Model ind 640 epoch 418 head B head_i_epoch 1 batch 100: avg loss -1.564658 avg loss no lamb -1.564658 time 2019-02-21 15:35:19.467696
Model ind 640 epoch 418 head B head_i_epoch 1 batch 200: avg loss -1.752691 avg loss no lamb -1.752691 time 2019-02-21 15:37:42.892877
last batch sz 160
Pre: time 2019-02-21 15:39:52.008715: 
 	std: 0.055107255
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49486667, 0.6078333, 0.6073167, 0.6070833, 0.49498335]
	train_accs: [0.49486667, 0.6078333, 0.6073167, 0.6070833, 0.49498335]
	best_train_sub_head: 1
	worst: 0.49486667
	avg: 0.5624167
	best: 0.6078333

Starting e_i: 419
Model ind 640 epoch 419 head A head_i_epoch 0 batch 0: avg loss -2.910797 avg loss no lamb -2.910797 time 2019-02-21 15:39:54.509365
Model ind 640 epoch 419 head A head_i_epoch 0 batch 100: avg loss -2.857871 avg loss no lamb -2.857871 time 2019-02-21 15:42:18.218693
Model ind 640 epoch 419 head A head_i_epoch 0 batch 200: avg loss -3.004331 avg loss no lamb -3.004331 time 2019-02-21 15:44:42.348252
last batch sz 160
Model ind 640 epoch 419 head B head_i_epoch 0 batch 0: avg loss -1.619018 avg loss no lamb -1.619018 time 2019-02-21 15:46:28.231472
Model ind 640 epoch 419 head B head_i_epoch 0 batch 100: avg loss -1.630405 avg loss no lamb -1.630405 time 2019-02-21 15:48:52.683341
Model ind 640 epoch 419 head B head_i_epoch 0 batch 200: avg loss -1.839512 avg loss no lamb -1.839512 time 2019-02-21 15:51:16.637458
last batch sz 160
Model ind 640 epoch 419 head B head_i_epoch 1 batch 0: avg loss -1.616964 avg loss no lamb -1.616964 time 2019-02-21 15:53:01.623070
Model ind 640 epoch 419 head B head_i_epoch 1 batch 100: avg loss -1.576683 avg loss no lamb -1.576683 time 2019-02-21 15:55:25.910470
Model ind 640 epoch 419 head B head_i_epoch 1 batch 200: avg loss -1.781325 avg loss no lamb -1.781325 time 2019-02-21 15:57:50.411902
last batch sz 160
Pre: time 2019-02-21 16:00:00.512420: 
 	std: 0.05241122
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49998334, 0.6073667, 0.60715, 0.6071333, 0.50048333]
	train_accs: [0.49998334, 0.6073667, 0.60715, 0.6071333, 0.50048333]
	best_train_sub_head: 1
	worst: 0.49998334
	avg: 0.5644233
	best: 0.6073667

Starting e_i: 420
Model ind 640 epoch 420 head A head_i_epoch 0 batch 0: avg loss -2.937322 avg loss no lamb -2.937322 time 2019-02-21 16:00:02.673669
Model ind 640 epoch 420 head A head_i_epoch 0 batch 100: avg loss -2.912856 avg loss no lamb -2.912856 time 2019-02-21 16:02:27.369751
Model ind 640 epoch 420 head A head_i_epoch 0 batch 200: avg loss -3.010626 avg loss no lamb -3.010626 time 2019-02-21 16:04:52.332861
last batch sz 160
Model ind 640 epoch 420 head B head_i_epoch 0 batch 0: avg loss -1.710274 avg loss no lamb -1.710274 time 2019-02-21 16:06:37.644608
Model ind 640 epoch 420 head B head_i_epoch 0 batch 100: avg loss -1.647636 avg loss no lamb -1.647636 time 2019-02-21 16:09:02.573063
Model ind 640 epoch 420 head B head_i_epoch 0 batch 200: avg loss -1.688532 avg loss no lamb -1.688532 time 2019-02-21 16:11:27.262551
last batch sz 160
Model ind 640 epoch 420 head B head_i_epoch 1 batch 0: avg loss -1.653334 avg loss no lamb -1.653334 time 2019-02-21 16:13:12.059951
Model ind 640 epoch 420 head B head_i_epoch 1 batch 100: avg loss -1.551333 avg loss no lamb -1.551333 time 2019-02-21 16:15:36.636431
Model ind 640 epoch 420 head B head_i_epoch 1 batch 200: avg loss -1.719079 avg loss no lamb -1.719079 time 2019-02-21 16:18:01.684873
last batch sz 160
Pre: time 2019-02-21 16:20:11.846618: 
 	std: 0.05265148
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49943334, 0.6081167, 0.6070167, 0.60715, 0.50048333]
	train_accs: [0.49943334, 0.6081167, 0.6070167, 0.60715, 0.50048333]
	best_train_sub_head: 1
	worst: 0.49943334
	avg: 0.56444
	best: 0.6081167

Starting e_i: 421
Model ind 640 epoch 421 head A head_i_epoch 0 batch 0: avg loss -2.984056 avg loss no lamb -2.984056 time 2019-02-21 16:20:17.755746
Model ind 640 epoch 421 head A head_i_epoch 0 batch 100: avg loss -2.928042 avg loss no lamb -2.928042 time 2019-02-21 16:22:42.554131
Model ind 640 epoch 421 head A head_i_epoch 0 batch 200: avg loss -3.041328 avg loss no lamb -3.041328 time 2019-02-21 16:25:07.527577
last batch sz 160
Model ind 640 epoch 421 head B head_i_epoch 0 batch 0: avg loss -1.699702 avg loss no lamb -1.699702 time 2019-02-21 16:26:52.906318
Model ind 640 epoch 421 head B head_i_epoch 0 batch 100: avg loss -1.574755 avg loss no lamb -1.574755 time 2019-02-21 16:29:17.761530
Model ind 640 epoch 421 head B head_i_epoch 0 batch 200: avg loss -1.709104 avg loss no lamb -1.709104 time 2019-02-21 16:31:42.210743
last batch sz 160
Model ind 640 epoch 421 head B head_i_epoch 1 batch 0: avg loss -1.592572 avg loss no lamb -1.592572 time 2019-02-21 16:33:27.344321
Model ind 640 epoch 421 head B head_i_epoch 1 batch 100: avg loss -1.527004 avg loss no lamb -1.527004 time 2019-02-21 16:35:52.406530
Model ind 640 epoch 421 head B head_i_epoch 1 batch 200: avg loss -1.775641 avg loss no lamb -1.775641 time 2019-02-21 16:38:16.416784
last batch sz 160
Pre: time 2019-02-21 16:40:25.558875: 
 	std: 0.05344032
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49596667, 0.6051, 0.60525, 0.6045, 0.49576667]
	train_accs: [0.49596667, 0.6051, 0.60525, 0.6045, 0.49576667]
	best_train_sub_head: 2
	worst: 0.49576667
	avg: 0.5613166
	best: 0.60525

Starting e_i: 422
Model ind 640 epoch 422 head A head_i_epoch 0 batch 0: avg loss -2.943834 avg loss no lamb -2.943834 time 2019-02-21 16:40:27.801419
Model ind 640 epoch 422 head A head_i_epoch 0 batch 100: avg loss -2.865609 avg loss no lamb -2.865609 time 2019-02-21 16:42:52.070479
Model ind 640 epoch 422 head A head_i_epoch 0 batch 200: avg loss -3.000650 avg loss no lamb -3.000650 time 2019-02-21 16:45:16.335527
last batch sz 160
Model ind 640 epoch 422 head B head_i_epoch 0 batch 0: avg loss -1.650219 avg loss no lamb -1.650219 time 2019-02-21 16:47:01.379243
Model ind 640 epoch 422 head B head_i_epoch 0 batch 100: avg loss -1.583395 avg loss no lamb -1.583395 time 2019-02-21 16:49:26.064446
Model ind 640 epoch 422 head B head_i_epoch 0 batch 200: avg loss -1.721364 avg loss no lamb -1.721364 time 2019-02-21 16:51:50.654965
last batch sz 160
Model ind 640 epoch 422 head B head_i_epoch 1 batch 0: avg loss -1.625573 avg loss no lamb -1.625573 time 2019-02-21 16:53:35.238237
Model ind 640 epoch 422 head B head_i_epoch 1 batch 100: avg loss -1.569653 avg loss no lamb -1.569653 time 2019-02-21 16:55:59.757790
Model ind 640 epoch 422 head B head_i_epoch 1 batch 200: avg loss -1.756266 avg loss no lamb -1.756266 time 2019-02-21 16:58:23.847472
last batch sz 160
Pre: time 2019-02-21 17:00:33.299555: 
 	std: 0.054737862
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49426666, 0.60688335, 0.6070333, 0.60646665, 0.49586666]
	train_accs: [0.49426666, 0.60688335, 0.6070333, 0.60646665, 0.49586666]
	best_train_sub_head: 2
	worst: 0.49426666
	avg: 0.5621034
	best: 0.6070333

Starting e_i: 423
Model ind 640 epoch 423 head A head_i_epoch 0 batch 0: avg loss -2.937505 avg loss no lamb -2.937505 time 2019-02-21 17:00:35.488639
Model ind 640 epoch 423 head A head_i_epoch 0 batch 100: avg loss -2.944674 avg loss no lamb -2.944674 time 2019-02-21 17:03:00.228619
Model ind 640 epoch 423 head A head_i_epoch 0 batch 200: avg loss -3.060467 avg loss no lamb -3.060467 time 2019-02-21 17:05:24.833804
last batch sz 160
Model ind 640 epoch 423 head B head_i_epoch 0 batch 0: avg loss -1.696998 avg loss no lamb -1.696998 time 2019-02-21 17:07:09.968881
Model ind 640 epoch 423 head B head_i_epoch 0 batch 100: avg loss -1.532813 avg loss no lamb -1.532813 time 2019-02-21 17:09:34.260755
Model ind 640 epoch 423 head B head_i_epoch 0 batch 200: avg loss -1.720891 avg loss no lamb -1.720891 time 2019-02-21 17:11:58.661165
last batch sz 160
Model ind 640 epoch 423 head B head_i_epoch 1 batch 0: avg loss -1.629154 avg loss no lamb -1.629154 time 2019-02-21 17:13:43.692391
Model ind 640 epoch 423 head B head_i_epoch 1 batch 100: avg loss -1.500011 avg loss no lamb -1.500011 time 2019-02-21 17:16:07.309255
Model ind 640 epoch 423 head B head_i_epoch 1 batch 200: avg loss -1.810051 avg loss no lamb -1.810051 time 2019-02-21 17:18:31.584964
last batch sz 160
Pre: time 2019-02-21 17:20:41.398974: 
 	std: 0.05392697
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49578333, 0.60641664, 0.6066333, 0.6057, 0.49656665]
	train_accs: [0.49578333, 0.60641664, 0.6066333, 0.6057, 0.49656665]
	best_train_sub_head: 2
	worst: 0.49578333
	avg: 0.56222
	best: 0.6066333

Starting e_i: 424
Model ind 640 epoch 424 head A head_i_epoch 0 batch 0: avg loss -3.009809 avg loss no lamb -3.009809 time 2019-02-21 17:20:43.602066
Model ind 640 epoch 424 head A head_i_epoch 0 batch 100: avg loss -2.967171 avg loss no lamb -2.967171 time 2019-02-21 17:23:08.291236
Model ind 640 epoch 424 head A head_i_epoch 0 batch 200: avg loss -2.975128 avg loss no lamb -2.975128 time 2019-02-21 17:25:32.924118
last batch sz 160
Model ind 640 epoch 424 head B head_i_epoch 0 batch 0: avg loss -1.623628 avg loss no lamb -1.623628 time 2019-02-21 17:27:18.252119
Model ind 640 epoch 424 head B head_i_epoch 0 batch 100: avg loss -1.563066 avg loss no lamb -1.563066 time 2019-02-21 17:29:43.933114
Model ind 640 epoch 424 head B head_i_epoch 0 batch 200: avg loss -1.705579 avg loss no lamb -1.705579 time 2019-02-21 17:32:10.968469
last batch sz 160
Model ind 640 epoch 424 head B head_i_epoch 1 batch 0: avg loss -1.629938 avg loss no lamb -1.629938 time 2019-02-21 17:33:56.764536
Model ind 640 epoch 424 head B head_i_epoch 1 batch 100: avg loss -1.503001 avg loss no lamb -1.503001 time 2019-02-21 17:36:21.271529
Model ind 640 epoch 424 head B head_i_epoch 1 batch 200: avg loss -1.786612 avg loss no lamb -1.786612 time 2019-02-21 17:38:45.252990
last batch sz 160
Pre: time 2019-02-21 17:40:55.000299: 
 	std: 0.052409757
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49995, 0.60763335, 0.60756665, 0.6074833, 0.50121665]
	train_accs: [0.49995, 0.60763335, 0.60756665, 0.6074833, 0.50121665]
	best_train_sub_head: 1
	worst: 0.49995
	avg: 0.56477
	best: 0.60763335

Starting e_i: 425
Model ind 640 epoch 425 head A head_i_epoch 0 batch 0: avg loss -2.970687 avg loss no lamb -2.970687 time 2019-02-21 17:40:57.207470
Model ind 640 epoch 425 head A head_i_epoch 0 batch 100: avg loss -2.874430 avg loss no lamb -2.874430 time 2019-02-21 17:43:21.637447
Model ind 640 epoch 425 head A head_i_epoch 0 batch 200: avg loss -3.043317 avg loss no lamb -3.043317 time 2019-02-21 17:45:46.140936
last batch sz 160
Model ind 640 epoch 425 head B head_i_epoch 0 batch 0: avg loss -1.624179 avg loss no lamb -1.624179 time 2019-02-21 17:47:31.413723
Model ind 640 epoch 425 head B head_i_epoch 0 batch 100: avg loss -1.567153 avg loss no lamb -1.567153 time 2019-02-21 17:49:56.079801
Model ind 640 epoch 425 head B head_i_epoch 0 batch 200: avg loss -1.788116 avg loss no lamb -1.788116 time 2019-02-21 17:52:20.435501
last batch sz 160
Model ind 640 epoch 425 head B head_i_epoch 1 batch 0: avg loss -1.606461 avg loss no lamb -1.606461 time 2019-02-21 17:54:05.734334
Model ind 640 epoch 425 head B head_i_epoch 1 batch 100: avg loss -1.635847 avg loss no lamb -1.635847 time 2019-02-21 17:56:29.719011
Model ind 640 epoch 425 head B head_i_epoch 1 batch 200: avg loss -1.833632 avg loss no lamb -1.833632 time 2019-02-21 17:58:52.933597
last batch sz 160
Pre: time 2019-02-21 18:01:01.731861: 
 	std: 0.053216893
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49973333, 0.60873336, 0.6085, 0.60815, 0.49993333]
	train_accs: [0.49973333, 0.60873336, 0.6085, 0.60815, 0.49993333]
	best_train_sub_head: 1
	worst: 0.49973333
	avg: 0.56500995
	best: 0.60873336

Starting e_i: 426
Model ind 640 epoch 426 head A head_i_epoch 0 batch 0: avg loss -3.012109 avg loss no lamb -3.012109 time 2019-02-21 18:01:04.333089
Model ind 640 epoch 426 head A head_i_epoch 0 batch 100: avg loss -2.892014 avg loss no lamb -2.892014 time 2019-02-21 18:03:28.646703
Model ind 640 epoch 426 head A head_i_epoch 0 batch 200: avg loss -2.999590 avg loss no lamb -2.999590 time 2019-02-21 18:05:53.021045
last batch sz 160
Model ind 640 epoch 426 head B head_i_epoch 0 batch 0: avg loss -1.658627 avg loss no lamb -1.658627 time 2019-02-21 18:07:38.171019
Model ind 640 epoch 426 head B head_i_epoch 0 batch 100: avg loss -1.545610 avg loss no lamb -1.545610 time 2019-02-21 18:10:03.007950
Model ind 640 epoch 426 head B head_i_epoch 0 batch 200: avg loss -1.731899 avg loss no lamb -1.731899 time 2019-02-21 18:12:26.400568
last batch sz 160
Model ind 640 epoch 426 head B head_i_epoch 1 batch 0: avg loss -1.671995 avg loss no lamb -1.671995 time 2019-02-21 18:14:10.690899
Model ind 640 epoch 426 head B head_i_epoch 1 batch 100: avg loss -1.497979 avg loss no lamb -1.497979 time 2019-02-21 18:16:36.839700
Model ind 640 epoch 426 head B head_i_epoch 1 batch 200: avg loss -1.719076 avg loss no lamb -1.719076 time 2019-02-21 18:19:01.722188
last batch sz 160
Pre: time 2019-02-21 18:21:12.263460: 
 	std: 0.05335956
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49806666, 0.6070333, 0.60711664, 0.6067333, 0.49801666]
	train_accs: [0.49806666, 0.6070333, 0.60711664, 0.6067333, 0.49801666]
	best_train_sub_head: 2
	worst: 0.49801666
	avg: 0.5633933
	best: 0.60711664

Starting e_i: 427
Model ind 640 epoch 427 head A head_i_epoch 0 batch 0: avg loss -3.000161 avg loss no lamb -3.000161 time 2019-02-21 18:21:14.511057
Model ind 640 epoch 427 head A head_i_epoch 0 batch 100: avg loss -2.933252 avg loss no lamb -2.933252 time 2019-02-21 18:23:39.207677
Model ind 640 epoch 427 head A head_i_epoch 0 batch 200: avg loss -2.975389 avg loss no lamb -2.975389 time 2019-02-21 18:26:04.189973
last batch sz 160
Model ind 640 epoch 427 head B head_i_epoch 0 batch 0: avg loss -1.711305 avg loss no lamb -1.711305 time 2019-02-21 18:27:49.588663
Model ind 640 epoch 427 head B head_i_epoch 0 batch 100: avg loss -1.515030 avg loss no lamb -1.515030 time 2019-02-21 18:30:13.553166
Model ind 640 epoch 427 head B head_i_epoch 0 batch 200: avg loss -1.817866 avg loss no lamb -1.817866 time 2019-02-21 18:32:37.587090
last batch sz 160
Model ind 640 epoch 427 head B head_i_epoch 1 batch 0: avg loss -1.695189 avg loss no lamb -1.695189 time 2019-02-21 18:34:22.425346
Model ind 640 epoch 427 head B head_i_epoch 1 batch 100: avg loss -1.585611 avg loss no lamb -1.585611 time 2019-02-21 18:36:46.466865
Model ind 640 epoch 427 head B head_i_epoch 1 batch 200: avg loss -1.767128 avg loss no lamb -1.767128 time 2019-02-21 18:39:10.379968
last batch sz 160
Pre: time 2019-02-21 18:41:20.309634: 
 	std: 0.05284539
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49903333, 0.60691667, 0.60726666, 0.60665, 0.49911666]
	train_accs: [0.49903333, 0.60691667, 0.60726666, 0.60665, 0.49911666]
	best_train_sub_head: 2
	worst: 0.49903333
	avg: 0.56379664
	best: 0.60726666

Starting e_i: 428
Model ind 640 epoch 428 head A head_i_epoch 0 batch 0: avg loss -2.981814 avg loss no lamb -2.981814 time 2019-02-21 18:41:22.482563
Model ind 640 epoch 428 head A head_i_epoch 0 batch 100: avg loss -2.866438 avg loss no lamb -2.866438 time 2019-02-21 18:43:46.722067
Model ind 640 epoch 428 head A head_i_epoch 0 batch 200: avg loss -3.098877 avg loss no lamb -3.098877 time 2019-02-21 18:46:11.504765
last batch sz 160
Model ind 640 epoch 428 head B head_i_epoch 0 batch 0: avg loss -1.786118 avg loss no lamb -1.786118 time 2019-02-21 18:47:56.858529
Model ind 640 epoch 428 head B head_i_epoch 0 batch 100: avg loss -1.535326 avg loss no lamb -1.535326 time 2019-02-21 18:50:21.822345
Model ind 640 epoch 428 head B head_i_epoch 0 batch 200: avg loss -1.828135 avg loss no lamb -1.828135 time 2019-02-21 18:52:46.366272
last batch sz 160
Model ind 640 epoch 428 head B head_i_epoch 1 batch 0: avg loss -1.744888 avg loss no lamb -1.744888 time 2019-02-21 18:54:31.932988
Model ind 640 epoch 428 head B head_i_epoch 1 batch 100: avg loss -1.517801 avg loss no lamb -1.517801 time 2019-02-21 18:56:56.700930
Model ind 640 epoch 428 head B head_i_epoch 1 batch 200: avg loss -1.744241 avg loss no lamb -1.744241 time 2019-02-21 18:59:22.105260
last batch sz 160
Pre: time 2019-02-21 19:01:32.536390: 
 	std: 0.053719938
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49453333, 0.6038333, 0.6037833, 0.6033667, 0.49348333]
	train_accs: [0.49453333, 0.6038333, 0.6037833, 0.6033667, 0.49348333]
	best_train_sub_head: 1
	worst: 0.49348333
	avg: 0.55979997
	best: 0.6038333

Starting e_i: 429
Model ind 640 epoch 429 head A head_i_epoch 0 batch 0: avg loss -2.962357 avg loss no lamb -2.962357 time 2019-02-21 19:01:34.639796
Model ind 640 epoch 429 head A head_i_epoch 0 batch 100: avg loss -2.899716 avg loss no lamb -2.899716 time 2019-02-21 19:03:58.683335
Model ind 640 epoch 429 head A head_i_epoch 0 batch 200: avg loss -3.074722 avg loss no lamb -3.074722 time 2019-02-21 19:06:23.343148
last batch sz 160
Model ind 640 epoch 429 head B head_i_epoch 0 batch 0: avg loss -1.649393 avg loss no lamb -1.649393 time 2019-02-21 19:08:09.082521
Model ind 640 epoch 429 head B head_i_epoch 0 batch 100: avg loss -1.710237 avg loss no lamb -1.710237 time 2019-02-21 19:10:33.396757
Model ind 640 epoch 429 head B head_i_epoch 0 batch 200: avg loss -1.786438 avg loss no lamb -1.786438 time 2019-02-21 19:12:57.890023
last batch sz 160
Model ind 640 epoch 429 head B head_i_epoch 1 batch 0: avg loss -1.691318 avg loss no lamb -1.691318 time 2019-02-21 19:14:43.120107
Model ind 640 epoch 429 head B head_i_epoch 1 batch 100: avg loss -1.667542 avg loss no lamb -1.667542 time 2019-02-21 19:17:07.733228
Model ind 640 epoch 429 head B head_i_epoch 1 batch 200: avg loss -1.769092 avg loss no lamb -1.769092 time 2019-02-21 19:19:32.479336
last batch sz 160
Pre: time 2019-02-21 19:21:41.789184: 
 	std: 0.05227149
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50226665, 0.6084833, 0.6084667, 0.60866666, 0.5014167]
	train_accs: [0.50226665, 0.6084833, 0.6084667, 0.60866666, 0.5014167]
	best_train_sub_head: 3
	worst: 0.5014167
	avg: 0.56586
	best: 0.60866666

Starting e_i: 430
Model ind 640 epoch 430 head A head_i_epoch 0 batch 0: avg loss -2.985673 avg loss no lamb -2.985673 time 2019-02-21 19:21:43.987030
Model ind 640 epoch 430 head A head_i_epoch 0 batch 100: avg loss -2.971330 avg loss no lamb -2.971330 time 2019-02-21 19:24:08.418804
Model ind 640 epoch 430 head A head_i_epoch 0 batch 200: avg loss -3.021590 avg loss no lamb -3.021590 time 2019-02-21 19:26:34.401945
last batch sz 160
Model ind 640 epoch 430 head B head_i_epoch 0 batch 0: avg loss -1.696394 avg loss no lamb -1.696394 time 2019-02-21 19:28:21.782284
Model ind 640 epoch 430 head B head_i_epoch 0 batch 100: avg loss -1.522013 avg loss no lamb -1.522013 time 2019-02-21 19:30:49.192664
Model ind 640 epoch 430 head B head_i_epoch 0 batch 200: avg loss -1.713292 avg loss no lamb -1.713292 time 2019-02-21 19:33:16.575924
last batch sz 160
Model ind 640 epoch 430 head B head_i_epoch 1 batch 0: avg loss -1.642108 avg loss no lamb -1.642108 time 2019-02-21 19:35:03.703286
Model ind 640 epoch 430 head B head_i_epoch 1 batch 100: avg loss -1.557216 avg loss no lamb -1.557216 time 2019-02-21 19:37:31.423318
Model ind 640 epoch 430 head B head_i_epoch 1 batch 200: avg loss -1.764374 avg loss no lamb -1.764374 time 2019-02-21 19:39:59.236088
last batch sz 160
Pre: time 2019-02-21 19:42:11.384772: 
 	std: 0.0523833
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49743333, 0.60373336, 0.60395, 0.60328335, 0.49603334]
	train_accs: [0.49743333, 0.60373336, 0.60395, 0.60328335, 0.49603334]
	best_train_sub_head: 2
	worst: 0.49603334
	avg: 0.56088674
	best: 0.60395

Starting e_i: 431
Model ind 640 epoch 431 head A head_i_epoch 0 batch 0: avg loss -2.925406 avg loss no lamb -2.925406 time 2019-02-21 19:42:18.037486
Model ind 640 epoch 431 head A head_i_epoch 0 batch 100: avg loss -2.916701 avg loss no lamb -2.916701 time 2019-02-21 19:44:44.571175
Model ind 640 epoch 431 head A head_i_epoch 0 batch 200: avg loss -3.074657 avg loss no lamb -3.074657 time 2019-02-21 19:47:09.216892
last batch sz 160
Model ind 640 epoch 431 head B head_i_epoch 0 batch 0: avg loss -1.712278 avg loss no lamb -1.712278 time 2019-02-21 19:48:54.663192
Model ind 640 epoch 431 head B head_i_epoch 0 batch 100: avg loss -1.523586 avg loss no lamb -1.523586 time 2019-02-21 19:51:19.497989
Model ind 640 epoch 431 head B head_i_epoch 0 batch 200: avg loss -1.770790 avg loss no lamb -1.770790 time 2019-02-21 19:53:43.681325
last batch sz 160
Model ind 640 epoch 431 head B head_i_epoch 1 batch 0: avg loss -1.710094 avg loss no lamb -1.710094 time 2019-02-21 19:55:29.173871
Model ind 640 epoch 431 head B head_i_epoch 1 batch 100: avg loss -1.567519 avg loss no lamb -1.567519 time 2019-02-21 19:57:54.336458
Model ind 640 epoch 431 head B head_i_epoch 1 batch 200: avg loss -1.706969 avg loss no lamb -1.706969 time 2019-02-21 20:00:18.579351
last batch sz 160
Pre: time 2019-02-21 20:02:28.247182: 
 	std: 0.0531223
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5002, 0.6081667, 0.60855, 0.60793334, 0.49936667]
	train_accs: [0.5002, 0.6081667, 0.60855, 0.60793334, 0.49936667]
	best_train_sub_head: 2
	worst: 0.49936667
	avg: 0.56484336
	best: 0.60855

Starting e_i: 432
Model ind 640 epoch 432 head A head_i_epoch 0 batch 0: avg loss -3.030275 avg loss no lamb -3.030275 time 2019-02-21 20:02:30.409290
Model ind 640 epoch 432 head A head_i_epoch 0 batch 100: avg loss -2.941935 avg loss no lamb -2.941935 time 2019-02-21 20:04:55.027488
Model ind 640 epoch 432 head A head_i_epoch 0 batch 200: avg loss -3.036917 avg loss no lamb -3.036917 time 2019-02-21 20:07:19.836295
last batch sz 160
Model ind 640 epoch 432 head B head_i_epoch 0 batch 0: avg loss -1.670880 avg loss no lamb -1.670880 time 2019-02-21 20:09:05.108463
Model ind 640 epoch 432 head B head_i_epoch 0 batch 100: avg loss -1.543995 avg loss no lamb -1.543995 time 2019-02-21 20:11:29.694473
Model ind 640 epoch 432 head B head_i_epoch 0 batch 200: avg loss -1.728474 avg loss no lamb -1.728474 time 2019-02-21 20:13:53.969836
last batch sz 160
Model ind 640 epoch 432 head B head_i_epoch 1 batch 0: avg loss -1.631481 avg loss no lamb -1.631481 time 2019-02-21 20:15:38.966936
Model ind 640 epoch 432 head B head_i_epoch 1 batch 100: avg loss -1.567741 avg loss no lamb -1.567741 time 2019-02-21 20:18:03.215317
Model ind 640 epoch 432 head B head_i_epoch 1 batch 200: avg loss -1.776936 avg loss no lamb -1.776936 time 2019-02-21 20:20:27.851691
last batch sz 160
Pre: time 2019-02-21 20:22:37.666460: 
 	std: 0.054120846
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49821666, 0.6081, 0.60786664, 0.60786664, 0.49673334]
	train_accs: [0.49821666, 0.6081, 0.60786664, 0.60786664, 0.49673334]
	best_train_sub_head: 1
	worst: 0.49673334
	avg: 0.5637567
	best: 0.6081

Starting e_i: 433
Model ind 640 epoch 433 head A head_i_epoch 0 batch 0: avg loss -2.983992 avg loss no lamb -2.983992 time 2019-02-21 20:22:39.860776
Model ind 640 epoch 433 head A head_i_epoch 0 batch 100: avg loss -2.874235 avg loss no lamb -2.874235 time 2019-02-21 20:25:05.044818
Model ind 640 epoch 433 head A head_i_epoch 0 batch 200: avg loss -2.954154 avg loss no lamb -2.954154 time 2019-02-21 20:27:30.110209
last batch sz 160
Model ind 640 epoch 433 head B head_i_epoch 0 batch 0: avg loss -1.754111 avg loss no lamb -1.754111 time 2019-02-21 20:29:15.129258
Model ind 640 epoch 433 head B head_i_epoch 0 batch 100: avg loss -1.595311 avg loss no lamb -1.595311 time 2019-02-21 20:31:39.611044
Model ind 640 epoch 433 head B head_i_epoch 0 batch 200: avg loss -1.765024 avg loss no lamb -1.765024 time 2019-02-21 20:34:04.358289
last batch sz 160
Model ind 640 epoch 433 head B head_i_epoch 1 batch 0: avg loss -1.632298 avg loss no lamb -1.632298 time 2019-02-21 20:35:49.249701
Model ind 640 epoch 433 head B head_i_epoch 1 batch 100: avg loss -1.561606 avg loss no lamb -1.561606 time 2019-02-21 20:38:12.945489
Model ind 640 epoch 433 head B head_i_epoch 1 batch 200: avg loss -1.774509 avg loss no lamb -1.774509 time 2019-02-21 20:40:37.194813
last batch sz 160
Pre: time 2019-02-21 20:42:47.952444: 
 	std: 0.053540587
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49926665, 0.60871667, 0.6088667, 0.60863334, 0.49963334]
	train_accs: [0.49926665, 0.60871667, 0.6088667, 0.60863334, 0.49963334]
	best_train_sub_head: 2
	worst: 0.49926665
	avg: 0.5650233
	best: 0.6088667

Starting e_i: 434
Model ind 640 epoch 434 head A head_i_epoch 0 batch 0: avg loss -2.918189 avg loss no lamb -2.918189 time 2019-02-21 20:42:50.145460
Model ind 640 epoch 434 head A head_i_epoch 0 batch 100: avg loss -2.915775 avg loss no lamb -2.915775 time 2019-02-21 20:45:15.157399
Model ind 640 epoch 434 head A head_i_epoch 0 batch 200: avg loss -3.024559 avg loss no lamb -3.024559 time 2019-02-21 20:47:40.080562
last batch sz 160
Model ind 640 epoch 434 head B head_i_epoch 0 batch 0: avg loss -1.648168 avg loss no lamb -1.648168 time 2019-02-21 20:49:25.179055
Model ind 640 epoch 434 head B head_i_epoch 0 batch 100: avg loss -1.603147 avg loss no lamb -1.603147 time 2019-02-21 20:51:48.545699
Model ind 640 epoch 434 head B head_i_epoch 0 batch 200: avg loss -1.816919 avg loss no lamb -1.816919 time 2019-02-21 20:54:11.674958
last batch sz 160
Model ind 640 epoch 434 head B head_i_epoch 1 batch 0: avg loss -1.664167 avg loss no lamb -1.664167 time 2019-02-21 20:55:56.230811
Model ind 640 epoch 434 head B head_i_epoch 1 batch 100: avg loss -1.552122 avg loss no lamb -1.552122 time 2019-02-21 20:58:20.126247
Model ind 640 epoch 434 head B head_i_epoch 1 batch 200: avg loss -1.771920 avg loss no lamb -1.771920 time 2019-02-21 21:00:44.291744
last batch sz 160
Pre: time 2019-02-21 21:02:54.351446: 
 	std: 0.051248804
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5000833, 0.60475, 0.6045667, 0.60471666, 0.50005]
	train_accs: [0.5000833, 0.60475, 0.6045667, 0.60471666, 0.50005]
	best_train_sub_head: 1
	worst: 0.50005
	avg: 0.56283337
	best: 0.60475

Starting e_i: 435
Model ind 640 epoch 435 head A head_i_epoch 0 batch 0: avg loss -3.010479 avg loss no lamb -3.010479 time 2019-02-21 21:02:56.578967
Model ind 640 epoch 435 head A head_i_epoch 0 batch 100: avg loss -2.971427 avg loss no lamb -2.971427 time 2019-02-21 21:05:21.234648
Model ind 640 epoch 435 head A head_i_epoch 0 batch 200: avg loss -3.058812 avg loss no lamb -3.058812 time 2019-02-21 21:07:45.803439
last batch sz 160
Model ind 640 epoch 435 head B head_i_epoch 0 batch 0: avg loss -1.691953 avg loss no lamb -1.691953 time 2019-02-21 21:09:30.935659
Model ind 640 epoch 435 head B head_i_epoch 0 batch 100: avg loss -1.563871 avg loss no lamb -1.563871 time 2019-02-21 21:11:54.829390
Model ind 640 epoch 435 head B head_i_epoch 0 batch 200: avg loss -1.767931 avg loss no lamb -1.767931 time 2019-02-21 21:14:18.784192
last batch sz 160
Model ind 640 epoch 435 head B head_i_epoch 1 batch 0: avg loss -1.571386 avg loss no lamb -1.571386 time 2019-02-21 21:16:02.767640
Model ind 640 epoch 435 head B head_i_epoch 1 batch 100: avg loss -1.483953 avg loss no lamb -1.483953 time 2019-02-21 21:18:26.474124
Model ind 640 epoch 435 head B head_i_epoch 1 batch 200: avg loss -1.765374 avg loss no lamb -1.765374 time 2019-02-21 21:20:50.638170
last batch sz 160
Pre: time 2019-02-21 21:23:00.091728: 
 	std: 0.053188283
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4983, 0.6067, 0.607, 0.60648334, 0.49801666]
	train_accs: [0.4983, 0.6067, 0.607, 0.60648334, 0.49801666]
	best_train_sub_head: 2
	worst: 0.49801666
	avg: 0.5633
	best: 0.607

Starting e_i: 436
Model ind 640 epoch 436 head A head_i_epoch 0 batch 0: avg loss -2.998384 avg loss no lamb -2.998384 time 2019-02-21 21:23:02.656924
Model ind 640 epoch 436 head A head_i_epoch 0 batch 100: avg loss -2.882307 avg loss no lamb -2.882307 time 2019-02-21 21:25:27.242512
Model ind 640 epoch 436 head A head_i_epoch 0 batch 200: avg loss -2.964363 avg loss no lamb -2.964363 time 2019-02-21 21:27:51.334938
last batch sz 160
Model ind 640 epoch 436 head B head_i_epoch 0 batch 0: avg loss -1.679073 avg loss no lamb -1.679073 time 2019-02-21 21:29:36.796906
Model ind 640 epoch 436 head B head_i_epoch 0 batch 100: avg loss -1.477666 avg loss no lamb -1.477666 time 2019-02-21 21:32:01.123015
Model ind 640 epoch 436 head B head_i_epoch 0 batch 200: avg loss -1.769822 avg loss no lamb -1.769822 time 2019-02-21 21:34:25.831167
last batch sz 160
Model ind 640 epoch 436 head B head_i_epoch 1 batch 0: avg loss -1.706925 avg loss no lamb -1.706925 time 2019-02-21 21:36:10.646298
Model ind 640 epoch 436 head B head_i_epoch 1 batch 100: avg loss -1.614242 avg loss no lamb -1.614242 time 2019-02-21 21:38:34.930192
Model ind 640 epoch 436 head B head_i_epoch 1 batch 200: avg loss -1.697077 avg loss no lamb -1.697077 time 2019-02-21 21:40:59.269377
last batch sz 160
Pre: time 2019-02-21 21:43:08.707445: 
 	std: 0.052345198
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5032, 0.60833335, 0.60871667, 0.60885, 0.5004]
	train_accs: [0.5032, 0.60833335, 0.60871667, 0.60885, 0.5004]
	best_train_sub_head: 3
	worst: 0.5004
	avg: 0.5659
	best: 0.60885

Starting e_i: 437
Model ind 640 epoch 437 head A head_i_epoch 0 batch 0: avg loss -2.933923 avg loss no lamb -2.933923 time 2019-02-21 21:43:10.872009
Model ind 640 epoch 437 head A head_i_epoch 0 batch 100: avg loss -2.925882 avg loss no lamb -2.925882 time 2019-02-21 21:45:35.696946
Model ind 640 epoch 437 head A head_i_epoch 0 batch 200: avg loss -3.028931 avg loss no lamb -3.028931 time 2019-02-21 21:48:00.640507
last batch sz 160
Model ind 640 epoch 437 head B head_i_epoch 0 batch 0: avg loss -1.617254 avg loss no lamb -1.617254 time 2019-02-21 21:49:46.087134
Model ind 640 epoch 437 head B head_i_epoch 0 batch 100: avg loss -1.582314 avg loss no lamb -1.582314 time 2019-02-21 21:52:10.955993
Model ind 640 epoch 437 head B head_i_epoch 0 batch 200: avg loss -1.733813 avg loss no lamb -1.733813 time 2019-02-21 21:54:35.060781
last batch sz 160
Model ind 640 epoch 437 head B head_i_epoch 1 batch 0: avg loss -1.676757 avg loss no lamb -1.676757 time 2019-02-21 21:56:20.460089
Model ind 640 epoch 437 head B head_i_epoch 1 batch 100: avg loss -1.520906 avg loss no lamb -1.520906 time 2019-02-21 21:58:44.993104
Model ind 640 epoch 437 head B head_i_epoch 1 batch 200: avg loss -1.716195 avg loss no lamb -1.716195 time 2019-02-21 22:01:09.742781
last batch sz 160
Pre: time 2019-02-21 22:03:19.852117: 
 	std: 0.05399229
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49696666, 0.6066833, 0.60643333, 0.606, 0.49536666]
	train_accs: [0.49696666, 0.6066833, 0.60643333, 0.606, 0.49536666]
	best_train_sub_head: 1
	worst: 0.49536666
	avg: 0.56228995
	best: 0.6066833

Starting e_i: 438
Model ind 640 epoch 438 head A head_i_epoch 0 batch 0: avg loss -2.998664 avg loss no lamb -2.998664 time 2019-02-21 22:03:22.083723
Model ind 640 epoch 438 head A head_i_epoch 0 batch 100: avg loss -2.884256 avg loss no lamb -2.884256 time 2019-02-21 22:05:46.279411
Model ind 640 epoch 438 head A head_i_epoch 0 batch 200: avg loss -3.017853 avg loss no lamb -3.017853 time 2019-02-21 22:08:10.153924
last batch sz 160
Model ind 640 epoch 438 head B head_i_epoch 0 batch 0: avg loss -1.715889 avg loss no lamb -1.715889 time 2019-02-21 22:09:55.646833
Model ind 640 epoch 438 head B head_i_epoch 0 batch 100: avg loss -1.586615 avg loss no lamb -1.586615 time 2019-02-21 22:12:20.549012
Model ind 640 epoch 438 head B head_i_epoch 0 batch 200: avg loss -1.757681 avg loss no lamb -1.757681 time 2019-02-21 22:14:44.692990
last batch sz 160
Model ind 640 epoch 438 head B head_i_epoch 1 batch 0: avg loss -1.700026 avg loss no lamb -1.700026 time 2019-02-21 22:16:29.771547
Model ind 640 epoch 438 head B head_i_epoch 1 batch 100: avg loss -1.560378 avg loss no lamb -1.560378 time 2019-02-21 22:18:54.943555
Model ind 640 epoch 438 head B head_i_epoch 1 batch 200: avg loss -1.777723 avg loss no lamb -1.777723 time 2019-02-21 22:21:19.497290
last batch sz 160
Pre: time 2019-02-21 22:23:29.553792: 
 	std: 0.05466645
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49933332, 0.61086667, 0.6105667, 0.61035, 0.49868333]
	train_accs: [0.49933332, 0.61086667, 0.6105667, 0.61035, 0.49868333]
	best_train_sub_head: 1
	worst: 0.49868333
	avg: 0.56596005
	best: 0.61086667

Starting e_i: 439
Model ind 640 epoch 439 head A head_i_epoch 0 batch 0: avg loss -2.962438 avg loss no lamb -2.962438 time 2019-02-21 22:23:31.741448
Model ind 640 epoch 439 head A head_i_epoch 0 batch 100: avg loss -2.843624 avg loss no lamb -2.843624 time 2019-02-21 22:25:56.488305
Model ind 640 epoch 439 head A head_i_epoch 0 batch 200: avg loss -3.018480 avg loss no lamb -3.018480 time 2019-02-21 22:28:20.875408
last batch sz 160
Model ind 640 epoch 439 head B head_i_epoch 0 batch 0: avg loss -1.756024 avg loss no lamb -1.756024 time 2019-02-21 22:30:05.802735
Model ind 640 epoch 439 head B head_i_epoch 0 batch 100: avg loss -1.636753 avg loss no lamb -1.636753 time 2019-02-21 22:32:30.524321
Model ind 640 epoch 439 head B head_i_epoch 0 batch 200: avg loss -1.778912 avg loss no lamb -1.778912 time 2019-02-21 22:34:55.363474
last batch sz 160
Model ind 640 epoch 439 head B head_i_epoch 1 batch 0: avg loss -1.701393 avg loss no lamb -1.701393 time 2019-02-21 22:36:40.714550
Model ind 640 epoch 439 head B head_i_epoch 1 batch 100: avg loss -1.524704 avg loss no lamb -1.524704 time 2019-02-21 22:39:04.841825
Model ind 640 epoch 439 head B head_i_epoch 1 batch 200: avg loss -1.810798 avg loss no lamb -1.810798 time 2019-02-21 22:41:29.276084
last batch sz 160
Pre: time 2019-02-21 22:43:38.871759: 
 	std: 0.054060366
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49786666, 0.60833335, 0.60801667, 0.60835, 0.4979]
	train_accs: [0.49786666, 0.60833335, 0.60801667, 0.60835, 0.4979]
	best_train_sub_head: 3
	worst: 0.49786666
	avg: 0.56409335
	best: 0.60835

Starting e_i: 440
Model ind 640 epoch 440 head A head_i_epoch 0 batch 0: avg loss -3.042173 avg loss no lamb -3.042173 time 2019-02-21 22:43:41.049473
Model ind 640 epoch 440 head A head_i_epoch 0 batch 100: avg loss -2.877120 avg loss no lamb -2.877120 time 2019-02-21 22:46:05.068712
Model ind 640 epoch 440 head A head_i_epoch 0 batch 200: avg loss -3.066339 avg loss no lamb -3.066339 time 2019-02-21 22:48:28.980920
last batch sz 160
Model ind 640 epoch 440 head B head_i_epoch 0 batch 0: avg loss -1.726015 avg loss no lamb -1.726015 time 2019-02-21 22:50:13.982013
Model ind 640 epoch 440 head B head_i_epoch 0 batch 100: avg loss -1.552481 avg loss no lamb -1.552481 time 2019-02-21 22:52:39.002967
Model ind 640 epoch 440 head B head_i_epoch 0 batch 200: avg loss -1.800333 avg loss no lamb -1.800333 time 2019-02-21 22:55:03.458140
last batch sz 160
Model ind 640 epoch 440 head B head_i_epoch 1 batch 0: avg loss -1.607271 avg loss no lamb -1.607271 time 2019-02-21 22:56:48.569278
Model ind 640 epoch 440 head B head_i_epoch 1 batch 100: avg loss -1.509883 avg loss no lamb -1.509883 time 2019-02-21 22:59:13.177426
Model ind 640 epoch 440 head B head_i_epoch 1 batch 200: avg loss -1.769304 avg loss no lamb -1.769304 time 2019-02-21 23:01:36.757910
last batch sz 160
Pre: time 2019-02-21 23:03:47.196337: 
 	std: 0.051005967
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50408334, 0.60798335, 0.6076, 0.6077833, 0.5032667]
	train_accs: [0.50408334, 0.60798335, 0.6076, 0.6077833, 0.5032667]
	best_train_sub_head: 1
	worst: 0.5032667
	avg: 0.56614333
	best: 0.60798335

Starting e_i: 441
Model ind 640 epoch 441 head A head_i_epoch 0 batch 0: avg loss -3.003177 avg loss no lamb -3.003177 time 2019-02-21 23:03:53.672358
Model ind 640 epoch 441 head A head_i_epoch 0 batch 100: avg loss -2.891937 avg loss no lamb -2.891937 time 2019-02-21 23:06:18.202538
Model ind 640 epoch 441 head A head_i_epoch 0 batch 200: avg loss -3.067532 avg loss no lamb -3.067532 time 2019-02-21 23:08:43.091565
last batch sz 160
Model ind 640 epoch 441 head B head_i_epoch 0 batch 0: avg loss -1.740421 avg loss no lamb -1.740421 time 2019-02-21 23:10:28.554663
Model ind 640 epoch 441 head B head_i_epoch 0 batch 100: avg loss -1.602941 avg loss no lamb -1.602941 time 2019-02-21 23:12:53.150782
Model ind 640 epoch 441 head B head_i_epoch 0 batch 200: avg loss -1.832091 avg loss no lamb -1.832091 time 2019-02-21 23:15:17.650517
last batch sz 160
Model ind 640 epoch 441 head B head_i_epoch 1 batch 0: avg loss -1.692411 avg loss no lamb -1.692411 time 2019-02-21 23:17:03.081255
Model ind 640 epoch 441 head B head_i_epoch 1 batch 100: avg loss -1.597214 avg loss no lamb -1.597214 time 2019-02-21 23:19:27.996320
Model ind 640 epoch 441 head B head_i_epoch 1 batch 200: avg loss -1.750910 avg loss no lamb -1.750910 time 2019-02-21 23:21:52.563039
last batch sz 160
Pre: time 2019-02-21 23:24:01.897526: 
 	std: 0.051324215
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5028333, 0.6081, 0.60755, 0.6073667, 0.50298333]
	train_accs: [0.5028333, 0.6081, 0.60755, 0.6073667, 0.50298333]
	best_train_sub_head: 1
	worst: 0.5028333
	avg: 0.5657667
	best: 0.6081

Starting e_i: 442
Model ind 640 epoch 442 head A head_i_epoch 0 batch 0: avg loss -2.990850 avg loss no lamb -2.990850 time 2019-02-21 23:24:04.067864
Model ind 640 epoch 442 head A head_i_epoch 0 batch 100: avg loss -2.890010 avg loss no lamb -2.890010 time 2019-02-21 23:26:28.557410
Model ind 640 epoch 442 head A head_i_epoch 0 batch 200: avg loss -2.935274 avg loss no lamb -2.935274 time 2019-02-21 23:28:53.010826
last batch sz 160
Model ind 640 epoch 442 head B head_i_epoch 0 batch 0: avg loss -1.689859 avg loss no lamb -1.689859 time 2019-02-21 23:30:38.025461
Model ind 640 epoch 442 head B head_i_epoch 0 batch 100: avg loss -1.544399 avg loss no lamb -1.544399 time 2019-02-21 23:33:01.933601
Model ind 640 epoch 442 head B head_i_epoch 0 batch 200: avg loss -1.692828 avg loss no lamb -1.692828 time 2019-02-21 23:35:26.503925
last batch sz 160
Model ind 640 epoch 442 head B head_i_epoch 1 batch 0: avg loss -1.701774 avg loss no lamb -1.701774 time 2019-02-21 23:37:11.898553
Model ind 640 epoch 442 head B head_i_epoch 1 batch 100: avg loss -1.631497 avg loss no lamb -1.631497 time 2019-02-21 23:39:36.200400
Model ind 640 epoch 442 head B head_i_epoch 1 batch 200: avg loss -1.730739 avg loss no lamb -1.730739 time 2019-02-21 23:42:00.071491
last batch sz 160
Pre: time 2019-02-21 23:44:09.294199: 
 	std: 0.053134542
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5017833, 0.6098833, 0.60955, 0.6098167, 0.5008]
	train_accs: [0.5017833, 0.6098833, 0.60955, 0.6098167, 0.5008]
	best_train_sub_head: 1
	worst: 0.5008
	avg: 0.5663667
	best: 0.6098833

Starting e_i: 443
Model ind 640 epoch 443 head A head_i_epoch 0 batch 0: avg loss -3.036504 avg loss no lamb -3.036504 time 2019-02-21 23:44:11.471074
Model ind 640 epoch 443 head A head_i_epoch 0 batch 100: avg loss -2.962109 avg loss no lamb -2.962109 time 2019-02-21 23:46:36.359558
Model ind 640 epoch 443 head A head_i_epoch 0 batch 200: avg loss -3.081121 avg loss no lamb -3.081121 time 2019-02-21 23:49:01.100092
last batch sz 160
Model ind 640 epoch 443 head B head_i_epoch 0 batch 0: avg loss -1.751395 avg loss no lamb -1.751395 time 2019-02-21 23:50:46.683566
Model ind 640 epoch 443 head B head_i_epoch 0 batch 100: avg loss -1.557510 avg loss no lamb -1.557510 time 2019-02-21 23:53:11.789526
Model ind 640 epoch 443 head B head_i_epoch 0 batch 200: avg loss -1.799735 avg loss no lamb -1.799735 time 2019-02-21 23:55:36.484780
last batch sz 160
Model ind 640 epoch 443 head B head_i_epoch 1 batch 0: avg loss -1.652537 avg loss no lamb -1.652537 time 2019-02-21 23:57:21.493972
Model ind 640 epoch 443 head B head_i_epoch 1 batch 100: avg loss -1.548223 avg loss no lamb -1.548223 time 2019-02-21 23:59:45.797374
Model ind 640 epoch 443 head B head_i_epoch 1 batch 200: avg loss -1.789845 avg loss no lamb -1.789845 time 2019-02-22 00:02:10.524165
last batch sz 160
Pre: time 2019-02-22 00:04:20.960784: 
 	std: 0.05242232
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4988, 0.60583335, 0.60536665, 0.60541666, 0.49826667]
	train_accs: [0.4988, 0.60583335, 0.60536665, 0.60541666, 0.49826667]
	best_train_sub_head: 1
	worst: 0.49826667
	avg: 0.5627367
	best: 0.60583335

Starting e_i: 444
Model ind 640 epoch 444 head A head_i_epoch 0 batch 0: avg loss -3.053175 avg loss no lamb -3.053175 time 2019-02-22 00:04:23.120842
Model ind 640 epoch 444 head A head_i_epoch 0 batch 100: avg loss -2.932521 avg loss no lamb -2.932521 time 2019-02-22 00:06:47.402677
Model ind 640 epoch 444 head A head_i_epoch 0 batch 200: avg loss -3.053151 avg loss no lamb -3.053151 time 2019-02-22 00:09:11.875067
last batch sz 160
Model ind 640 epoch 444 head B head_i_epoch 0 batch 0: avg loss -1.667878 avg loss no lamb -1.667878 time 2019-02-22 00:10:56.873531
Model ind 640 epoch 444 head B head_i_epoch 0 batch 100: avg loss -1.532339 avg loss no lamb -1.532339 time 2019-02-22 00:13:20.996795
Model ind 640 epoch 444 head B head_i_epoch 0 batch 200: avg loss -1.814689 avg loss no lamb -1.814689 time 2019-02-22 00:15:45.897377
last batch sz 160
Model ind 640 epoch 444 head B head_i_epoch 1 batch 0: avg loss -1.630630 avg loss no lamb -1.630630 time 2019-02-22 00:17:31.161333
Model ind 640 epoch 444 head B head_i_epoch 1 batch 100: avg loss -1.586397 avg loss no lamb -1.586397 time 2019-02-22 00:19:55.089827
Model ind 640 epoch 444 head B head_i_epoch 1 batch 200: avg loss -1.759380 avg loss no lamb -1.759380 time 2019-02-22 00:22:20.008759
last batch sz 160
Pre: time 2019-02-22 00:24:29.977570: 
 	std: 0.05329674
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4976, 0.6059833, 0.6059167, 0.6056167, 0.4965]
	train_accs: [0.4976, 0.6059833, 0.6059167, 0.6056167, 0.4965]
	best_train_sub_head: 1
	worst: 0.4965
	avg: 0.5623234
	best: 0.6059833

Starting e_i: 445
Model ind 640 epoch 445 head A head_i_epoch 0 batch 0: avg loss -2.997535 avg loss no lamb -2.997535 time 2019-02-22 00:24:32.203290
Model ind 640 epoch 445 head A head_i_epoch 0 batch 100: avg loss -2.911313 avg loss no lamb -2.911313 time 2019-02-22 00:26:57.203916
Model ind 640 epoch 445 head A head_i_epoch 0 batch 200: avg loss -3.090472 avg loss no lamb -3.090472 time 2019-02-22 00:29:22.068029
last batch sz 160
Model ind 640 epoch 445 head B head_i_epoch 0 batch 0: avg loss -1.610097 avg loss no lamb -1.610097 time 2019-02-22 00:31:07.476519
Model ind 640 epoch 445 head B head_i_epoch 0 batch 100: avg loss -1.591905 avg loss no lamb -1.591905 time 2019-02-22 00:33:31.569252
Model ind 640 epoch 445 head B head_i_epoch 0 batch 200: avg loss -1.770705 avg loss no lamb -1.770705 time 2019-02-22 00:35:56.048577
last batch sz 160
Model ind 640 epoch 445 head B head_i_epoch 1 batch 0: avg loss -1.700184 avg loss no lamb -1.700184 time 2019-02-22 00:37:41.273527
Model ind 640 epoch 445 head B head_i_epoch 1 batch 100: avg loss -1.488267 avg loss no lamb -1.488267 time 2019-02-22 00:40:05.805780
Model ind 640 epoch 445 head B head_i_epoch 1 batch 200: avg loss -1.774006 avg loss no lamb -1.774006 time 2019-02-22 00:42:29.720514
last batch sz 160
Pre: time 2019-02-22 00:44:39.742746: 
 	std: 0.053985875
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49681666, 0.60678333, 0.6069, 0.60648334, 0.49623334]
	train_accs: [0.49681666, 0.60678333, 0.6069, 0.60648334, 0.49623334]
	best_train_sub_head: 2
	worst: 0.49623334
	avg: 0.56264335
	best: 0.6069

Starting e_i: 446
Model ind 640 epoch 446 head A head_i_epoch 0 batch 0: avg loss -3.014553 avg loss no lamb -3.014553 time 2019-02-22 00:44:41.900881
Model ind 640 epoch 446 head A head_i_epoch 0 batch 100: avg loss -2.868302 avg loss no lamb -2.868302 time 2019-02-22 00:47:06.343630
Model ind 640 epoch 446 head A head_i_epoch 0 batch 200: avg loss -3.043897 avg loss no lamb -3.043897 time 2019-02-22 00:49:31.312538
last batch sz 160
Model ind 640 epoch 446 head B head_i_epoch 0 batch 0: avg loss -1.740678 avg loss no lamb -1.740678 time 2019-02-22 00:51:16.273453
Model ind 640 epoch 446 head B head_i_epoch 0 batch 100: avg loss -1.588216 avg loss no lamb -1.588216 time 2019-02-22 00:53:40.442122
Model ind 640 epoch 446 head B head_i_epoch 0 batch 200: avg loss -1.760994 avg loss no lamb -1.760994 time 2019-02-22 00:56:04.739836
last batch sz 160
Model ind 640 epoch 446 head B head_i_epoch 1 batch 0: avg loss -1.683670 avg loss no lamb -1.683670 time 2019-02-22 00:57:49.558697
Model ind 640 epoch 446 head B head_i_epoch 1 batch 100: avg loss -1.525585 avg loss no lamb -1.525585 time 2019-02-22 01:00:13.861471
Model ind 640 epoch 446 head B head_i_epoch 1 batch 200: avg loss -1.745459 avg loss no lamb -1.745459 time 2019-02-22 01:02:38.565766
last batch sz 160
Pre: time 2019-02-22 01:04:47.873854: 
 	std: 0.054082066
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49573332, 0.60608333, 0.6059667, 0.60613334, 0.4956]
	train_accs: [0.49573332, 0.60608333, 0.6059667, 0.60613334, 0.4956]
	best_train_sub_head: 3
	worst: 0.4956
	avg: 0.56190336
	best: 0.60613334

Starting e_i: 447
Model ind 640 epoch 447 head A head_i_epoch 0 batch 0: avg loss -2.997396 avg loss no lamb -2.997396 time 2019-02-22 01:04:50.300992
Model ind 640 epoch 447 head A head_i_epoch 0 batch 100: avg loss -2.955230 avg loss no lamb -2.955230 time 2019-02-22 01:07:14.874868
Model ind 640 epoch 447 head A head_i_epoch 0 batch 200: avg loss -3.065608 avg loss no lamb -3.065608 time 2019-02-22 01:09:39.496383
last batch sz 160
Model ind 640 epoch 447 head B head_i_epoch 0 batch 0: avg loss -1.730514 avg loss no lamb -1.730514 time 2019-02-22 01:11:24.609917
Model ind 640 epoch 447 head B head_i_epoch 0 batch 100: avg loss -1.623861 avg loss no lamb -1.623861 time 2019-02-22 01:13:49.312240
Model ind 640 epoch 447 head B head_i_epoch 0 batch 200: avg loss -1.736025 avg loss no lamb -1.736025 time 2019-02-22 01:16:13.452478
last batch sz 160
Model ind 640 epoch 447 head B head_i_epoch 1 batch 0: avg loss -1.662007 avg loss no lamb -1.662007 time 2019-02-22 01:17:58.265897
Model ind 640 epoch 447 head B head_i_epoch 1 batch 100: avg loss -1.677903 avg loss no lamb -1.677903 time 2019-02-22 01:20:22.391313
Model ind 640 epoch 447 head B head_i_epoch 1 batch 200: avg loss -1.634121 avg loss no lamb -1.634121 time 2019-02-22 01:22:46.008126
last batch sz 160
Pre: time 2019-02-22 01:24:55.158368: 
 	std: 0.05317602
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49705, 0.60538334, 0.6052, 0.6054, 0.49651667]
	train_accs: [0.49705, 0.60538334, 0.6052, 0.6054, 0.49651667]
	best_train_sub_head: 3
	worst: 0.49651667
	avg: 0.56191003
	best: 0.6054

Starting e_i: 448
Model ind 640 epoch 448 head A head_i_epoch 0 batch 0: avg loss -3.055785 avg loss no lamb -3.055785 time 2019-02-22 01:24:57.809031
Model ind 640 epoch 448 head A head_i_epoch 0 batch 100: avg loss -2.955758 avg loss no lamb -2.955758 time 2019-02-22 01:27:21.925216
Model ind 640 epoch 448 head A head_i_epoch 0 batch 200: avg loss -2.982144 avg loss no lamb -2.982144 time 2019-02-22 01:29:46.216222
last batch sz 160
Model ind 640 epoch 448 head B head_i_epoch 0 batch 0: avg loss -1.651364 avg loss no lamb -1.651364 time 2019-02-22 01:31:31.471025
Model ind 640 epoch 448 head B head_i_epoch 0 batch 100: avg loss -1.590930 avg loss no lamb -1.590930 time 2019-02-22 01:33:55.815625
Model ind 640 epoch 448 head B head_i_epoch 0 batch 200: avg loss -1.725985 avg loss no lamb -1.725985 time 2019-02-22 01:36:20.066295
last batch sz 160
Model ind 640 epoch 448 head B head_i_epoch 1 batch 0: avg loss -1.751190 avg loss no lamb -1.751190 time 2019-02-22 01:38:05.160306
Model ind 640 epoch 448 head B head_i_epoch 1 batch 100: avg loss -1.584689 avg loss no lamb -1.584689 time 2019-02-22 01:40:29.433913
Model ind 640 epoch 448 head B head_i_epoch 1 batch 200: avg loss -1.802468 avg loss no lamb -1.802468 time 2019-02-22 01:42:54.097467
last batch sz 160
Pre: time 2019-02-22 01:45:04.166939: 
 	std: 0.05295713
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49821666, 0.60606664, 0.6059333, 0.6058667, 0.4975]
	train_accs: [0.49821666, 0.60606664, 0.6059333, 0.6058667, 0.4975]
	best_train_sub_head: 1
	worst: 0.4975
	avg: 0.56271666
	best: 0.60606664

Starting e_i: 449
Model ind 640 epoch 449 head A head_i_epoch 0 batch 0: avg loss -3.030971 avg loss no lamb -3.030971 time 2019-02-22 01:45:06.397772
Model ind 640 epoch 449 head A head_i_epoch 0 batch 100: avg loss -2.862606 avg loss no lamb -2.862606 time 2019-02-22 01:47:30.898173
Model ind 640 epoch 449 head A head_i_epoch 0 batch 200: avg loss -3.046239 avg loss no lamb -3.046239 time 2019-02-22 01:49:55.661619
last batch sz 160
Model ind 640 epoch 449 head B head_i_epoch 0 batch 0: avg loss -1.668394 avg loss no lamb -1.668394 time 2019-02-22 01:51:40.994811
Model ind 640 epoch 449 head B head_i_epoch 0 batch 100: avg loss -1.616171 avg loss no lamb -1.616171 time 2019-02-22 01:54:05.629320
Model ind 640 epoch 449 head B head_i_epoch 0 batch 200: avg loss -1.851340 avg loss no lamb -1.851340 time 2019-02-22 01:56:29.873655
last batch sz 160
Model ind 640 epoch 449 head B head_i_epoch 1 batch 0: avg loss -1.695711 avg loss no lamb -1.695711 time 2019-02-22 01:58:14.836026
Model ind 640 epoch 449 head B head_i_epoch 1 batch 100: avg loss -1.566654 avg loss no lamb -1.566654 time 2019-02-22 02:00:39.449957
Model ind 640 epoch 449 head B head_i_epoch 1 batch 200: avg loss -1.786915 avg loss no lamb -1.786915 time 2019-02-22 02:03:03.621848
last batch sz 160
Pre: time 2019-02-22 02:05:13.568625: 
 	std: 0.052416556
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49815, 0.6052667, 0.60485, 0.6051667, 0.49805]
	train_accs: [0.49815, 0.6052667, 0.60485, 0.6051667, 0.49805]
	best_train_sub_head: 1
	worst: 0.49805
	avg: 0.5622967
	best: 0.6052667

Starting e_i: 450
Model ind 640 epoch 450 head A head_i_epoch 0 batch 0: avg loss -3.063770 avg loss no lamb -3.063770 time 2019-02-22 02:05:15.738361
Model ind 640 epoch 450 head A head_i_epoch 0 batch 100: avg loss -2.962142 avg loss no lamb -2.962142 time 2019-02-22 02:07:40.387822
Model ind 640 epoch 450 head A head_i_epoch 0 batch 200: avg loss -3.114987 avg loss no lamb -3.114987 time 2019-02-22 02:10:04.966678
last batch sz 160
Model ind 640 epoch 450 head B head_i_epoch 0 batch 0: avg loss -1.599982 avg loss no lamb -1.599982 time 2019-02-22 02:11:50.085624
Model ind 640 epoch 450 head B head_i_epoch 0 batch 100: avg loss -1.608006 avg loss no lamb -1.608006 time 2019-02-22 02:14:14.850297
Model ind 640 epoch 450 head B head_i_epoch 0 batch 200: avg loss -1.769129 avg loss no lamb -1.769129 time 2019-02-22 02:16:39.000305
last batch sz 160
Model ind 640 epoch 450 head B head_i_epoch 1 batch 0: avg loss -1.712923 avg loss no lamb -1.712923 time 2019-02-22 02:18:24.027386
Model ind 640 epoch 450 head B head_i_epoch 1 batch 100: avg loss -1.602771 avg loss no lamb -1.602771 time 2019-02-22 02:20:48.522470
Model ind 640 epoch 450 head B head_i_epoch 1 batch 200: avg loss -1.831384 avg loss no lamb -1.831384 time 2019-02-22 02:23:13.257752
last batch sz 160
Pre: time 2019-02-22 02:25:23.973782: 
 	std: 0.052324887
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4963, 0.6027333, 0.60251665, 0.60251665, 0.49526668]
	train_accs: [0.4963, 0.6027333, 0.60251665, 0.60251665, 0.49526668]
	best_train_sub_head: 1
	worst: 0.49526668
	avg: 0.55986667
	best: 0.6027333

Starting e_i: 451
Model ind 640 epoch 451 head A head_i_epoch 0 batch 0: avg loss -2.974033 avg loss no lamb -2.974033 time 2019-02-22 02:25:29.718242
Model ind 640 epoch 451 head A head_i_epoch 0 batch 100: avg loss -2.957167 avg loss no lamb -2.957167 time 2019-02-22 02:27:54.447339
Model ind 640 epoch 451 head A head_i_epoch 0 batch 200: avg loss -2.965154 avg loss no lamb -2.965154 time 2019-02-22 02:30:19.154792
last batch sz 160
Model ind 640 epoch 451 head B head_i_epoch 0 batch 0: avg loss -1.693437 avg loss no lamb -1.693437 time 2019-02-22 02:32:04.320263
Model ind 640 epoch 451 head B head_i_epoch 0 batch 100: avg loss -1.633783 avg loss no lamb -1.633783 time 2019-02-22 02:34:28.410168
Model ind 640 epoch 451 head B head_i_epoch 0 batch 200: avg loss -1.719066 avg loss no lamb -1.719066 time 2019-02-22 02:36:52.935852
last batch sz 160
Model ind 640 epoch 451 head B head_i_epoch 1 batch 0: avg loss -1.690974 avg loss no lamb -1.690974 time 2019-02-22 02:38:38.812695
Model ind 640 epoch 451 head B head_i_epoch 1 batch 100: avg loss -1.643040 avg loss no lamb -1.643040 time 2019-02-22 02:41:02.921560
Model ind 640 epoch 451 head B head_i_epoch 1 batch 200: avg loss -1.779783 avg loss no lamb -1.779783 time 2019-02-22 02:43:26.777437
last batch sz 160
Pre: time 2019-02-22 02:45:36.693093: 
 	std: 0.05385056
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49443334, 0.60476667, 0.6041667, 0.60365, 0.49411666]
	train_accs: [0.49443334, 0.60476667, 0.6041667, 0.60365, 0.49411666]
	best_train_sub_head: 1
	worst: 0.49411666
	avg: 0.56022674
	best: 0.60476667

Starting e_i: 452
Model ind 640 epoch 452 head A head_i_epoch 0 batch 0: avg loss -3.033672 avg loss no lamb -3.033672 time 2019-02-22 02:45:39.015967
Model ind 640 epoch 452 head A head_i_epoch 0 batch 100: avg loss -2.906995 avg loss no lamb -2.906995 time 2019-02-22 02:48:03.196126
Model ind 640 epoch 452 head A head_i_epoch 0 batch 200: avg loss -3.034690 avg loss no lamb -3.034690 time 2019-02-22 02:50:27.858817
last batch sz 160
Model ind 640 epoch 452 head B head_i_epoch 0 batch 0: avg loss -1.683931 avg loss no lamb -1.683931 time 2019-02-22 02:52:12.597282
Model ind 640 epoch 452 head B head_i_epoch 0 batch 100: avg loss -1.627848 avg loss no lamb -1.627848 time 2019-02-22 02:54:36.838017
Model ind 640 epoch 452 head B head_i_epoch 0 batch 200: avg loss -1.755351 avg loss no lamb -1.755351 time 2019-02-22 02:57:01.117795
last batch sz 160
Model ind 640 epoch 452 head B head_i_epoch 1 batch 0: avg loss -1.666179 avg loss no lamb -1.666179 time 2019-02-22 02:58:46.308030
Model ind 640 epoch 452 head B head_i_epoch 1 batch 100: avg loss -1.538235 avg loss no lamb -1.538235 time 2019-02-22 03:01:10.653922
Model ind 640 epoch 452 head B head_i_epoch 1 batch 200: avg loss -1.747989 avg loss no lamb -1.747989 time 2019-02-22 03:03:35.504021
last batch sz 160
Pre: time 2019-02-22 03:05:45.266778: 
 	std: 0.05097884
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5021833, 0.60681665, 0.6061, 0.6059333, 0.50226665]
	train_accs: [0.5021833, 0.60681665, 0.6061, 0.6059333, 0.50226665]
	best_train_sub_head: 1
	worst: 0.5021833
	avg: 0.56466
	best: 0.60681665

Starting e_i: 453
Model ind 640 epoch 453 head A head_i_epoch 0 batch 0: avg loss -3.049574 avg loss no lamb -3.049574 time 2019-02-22 03:05:48.040733
Model ind 640 epoch 453 head A head_i_epoch 0 batch 100: avg loss -2.935511 avg loss no lamb -2.935511 time 2019-02-22 03:08:12.561206
Model ind 640 epoch 453 head A head_i_epoch 0 batch 200: avg loss -3.062360 avg loss no lamb -3.062360 time 2019-02-22 03:10:37.326625
last batch sz 160
Model ind 640 epoch 453 head B head_i_epoch 0 batch 0: avg loss -1.660949 avg loss no lamb -1.660949 time 2019-02-22 03:12:22.888245
Model ind 640 epoch 453 head B head_i_epoch 0 batch 100: avg loss -1.592741 avg loss no lamb -1.592741 time 2019-02-22 03:14:47.823158
Model ind 640 epoch 453 head B head_i_epoch 0 batch 200: avg loss -1.790522 avg loss no lamb -1.790522 time 2019-02-22 03:17:12.271235
last batch sz 160
Model ind 640 epoch 453 head B head_i_epoch 1 batch 0: avg loss -1.706892 avg loss no lamb -1.706892 time 2019-02-22 03:18:57.256016
Model ind 640 epoch 453 head B head_i_epoch 1 batch 100: avg loss -1.578050 avg loss no lamb -1.578050 time 2019-02-22 03:21:21.672766
Model ind 640 epoch 453 head B head_i_epoch 1 batch 200: avg loss -1.828131 avg loss no lamb -1.828131 time 2019-02-22 03:23:46.081558
last batch sz 160
Pre: time 2019-02-22 03:25:56.024800: 
 	std: 0.05396115
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.501, 0.61155, 0.61116666, 0.6112, 0.50131667]
	train_accs: [0.501, 0.61155, 0.61116666, 0.6112, 0.50131667]
	best_train_sub_head: 1
	worst: 0.501
	avg: 0.5672466
	best: 0.61155

Starting e_i: 454
Model ind 640 epoch 454 head A head_i_epoch 0 batch 0: avg loss -3.112271 avg loss no lamb -3.112271 time 2019-02-22 03:25:58.190465
Model ind 640 epoch 454 head A head_i_epoch 0 batch 100: avg loss -2.940962 avg loss no lamb -2.940962 time 2019-02-22 03:28:23.079990
Model ind 640 epoch 454 head A head_i_epoch 0 batch 200: avg loss -3.092579 avg loss no lamb -3.092579 time 2019-02-22 03:30:47.936441
last batch sz 160
Model ind 640 epoch 454 head B head_i_epoch 0 batch 0: avg loss -1.721413 avg loss no lamb -1.721413 time 2019-02-22 03:32:33.212048
Model ind 640 epoch 454 head B head_i_epoch 0 batch 100: avg loss -1.682341 avg loss no lamb -1.682341 time 2019-02-22 03:34:58.035425
Model ind 640 epoch 454 head B head_i_epoch 0 batch 200: avg loss -1.834303 avg loss no lamb -1.834303 time 2019-02-22 03:37:22.719135
last batch sz 160
Model ind 640 epoch 454 head B head_i_epoch 1 batch 0: avg loss -1.716248 avg loss no lamb -1.716248 time 2019-02-22 03:39:07.835110
Model ind 640 epoch 454 head B head_i_epoch 1 batch 100: avg loss -1.671939 avg loss no lamb -1.671939 time 2019-02-22 03:41:31.797607
Model ind 640 epoch 454 head B head_i_epoch 1 batch 200: avg loss -1.816244 avg loss no lamb -1.816244 time 2019-02-22 03:43:55.774116
last batch sz 160
Pre: time 2019-02-22 03:46:05.702142: 
 	std: 0.05326692
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49868333, 0.6073833, 0.6073167, 0.60726666, 0.4985]
	train_accs: [0.49868333, 0.6073833, 0.6073167, 0.60726666, 0.4985]
	best_train_sub_head: 1
	worst: 0.4985
	avg: 0.56383
	best: 0.6073833

Starting e_i: 455
Model ind 640 epoch 455 head A head_i_epoch 0 batch 0: avg loss -3.055526 avg loss no lamb -3.055526 time 2019-02-22 03:46:07.954475
Model ind 640 epoch 455 head A head_i_epoch 0 batch 100: avg loss -2.882211 avg loss no lamb -2.882211 time 2019-02-22 03:48:32.654167
Model ind 640 epoch 455 head A head_i_epoch 0 batch 200: avg loss -2.988542 avg loss no lamb -2.988542 time 2019-02-22 03:50:57.486846
last batch sz 160
Model ind 640 epoch 455 head B head_i_epoch 0 batch 0: avg loss -1.727061 avg loss no lamb -1.727061 time 2019-02-22 03:52:42.877349
Model ind 640 epoch 455 head B head_i_epoch 0 batch 100: avg loss -1.562766 avg loss no lamb -1.562766 time 2019-02-22 03:55:07.176505
Model ind 640 epoch 455 head B head_i_epoch 0 batch 200: avg loss -1.750576 avg loss no lamb -1.750576 time 2019-02-22 03:57:31.713531
last batch sz 160
Model ind 640 epoch 455 head B head_i_epoch 1 batch 0: avg loss -1.616516 avg loss no lamb -1.616516 time 2019-02-22 03:59:16.887762
Model ind 640 epoch 455 head B head_i_epoch 1 batch 100: avg loss -1.571681 avg loss no lamb -1.571681 time 2019-02-22 04:01:41.594129
Model ind 640 epoch 455 head B head_i_epoch 1 batch 200: avg loss -1.739994 avg loss no lamb -1.739994 time 2019-02-22 04:04:06.239391
last batch sz 160
Pre: time 2019-02-22 04:06:16.654351: 
 	std: 0.054351974
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49665, 0.60815, 0.60768336, 0.6078, 0.49721667]
	train_accs: [0.49665, 0.60815, 0.60768336, 0.6078, 0.49721667]
	best_train_sub_head: 1
	worst: 0.49665
	avg: 0.56350005
	best: 0.60815

Starting e_i: 456
Model ind 640 epoch 456 head A head_i_epoch 0 batch 0: avg loss -3.004864 avg loss no lamb -3.004864 time 2019-02-22 04:06:18.819153
Model ind 640 epoch 456 head A head_i_epoch 0 batch 100: avg loss -2.891730 avg loss no lamb -2.891730 time 2019-02-22 04:08:43.707599
Model ind 640 epoch 456 head A head_i_epoch 0 batch 200: avg loss -3.022742 avg loss no lamb -3.022742 time 2019-02-22 04:11:08.113635
last batch sz 160
Model ind 640 epoch 456 head B head_i_epoch 0 batch 0: avg loss -1.681621 avg loss no lamb -1.681621 time 2019-02-22 04:12:53.405325
Model ind 640 epoch 456 head B head_i_epoch 0 batch 100: avg loss -1.579270 avg loss no lamb -1.579270 time 2019-02-22 04:15:17.479056
Model ind 640 epoch 456 head B head_i_epoch 0 batch 200: avg loss -1.765369 avg loss no lamb -1.765369 time 2019-02-22 04:17:41.705289
last batch sz 160
Model ind 640 epoch 456 head B head_i_epoch 1 batch 0: avg loss -1.699287 avg loss no lamb -1.699287 time 2019-02-22 04:19:26.937247
Model ind 640 epoch 456 head B head_i_epoch 1 batch 100: avg loss -1.582704 avg loss no lamb -1.582704 time 2019-02-22 04:21:51.298356
Model ind 640 epoch 456 head B head_i_epoch 1 batch 200: avg loss -1.764360 avg loss no lamb -1.764360 time 2019-02-22 04:24:16.127455
last batch sz 160
Pre: time 2019-02-22 04:26:25.809370: 
 	std: 0.05362642
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49835, 0.6081833, 0.60768336, 0.6079, 0.49856666]
	train_accs: [0.49835, 0.6081833, 0.60768336, 0.6079, 0.49856666]
	best_train_sub_head: 1
	worst: 0.49835
	avg: 0.5641367
	best: 0.6081833

Starting e_i: 457
Model ind 640 epoch 457 head A head_i_epoch 0 batch 0: avg loss -3.000412 avg loss no lamb -3.000412 time 2019-02-22 04:26:28.249265
Model ind 640 epoch 457 head A head_i_epoch 0 batch 100: avg loss -2.893419 avg loss no lamb -2.893419 time 2019-02-22 04:28:53.358950
Model ind 640 epoch 457 head A head_i_epoch 0 batch 200: avg loss -3.053901 avg loss no lamb -3.053901 time 2019-02-22 04:31:18.069085
last batch sz 160
Model ind 640 epoch 457 head B head_i_epoch 0 batch 0: avg loss -1.559471 avg loss no lamb -1.559471 time 2019-02-22 04:33:03.245656
Model ind 640 epoch 457 head B head_i_epoch 0 batch 100: avg loss -1.662663 avg loss no lamb -1.662663 time 2019-02-22 04:35:27.631892
Model ind 640 epoch 457 head B head_i_epoch 0 batch 200: avg loss -1.797002 avg loss no lamb -1.797002 time 2019-02-22 04:37:51.648050
last batch sz 160
Model ind 640 epoch 457 head B head_i_epoch 1 batch 0: avg loss -1.663606 avg loss no lamb -1.663606 time 2019-02-22 04:39:36.693689
Model ind 640 epoch 457 head B head_i_epoch 1 batch 100: avg loss -1.602224 avg loss no lamb -1.602224 time 2019-02-22 04:42:01.864486
Model ind 640 epoch 457 head B head_i_epoch 1 batch 200: avg loss -1.793303 avg loss no lamb -1.793303 time 2019-02-22 04:44:26.019950
last batch sz 160
Pre: time 2019-02-22 04:46:35.874897: 
 	std: 0.053975556
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49386665, 0.60513335, 0.60433334, 0.60475, 0.49526668]
	train_accs: [0.49386665, 0.60513335, 0.60433334, 0.60475, 0.49526668]
	best_train_sub_head: 1
	worst: 0.49386665
	avg: 0.56067
	best: 0.60513335

Starting e_i: 458
Model ind 640 epoch 458 head A head_i_epoch 0 batch 0: avg loss -3.026272 avg loss no lamb -3.026272 time 2019-02-22 04:46:38.736687
Model ind 640 epoch 458 head A head_i_epoch 0 batch 100: avg loss -2.961318 avg loss no lamb -2.961318 time 2019-02-22 04:49:02.264574
Model ind 640 epoch 458 head A head_i_epoch 0 batch 200: avg loss -3.020763 avg loss no lamb -3.020763 time 2019-02-22 04:51:26.743566
last batch sz 160
Model ind 640 epoch 458 head B head_i_epoch 0 batch 0: avg loss -1.641549 avg loss no lamb -1.641549 time 2019-02-22 04:53:11.581354
Model ind 640 epoch 458 head B head_i_epoch 0 batch 100: avg loss -1.586450 avg loss no lamb -1.586450 time 2019-02-22 04:55:35.244754
Model ind 640 epoch 458 head B head_i_epoch 0 batch 200: avg loss -1.851586 avg loss no lamb -1.851586 time 2019-02-22 04:57:59.796273
last batch sz 160
Model ind 640 epoch 458 head B head_i_epoch 1 batch 0: avg loss -1.598478 avg loss no lamb -1.598478 time 2019-02-22 04:59:45.049801
Model ind 640 epoch 458 head B head_i_epoch 1 batch 100: avg loss -1.662053 avg loss no lamb -1.662053 time 2019-02-22 05:02:09.906822
Model ind 640 epoch 458 head B head_i_epoch 1 batch 200: avg loss -1.793998 avg loss no lamb -1.793998 time 2019-02-22 05:04:34.501419
last batch sz 160
Pre: time 2019-02-22 05:06:44.265974: 
 	std: 0.054391462
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49441665, 0.6070333, 0.60648334, 0.60691667, 0.49718332]
	train_accs: [0.49441665, 0.6070333, 0.60648334, 0.60691667, 0.49718332]
	best_train_sub_head: 1
	worst: 0.49441665
	avg: 0.56240666
	best: 0.6070333

Starting e_i: 459
Model ind 640 epoch 459 head A head_i_epoch 0 batch 0: avg loss -3.053734 avg loss no lamb -3.053734 time 2019-02-22 05:06:46.496864
Model ind 640 epoch 459 head A head_i_epoch 0 batch 100: avg loss -2.915407 avg loss no lamb -2.915407 time 2019-02-22 05:09:10.344649
Model ind 640 epoch 459 head A head_i_epoch 0 batch 200: avg loss -3.008196 avg loss no lamb -3.008196 time 2019-02-22 05:11:34.237376
last batch sz 160
Model ind 640 epoch 459 head B head_i_epoch 0 batch 0: avg loss -1.603409 avg loss no lamb -1.603409 time 2019-02-22 05:13:19.147362
Model ind 640 epoch 459 head B head_i_epoch 0 batch 100: avg loss -1.648127 avg loss no lamb -1.648127 time 2019-02-22 05:15:43.133746
Model ind 640 epoch 459 head B head_i_epoch 0 batch 200: avg loss -1.718361 avg loss no lamb -1.718361 time 2019-02-22 05:18:07.419728
last batch sz 160
Model ind 640 epoch 459 head B head_i_epoch 1 batch 0: avg loss -1.724874 avg loss no lamb -1.724874 time 2019-02-22 05:19:52.428046
Model ind 640 epoch 459 head B head_i_epoch 1 batch 100: avg loss -1.641616 avg loss no lamb -1.641616 time 2019-02-22 05:22:16.390491
Model ind 640 epoch 459 head B head_i_epoch 1 batch 200: avg loss -1.712617 avg loss no lamb -1.712617 time 2019-02-22 05:24:40.920737
last batch sz 160
Pre: time 2019-02-22 05:26:50.762424: 
 	std: 0.051390953
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.502, 0.60716665, 0.6069833, 0.6075, 0.50263333]
	train_accs: [0.502, 0.60716665, 0.6069833, 0.6075, 0.50263333]
	best_train_sub_head: 3
	worst: 0.502
	avg: 0.56525666
	best: 0.6075

Starting e_i: 460
Model ind 640 epoch 460 head A head_i_epoch 0 batch 0: avg loss -3.044504 avg loss no lamb -3.044504 time 2019-02-22 05:26:52.939922
Model ind 640 epoch 460 head A head_i_epoch 0 batch 100: avg loss -2.941664 avg loss no lamb -2.941664 time 2019-02-22 05:29:17.435913
Model ind 640 epoch 460 head A head_i_epoch 0 batch 200: avg loss -3.069990 avg loss no lamb -3.069990 time 2019-02-22 05:31:41.605768
last batch sz 160
Model ind 640 epoch 460 head B head_i_epoch 0 batch 0: avg loss -1.656717 avg loss no lamb -1.656717 time 2019-02-22 05:33:27.033998
Model ind 640 epoch 460 head B head_i_epoch 0 batch 100: avg loss -1.602820 avg loss no lamb -1.602820 time 2019-02-22 05:35:50.865940
Model ind 640 epoch 460 head B head_i_epoch 0 batch 200: avg loss -1.811810 avg loss no lamb -1.811810 time 2019-02-22 05:38:15.337256
last batch sz 160
Model ind 640 epoch 460 head B head_i_epoch 1 batch 0: avg loss -1.668530 avg loss no lamb -1.668530 time 2019-02-22 05:40:00.186646
Model ind 640 epoch 460 head B head_i_epoch 1 batch 100: avg loss -1.646918 avg loss no lamb -1.646918 time 2019-02-22 05:42:24.137127
Model ind 640 epoch 460 head B head_i_epoch 1 batch 200: avg loss -1.757323 avg loss no lamb -1.757323 time 2019-02-22 05:44:48.215477
last batch sz 160
Pre: time 2019-02-22 05:46:58.509354: 
 	std: 0.053194333
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.4986, 0.60761666, 0.6073833, 0.60786664, 0.49948335]
	train_accs: [0.4986, 0.60761666, 0.6073833, 0.60786664, 0.49948335]
	best_train_sub_head: 3
	worst: 0.4986
	avg: 0.56419
	best: 0.60786664

Starting e_i: 461
Model ind 640 epoch 461 head A head_i_epoch 0 batch 0: avg loss -3.010131 avg loss no lamb -3.010131 time 2019-02-22 05:47:03.968207
Model ind 640 epoch 461 head A head_i_epoch 0 batch 100: avg loss -2.948426 avg loss no lamb -2.948426 time 2019-02-22 05:49:28.728119
Model ind 640 epoch 461 head A head_i_epoch 0 batch 200: avg loss -3.014954 avg loss no lamb -3.014954 time 2019-02-22 05:51:53.464448
last batch sz 160
Model ind 640 epoch 461 head B head_i_epoch 0 batch 0: avg loss -1.657358 avg loss no lamb -1.657358 time 2019-02-22 05:53:38.931341
Model ind 640 epoch 461 head B head_i_epoch 0 batch 100: avg loss -1.636635 avg loss no lamb -1.636635 time 2019-02-22 05:56:03.707057
Model ind 640 epoch 461 head B head_i_epoch 0 batch 200: avg loss -1.724590 avg loss no lamb -1.724590 time 2019-02-22 05:58:28.098574
last batch sz 160
Model ind 640 epoch 461 head B head_i_epoch 1 batch 0: avg loss -1.479785 avg loss no lamb -1.479785 time 2019-02-22 06:00:12.688330
Model ind 640 epoch 461 head B head_i_epoch 1 batch 100: avg loss -1.549627 avg loss no lamb -1.549627 time 2019-02-22 06:02:37.135182
Model ind 640 epoch 461 head B head_i_epoch 1 batch 200: avg loss -1.781068 avg loss no lamb -1.781068 time 2019-02-22 06:05:01.569864
last batch sz 160
Pre: time 2019-02-22 06:07:11.454982: 
 	std: 0.054543037
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49438334, 0.60623336, 0.6063833, 0.6056333, 0.49511668]
	train_accs: [0.49438334, 0.60623336, 0.6063833, 0.6056333, 0.49511668]
	best_train_sub_head: 2
	worst: 0.49438334
	avg: 0.56155
	best: 0.6063833

Starting e_i: 462
Model ind 640 epoch 462 head A head_i_epoch 0 batch 0: avg loss -2.889646 avg loss no lamb -2.889646 time 2019-02-22 06:07:13.740241
Model ind 640 epoch 462 head A head_i_epoch 0 batch 100: avg loss -2.905047 avg loss no lamb -2.905047 time 2019-02-22 06:09:38.605352
Model ind 640 epoch 462 head A head_i_epoch 0 batch 200: avg loss -2.976445 avg loss no lamb -2.976445 time 2019-02-22 06:12:03.026241
last batch sz 160
Model ind 640 epoch 462 head B head_i_epoch 0 batch 0: avg loss -1.729919 avg loss no lamb -1.729919 time 2019-02-22 06:13:48.396768
Model ind 640 epoch 462 head B head_i_epoch 0 batch 100: avg loss -1.535318 avg loss no lamb -1.535318 time 2019-02-22 06:16:13.341675
Model ind 640 epoch 462 head B head_i_epoch 0 batch 200: avg loss -1.781086 avg loss no lamb -1.781086 time 2019-02-22 06:18:37.797726
last batch sz 160
Model ind 640 epoch 462 head B head_i_epoch 1 batch 0: avg loss -1.650734 avg loss no lamb -1.650734 time 2019-02-22 06:20:23.166420
Model ind 640 epoch 462 head B head_i_epoch 1 batch 100: avg loss -1.497042 avg loss no lamb -1.497042 time 2019-02-22 06:22:47.886696
Model ind 640 epoch 462 head B head_i_epoch 1 batch 200: avg loss -1.841151 avg loss no lamb -1.841151 time 2019-02-22 06:25:12.319080
last batch sz 160
Pre: time 2019-02-22 06:27:21.764566: 
 	std: 0.053146657
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49795, 0.6071333, 0.6066667, 0.60675, 0.49878332]
	train_accs: [0.49795, 0.6071333, 0.6066667, 0.60675, 0.49878332]
	best_train_sub_head: 1
	worst: 0.49795
	avg: 0.56345665
	best: 0.6071333

Starting e_i: 463
Model ind 640 epoch 463 head A head_i_epoch 0 batch 0: avg loss -3.018418 avg loss no lamb -3.018418 time 2019-02-22 06:27:24.524738
Model ind 640 epoch 463 head A head_i_epoch 0 batch 100: avg loss -2.954948 avg loss no lamb -2.954948 time 2019-02-22 06:29:49.169972
Model ind 640 epoch 463 head A head_i_epoch 0 batch 200: avg loss -3.035825 avg loss no lamb -3.035825 time 2019-02-22 06:32:14.082830
last batch sz 160
Model ind 640 epoch 463 head B head_i_epoch 0 batch 0: avg loss -1.717873 avg loss no lamb -1.717873 time 2019-02-22 06:33:58.947902
Model ind 640 epoch 463 head B head_i_epoch 0 batch 100: avg loss -1.526992 avg loss no lamb -1.526992 time 2019-02-22 06:36:23.206608
Model ind 640 epoch 463 head B head_i_epoch 0 batch 200: avg loss -1.774946 avg loss no lamb -1.774946 time 2019-02-22 06:38:47.936311
last batch sz 160
Model ind 640 epoch 463 head B head_i_epoch 1 batch 0: avg loss -1.714483 avg loss no lamb -1.714483 time 2019-02-22 06:40:33.183279
Model ind 640 epoch 463 head B head_i_epoch 1 batch 100: avg loss -1.650577 avg loss no lamb -1.650577 time 2019-02-22 06:42:57.922617
Model ind 640 epoch 463 head B head_i_epoch 1 batch 200: avg loss -1.758650 avg loss no lamb -1.758650 time 2019-02-22 06:45:22.504211
last batch sz 160
Pre: time 2019-02-22 06:47:32.296494: 
 	std: 0.052713092
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50385, 0.6113167, 0.61141664, 0.6112667, 0.5036167]
	train_accs: [0.50385, 0.6113167, 0.61141664, 0.6112667, 0.5036167]
	best_train_sub_head: 2
	worst: 0.5036167
	avg: 0.5682933
	best: 0.61141664

Starting e_i: 464
Model ind 640 epoch 464 head A head_i_epoch 0 batch 0: avg loss -2.971476 avg loss no lamb -2.971476 time 2019-02-22 06:47:34.473033
Model ind 640 epoch 464 head A head_i_epoch 0 batch 100: avg loss -2.907125 avg loss no lamb -2.907125 time 2019-02-22 06:49:59.016231
Model ind 640 epoch 464 head A head_i_epoch 0 batch 200: avg loss -2.965185 avg loss no lamb -2.965185 time 2019-02-22 06:52:23.940313
last batch sz 160
Model ind 640 epoch 464 head B head_i_epoch 0 batch 0: avg loss -1.600427 avg loss no lamb -1.600427 time 2019-02-22 06:54:09.300079
Model ind 640 epoch 464 head B head_i_epoch 0 batch 100: avg loss -1.560509 avg loss no lamb -1.560509 time 2019-02-22 06:56:33.721345
Model ind 640 epoch 464 head B head_i_epoch 0 batch 200: avg loss -1.755723 avg loss no lamb -1.755723 time 2019-02-22 06:58:58.253315
last batch sz 160
Model ind 640 epoch 464 head B head_i_epoch 1 batch 0: avg loss -1.737088 avg loss no lamb -1.737088 time 2019-02-22 07:00:43.460148
Model ind 640 epoch 464 head B head_i_epoch 1 batch 100: avg loss -1.526567 avg loss no lamb -1.526567 time 2019-02-22 07:03:08.132346
Model ind 640 epoch 464 head B head_i_epoch 1 batch 200: avg loss -1.721226 avg loss no lamb -1.721226 time 2019-02-22 07:05:32.664764
last batch sz 160
Pre: time 2019-02-22 07:07:43.060418: 
 	std: 0.054861374
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4959, 0.6081833, 0.6086, 0.60826665, 0.49683332]
	train_accs: [0.4959, 0.6081833, 0.6086, 0.60826665, 0.49683332]
	best_train_sub_head: 2
	worst: 0.4959
	avg: 0.5635567
	best: 0.6086

Starting e_i: 465
Model ind 640 epoch 465 head A head_i_epoch 0 batch 0: avg loss -3.019239 avg loss no lamb -3.019239 time 2019-02-22 07:07:45.278959
Model ind 640 epoch 465 head A head_i_epoch 0 batch 100: avg loss -2.897468 avg loss no lamb -2.897468 time 2019-02-22 07:10:10.022934
Model ind 640 epoch 465 head A head_i_epoch 0 batch 200: avg loss -3.050395 avg loss no lamb -3.050395 time 2019-02-22 07:12:34.556676
last batch sz 160
Model ind 640 epoch 465 head B head_i_epoch 0 batch 0: avg loss -1.643761 avg loss no lamb -1.643761 time 2019-02-22 07:14:19.902421
Model ind 640 epoch 465 head B head_i_epoch 0 batch 100: avg loss -1.621316 avg loss no lamb -1.621316 time 2019-02-22 07:16:44.819017
Model ind 640 epoch 465 head B head_i_epoch 0 batch 200: avg loss -1.739262 avg loss no lamb -1.739262 time 2019-02-22 07:19:08.959673
last batch sz 160
Model ind 640 epoch 465 head B head_i_epoch 1 batch 0: avg loss -1.678027 avg loss no lamb -1.678027 time 2019-02-22 07:20:53.461356
Model ind 640 epoch 465 head B head_i_epoch 1 batch 100: avg loss -1.627835 avg loss no lamb -1.627835 time 2019-02-22 07:23:17.387340
Model ind 640 epoch 465 head B head_i_epoch 1 batch 200: avg loss -1.721078 avg loss no lamb -1.721078 time 2019-02-22 07:25:41.872988
last batch sz 160
Pre: time 2019-02-22 07:27:52.195509: 
 	std: 0.05291822
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49883333, 0.60751665, 0.60733336, 0.60725, 0.49986666]
	train_accs: [0.49883333, 0.60751665, 0.60733336, 0.60725, 0.49986666]
	best_train_sub_head: 1
	worst: 0.49883333
	avg: 0.56416
	best: 0.60751665

Starting e_i: 466
Model ind 640 epoch 466 head A head_i_epoch 0 batch 0: avg loss -2.991700 avg loss no lamb -2.991700 time 2019-02-22 07:27:54.428755
Model ind 640 epoch 466 head A head_i_epoch 0 batch 100: avg loss -2.912049 avg loss no lamb -2.912049 time 2019-02-22 07:30:19.304525
Model ind 640 epoch 466 head A head_i_epoch 0 batch 200: avg loss -3.070260 avg loss no lamb -3.070260 time 2019-02-22 07:32:44.270889
last batch sz 160
Model ind 640 epoch 466 head B head_i_epoch 0 batch 0: avg loss -1.708127 avg loss no lamb -1.708127 time 2019-02-22 07:34:29.614590
Model ind 640 epoch 466 head B head_i_epoch 0 batch 100: avg loss -1.650662 avg loss no lamb -1.650662 time 2019-02-22 07:36:54.376622
Model ind 640 epoch 466 head B head_i_epoch 0 batch 200: avg loss -1.792796 avg loss no lamb -1.792796 time 2019-02-22 07:39:18.443679
last batch sz 160
Model ind 640 epoch 466 head B head_i_epoch 1 batch 0: avg loss -1.660157 avg loss no lamb -1.660157 time 2019-02-22 07:41:03.532272
Model ind 640 epoch 466 head B head_i_epoch 1 batch 100: avg loss -1.638791 avg loss no lamb -1.638791 time 2019-02-22 07:43:27.876931
Model ind 640 epoch 466 head B head_i_epoch 1 batch 200: avg loss -1.749475 avg loss no lamb -1.749475 time 2019-02-22 07:45:52.217577
last batch sz 160
Pre: time 2019-02-22 07:48:01.773222: 
 	std: 0.053555466
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5018167, 0.6113167, 0.61113334, 0.6113333, 0.5020667]
	train_accs: [0.5018167, 0.6113167, 0.61113334, 0.6113333, 0.5020667]
	best_train_sub_head: 3
	worst: 0.5018167
	avg: 0.5675334
	best: 0.6113333

Starting e_i: 467
Model ind 640 epoch 467 head A head_i_epoch 0 batch 0: avg loss -3.045738 avg loss no lamb -3.045738 time 2019-02-22 07:48:03.975970
Model ind 640 epoch 467 head A head_i_epoch 0 batch 100: avg loss -2.881927 avg loss no lamb -2.881927 time 2019-02-22 07:50:28.239248
Model ind 640 epoch 467 head A head_i_epoch 0 batch 200: avg loss -3.070643 avg loss no lamb -3.070643 time 2019-02-22 07:52:52.604885
last batch sz 160
Model ind 640 epoch 467 head B head_i_epoch 0 batch 0: avg loss -1.647099 avg loss no lamb -1.647099 time 2019-02-22 07:54:38.233706
Model ind 640 epoch 467 head B head_i_epoch 0 batch 100: avg loss -1.617754 avg loss no lamb -1.617754 time 2019-02-22 07:57:03.722253
Model ind 640 epoch 467 head B head_i_epoch 0 batch 200: avg loss -1.780025 avg loss no lamb -1.780025 time 2019-02-22 07:59:28.052767
last batch sz 160
Model ind 640 epoch 467 head B head_i_epoch 1 batch 0: avg loss -1.670390 avg loss no lamb -1.670390 time 2019-02-22 08:01:13.020455
Model ind 640 epoch 467 head B head_i_epoch 1 batch 100: avg loss -1.599697 avg loss no lamb -1.599697 time 2019-02-22 08:03:37.220911
Model ind 640 epoch 467 head B head_i_epoch 1 batch 200: avg loss -1.843041 avg loss no lamb -1.843041 time 2019-02-22 08:06:02.295082
last batch sz 160
Pre: time 2019-02-22 08:08:12.432701: 
 	std: 0.052396253
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50123334, 0.6085, 0.6084167, 0.6084667, 0.5017833]
	train_accs: [0.50123334, 0.6085, 0.6084167, 0.6084667, 0.5017833]
	best_train_sub_head: 1
	worst: 0.50123334
	avg: 0.56568
	best: 0.6085

Starting e_i: 468
Model ind 640 epoch 468 head A head_i_epoch 0 batch 0: avg loss -3.060409 avg loss no lamb -3.060409 time 2019-02-22 08:08:15.120624
Model ind 640 epoch 468 head A head_i_epoch 0 batch 100: avg loss -2.989386 avg loss no lamb -2.989386 time 2019-02-22 08:10:39.821947
Model ind 640 epoch 468 head A head_i_epoch 0 batch 200: avg loss -3.067483 avg loss no lamb -3.067483 time 2019-02-22 08:13:04.664676
last batch sz 160
Model ind 640 epoch 468 head B head_i_epoch 0 batch 0: avg loss -1.721999 avg loss no lamb -1.721999 time 2019-02-22 08:14:49.407960
Model ind 640 epoch 468 head B head_i_epoch 0 batch 100: avg loss -1.656019 avg loss no lamb -1.656019 time 2019-02-22 08:17:14.009116
Model ind 640 epoch 468 head B head_i_epoch 0 batch 200: avg loss -1.714861 avg loss no lamb -1.714861 time 2019-02-22 08:19:38.332880
last batch sz 160
Model ind 640 epoch 468 head B head_i_epoch 1 batch 0: avg loss -1.652907 avg loss no lamb -1.652907 time 2019-02-22 08:21:23.577875
Model ind 640 epoch 468 head B head_i_epoch 1 batch 100: avg loss -1.527262 avg loss no lamb -1.527262 time 2019-02-22 08:23:48.416790
Model ind 640 epoch 468 head B head_i_epoch 1 batch 200: avg loss -1.748178 avg loss no lamb -1.748178 time 2019-02-22 08:26:13.386720
last batch sz 160
Pre: time 2019-02-22 08:28:23.248678: 
 	std: 0.05296235
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4984, 0.60621667, 0.6063833, 0.60615, 0.49788332]
	train_accs: [0.4984, 0.60621667, 0.6063833, 0.60615, 0.49788332]
	best_train_sub_head: 2
	worst: 0.49788332
	avg: 0.56300664
	best: 0.6063833

Starting e_i: 469
Model ind 640 epoch 469 head A head_i_epoch 0 batch 0: avg loss -2.987523 avg loss no lamb -2.987523 time 2019-02-22 08:28:25.479540
Model ind 640 epoch 469 head A head_i_epoch 0 batch 100: avg loss -2.933947 avg loss no lamb -2.933947 time 2019-02-22 08:30:49.632255
Model ind 640 epoch 469 head A head_i_epoch 0 batch 200: avg loss -3.049134 avg loss no lamb -3.049134 time 2019-02-22 08:33:14.436408
last batch sz 160
Model ind 640 epoch 469 head B head_i_epoch 0 batch 0: avg loss -1.687804 avg loss no lamb -1.687804 time 2019-02-22 08:34:59.855693
Model ind 640 epoch 469 head B head_i_epoch 0 batch 100: avg loss -1.703879 avg loss no lamb -1.703879 time 2019-02-22 08:37:24.198107
Model ind 640 epoch 469 head B head_i_epoch 0 batch 200: avg loss -1.791888 avg loss no lamb -1.791888 time 2019-02-22 08:39:48.953395
last batch sz 160
Model ind 640 epoch 469 head B head_i_epoch 1 batch 0: avg loss -1.601754 avg loss no lamb -1.601754 time 2019-02-22 08:41:34.068740
Model ind 640 epoch 469 head B head_i_epoch 1 batch 100: avg loss -1.624782 avg loss no lamb -1.624782 time 2019-02-22 08:43:58.359919
Model ind 640 epoch 469 head B head_i_epoch 1 batch 200: avg loss -1.783255 avg loss no lamb -1.783255 time 2019-02-22 08:46:22.477579
last batch sz 160
Pre: time 2019-02-22 08:48:32.527359: 
 	std: 0.05079883
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50053334, 0.6037, 0.604, 0.60415, 0.49998334]
	train_accs: [0.50053334, 0.6037, 0.604, 0.60415, 0.49998334]
	best_train_sub_head: 3
	worst: 0.49998334
	avg: 0.5624733
	best: 0.60415

Starting e_i: 470
Model ind 640 epoch 470 head A head_i_epoch 0 batch 0: avg loss -2.991670 avg loss no lamb -2.991670 time 2019-02-22 08:48:34.818607
Model ind 640 epoch 470 head A head_i_epoch 0 batch 100: avg loss -2.888412 avg loss no lamb -2.888412 time 2019-02-22 08:50:59.588726
Model ind 640 epoch 470 head A head_i_epoch 0 batch 200: avg loss -3.091539 avg loss no lamb -3.091539 time 2019-02-22 08:53:24.064849
last batch sz 160
Model ind 640 epoch 470 head B head_i_epoch 0 batch 0: avg loss -1.715627 avg loss no lamb -1.715627 time 2019-02-22 08:55:09.163424
Model ind 640 epoch 470 head B head_i_epoch 0 batch 100: avg loss -1.533900 avg loss no lamb -1.533900 time 2019-02-22 08:57:33.746734
Model ind 640 epoch 470 head B head_i_epoch 0 batch 200: avg loss -1.759662 avg loss no lamb -1.759662 time 2019-02-22 08:59:58.712253
last batch sz 160
Model ind 640 epoch 470 head B head_i_epoch 1 batch 0: avg loss -1.664556 avg loss no lamb -1.664556 time 2019-02-22 09:01:44.127593
Model ind 640 epoch 470 head B head_i_epoch 1 batch 100: avg loss -1.658434 avg loss no lamb -1.658434 time 2019-02-22 09:04:08.380816
Model ind 640 epoch 470 head B head_i_epoch 1 batch 200: avg loss -1.747321 avg loss no lamb -1.747321 time 2019-02-22 09:06:32.251330
last batch sz 160
Pre: time 2019-02-22 09:08:42.628014: 
 	std: 0.05282503
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49918333, 0.60665, 0.60715, 0.6066833, 0.49881667]
	train_accs: [0.49918333, 0.60665, 0.60715, 0.6066833, 0.49881667]
	best_train_sub_head: 2
	worst: 0.49881667
	avg: 0.5636967
	best: 0.60715

Starting e_i: 471
Model ind 640 epoch 471 head A head_i_epoch 0 batch 0: avg loss -2.997225 avg loss no lamb -2.997225 time 2019-02-22 09:08:48.092299
Model ind 640 epoch 471 head A head_i_epoch 0 batch 100: avg loss -2.978098 avg loss no lamb -2.978098 time 2019-02-22 09:11:12.924179
Model ind 640 epoch 471 head A head_i_epoch 0 batch 200: avg loss -2.979710 avg loss no lamb -2.979710 time 2019-02-22 09:13:37.665909
last batch sz 160
Model ind 640 epoch 471 head B head_i_epoch 0 batch 0: avg loss -1.635354 avg loss no lamb -1.635354 time 2019-02-22 09:15:23.106341
Model ind 640 epoch 471 head B head_i_epoch 0 batch 100: avg loss -1.595688 avg loss no lamb -1.595688 time 2019-02-22 09:17:47.496229
Model ind 640 epoch 471 head B head_i_epoch 0 batch 200: avg loss -1.830121 avg loss no lamb -1.830121 time 2019-02-22 09:20:11.344880
last batch sz 160
Model ind 640 epoch 471 head B head_i_epoch 1 batch 0: avg loss -1.723628 avg loss no lamb -1.723628 time 2019-02-22 09:21:56.926248
Model ind 640 epoch 471 head B head_i_epoch 1 batch 100: avg loss -1.601426 avg loss no lamb -1.601426 time 2019-02-22 09:24:21.929155
Model ind 640 epoch 471 head B head_i_epoch 1 batch 200: avg loss -1.751423 avg loss no lamb -1.751423 time 2019-02-22 09:26:46.399034
last batch sz 160
Pre: time 2019-02-22 09:28:55.780291: 
 	std: 0.051542804
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5027, 0.60815, 0.6088167, 0.60788333, 0.50345]
	train_accs: [0.5027, 0.60815, 0.6088167, 0.60788333, 0.50345]
	best_train_sub_head: 2
	worst: 0.5027
	avg: 0.56619996
	best: 0.6088167

Starting e_i: 472
Model ind 640 epoch 472 head A head_i_epoch 0 batch 0: avg loss -2.980050 avg loss no lamb -2.980050 time 2019-02-22 09:28:57.996307
Model ind 640 epoch 472 head A head_i_epoch 0 batch 100: avg loss -2.892159 avg loss no lamb -2.892159 time 2019-02-22 09:31:22.918339
Model ind 640 epoch 472 head A head_i_epoch 0 batch 200: avg loss -3.051055 avg loss no lamb -3.051055 time 2019-02-22 09:33:48.054529
last batch sz 160
Model ind 640 epoch 472 head B head_i_epoch 0 batch 0: avg loss -1.760129 avg loss no lamb -1.760129 time 2019-02-22 09:35:33.342515
Model ind 640 epoch 472 head B head_i_epoch 0 batch 100: avg loss -1.667554 avg loss no lamb -1.667554 time 2019-02-22 09:37:57.832823
Model ind 640 epoch 472 head B head_i_epoch 0 batch 200: avg loss -1.694346 avg loss no lamb -1.694346 time 2019-02-22 09:40:22.217106
last batch sz 160
Model ind 640 epoch 472 head B head_i_epoch 1 batch 0: avg loss -1.708794 avg loss no lamb -1.708794 time 2019-02-22 09:42:07.531854
Model ind 640 epoch 472 head B head_i_epoch 1 batch 100: avg loss -1.704005 avg loss no lamb -1.704005 time 2019-02-22 09:44:31.983789
Model ind 640 epoch 472 head B head_i_epoch 1 batch 200: avg loss -1.802857 avg loss no lamb -1.802857 time 2019-02-22 09:46:56.777964
last batch sz 160
Pre: time 2019-02-22 09:49:06.634804: 
 	std: 0.05153462
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5031, 0.60901666, 0.60943335, 0.60871667, 0.5046333]
	train_accs: [0.5031, 0.60901666, 0.60943335, 0.60871667, 0.5046333]
	best_train_sub_head: 2
	worst: 0.5031
	avg: 0.56698
	best: 0.60943335

Starting e_i: 473
Model ind 640 epoch 473 head A head_i_epoch 0 batch 0: avg loss -3.052105 avg loss no lamb -3.052105 time 2019-02-22 09:49:09.327505
Model ind 640 epoch 473 head A head_i_epoch 0 batch 100: avg loss -2.893106 avg loss no lamb -2.893106 time 2019-02-22 09:51:33.739980
Model ind 640 epoch 473 head A head_i_epoch 0 batch 200: avg loss -3.091259 avg loss no lamb -3.091259 time 2019-02-22 09:53:58.114835
last batch sz 160
Model ind 640 epoch 473 head B head_i_epoch 0 batch 0: avg loss -1.667249 avg loss no lamb -1.667249 time 2019-02-22 09:55:43.522126
Model ind 640 epoch 473 head B head_i_epoch 0 batch 100: avg loss -1.583742 avg loss no lamb -1.583742 time 2019-02-22 09:58:08.240702
Model ind 640 epoch 473 head B head_i_epoch 0 batch 200: avg loss -1.749715 avg loss no lamb -1.749715 time 2019-02-22 10:00:32.948463
last batch sz 160
Model ind 640 epoch 473 head B head_i_epoch 1 batch 0: avg loss -1.707592 avg loss no lamb -1.707592 time 2019-02-22 10:02:18.383375
Model ind 640 epoch 473 head B head_i_epoch 1 batch 100: avg loss -1.568270 avg loss no lamb -1.568270 time 2019-02-22 10:04:43.364089
Model ind 640 epoch 473 head B head_i_epoch 1 batch 200: avg loss -1.751860 avg loss no lamb -1.751860 time 2019-02-22 10:07:07.917557
last batch sz 160
Pre: time 2019-02-22 10:09:17.856562: 
 	std: 0.053020004
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5025667, 0.6109333, 0.6110333, 0.61148334, 0.5032833]
	train_accs: [0.5025667, 0.6109333, 0.6110333, 0.61148334, 0.5032833]
	best_train_sub_head: 3
	worst: 0.5025667
	avg: 0.56786
	best: 0.61148334

Starting e_i: 474
Model ind 640 epoch 474 head A head_i_epoch 0 batch 0: avg loss -3.023185 avg loss no lamb -3.023185 time 2019-02-22 10:09:20.176149
Model ind 640 epoch 474 head A head_i_epoch 0 batch 100: avg loss -2.960473 avg loss no lamb -2.960473 time 2019-02-22 10:11:44.639570
Model ind 640 epoch 474 head A head_i_epoch 0 batch 200: avg loss -3.086116 avg loss no lamb -3.086116 time 2019-02-22 10:14:09.835659
last batch sz 160
Model ind 640 epoch 474 head B head_i_epoch 0 batch 0: avg loss -1.706604 avg loss no lamb -1.706604 time 2019-02-22 10:15:55.190777
Model ind 640 epoch 474 head B head_i_epoch 0 batch 100: avg loss -1.698683 avg loss no lamb -1.698683 time 2019-02-22 10:18:20.033162
Model ind 640 epoch 474 head B head_i_epoch 0 batch 200: avg loss -1.784567 avg loss no lamb -1.784567 time 2019-02-22 10:20:44.506480
last batch sz 160
Model ind 640 epoch 474 head B head_i_epoch 1 batch 0: avg loss -1.625766 avg loss no lamb -1.625766 time 2019-02-22 10:22:29.681179
Model ind 640 epoch 474 head B head_i_epoch 1 batch 100: avg loss -1.573551 avg loss no lamb -1.573551 time 2019-02-22 10:24:54.650384
Model ind 640 epoch 474 head B head_i_epoch 1 batch 200: avg loss -1.871419 avg loss no lamb -1.871419 time 2019-02-22 10:27:19.352167
last batch sz 160
Pre: time 2019-02-22 10:29:29.197479: 
 	std: 0.05262299
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50236666, 0.60995, 0.61046666, 0.61035, 0.50331664]
	train_accs: [0.50236666, 0.60995, 0.61046666, 0.61035, 0.50331664]
	best_train_sub_head: 2
	worst: 0.50236666
	avg: 0.56728995
	best: 0.61046666

Starting e_i: 475
Model ind 640 epoch 475 head A head_i_epoch 0 batch 0: avg loss -2.954387 avg loss no lamb -2.954387 time 2019-02-22 10:29:31.493840
Model ind 640 epoch 475 head A head_i_epoch 0 batch 100: avg loss -2.895633 avg loss no lamb -2.895633 time 2019-02-22 10:31:56.330306
Model ind 640 epoch 475 head A head_i_epoch 0 batch 200: avg loss -3.058575 avg loss no lamb -3.058575 time 2019-02-22 10:34:20.870093
last batch sz 160
Model ind 640 epoch 475 head B head_i_epoch 0 batch 0: avg loss -1.658065 avg loss no lamb -1.658065 time 2019-02-22 10:36:06.092826
Model ind 640 epoch 475 head B head_i_epoch 0 batch 100: avg loss -1.629545 avg loss no lamb -1.629545 time 2019-02-22 10:38:30.571167
Model ind 640 epoch 475 head B head_i_epoch 0 batch 200: avg loss -1.853422 avg loss no lamb -1.853422 time 2019-02-22 10:40:54.793179
last batch sz 160
Model ind 640 epoch 475 head B head_i_epoch 1 batch 0: avg loss -1.713143 avg loss no lamb -1.713143 time 2019-02-22 10:42:40.049474
Model ind 640 epoch 475 head B head_i_epoch 1 batch 100: avg loss -1.604302 avg loss no lamb -1.604302 time 2019-02-22 10:45:04.834924
Model ind 640 epoch 475 head B head_i_epoch 1 batch 200: avg loss -1.741052 avg loss no lamb -1.741052 time 2019-02-22 10:47:28.619333
last batch sz 160
Pre: time 2019-02-22 10:49:38.605716: 
 	std: 0.052630298
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49913332, 0.6066, 0.6069, 0.6063667, 0.49925]
	train_accs: [0.49913332, 0.6066, 0.6069, 0.6063667, 0.49925]
	best_train_sub_head: 2
	worst: 0.49913332
	avg: 0.56365
	best: 0.6069

Starting e_i: 476
Model ind 640 epoch 476 head A head_i_epoch 0 batch 0: avg loss -2.934887 avg loss no lamb -2.934887 time 2019-02-22 10:49:40.830207
Model ind 640 epoch 476 head A head_i_epoch 0 batch 100: avg loss -2.922455 avg loss no lamb -2.922455 time 2019-02-22 10:52:07.110316
Model ind 640 epoch 476 head A head_i_epoch 0 batch 200: avg loss -3.051187 avg loss no lamb -3.051187 time 2019-02-22 10:54:32.151626
last batch sz 160
Model ind 640 epoch 476 head B head_i_epoch 0 batch 0: avg loss -1.632481 avg loss no lamb -1.632481 time 2019-02-22 10:56:17.653747
Model ind 640 epoch 476 head B head_i_epoch 0 batch 100: avg loss -1.511006 avg loss no lamb -1.511006 time 2019-02-22 10:58:42.657632
Model ind 640 epoch 476 head B head_i_epoch 0 batch 200: avg loss -1.781506 avg loss no lamb -1.781506 time 2019-02-22 11:01:07.864381
last batch sz 160
Model ind 640 epoch 476 head B head_i_epoch 1 batch 0: avg loss -1.693310 avg loss no lamb -1.693310 time 2019-02-22 11:02:53.479846
