Debug: False
Config: Config: -----
batchnorm_track: False
data_mean: [0.5, 0.5, 0.5]
out_dir: /scratch/shared/slow/xuji/iid_private/685
always_rot: False
dataset: MNIST
num_epochs: 3200
lr_schedule: []
mapping_test_partitions: [True, False]
lamb_B: 1.0
lamb_A: 1.0
crop_other: True
output_k_B: 10
save_freq: 20
output_k_A: 50
lr_mult: 0.1
tf3_crop_sz: 0
input_sz: 24
dataloader_batch_sz: 140
eval_mode: hung
in_channels: 1
lr: 0.0001
gt_k: 10
no_flip: True
tf3_crop_diff: False
opt: Adam
restart_from_best: False
double_eval: False
output_k: 10
num_dataloaders: 5
tf2_crop: random
tf2_crop_szs: [16, 20, 24]
head_B_epochs: 2
num_sub_heads: 5
crop_orig: True
tf1_crop: centre_half
test_code: False
twohead: True
tf1_crop_sz: 20
mapping_assignment_partitions: [True, False]
per_img_demean: False
dataset_root: /scratch/local/ssd/xuji/MNIST
arch: ClusterNet6cTwoHead
head_A_first: False
restart: False
batch_sz: 700
out_root: /scratch/shared/slow/xuji/iid_private
model_ind: 685
data_std: [0.5, 0.5, 0.5]
head_A_epochs: 1
demean: False
mode: IID
train_partitions: [True, False]
rot_val: 25.0
no_jitter: False
----------
selected centre_half crop for tf1
tf3 crop size is same as tf1
adding rotation option for imgs_tf: 25
not always_rot
selected random crop for tf2
adding crop size option for imgs_tf: 16
selected random crop for tf2
adding crop size option for imgs_tf: 20
selected random crop for tf2
adding crop size option for imgs_tf: 24
not adding flip
adding jitter
not demeaning data
not per image demeaning data
Making datasets with <class 'torchvision.datasets.mnist.MNIST'> and None
Creating auxiliary dataloader ind 0 out of 5 time 2019-02-21 18:48:45.025040
Creating auxiliary dataloader ind 1 out of 5 time 2019-02-21 18:48:45.074542
Creating auxiliary dataloader ind 2 out of 5 time 2019-02-21 18:48:45.129966
Creating auxiliary dataloader ind 3 out of 5 time 2019-02-21 18:48:45.187893
Creating auxiliary dataloader ind 4 out of 5 time 2019-02-21 18:48:45.232499
Length of datasets vector 6
Number of batches per epoch: 500
Creating auxiliary dataloader ind 0 out of 5 time 2019-02-21 18:48:45.319005
Creating auxiliary dataloader ind 1 out of 5 time 2019-02-21 18:48:45.359538
Creating auxiliary dataloader ind 2 out of 5 time 2019-02-21 18:48:45.405495
Creating auxiliary dataloader ind 3 out of 5 time 2019-02-21 18:48:45.448094
Creating auxiliary dataloader ind 4 out of 5 time 2019-02-21 18:48:45.497973
Length of datasets vector 6
Number of batches per epoch: 500
semisup: False
Pre: time 2019-02-21 18:49:00.249597: 
 	std: 0.01260446
	best_train_sub_head_match: [(0, 3), (1, 5), (2, 1), (3, 6), (4, 9), (5, 8), (6, 2), (7, 4), (8, 7), (9, 0)]
	test_accs: [0.22284286, 0.19928572, 0.19282857, 0.20842858, 0.18692857]
	train_accs: [0.22284286, 0.19928572, 0.19282857, 0.20842858, 0.18692857]
	best_train_sub_head: 0
	worst: 0.18692857
	avg: 0.20206285
	best: 0.22284286

Starting e_i: 1
Model ind 685 epoch 1 head B batch: 0 avg loss -0.000033 avg loss no lamb -0.000033 time 2019-02-21 18:49:00.750554
Model ind 685 epoch 1 head B batch: 1 avg loss -0.000280 avg loss no lamb -0.000280 time 2019-02-21 18:49:01.354227
Model ind 685 epoch 1 head B batch: 2 avg loss -0.002348 avg loss no lamb -0.002348 time 2019-02-21 18:49:01.929224
Model ind 685 epoch 1 head B batch: 3 avg loss -0.008303 avg loss no lamb -0.008303 time 2019-02-21 18:49:02.505143
Model ind 685 epoch 1 head B batch: 4 avg loss -0.011104 avg loss no lamb -0.011104 time 2019-02-21 18:49:03.140007
Model ind 685 epoch 1 head B batch: 5 avg loss -0.013754 avg loss no lamb -0.013754 time 2019-02-21 18:49:03.654731
Model ind 685 epoch 1 head B batch: 6 avg loss -0.025951 avg loss no lamb -0.025951 time 2019-02-21 18:49:04.207134
Model ind 685 epoch 1 head B batch: 7 avg loss -0.039980 avg loss no lamb -0.039980 time 2019-02-21 18:49:04.768620
Model ind 685 epoch 1 head B batch: 8 avg loss -0.051420 avg loss no lamb -0.051420 time 2019-02-21 18:49:05.336061
Model ind 685 epoch 1 head B batch: 9 avg loss -0.042005 avg loss no lamb -0.042005 time 2019-02-21 18:49:05.945808
Model ind 685 epoch 1 head B batch: 10 avg loss -0.061521 avg loss no lamb -0.061521 time 2019-02-21 18:49:06.571077
Model ind 685 epoch 1 head B batch: 11 avg loss -0.063650 avg loss no lamb -0.063650 time 2019-02-21 18:49:07.148740
Model ind 685 epoch 1 head B batch: 12 avg loss -0.080131 avg loss no lamb -0.080131 time 2019-02-21 18:49:07.694988
Model ind 685 epoch 1 head B batch: 13 avg loss -0.114335 avg loss no lamb -0.114335 time 2019-02-21 18:49:08.218753
Model ind 685 epoch 1 head B batch: 14 avg loss -0.146583 avg loss no lamb -0.146583 time 2019-02-21 18:49:08.727419
Model ind 685 epoch 1 head B batch: 15 avg loss -0.148283 avg loss no lamb -0.148283 time 2019-02-21 18:49:09.246944
Model ind 685 epoch 1 head B batch: 16 avg loss -0.123003 avg loss no lamb -0.123003 time 2019-02-21 18:49:09.758887
Model ind 685 epoch 1 head B batch: 17 avg loss -0.142246 avg loss no lamb -0.142246 time 2019-02-21 18:49:10.267423
Model ind 685 epoch 1 head B batch: 18 avg loss -0.184152 avg loss no lamb -0.184152 time 2019-02-21 18:49:10.760400
Model ind 685 epoch 1 head B batch: 19 avg loss -0.212709 avg loss no lamb -0.212709 time 2019-02-21 18:49:11.277181
Model ind 685 epoch 1 head B batch: 20 avg loss -0.236484 avg loss no lamb -0.236484 time 2019-02-21 18:49:11.782923
Model ind 685 epoch 1 head B batch: 21 avg loss -0.173478 avg loss no lamb -0.173478 time 2019-02-21 18:49:12.333440
Model ind 685 epoch 1 head B batch: 22 avg loss -0.183565 avg loss no lamb -0.183565 time 2019-02-21 18:49:12.878400
Model ind 685 epoch 1 head B batch: 23 avg loss -0.185163 avg loss no lamb -0.185163 time 2019-02-21 18:49:13.400224
Model ind 685 epoch 1 head B batch: 24 avg loss -0.258415 avg loss no lamb -0.258415 time 2019-02-21 18:49:13.945743
Model ind 685 epoch 1 head B batch: 25 avg loss -0.263569 avg loss no lamb -0.263569 time 2019-02-21 18:49:14.462051
Model ind 685 epoch 1 head B batch: 26 avg loss -0.261647 avg loss no lamb -0.261647 time 2019-02-21 18:49:14.982978
Model ind 685 epoch 1 head B batch: 27 avg loss -0.242802 avg loss no lamb -0.242802 time 2019-02-21 18:49:15.524861
Model ind 685 epoch 1 head B batch: 28 avg loss -0.252415 avg loss no lamb -0.252415 time 2019-02-21 18:49:16.133016
Model ind 685 epoch 1 head B batch: 29 avg loss -0.265290 avg loss no lamb -0.265290 time 2019-02-21 18:49:16.697108
Model ind 685 epoch 1 head B batch: 30 avg loss -0.238226 avg loss no lamb -0.238226 time 2019-02-21 18:49:17.227459
Model ind 685 epoch 1 head B batch: 31 avg loss -0.213025 avg loss no lamb -0.213025 time 2019-02-21 18:49:17.762650
Model ind 685 epoch 1 head B batch: 32 avg loss -0.345592 avg loss no lamb -0.345592 time 2019-02-21 18:49:18.329623
Model ind 685 epoch 1 head B batch: 33 avg loss -0.303930 avg loss no lamb -0.303930 time 2019-02-21 18:49:18.824769
Model ind 685 epoch 1 head B batch: 34 avg loss -0.329325 avg loss no lamb -0.329325 time 2019-02-21 18:49:19.350312
Model ind 685 epoch 1 head B batch: 35 avg loss -0.408095 avg loss no lamb -0.408095 time 2019-02-21 18:49:19.893213
Model ind 685 epoch 1 head B batch: 36 avg loss -0.342776 avg loss no lamb -0.342776 time 2019-02-21 18:49:20.428421
Model ind 685 epoch 1 head B batch: 37 avg loss -0.310439 avg loss no lamb -0.310439 time 2019-02-21 18:49:20.985873
Model ind 685 epoch 1 head B batch: 38 avg loss -0.314825 avg loss no lamb -0.314825 time 2019-02-21 18:49:21.511443
Model ind 685 epoch 1 head B batch: 39 avg loss -0.394279 avg loss no lamb -0.394279 time 2019-02-21 18:49:22.062386
Model ind 685 epoch 1 head B batch: 40 avg loss -0.375588 avg loss no lamb -0.375588 time 2019-02-21 18:49:22.610550
Model ind 685 epoch 1 head B batch: 41 avg loss -0.329531 avg loss no lamb -0.329531 time 2019-02-21 18:49:23.154022
Model ind 685 epoch 1 head B batch: 42 avg loss -0.359448 avg loss no lamb -0.359448 time 2019-02-21 18:49:23.758449
Model ind 685 epoch 1 head B batch: 43 avg loss -0.407339 avg loss no lamb -0.407339 time 2019-02-21 18:49:24.297268
Model ind 685 epoch 1 head B batch: 44 avg loss -0.418310 avg loss no lamb -0.418310 time 2019-02-21 18:49:24.815264
Model ind 685 epoch 1 head B batch: 45 avg loss -0.407252 avg loss no lamb -0.407252 time 2019-02-21 18:49:25.431520
Model ind 685 epoch 1 head B batch: 46 avg loss -0.387159 avg loss no lamb -0.387159 time 2019-02-21 18:49:25.953660
Model ind 685 epoch 1 head B batch: 47 avg loss -0.401584 avg loss no lamb -0.401584 time 2019-02-21 18:49:26.509608
Model ind 685 epoch 1 head B batch: 48 avg loss -0.362145 avg loss no lamb -0.362145 time 2019-02-21 18:49:27.043113
Model ind 685 epoch 1 head B batch: 49 avg loss -0.418638 avg loss no lamb -0.418638 time 2019-02-21 18:49:27.630078
Model ind 685 epoch 1 head B batch: 50 avg loss -0.405865 avg loss no lamb -0.405865 time 2019-02-21 18:49:28.175914
Model ind 685 epoch 1 head B batch: 51 avg loss -0.383813 avg loss no lamb -0.383813 time 2019-02-21 18:49:28.764949
Model ind 685 epoch 1 head B batch: 52 avg loss -0.368304 avg loss no lamb -0.368304 time 2019-02-21 18:49:29.298624
Model ind 685 epoch 1 head B batch: 53 avg loss -0.478625 avg loss no lamb -0.478625 time 2019-02-21 18:49:29.916496
Model ind 685 epoch 1 head B batch: 54 avg loss -0.454323 avg loss no lamb -0.454323 time 2019-02-21 18:49:30.457891
Model ind 685 epoch 1 head B batch: 55 avg loss -0.442845 avg loss no lamb -0.442845 time 2019-02-21 18:49:30.994203
Model ind 685 epoch 1 head B batch: 56 avg loss -0.459630 avg loss no lamb -0.459630 time 2019-02-21 18:49:31.598218
Model ind 685 epoch 1 head B batch: 57 avg loss -0.479447 avg loss no lamb -0.479447 time 2019-02-21 18:49:32.183671
Model ind 685 epoch 1 head B batch: 58 avg loss -0.456908 avg loss no lamb -0.456908 time 2019-02-21 18:49:32.787626
Model ind 685 epoch 1 head B batch: 59 avg loss -0.464242 avg loss no lamb -0.464242 time 2019-02-21 18:49:33.381434
Model ind 685 epoch 1 head B batch: 60 avg loss -0.491208 avg loss no lamb -0.491208 time 2019-02-21 18:49:33.904513
Model ind 685 epoch 1 head B batch: 61 avg loss -0.579164 avg loss no lamb -0.579164 time 2019-02-21 18:49:34.417637
Model ind 685 epoch 1 head B batch: 62 avg loss -0.419511 avg loss no lamb -0.419511 time 2019-02-21 18:49:35.012399
Model ind 685 epoch 1 head B batch: 63 avg loss -0.446423 avg loss no lamb -0.446423 time 2019-02-21 18:49:35.568636
Model ind 685 epoch 1 head B batch: 64 avg loss -0.570957 avg loss no lamb -0.570957 time 2019-02-21 18:49:36.164839
Model ind 685 epoch 1 head B batch: 65 avg loss -0.538875 avg loss no lamb -0.538875 time 2019-02-21 18:49:36.712457
Model ind 685 epoch 1 head B batch: 66 avg loss -0.529783 avg loss no lamb -0.529783 time 2019-02-21 18:49:37.265602
Model ind 685 epoch 1 head B batch: 67 avg loss -0.472610 avg loss no lamb -0.472610 time 2019-02-21 18:49:37.875004
Model ind 685 epoch 1 head B batch: 68 avg loss -0.528186 avg loss no lamb -0.528186 time 2019-02-21 18:49:38.457386
Model ind 685 epoch 1 head B batch: 69 avg loss -0.501411 avg loss no lamb -0.501411 time 2019-02-21 18:49:38.977067
Model ind 685 epoch 1 head B batch: 70 avg loss -0.537921 avg loss no lamb -0.537921 time 2019-02-21 18:49:39.587099
Model ind 685 epoch 1 head B batch: 71 avg loss -0.526153 avg loss no lamb -0.526153 time 2019-02-21 18:49:40.166557
Model ind 685 epoch 1 head B batch: 72 avg loss -0.501028 avg loss no lamb -0.501028 time 2019-02-21 18:49:40.734390
Model ind 685 epoch 1 head B batch: 73 avg loss -0.533894 avg loss no lamb -0.533894 time 2019-02-21 18:49:41.307946
Model ind 685 epoch 1 head B batch: 74 avg loss -0.527928 avg loss no lamb -0.527928 time 2019-02-21 18:49:41.855177
Model ind 685 epoch 1 head B batch: 75 avg loss -0.565695 avg loss no lamb -0.565695 time 2019-02-21 18:49:42.499064
Model ind 685 epoch 1 head B batch: 76 avg loss -0.489635 avg loss no lamb -0.489635 time 2019-02-21 18:49:43.063040
Model ind 685 epoch 1 head B batch: 77 avg loss -0.621102 avg loss no lamb -0.621102 time 2019-02-21 18:49:43.720600
Model ind 685 epoch 1 head B batch: 78 avg loss -0.561004 avg loss no lamb -0.561004 time 2019-02-21 18:49:44.270368
Model ind 685 epoch 1 head B batch: 79 avg loss -0.599661 avg loss no lamb -0.599661 time 2019-02-21 18:49:44.892441
Model ind 685 epoch 1 head B batch: 80 avg loss -0.627332 avg loss no lamb -0.627332 time 2019-02-21 18:49:45.479152
Model ind 685 epoch 1 head B batch: 81 avg loss -0.567167 avg loss no lamb -0.567167 time 2019-02-21 18:49:46.030338
Model ind 685 epoch 1 head B batch: 82 avg loss -0.599100 avg loss no lamb -0.599100 time 2019-02-21 18:49:46.588138
Model ind 685 epoch 1 head B batch: 83 avg loss -0.514596 avg loss no lamb -0.514596 time 2019-02-21 18:49:47.131688
Model ind 685 epoch 1 head B batch: 84 avg loss -0.508087 avg loss no lamb -0.508087 time 2019-02-21 18:49:47.664386
Model ind 685 epoch 1 head B batch: 85 avg loss -0.557914 avg loss no lamb -0.557914 time 2019-02-21 18:49:48.161951
Model ind 685 epoch 1 head B batch: 86 avg loss -0.622647 avg loss no lamb -0.622647 time 2019-02-21 18:49:48.628415
Model ind 685 epoch 1 head B batch: 87 avg loss -0.519191 avg loss no lamb -0.519191 time 2019-02-21 18:49:49.144989
Model ind 685 epoch 1 head B batch: 88 avg loss -0.619803 avg loss no lamb -0.619803 time 2019-02-21 18:49:49.639209
Model ind 685 epoch 1 head B batch: 89 avg loss -0.533668 avg loss no lamb -0.533668 time 2019-02-21 18:49:50.127474
Model ind 685 epoch 1 head B batch: 90 avg loss -0.619798 avg loss no lamb -0.619798 time 2019-02-21 18:49:50.639496
Model ind 685 epoch 1 head B batch: 91 avg loss -0.644135 avg loss no lamb -0.644135 time 2019-02-21 18:49:51.158622
Model ind 685 epoch 1 head B batch: 92 avg loss -0.530197 avg loss no lamb -0.530197 time 2019-02-21 18:49:51.673676
Model ind 685 epoch 1 head B batch: 93 avg loss -0.501597 avg loss no lamb -0.501597 time 2019-02-21 18:49:52.187788
Model ind 685 epoch 1 head B batch: 94 avg loss -0.519991 avg loss no lamb -0.519991 time 2019-02-21 18:49:52.684273
Model ind 685 epoch 1 head B batch: 95 avg loss -0.506933 avg loss no lamb -0.506933 time 2019-02-21 18:49:53.191682
Model ind 685 epoch 1 head B batch: 96 avg loss -0.614335 avg loss no lamb -0.614335 time 2019-02-21 18:49:53.674575
Model ind 685 epoch 1 head B batch: 97 avg loss -0.630593 avg loss no lamb -0.630593 time 2019-02-21 18:49:54.190143
Model ind 685 epoch 1 head B batch: 98 avg loss -0.541075 avg loss no lamb -0.541075 time 2019-02-21 18:49:54.685048
Model ind 685 epoch 1 head B batch: 99 avg loss -0.498121 avg loss no lamb -0.498121 time 2019-02-21 18:49:55.251004
Model ind 685 epoch 1 head B batch: 100 avg loss -0.561403 avg loss no lamb -0.561403 time 2019-02-21 18:49:55.758601
Model ind 685 epoch 1 head B batch: 101 avg loss -0.616845 avg loss no lamb -0.616845 time 2019-02-21 18:49:56.328250
Model ind 685 epoch 1 head B batch: 102 avg loss -0.586953 avg loss no lamb -0.586953 time 2019-02-21 18:49:56.841299
Model ind 685 epoch 1 head B batch: 103 avg loss -0.507215 avg loss no lamb -0.507215 time 2019-02-21 18:49:57.462440
Model ind 685 epoch 1 head B batch: 104 avg loss -0.615683 avg loss no lamb -0.615683 time 2019-02-21 18:49:58.036789
Model ind 685 epoch 1 head B batch: 105 avg loss -0.588312 avg loss no lamb -0.588312 time 2019-02-21 18:49:58.642248
Model ind 685 epoch 1 head B batch: 106 avg loss -0.665089 avg loss no lamb -0.665089 time 2019-02-21 18:49:59.233621
Model ind 685 epoch 1 head B batch: 107 avg loss -0.689338 avg loss no lamb -0.689338 time 2019-02-21 18:49:59.835101
Model ind 685 epoch 1 head B batch: 108 avg loss -0.576407 avg loss no lamb -0.576407 time 2019-02-21 18:50:00.405924
Model ind 685 epoch 1 head B batch: 109 avg loss -0.691060 avg loss no lamb -0.691060 time 2019-02-21 18:50:00.952439
Model ind 685 epoch 1 head B batch: 110 avg loss -0.535593 avg loss no lamb -0.535593 time 2019-02-21 18:50:01.477880
Model ind 685 epoch 1 head B batch: 111 avg loss -0.647842 avg loss no lamb -0.647842 time 2019-02-21 18:50:01.968515
Model ind 685 epoch 1 head B batch: 112 avg loss -0.615432 avg loss no lamb -0.615432 time 2019-02-21 18:50:02.473717
Model ind 685 epoch 1 head B batch: 113 avg loss -0.534466 avg loss no lamb -0.534466 time 2019-02-21 18:50:02.951543
Model ind 685 epoch 1 head B batch: 114 avg loss -0.601300 avg loss no lamb -0.601300 time 2019-02-21 18:50:03.435621
Model ind 685 epoch 1 head B batch: 115 avg loss -0.686761 avg loss no lamb -0.686761 time 2019-02-21 18:50:03.928759
Model ind 685 epoch 1 head B batch: 116 avg loss -0.759341 avg loss no lamb -0.759341 time 2019-02-21 18:50:04.442989
Model ind 685 epoch 1 head B batch: 117 avg loss -0.638375 avg loss no lamb -0.638375 time 2019-02-21 18:50:04.934505
Model ind 685 epoch 1 head B batch: 118 avg loss -0.710253 avg loss no lamb -0.710253 time 2019-02-21 18:50:05.441084
Model ind 685 epoch 1 head B batch: 119 avg loss -0.672725 avg loss no lamb -0.672725 time 2019-02-21 18:50:05.939050
Model ind 685 epoch 1 head B batch: 120 avg loss -0.658217 avg loss no lamb -0.658217 time 2019-02-21 18:50:06.460219
Model ind 685 epoch 1 head B batch: 121 avg loss -0.653200 avg loss no lamb -0.653200 time 2019-02-21 18:50:06.949025
Model ind 685 epoch 1 head B batch: 122 avg loss -0.576949 avg loss no lamb -0.576949 time 2019-02-21 18:50:07.463308
Model ind 685 epoch 1 head B batch: 123 avg loss -0.615838 avg loss no lamb -0.615838 time 2019-02-21 18:50:08.034032
Model ind 685 epoch 1 head B batch: 124 avg loss -0.684957 avg loss no lamb -0.684957 time 2019-02-21 18:50:08.616774
Model ind 685 epoch 1 head B batch: 125 avg loss -0.599135 avg loss no lamb -0.599135 time 2019-02-21 18:50:09.186170
Model ind 685 epoch 1 head B batch: 126 avg loss -0.701883 avg loss no lamb -0.701883 time 2019-02-21 18:50:09.789301
Model ind 685 epoch 1 head B batch: 127 avg loss -0.641827 avg loss no lamb -0.641827 time 2019-02-21 18:50:10.352456
Model ind 685 epoch 1 head B batch: 128 avg loss -0.619470 avg loss no lamb -0.619470 time 2019-02-21 18:50:10.898428
Model ind 685 epoch 1 head B batch: 129 avg loss -0.698978 avg loss no lamb -0.698978 time 2019-02-21 18:50:11.457958
Model ind 685 epoch 1 head B batch: 130 avg loss -0.703349 avg loss no lamb -0.703349 time 2019-02-21 18:50:11.971416
Model ind 685 epoch 1 head B batch: 131 avg loss -0.668842 avg loss no lamb -0.668842 time 2019-02-21 18:50:12.490955
Model ind 685 epoch 1 head B batch: 132 avg loss -0.637060 avg loss no lamb -0.637060 time 2019-02-21 18:50:12.977624
Model ind 685 epoch 1 head B batch: 133 avg loss -0.718163 avg loss no lamb -0.718163 time 2019-02-21 18:50:13.570262
Model ind 685 epoch 1 head B batch: 134 avg loss -0.684765 avg loss no lamb -0.684765 time 2019-02-21 18:50:14.134676
Model ind 685 epoch 1 head B batch: 135 avg loss -0.672564 avg loss no lamb -0.672564 time 2019-02-21 18:50:14.642171
Model ind 685 epoch 1 head B batch: 136 avg loss -0.725257 avg loss no lamb -0.725257 time 2019-02-21 18:50:15.137656
Model ind 685 epoch 1 head B batch: 137 avg loss -0.591482 avg loss no lamb -0.591482 time 2019-02-21 18:50:15.642938
Model ind 685 epoch 1 head B batch: 138 avg loss -0.689784 avg loss no lamb -0.689784 time 2019-02-21 18:50:16.144640
Model ind 685 epoch 1 head B batch: 139 avg loss -0.671657 avg loss no lamb -0.671657 time 2019-02-21 18:50:16.660597
Model ind 685 epoch 1 head B batch: 140 avg loss -0.684433 avg loss no lamb -0.684433 time 2019-02-21 18:50:17.235499
Model ind 685 epoch 1 head B batch: 141 avg loss -0.650719 avg loss no lamb -0.650719 time 2019-02-21 18:50:17.822155
Model ind 685 epoch 1 head B batch: 142 avg loss -0.717391 avg loss no lamb -0.717391 time 2019-02-21 18:50:18.354673
Model ind 685 epoch 1 head B batch: 143 avg loss -0.768978 avg loss no lamb -0.768978 time 2019-02-21 18:50:18.888734
Model ind 685 epoch 1 head B batch: 144 avg loss -0.685316 avg loss no lamb -0.685316 time 2019-02-21 18:50:19.402754
Model ind 685 epoch 1 head B batch: 145 avg loss -0.704848 avg loss no lamb -0.704848 time 2019-02-21 18:50:19.911058
Model ind 685 epoch 1 head B batch: 146 avg loss -0.693452 avg loss no lamb -0.693452 time 2019-02-21 18:50:20.450181
Model ind 685 epoch 1 head B batch: 147 avg loss -0.708507 avg loss no lamb -0.708507 time 2019-02-21 18:50:21.031800
Model ind 685 epoch 1 head B batch: 148 avg loss -0.695572 avg loss no lamb -0.695572 time 2019-02-21 18:50:21.589871
Model ind 685 epoch 1 head B batch: 149 avg loss -0.660723 avg loss no lamb -0.660723 time 2019-02-21 18:50:22.115993
Model ind 685 epoch 1 head B batch: 150 avg loss -0.744680 avg loss no lamb -0.744680 time 2019-02-21 18:50:22.634575
Model ind 685 epoch 1 head B batch: 151 avg loss -0.827515 avg loss no lamb -0.827515 time 2019-02-21 18:50:23.140622
Model ind 685 epoch 1 head B batch: 152 avg loss -0.800944 avg loss no lamb -0.800944 time 2019-02-21 18:50:23.652894
Model ind 685 epoch 1 head B batch: 153 avg loss -0.713104 avg loss no lamb -0.713104 time 2019-02-21 18:50:24.181517
Model ind 685 epoch 1 head B batch: 154 avg loss -0.656054 avg loss no lamb -0.656054 time 2019-02-21 18:50:24.695477
Model ind 685 epoch 1 head B batch: 155 avg loss -0.799184 avg loss no lamb -0.799184 time 2019-02-21 18:50:25.219560
Model ind 685 epoch 1 head B batch: 156 avg loss -0.773426 avg loss no lamb -0.773426 time 2019-02-21 18:50:25.721038
Model ind 685 epoch 1 head B batch: 157 avg loss -0.730318 avg loss no lamb -0.730318 time 2019-02-21 18:50:26.233031
Model ind 685 epoch 1 head B batch: 158 avg loss -0.782606 avg loss no lamb -0.782606 time 2019-02-21 18:50:26.769051
Model ind 685 epoch 1 head B batch: 159 avg loss -0.722220 avg loss no lamb -0.722220 time 2019-02-21 18:50:27.278903
Model ind 685 epoch 1 head B batch: 160 avg loss -0.728383 avg loss no lamb -0.728383 time 2019-02-21 18:50:27.848424
Model ind 685 epoch 1 head B batch: 161 avg loss -0.702146 avg loss no lamb -0.702146 time 2019-02-21 18:50:28.468847
Model ind 685 epoch 1 head B batch: 162 avg loss -0.799506 avg loss no lamb -0.799506 time 2019-02-21 18:50:29.158387
Model ind 685 epoch 1 head B batch: 163 avg loss -0.705224 avg loss no lamb -0.705224 time 2019-02-21 18:50:29.908631
Model ind 685 epoch 1 head B batch: 164 avg loss -0.715682 avg loss no lamb -0.715682 time 2019-02-21 18:50:30.610368
Model ind 685 epoch 1 head B batch: 165 avg loss -0.760330 avg loss no lamb -0.760330 time 2019-02-21 18:50:31.327502
Model ind 685 epoch 1 head B batch: 166 avg loss -0.765563 avg loss no lamb -0.765563 time 2019-02-21 18:50:32.034568
Model ind 685 epoch 1 head B batch: 167 avg loss -0.771974 avg loss no lamb -0.771974 time 2019-02-21 18:50:32.762831
Model ind 685 epoch 1 head B batch: 168 avg loss -0.661299 avg loss no lamb -0.661299 time 2019-02-21 18:50:33.381162
Model ind 685 epoch 1 head B batch: 169 avg loss -0.707423 avg loss no lamb -0.707423 time 2019-02-21 18:50:33.956765
Model ind 685 epoch 1 head B batch: 170 avg loss -0.742331 avg loss no lamb -0.742331 time 2019-02-21 18:50:34.472007
Model ind 685 epoch 1 head B batch: 171 avg loss -0.672205 avg loss no lamb -0.672205 time 2019-02-21 18:50:35.056862
Model ind 685 epoch 1 head B batch: 172 avg loss -0.763999 avg loss no lamb -0.763999 time 2019-02-21 18:50:35.592955
Model ind 685 epoch 1 head B batch: 173 avg loss -0.765116 avg loss no lamb -0.765116 time 2019-02-21 18:50:36.122359
Model ind 685 epoch 1 head B batch: 174 avg loss -0.734669 avg loss no lamb -0.734669 time 2019-02-21 18:50:36.632130
Model ind 685 epoch 1 head B batch: 175 avg loss -0.765816 avg loss no lamb -0.765816 time 2019-02-21 18:50:37.145291
Model ind 685 epoch 1 head B batch: 176 avg loss -0.742366 avg loss no lamb -0.742366 time 2019-02-21 18:50:37.661034
Model ind 685 epoch 1 head B batch: 177 avg loss -0.781625 avg loss no lamb -0.781625 time 2019-02-21 18:50:38.243513
Model ind 685 epoch 1 head B batch: 178 avg loss -0.836062 avg loss no lamb -0.836062 time 2019-02-21 18:50:38.824027
Model ind 685 epoch 1 head B batch: 179 avg loss -0.805172 avg loss no lamb -0.805172 time 2019-02-21 18:50:39.363490
Model ind 685 epoch 1 head B batch: 180 avg loss -0.771374 avg loss no lamb -0.771374 time 2019-02-21 18:50:39.883017
Model ind 685 epoch 1 head B batch: 181 avg loss -0.874112 avg loss no lamb -0.874112 time 2019-02-21 18:50:40.399052
Model ind 685 epoch 1 head B batch: 182 avg loss -0.822753 avg loss no lamb -0.822753 time 2019-02-21 18:50:40.911951
Model ind 685 epoch 1 head B batch: 183 avg loss -0.858853 avg loss no lamb -0.858853 time 2019-02-21 18:50:41.466841
Model ind 685 epoch 1 head B batch: 184 avg loss -0.761019 avg loss no lamb -0.761019 time 2019-02-21 18:50:42.041042
Model ind 685 epoch 1 head B batch: 185 avg loss -0.775097 avg loss no lamb -0.775097 time 2019-02-21 18:50:42.620771
Model ind 685 epoch 1 head B batch: 186 avg loss -0.848509 avg loss no lamb -0.848509 time 2019-02-21 18:50:43.166248
Model ind 685 epoch 1 head B batch: 187 avg loss -0.891370 avg loss no lamb -0.891370 time 2019-02-21 18:50:43.683080
Model ind 685 epoch 1 head B batch: 188 avg loss -0.747528 avg loss no lamb -0.747528 time 2019-02-21 18:50:44.212841
Model ind 685 epoch 1 head B batch: 189 avg loss -0.940829 avg loss no lamb -0.940829 time 2019-02-21 18:50:44.800561
Model ind 685 epoch 1 head B batch: 190 avg loss -0.799207 avg loss no lamb -0.799207 time 2019-02-21 18:50:45.369712
Model ind 685 epoch 1 head B batch: 191 avg loss -0.836232 avg loss no lamb -0.836232 time 2019-02-21 18:50:45.888276
Model ind 685 epoch 1 head B batch: 192 avg loss -0.822111 avg loss no lamb -0.822111 time 2019-02-21 18:50:46.390341
Model ind 685 epoch 1 head B batch: 193 avg loss -0.863604 avg loss no lamb -0.863604 time 2019-02-21 18:50:46.889099
Model ind 685 epoch 1 head B batch: 194 avg loss -0.818050 avg loss no lamb -0.818050 time 2019-02-21 18:50:47.397159
Model ind 685 epoch 1 head B batch: 195 avg loss -0.827388 avg loss no lamb -0.827388 time 2019-02-21 18:50:47.891082
Model ind 685 epoch 1 head B batch: 196 avg loss -0.867092 avg loss no lamb -0.867092 time 2019-02-21 18:50:48.401663
Model ind 685 epoch 1 head B batch: 197 avg loss -0.865634 avg loss no lamb -0.865634 time 2019-02-21 18:50:48.947508
Model ind 685 epoch 1 head B batch: 198 avg loss -0.750015 avg loss no lamb -0.750015 time 2019-02-21 18:50:49.493349
Model ind 685 epoch 1 head B batch: 199 avg loss -0.819882 avg loss no lamb -0.819882 time 2019-02-21 18:50:50.039709
Model ind 685 epoch 1 head B batch: 200 avg loss -0.874457 avg loss no lamb -0.874457 time 2019-02-21 18:50:50.604056
Model ind 685 epoch 1 head B batch: 201 avg loss -0.834539 avg loss no lamb -0.834539 time 2019-02-21 18:50:51.242703
Model ind 685 epoch 1 head B batch: 202 avg loss -0.840520 avg loss no lamb -0.840520 time 2019-02-21 18:50:51.827225
Model ind 685 epoch 1 head B batch: 203 avg loss -0.788019 avg loss no lamb -0.788019 time 2019-02-21 18:50:52.397427
Model ind 685 epoch 1 head B batch: 204 avg loss -0.800063 avg loss no lamb -0.800063 time 2019-02-21 18:50:52.998642
Model ind 685 epoch 1 head B batch: 205 avg loss -0.813299 avg loss no lamb -0.813299 time 2019-02-21 18:50:53.652049
Model ind 685 epoch 1 head B batch: 206 avg loss -0.756161 avg loss no lamb -0.756161 time 2019-02-21 18:50:54.204690
Model ind 685 epoch 1 head B batch: 207 avg loss -0.773760 avg loss no lamb -0.773760 time 2019-02-21 18:50:54.781133
Model ind 685 epoch 1 head B batch: 208 avg loss -0.780740 avg loss no lamb -0.780740 time 2019-02-21 18:50:55.321357
Model ind 685 epoch 1 head B batch: 209 avg loss -0.849201 avg loss no lamb -0.849201 time 2019-02-21 18:50:55.805714
Model ind 685 epoch 1 head B batch: 210 avg loss -0.876545 avg loss no lamb -0.876545 time 2019-02-21 18:50:56.335402
Model ind 685 epoch 1 head B batch: 211 avg loss -0.860538 avg loss no lamb -0.860538 time 2019-02-21 18:50:56.857555
Model ind 685 epoch 1 head B batch: 212 avg loss -0.709847 avg loss no lamb -0.709847 time 2019-02-21 18:50:57.381069
Model ind 685 epoch 1 head B batch: 213 avg loss -0.814451 avg loss no lamb -0.814451 time 2019-02-21 18:50:57.956662
Model ind 685 epoch 1 head B batch: 214 avg loss -0.804451 avg loss no lamb -0.804451 time 2019-02-21 18:50:58.476976
Model ind 685 epoch 1 head B batch: 215 avg loss -0.884597 avg loss no lamb -0.884597 time 2019-02-21 18:50:58.983399
Model ind 685 epoch 1 head B batch: 216 avg loss -0.926242 avg loss no lamb -0.926242 time 2019-02-21 18:50:59.502688
Model ind 685 epoch 1 head B batch: 217 avg loss -0.848710 avg loss no lamb -0.848710 time 2019-02-21 18:51:00.031003
Model ind 685 epoch 1 head B batch: 218 avg loss -0.834357 avg loss no lamb -0.834357 time 2019-02-21 18:51:00.611293
Model ind 685 epoch 1 head B batch: 219 avg loss -0.819722 avg loss no lamb -0.819722 time 2019-02-21 18:51:01.138154
Model ind 685 epoch 1 head B batch: 220 avg loss -0.907448 avg loss no lamb -0.907448 time 2019-02-21 18:51:01.680099
Model ind 685 epoch 1 head B batch: 221 avg loss -0.958598 avg loss no lamb -0.958598 time 2019-02-21 18:51:02.193886
Model ind 685 epoch 1 head B batch: 222 avg loss -0.859867 avg loss no lamb -0.859867 time 2019-02-21 18:51:02.676372
Model ind 685 epoch 1 head B batch: 223 avg loss -0.866611 avg loss no lamb -0.866611 time 2019-02-21 18:51:03.157726
Model ind 685 epoch 1 head B batch: 224 avg loss -0.892225 avg loss no lamb -0.892225 time 2019-02-21 18:51:03.648519
Model ind 685 epoch 1 head B batch: 225 avg loss -0.841834 avg loss no lamb -0.841834 time 2019-02-21 18:51:04.222773
Model ind 685 epoch 1 head B batch: 226 avg loss -0.889596 avg loss no lamb -0.889596 time 2019-02-21 18:51:04.752013
Model ind 685 epoch 1 head B batch: 227 avg loss -0.911588 avg loss no lamb -0.911588 time 2019-02-21 18:51:05.296155
Model ind 685 epoch 1 head B batch: 228 avg loss -0.907897 avg loss no lamb -0.907897 time 2019-02-21 18:51:05.819749
Model ind 685 epoch 1 head B batch: 229 avg loss -0.888818 avg loss no lamb -0.888818 time 2019-02-21 18:51:06.292778
Model ind 685 epoch 1 head B batch: 230 avg loss -0.834577 avg loss no lamb -0.834577 time 2019-02-21 18:51:06.828109
Model ind 685 epoch 1 head B batch: 231 avg loss -0.916500 avg loss no lamb -0.916500 time 2019-02-21 18:51:07.371033
Model ind 685 epoch 1 head B batch: 232 avg loss -0.905401 avg loss no lamb -0.905401 time 2019-02-21 18:51:07.896893
Model ind 685 epoch 1 head B batch: 233 avg loss -0.946962 avg loss no lamb -0.946962 time 2019-02-21 18:51:08.402660
Model ind 685 epoch 1 head B batch: 234 avg loss -0.944749 avg loss no lamb -0.944749 time 2019-02-21 18:51:08.913292
Model ind 685 epoch 1 head B batch: 235 avg loss -0.888215 avg loss no lamb -0.888215 time 2019-02-21 18:51:09.429400
Model ind 685 epoch 1 head B batch: 236 avg loss -0.855490 avg loss no lamb -0.855490 time 2019-02-21 18:51:09.997527
Model ind 685 epoch 1 head B batch: 237 avg loss -0.858629 avg loss no lamb -0.858629 time 2019-02-21 18:51:10.548023
Model ind 685 epoch 1 head B batch: 238 avg loss -0.926246 avg loss no lamb -0.926246 time 2019-02-21 18:51:11.031450
Model ind 685 epoch 1 head B batch: 239 avg loss -1.006301 avg loss no lamb -1.006301 time 2019-02-21 18:51:11.556172
Model ind 685 epoch 1 head B batch: 240 avg loss -0.970780 avg loss no lamb -0.970780 time 2019-02-21 18:51:12.119533
Model ind 685 epoch 1 head B batch: 241 avg loss -0.896055 avg loss no lamb -0.896055 time 2019-02-21 18:51:12.646658
Model ind 685 epoch 1 head B batch: 242 avg loss -1.010914 avg loss no lamb -1.010914 time 2019-02-21 18:51:13.145844
Model ind 685 epoch 1 head B batch: 243 avg loss -0.899998 avg loss no lamb -0.899998 time 2019-02-21 18:51:13.623974
Model ind 685 epoch 1 head B batch: 244 avg loss -0.998788 avg loss no lamb -0.998788 time 2019-02-21 18:51:14.222273
Model ind 685 epoch 1 head B batch: 245 avg loss -0.914225 avg loss no lamb -0.914225 time 2019-02-21 18:51:14.782854
Model ind 685 epoch 1 head B batch: 246 avg loss -0.867982 avg loss no lamb -0.867982 time 2019-02-21 18:51:15.304815
Model ind 685 epoch 1 head B batch: 247 avg loss -0.955054 avg loss no lamb -0.955054 time 2019-02-21 18:51:15.848230
Model ind 685 epoch 1 head B batch: 248 avg loss -0.918383 avg loss no lamb -0.918383 time 2019-02-21 18:51:16.398831
Model ind 685 epoch 1 head B batch: 249 avg loss -0.999591 avg loss no lamb -0.999591 time 2019-02-21 18:51:16.927789
Model ind 685 epoch 1 head B batch: 250 avg loss -0.906200 avg loss no lamb -0.906200 time 2019-02-21 18:51:17.485248
Model ind 685 epoch 1 head B batch: 251 avg loss -0.927828 avg loss no lamb -0.927828 time 2019-02-21 18:51:17.976585
Model ind 685 epoch 1 head B batch: 252 avg loss -0.955029 avg loss no lamb -0.955029 time 2019-02-21 18:51:18.538026
Model ind 685 epoch 1 head B batch: 253 avg loss -0.954494 avg loss no lamb -0.954494 time 2019-02-21 18:51:19.035300
Model ind 685 epoch 1 head B batch: 254 avg loss -0.984950 avg loss no lamb -0.984950 time 2019-02-21 18:51:19.539312
Model ind 685 epoch 1 head B batch: 255 avg loss -1.044152 avg loss no lamb -1.044152 time 2019-02-21 18:51:20.117519
Model ind 685 epoch 1 head B batch: 256 avg loss -0.939435 avg loss no lamb -0.939435 time 2019-02-21 18:51:20.595463
Model ind 685 epoch 1 head B batch: 257 avg loss -0.911022 avg loss no lamb -0.911022 time 2019-02-21 18:51:21.124262
Model ind 685 epoch 1 head B batch: 258 avg loss -0.935285 avg loss no lamb -0.935285 time 2019-02-21 18:51:21.729563
Model ind 685 epoch 1 head B batch: 259 avg loss -1.001745 avg loss no lamb -1.001745 time 2019-02-21 18:51:22.275497
Model ind 685 epoch 1 head B batch: 260 avg loss -0.914895 avg loss no lamb -0.914895 time 2019-02-21 18:51:22.826419
Model ind 685 epoch 1 head B batch: 261 avg loss -0.948204 avg loss no lamb -0.948204 time 2019-02-21 18:51:23.343548
Model ind 685 epoch 1 head B batch: 262 avg loss -0.922105 avg loss no lamb -0.922105 time 2019-02-21 18:51:23.869202
Model ind 685 epoch 1 head B batch: 263 avg loss -0.947069 avg loss no lamb -0.947069 time 2019-02-21 18:51:24.377040
Model ind 685 epoch 1 head B batch: 264 avg loss -0.888158 avg loss no lamb -0.888158 time 2019-02-21 18:51:24.919267
Model ind 685 epoch 1 head B batch: 265 avg loss -1.028407 avg loss no lamb -1.028407 time 2019-02-21 18:51:25.486081
Model ind 685 epoch 1 head B batch: 266 avg loss -0.987800 avg loss no lamb -0.987800 time 2019-02-21 18:51:26.077921
Model ind 685 epoch 1 head B batch: 267 avg loss -0.861225 avg loss no lamb -0.861225 time 2019-02-21 18:51:26.643878
Model ind 685 epoch 1 head B batch: 268 avg loss -1.020029 avg loss no lamb -1.020029 time 2019-02-21 18:51:27.229056
Model ind 685 epoch 1 head B batch: 269 avg loss -0.984978 avg loss no lamb -0.984978 time 2019-02-21 18:51:27.900394
Model ind 685 epoch 1 head B batch: 270 avg loss -0.958207 avg loss no lamb -0.958207 time 2019-02-21 18:51:28.471097
Model ind 685 epoch 1 head B batch: 271 avg loss -0.935732 avg loss no lamb -0.935732 time 2019-02-21 18:51:29.068861
Model ind 685 epoch 1 head B batch: 272 avg loss -0.899656 avg loss no lamb -0.899656 time 2019-02-21 18:51:29.672792
Model ind 685 epoch 1 head B batch: 273 avg loss -0.965843 avg loss no lamb -0.965843 time 2019-02-21 18:51:30.269069
Model ind 685 epoch 1 head B batch: 274 avg loss -1.034582 avg loss no lamb -1.034582 time 2019-02-21 18:51:30.865882
Model ind 685 epoch 1 head B batch: 275 avg loss -0.918013 avg loss no lamb -0.918013 time 2019-02-21 18:51:31.488983
Model ind 685 epoch 1 head B batch: 276 avg loss -1.034588 avg loss no lamb -1.034588 time 2019-02-21 18:51:31.971069
Model ind 685 epoch 1 head B batch: 277 avg loss -1.069693 avg loss no lamb -1.069693 time 2019-02-21 18:51:32.535773
Model ind 685 epoch 1 head B batch: 278 avg loss -1.009457 avg loss no lamb -1.009457 time 2019-02-21 18:51:33.092032
Model ind 685 epoch 1 head B batch: 279 avg loss -1.073100 avg loss no lamb -1.073100 time 2019-02-21 18:51:33.611405
Model ind 685 epoch 1 head B batch: 280 avg loss -0.949580 avg loss no lamb -0.949580 time 2019-02-21 18:51:34.119562
Model ind 685 epoch 1 head B batch: 281 avg loss -0.843882 avg loss no lamb -0.843882 time 2019-02-21 18:51:34.621129
Model ind 685 epoch 1 head B batch: 282 avg loss -0.988436 avg loss no lamb -0.988436 time 2019-02-21 18:51:35.132838
Model ind 685 epoch 1 head B batch: 283 avg loss -0.955257 avg loss no lamb -0.955257 time 2019-02-21 18:51:35.665302
Model ind 685 epoch 1 head B batch: 284 avg loss -1.090001 avg loss no lamb -1.090001 time 2019-02-21 18:51:36.263592
Model ind 685 epoch 1 head B batch: 285 avg loss -1.027057 avg loss no lamb -1.027057 time 2019-02-21 18:51:36.853353
Model ind 685 epoch 1 head B batch: 286 avg loss -1.067614 avg loss no lamb -1.067614 time 2019-02-21 18:51:37.383770
Model ind 685 epoch 1 head B batch: 287 avg loss -1.021291 avg loss no lamb -1.021291 time 2019-02-21 18:51:37.877849
Model ind 685 epoch 1 head B batch: 288 avg loss -1.029042 avg loss no lamb -1.029042 time 2019-02-21 18:51:38.377857
Model ind 685 epoch 1 head B batch: 289 avg loss -1.050618 avg loss no lamb -1.050618 time 2019-02-21 18:51:38.889979
Model ind 685 epoch 1 head B batch: 290 avg loss -1.105169 avg loss no lamb -1.105169 time 2019-02-21 18:51:39.407884
Model ind 685 epoch 1 head B batch: 291 avg loss -1.054497 avg loss no lamb -1.054497 time 2019-02-21 18:51:39.916618
Model ind 685 epoch 1 head B batch: 292 avg loss -1.104004 avg loss no lamb -1.104004 time 2019-02-21 18:51:40.406114
Model ind 685 epoch 1 head B batch: 293 avg loss -1.073948 avg loss no lamb -1.073948 time 2019-02-21 18:51:40.950605
Model ind 685 epoch 1 head B batch: 294 avg loss -0.999370 avg loss no lamb -0.999370 time 2019-02-21 18:51:41.521700
Model ind 685 epoch 1 head B batch: 295 avg loss -0.873149 avg loss no lamb -0.873149 time 2019-02-21 18:51:42.076702
Model ind 685 epoch 1 head B batch: 296 avg loss -1.076089 avg loss no lamb -1.076089 time 2019-02-21 18:51:42.604942
Model ind 685 epoch 1 head B batch: 297 avg loss -1.027761 avg loss no lamb -1.027761 time 2019-02-21 18:51:43.093502
Model ind 685 epoch 1 head B batch: 298 avg loss -1.082114 avg loss no lamb -1.082114 time 2019-02-21 18:51:43.609611
Model ind 685 epoch 1 head B batch: 299 avg loss -1.105197 avg loss no lamb -1.105197 time 2019-02-21 18:51:44.140190
Model ind 685 epoch 1 head B batch: 300 avg loss -0.993637 avg loss no lamb -0.993637 time 2019-02-21 18:51:44.658445
Model ind 685 epoch 1 head B batch: 301 avg loss -1.073159 avg loss no lamb -1.073159 time 2019-02-21 18:51:45.161309
Model ind 685 epoch 1 head B batch: 302 avg loss -0.965357 avg loss no lamb -0.965357 time 2019-02-21 18:51:45.686574
Model ind 685 epoch 1 head B batch: 303 avg loss -1.026546 avg loss no lamb -1.026546 time 2019-02-21 18:51:46.245200
Model ind 685 epoch 1 head B batch: 304 avg loss -1.077959 avg loss no lamb -1.077959 time 2019-02-21 18:51:46.754809
Model ind 685 epoch 1 head B batch: 305 avg loss -1.009712 avg loss no lamb -1.009712 time 2019-02-21 18:51:47.238855
Model ind 685 epoch 1 head B batch: 306 avg loss -1.142582 avg loss no lamb -1.142582 time 2019-02-21 18:51:47.804323
Model ind 685 epoch 1 head B batch: 307 avg loss -0.969000 avg loss no lamb -0.969000 time 2019-02-21 18:51:48.390062
Model ind 685 epoch 1 head B batch: 308 avg loss -1.020320 avg loss no lamb -1.020320 time 2019-02-21 18:51:48.968088
Model ind 685 epoch 1 head B batch: 309 avg loss -1.147442 avg loss no lamb -1.147442 time 2019-02-21 18:51:49.465058
Model ind 685 epoch 1 head B batch: 310 avg loss -1.034560 avg loss no lamb -1.034560 time 2019-02-21 18:51:49.953953
Model ind 685 epoch 1 head B batch: 311 avg loss -1.070247 avg loss no lamb -1.070247 time 2019-02-21 18:51:50.499960
Model ind 685 epoch 1 head B batch: 312 avg loss -1.128854 avg loss no lamb -1.128854 time 2019-02-21 18:51:51.016396
Model ind 685 epoch 1 head B batch: 313 avg loss -0.998548 avg loss no lamb -0.998548 time 2019-02-21 18:51:51.529958
Model ind 685 epoch 1 head B batch: 314 avg loss -1.055612 avg loss no lamb -1.055612 time 2019-02-21 18:51:52.011344
Model ind 685 epoch 1 head B batch: 315 avg loss -1.065776 avg loss no lamb -1.065776 time 2019-02-21 18:51:52.525356
Model ind 685 epoch 1 head B batch: 316 avg loss -1.046885 avg loss no lamb -1.046885 time 2019-02-21 18:51:53.014537
Model ind 685 epoch 1 head B batch: 317 avg loss -1.028453 avg loss no lamb -1.028453 time 2019-02-21 18:51:53.559766
Model ind 685 epoch 1 head B batch: 318 avg loss -1.150383 avg loss no lamb -1.150383 time 2019-02-21 18:51:54.084023
Model ind 685 epoch 1 head B batch: 319 avg loss -1.140914 avg loss no lamb -1.140914 time 2019-02-21 18:51:54.636985
Model ind 685 epoch 1 head B batch: 320 avg loss -1.092719 avg loss no lamb -1.092719 time 2019-02-21 18:51:55.122357
Model ind 685 epoch 1 head B batch: 321 avg loss -1.041118 avg loss no lamb -1.041118 time 2019-02-21 18:51:55.623764
Model ind 685 epoch 1 head B batch: 322 avg loss -1.084128 avg loss no lamb -1.084128 time 2019-02-21 18:51:56.113766
Model ind 685 epoch 1 head B batch: 323 avg loss -1.116642 avg loss no lamb -1.116642 time 2019-02-21 18:51:56.669253
Model ind 685 epoch 1 head B batch: 324 avg loss -1.019080 avg loss no lamb -1.019080 time 2019-02-21 18:51:57.161251
Model ind 685 epoch 1 head B batch: 325 avg loss -1.189378 avg loss no lamb -1.189378 time 2019-02-21 18:51:57.680910
Model ind 685 epoch 1 head B batch: 326 avg loss -1.151347 avg loss no lamb -1.151347 time 2019-02-21 18:51:58.198032
Model ind 685 epoch 1 head B batch: 327 avg loss -1.047883 avg loss no lamb -1.047883 time 2019-02-21 18:51:58.706093
Model ind 685 epoch 1 head B batch: 328 avg loss -1.067340 avg loss no lamb -1.067340 time 2019-02-21 18:51:59.204289
Model ind 685 epoch 1 head B batch: 329 avg loss -1.072019 avg loss no lamb -1.072019 time 2019-02-21 18:51:59.706640
Model ind 685 epoch 1 head B batch: 330 avg loss -1.042554 avg loss no lamb -1.042554 time 2019-02-21 18:52:00.189050
Model ind 685 epoch 1 head B batch: 331 avg loss -1.058908 avg loss no lamb -1.058908 time 2019-02-21 18:52:00.737007
Model ind 685 epoch 1 head B batch: 332 avg loss -1.152669 avg loss no lamb -1.152669 time 2019-02-21 18:52:01.285367
Model ind 685 epoch 1 head B batch: 333 avg loss -1.106309 avg loss no lamb -1.106309 time 2019-02-21 18:52:01.880744
Model ind 685 epoch 1 head B batch: 334 avg loss -1.165059 avg loss no lamb -1.165059 time 2019-02-21 18:52:02.413422
Model ind 685 epoch 1 head B batch: 335 avg loss -1.127170 avg loss no lamb -1.127170 time 2019-02-21 18:52:02.905613
Model ind 685 epoch 1 head B batch: 336 avg loss -1.136165 avg loss no lamb -1.136165 time 2019-02-21 18:52:03.422259
Model ind 685 epoch 1 head B batch: 337 avg loss -1.140750 avg loss no lamb -1.140750 time 2019-02-21 18:52:03.934102
Model ind 685 epoch 1 head B batch: 338 avg loss -1.174450 avg loss no lamb -1.174450 time 2019-02-21 18:52:04.423943
Model ind 685 epoch 1 head B batch: 339 avg loss -1.046415 avg loss no lamb -1.046415 time 2019-02-21 18:52:04.953551
Model ind 685 epoch 1 head B batch: 340 avg loss -1.126456 avg loss no lamb -1.126456 time 2019-02-21 18:52:05.500538
Model ind 685 epoch 1 head B batch: 341 avg loss -1.248140 avg loss no lamb -1.248140 time 2019-02-21 18:52:06.021870
Model ind 685 epoch 1 head B batch: 342 avg loss -1.219026 avg loss no lamb -1.219026 time 2019-02-21 18:52:06.517597
Model ind 685 epoch 1 head B batch: 343 avg loss -1.165764 avg loss no lamb -1.165764 time 2019-02-21 18:52:07.050092
Model ind 685 epoch 1 head B batch: 344 avg loss -1.159370 avg loss no lamb -1.159370 time 2019-02-21 18:52:07.579179
Model ind 685 epoch 1 head B batch: 345 avg loss -1.169008 avg loss no lamb -1.169008 time 2019-02-21 18:52:08.162582
Model ind 685 epoch 1 head B batch: 346 avg loss -1.145362 avg loss no lamb -1.145362 time 2019-02-21 18:52:08.640712
Model ind 685 epoch 1 head B batch: 347 avg loss -1.139741 avg loss no lamb -1.139741 time 2019-02-21 18:52:09.125571
Model ind 685 epoch 1 head B batch: 348 avg loss -1.162673 avg loss no lamb -1.162673 time 2019-02-21 18:52:09.671127
Model ind 685 epoch 1 head B batch: 349 avg loss -1.059702 avg loss no lamb -1.059702 time 2019-02-21 18:52:10.195176
Model ind 685 epoch 1 head B batch: 350 avg loss -1.106018 avg loss no lamb -1.106018 time 2019-02-21 18:52:10.687394
Model ind 685 epoch 1 head B batch: 351 avg loss -1.106333 avg loss no lamb -1.106333 time 2019-02-21 18:52:11.253437
Model ind 685 epoch 1 head B batch: 352 avg loss -1.204288 avg loss no lamb -1.204288 time 2019-02-21 18:52:11.793754
Model ind 685 epoch 1 head B batch: 353 avg loss -1.041424 avg loss no lamb -1.041424 time 2019-02-21 18:52:12.320949
Model ind 685 epoch 1 head B batch: 354 avg loss -1.029564 avg loss no lamb -1.029564 time 2019-02-21 18:52:12.880984
Model ind 685 epoch 1 head B batch: 355 avg loss -1.141072 avg loss no lamb -1.141072 time 2019-02-21 18:52:13.387894
Model ind 685 epoch 1 head B batch: 356 avg loss -1.173628 avg loss no lamb -1.173628 time 2019-02-21 18:52:13.928750
Model ind 685 epoch 1 head B batch: 357 avg loss -1.182012 avg loss no lamb -1.182012 time 2019-02-21 18:52:14.407658
Model ind 685 epoch 1 head B batch: 358 avg loss -1.206003 avg loss no lamb -1.206003 time 2019-02-21 18:52:14.942040
Model ind 685 epoch 1 head B batch: 359 avg loss -1.097504 avg loss no lamb -1.097504 time 2019-02-21 18:52:15.476649
Model ind 685 epoch 1 head B batch: 360 avg loss -1.145344 avg loss no lamb -1.145344 time 2019-02-21 18:52:16.013351
Model ind 685 epoch 1 head B batch: 361 avg loss -1.232845 avg loss no lamb -1.232845 time 2019-02-21 18:52:16.590061
Model ind 685 epoch 1 head B batch: 362 avg loss -1.053045 avg loss no lamb -1.053045 time 2019-02-21 18:52:17.155067
Model ind 685 epoch 1 head B batch: 363 avg loss -1.270829 avg loss no lamb -1.270829 time 2019-02-21 18:52:17.648804
Model ind 685 epoch 1 head B batch: 364 avg loss -1.225929 avg loss no lamb -1.225929 time 2019-02-21 18:52:18.153350
Model ind 685 epoch 1 head B batch: 365 avg loss -1.134014 avg loss no lamb -1.134014 time 2019-02-21 18:52:18.660937
Model ind 685 epoch 1 head B batch: 366 avg loss -1.091903 avg loss no lamb -1.091903 time 2019-02-21 18:52:19.164504
Model ind 685 epoch 1 head B batch: 367 avg loss -1.244120 avg loss no lamb -1.244120 time 2019-02-21 18:52:19.683190
Model ind 685 epoch 1 head B batch: 368 avg loss -1.233866 avg loss no lamb -1.233866 time 2019-02-21 18:52:20.237454
Model ind 685 epoch 1 head B batch: 369 avg loss -1.224242 avg loss no lamb -1.224242 time 2019-02-21 18:52:20.739316
Model ind 685 epoch 1 head B batch: 370 avg loss -1.235852 avg loss no lamb -1.235852 time 2019-02-21 18:52:21.253238
Model ind 685 epoch 1 head B batch: 371 avg loss -1.194358 avg loss no lamb -1.194358 time 2019-02-21 18:52:21.832513
Model ind 685 epoch 1 head B batch: 372 avg loss -1.089767 avg loss no lamb -1.089767 time 2019-02-21 18:52:22.369897
Model ind 685 epoch 1 head B batch: 373 avg loss -1.236755 avg loss no lamb -1.236755 time 2019-02-21 18:52:22.902297
Model ind 685 epoch 1 head B batch: 374 avg loss -1.327370 avg loss no lamb -1.327370 time 2019-02-21 18:52:23.417558
Model ind 685 epoch 1 head B batch: 375 avg loss -1.275718 avg loss no lamb -1.275718 time 2019-02-21 18:52:23.938355
Model ind 685 epoch 1 head B batch: 376 avg loss -1.196797 avg loss no lamb -1.196797 time 2019-02-21 18:52:24.461775
Model ind 685 epoch 1 head B batch: 377 avg loss -1.133596 avg loss no lamb -1.133596 time 2019-02-21 18:52:24.963932
Model ind 685 epoch 1 head B batch: 378 avg loss -1.177882 avg loss no lamb -1.177882 time 2019-02-21 18:52:25.515629
Model ind 685 epoch 1 head B batch: 379 avg loss -1.214529 avg loss no lamb -1.214529 time 2019-02-21 18:52:26.054031
Model ind 685 epoch 1 head B batch: 380 avg loss -1.279229 avg loss no lamb -1.279229 time 2019-02-21 18:52:26.597948
Model ind 685 epoch 1 head B batch: 381 avg loss -1.263894 avg loss no lamb -1.263894 time 2019-02-21 18:52:27.134342
Model ind 685 epoch 1 head B batch: 382 avg loss -1.284557 avg loss no lamb -1.284557 time 2019-02-21 18:52:27.700334
Model ind 685 epoch 1 head B batch: 383 avg loss -1.140733 avg loss no lamb -1.140733 time 2019-02-21 18:52:28.206967
Model ind 685 epoch 1 head B batch: 384 avg loss -1.322831 avg loss no lamb -1.322831 time 2019-02-21 18:52:28.688082
Model ind 685 epoch 1 head B batch: 385 avg loss -1.162086 avg loss no lamb -1.162086 time 2019-02-21 18:52:29.180615
Model ind 685 epoch 1 head B batch: 386 avg loss -1.241974 avg loss no lamb -1.241974 time 2019-02-21 18:52:29.704812
Model ind 685 epoch 1 head B batch: 387 avg loss -1.301717 avg loss no lamb -1.301717 time 2019-02-21 18:52:30.198236
Model ind 685 epoch 1 head B batch: 388 avg loss -1.321046 avg loss no lamb -1.321046 time 2019-02-21 18:52:30.828393
Model ind 685 epoch 1 head B batch: 389 avg loss -1.294795 avg loss no lamb -1.294795 time 2019-02-21 18:52:31.425310
Model ind 685 epoch 1 head B batch: 390 avg loss -1.346837 avg loss no lamb -1.346837 time 2019-02-21 18:52:32.007322
Model ind 685 epoch 1 head B batch: 391 avg loss -1.226978 avg loss no lamb -1.226978 time 2019-02-21 18:52:32.619357
Model ind 685 epoch 1 head B batch: 392 avg loss -1.167150 avg loss no lamb -1.167150 time 2019-02-21 18:52:33.209796
Model ind 685 epoch 1 head B batch: 393 avg loss -1.254299 avg loss no lamb -1.254299 time 2019-02-21 18:52:33.754183
Model ind 685 epoch 1 head B batch: 394 avg loss -1.357967 avg loss no lamb -1.357967 time 2019-02-21 18:52:34.270202
Model ind 685 epoch 1 head B batch: 395 avg loss -1.321976 avg loss no lamb -1.321976 time 2019-02-21 18:52:34.785367
Model ind 685 epoch 1 head B batch: 396 avg loss -1.316964 avg loss no lamb -1.316964 time 2019-02-21 18:52:35.285036
Model ind 685 epoch 1 head B batch: 397 avg loss -1.330716 avg loss no lamb -1.330716 time 2019-02-21 18:52:35.772787
Model ind 685 epoch 1 head B batch: 398 avg loss -1.333945 avg loss no lamb -1.333945 time 2019-02-21 18:52:36.333135
Model ind 685 epoch 1 head B batch: 399 avg loss -1.284899 avg loss no lamb -1.284899 time 2019-02-21 18:52:36.818762
Model ind 685 epoch 1 head B batch: 400 avg loss -1.416556 avg loss no lamb -1.416556 time 2019-02-21 18:52:37.341443
Model ind 685 epoch 1 head B batch: 401 avg loss -1.179646 avg loss no lamb -1.179646 time 2019-02-21 18:52:37.908338
Model ind 685 epoch 1 head B batch: 402 avg loss -1.284406 avg loss no lamb -1.284406 time 2019-02-21 18:52:38.440717
Model ind 685 epoch 1 head B batch: 403 avg loss -1.343788 avg loss no lamb -1.343788 time 2019-02-21 18:52:39.000629
Model ind 685 epoch 1 head B batch: 404 avg loss -1.313808 avg loss no lamb -1.313808 time 2019-02-21 18:52:39.581578
Model ind 685 epoch 1 head B batch: 405 avg loss -1.306008 avg loss no lamb -1.306008 time 2019-02-21 18:52:40.190750
Model ind 685 epoch 1 head B batch: 406 avg loss -1.367751 avg loss no lamb -1.367751 time 2019-02-21 18:52:40.779990
Model ind 685 epoch 1 head B batch: 407 avg loss -1.381890 avg loss no lamb -1.381890 time 2019-02-21 18:52:41.371300
Model ind 685 epoch 1 head B batch: 408 avg loss -1.369540 avg loss no lamb -1.369540 time 2019-02-21 18:52:41.920949
Model ind 685 epoch 1 head B batch: 409 avg loss -1.338482 avg loss no lamb -1.338482 time 2019-02-21 18:52:42.422285
Model ind 685 epoch 1 head B batch: 410 avg loss -1.312574 avg loss no lamb -1.312574 time 2019-02-21 18:52:42.930290
Model ind 685 epoch 1 head B batch: 411 avg loss -1.268625 avg loss no lamb -1.268625 time 2019-02-21 18:52:43.425859
Model ind 685 epoch 1 head B batch: 412 avg loss -1.317937 avg loss no lamb -1.317937 time 2019-02-21 18:52:43.948005
Model ind 685 epoch 1 head B batch: 413 avg loss -1.263508 avg loss no lamb -1.263508 time 2019-02-21 18:52:44.543695
Model ind 685 epoch 1 head B batch: 414 avg loss -1.387258 avg loss no lamb -1.387258 time 2019-02-21 18:52:45.087856
Model ind 685 epoch 1 head B batch: 415 avg loss -1.432383 avg loss no lamb -1.432383 time 2019-02-21 18:52:45.608937
Model ind 685 epoch 1 head B batch: 416 avg loss -1.431935 avg loss no lamb -1.431935 time 2019-02-21 18:52:46.130905
Model ind 685 epoch 1 head B batch: 417 avg loss -1.337722 avg loss no lamb -1.337722 time 2019-02-21 18:52:46.614990
Model ind 685 epoch 1 head B batch: 418 avg loss -1.347122 avg loss no lamb -1.347122 time 2019-02-21 18:52:47.103292
Model ind 685 epoch 1 head B batch: 419 avg loss -1.516757 avg loss no lamb -1.516757 time 2019-02-21 18:52:47.627806
Model ind 685 epoch 1 head B batch: 420 avg loss -1.324038 avg loss no lamb -1.324038 time 2019-02-21 18:52:48.166258
Model ind 685 epoch 1 head B batch: 421 avg loss -1.513731 avg loss no lamb -1.513731 time 2019-02-21 18:52:48.678565
Model ind 685 epoch 1 head B batch: 422 avg loss -1.503048 avg loss no lamb -1.503048 time 2019-02-21 18:52:49.218365
Model ind 685 epoch 1 head B batch: 423 avg loss -1.122771 avg loss no lamb -1.122771 time 2019-02-21 18:52:49.773632
Model ind 685 epoch 1 head B batch: 424 avg loss -1.302459 avg loss no lamb -1.302459 time 2019-02-21 18:52:50.294830
Model ind 685 epoch 1 head B batch: 425 avg loss -1.373416 avg loss no lamb -1.373416 time 2019-02-21 18:52:50.788440
Model ind 685 epoch 1 head B batch: 426 avg loss -1.215152 avg loss no lamb -1.215152 time 2019-02-21 18:52:51.286178
Model ind 685 epoch 1 head B batch: 427 avg loss -1.476725 avg loss no lamb -1.476725 time 2019-02-21 18:52:51.779234
Model ind 685 epoch 1 head B batch: 428 avg loss -1.365437 avg loss no lamb -1.365437 time 2019-02-21 18:52:52.297619
Model ind 685 epoch 1 head B batch: 429 avg loss -1.311394 avg loss no lamb -1.311394 time 2019-02-21 18:52:52.794956
Model ind 685 epoch 1 head B batch: 430 avg loss -1.325074 avg loss no lamb -1.325074 time 2019-02-21 18:52:53.318263
Model ind 685 epoch 1 head B batch: 431 avg loss -1.355969 avg loss no lamb -1.355969 time 2019-02-21 18:52:53.839731
Model ind 685 epoch 1 head B batch: 432 avg loss -1.186467 avg loss no lamb -1.186467 time 2019-02-21 18:52:54.317761
Model ind 685 epoch 1 head B batch: 433 avg loss -1.223162 avg loss no lamb -1.223162 time 2019-02-21 18:52:54.796831
Model ind 685 epoch 1 head B batch: 434 avg loss -1.382225 avg loss no lamb -1.382225 time 2019-02-21 18:52:55.275896
Model ind 685 epoch 1 head B batch: 435 avg loss -1.314423 avg loss no lamb -1.314423 time 2019-02-21 18:52:55.810451
Model ind 685 epoch 1 head B batch: 436 avg loss -1.265441 avg loss no lamb -1.265441 time 2019-02-21 18:52:56.386202
Model ind 685 epoch 1 head B batch: 437 avg loss -1.239663 avg loss no lamb -1.239663 time 2019-02-21 18:52:56.927650
Model ind 685 epoch 1 head B batch: 438 avg loss -1.207461 avg loss no lamb -1.207461 time 2019-02-21 18:52:57.439519
Model ind 685 epoch 1 head B batch: 439 avg loss -1.231309 avg loss no lamb -1.231309 time 2019-02-21 18:52:57.919575
Model ind 685 epoch 1 head B batch: 440 avg loss -1.322811 avg loss no lamb -1.322811 time 2019-02-21 18:52:58.434470
Model ind 685 epoch 1 head B batch: 441 avg loss -1.299459 avg loss no lamb -1.299459 time 2019-02-21 18:52:59.042434
Model ind 685 epoch 1 head B batch: 442 avg loss -1.274249 avg loss no lamb -1.274249 time 2019-02-21 18:52:59.574314
Model ind 685 epoch 1 head B batch: 443 avg loss -1.193228 avg loss no lamb -1.193228 time 2019-02-21 18:53:00.188710
Model ind 685 epoch 1 head B batch: 444 avg loss -1.353590 avg loss no lamb -1.353590 time 2019-02-21 18:53:00.706585
Model ind 685 epoch 1 head B batch: 445 avg loss -1.296400 avg loss no lamb -1.296400 time 2019-02-21 18:53:01.202689
Model ind 685 epoch 1 head B batch: 446 avg loss -1.300065 avg loss no lamb -1.300065 time 2019-02-21 18:53:01.697436
Model ind 685 epoch 1 head B batch: 447 avg loss -1.322886 avg loss no lamb -1.322886 time 2019-02-21 18:53:02.214343
Model ind 685 epoch 1 head B batch: 448 avg loss -1.304769 avg loss no lamb -1.304769 time 2019-02-21 18:53:02.712710
Model ind 685 epoch 1 head B batch: 449 avg loss -1.332175 avg loss no lamb -1.332175 time 2019-02-21 18:53:03.211845
Model ind 685 epoch 1 head B batch: 450 avg loss -1.377848 avg loss no lamb -1.377848 time 2019-02-21 18:53:03.741468
Model ind 685 epoch 1 head B batch: 451 avg loss -1.246006 avg loss no lamb -1.246006 time 2019-02-21 18:53:04.290285
Model ind 685 epoch 1 head B batch: 452 avg loss -1.288287 avg loss no lamb -1.288287 time 2019-02-21 18:53:04.797872
Model ind 685 epoch 1 head B batch: 453 avg loss -1.312992 avg loss no lamb -1.312992 time 2019-02-21 18:53:05.331002
Model ind 685 epoch 1 head B batch: 454 avg loss -1.409709 avg loss no lamb -1.409709 time 2019-02-21 18:53:05.827335
Model ind 685 epoch 1 head B batch: 455 avg loss -1.250463 avg loss no lamb -1.250463 time 2019-02-21 18:53:06.299014
Model ind 685 epoch 1 head B batch: 456 avg loss -1.277864 avg loss no lamb -1.277864 time 2019-02-21 18:53:06.801568
Model ind 685 epoch 1 head B batch: 457 avg loss -1.263488 avg loss no lamb -1.263488 time 2019-02-21 18:53:07.338871
Model ind 685 epoch 1 head B batch: 458 avg loss -1.311003 avg loss no lamb -1.311003 time 2019-02-21 18:53:07.861187
Model ind 685 epoch 1 head B batch: 459 avg loss -1.303446 avg loss no lamb -1.303446 time 2019-02-21 18:53:08.463107
Model ind 685 epoch 1 head B batch: 460 avg loss -1.283220 avg loss no lamb -1.283220 time 2019-02-21 18:53:09.054111
Model ind 685 epoch 1 head B batch: 461 avg loss -1.426350 avg loss no lamb -1.426350 time 2019-02-21 18:53:09.638706
Model ind 685 epoch 1 head B batch: 462 avg loss -1.404660 avg loss no lamb -1.404660 time 2019-02-21 18:53:10.235993
Model ind 685 epoch 1 head B batch: 463 avg loss -1.353052 avg loss no lamb -1.353052 time 2019-02-21 18:53:10.924358
Model ind 685 epoch 1 head B batch: 464 avg loss -1.418691 avg loss no lamb -1.418691 time 2019-02-21 18:53:11.585473
Model ind 685 epoch 1 head B batch: 465 avg loss -1.445470 avg loss no lamb -1.445470 time 2019-02-21 18:53:12.170148
Model ind 685 epoch 1 head B batch: 466 avg loss -1.457697 avg loss no lamb -1.457697 time 2019-02-21 18:53:12.721478
Model ind 685 epoch 1 head B batch: 467 avg loss -1.497908 avg loss no lamb -1.497908 time 2019-02-21 18:53:13.266352
Model ind 685 epoch 1 head B batch: 468 avg loss -1.428151 avg loss no lamb -1.428151 time 2019-02-21 18:53:13.837336
Model ind 685 epoch 1 head B batch: 469 avg loss -1.410523 avg loss no lamb -1.410523 time 2019-02-21 18:53:14.349405
Model ind 685 epoch 1 head B batch: 470 avg loss -1.383213 avg loss no lamb -1.383213 time 2019-02-21 18:53:14.843923
Model ind 685 epoch 1 head B batch: 471 avg loss -1.406876 avg loss no lamb -1.406876 time 2019-02-21 18:53:15.379215
Model ind 685 epoch 1 head B batch: 472 avg loss -1.317881 avg loss no lamb -1.317881 time 2019-02-21 18:53:15.919090
Model ind 685 epoch 1 head B batch: 473 avg loss -1.608247 avg loss no lamb -1.608247 time 2019-02-21 18:53:16.470019
Model ind 685 epoch 1 head B batch: 474 avg loss -1.563122 avg loss no lamb -1.563122 time 2019-02-21 18:53:17.031733
Model ind 685 epoch 1 head B batch: 475 avg loss -1.243919 avg loss no lamb -1.243919 time 2019-02-21 18:53:17.521223
Model ind 685 epoch 1 head B batch: 476 avg loss -1.276032 avg loss no lamb -1.276032 time 2019-02-21 18:53:18.022914
Model ind 685 epoch 1 head B batch: 477 avg loss -1.449295 avg loss no lamb -1.449295 time 2019-02-21 18:53:18.548880
Model ind 685 epoch 1 head B batch: 478 avg loss -1.531249 avg loss no lamb -1.531249 time 2019-02-21 18:53:19.101431
Model ind 685 epoch 1 head B batch: 479 avg loss -1.486413 avg loss no lamb -1.486413 time 2019-02-21 18:53:19.672482
Model ind 685 epoch 1 head B batch: 480 avg loss -1.464277 avg loss no lamb -1.464277 time 2019-02-21 18:53:20.264356
Model ind 685 epoch 1 head B batch: 481 avg loss -1.387956 avg loss no lamb -1.387956 time 2019-02-21 18:53:20.828397
Model ind 685 epoch 1 head B batch: 482 avg loss -1.466737 avg loss no lamb -1.466737 time 2019-02-21 18:53:21.423270
Model ind 685 epoch 1 head B batch: 483 avg loss -1.439727 avg loss no lamb -1.439727 time 2019-02-21 18:53:22.071066
Model ind 685 epoch 1 head B batch: 484 avg loss -1.407276 avg loss no lamb -1.407276 time 2019-02-21 18:53:22.623687
Model ind 685 epoch 1 head B batch: 485 avg loss -1.417067 avg loss no lamb -1.417067 time 2019-02-21 18:53:23.195053
Model ind 685 epoch 1 head B batch: 486 avg loss -1.383656 avg loss no lamb -1.383656 time 2019-02-21 18:53:23.730949
Model ind 685 epoch 1 head B batch: 487 avg loss -1.340149 avg loss no lamb -1.340149 time 2019-02-21 18:53:24.262546
Model ind 685 epoch 1 head B batch: 488 avg loss -1.450312 avg loss no lamb -1.450312 time 2019-02-21 18:53:24.816385
Model ind 685 epoch 1 head B batch: 489 avg loss -1.433637 avg loss no lamb -1.433637 time 2019-02-21 18:53:25.397073
Model ind 685 epoch 1 head B batch: 490 avg loss -1.490985 avg loss no lamb -1.490985 time 2019-02-21 18:53:26.020056
Model ind 685 epoch 1 head B batch: 491 avg loss -1.509741 avg loss no lamb -1.509741 time 2019-02-21 18:53:26.607267
Model ind 685 epoch 1 head B batch: 492 avg loss -1.585308 avg loss no lamb -1.585308 time 2019-02-21 18:53:27.151255
Model ind 685 epoch 1 head B batch: 493 avg loss -1.446148 avg loss no lamb -1.446148 time 2019-02-21 18:53:27.699878
Model ind 685 epoch 1 head B batch: 494 avg loss -1.466051 avg loss no lamb -1.466051 time 2019-02-21 18:53:28.252257
Model ind 685 epoch 1 head B batch: 495 avg loss -1.517087 avg loss no lamb -1.517087 time 2019-02-21 18:53:28.861566
Model ind 685 epoch 1 head B batch: 496 avg loss -1.476985 avg loss no lamb -1.476985 time 2019-02-21 18:53:29.474649
Model ind 685 epoch 1 head B batch: 497 avg loss -1.404894 avg loss no lamb -1.404894 time 2019-02-21 18:53:30.034496
Model ind 685 epoch 1 head B batch: 498 avg loss -1.209374 avg loss no lamb -1.209374 time 2019-02-21 18:53:30.540886
Model ind 685 epoch 1 head B batch: 499 avg loss -1.329357 avg loss no lamb -1.329357 time 2019-02-21 18:53:31.065602
Model ind 685 epoch 1 head B batch: 0 avg loss -1.342285 avg loss no lamb -1.342285 time 2019-02-21 18:53:31.590125
Model ind 685 epoch 1 head B batch: 1 avg loss -1.440537 avg loss no lamb -1.440537 time 2019-02-21 18:53:32.080463
Model ind 685 epoch 1 head B batch: 2 avg loss -1.411358 avg loss no lamb -1.411358 time 2019-02-21 18:53:32.599649
Model ind 685 epoch 1 head B batch: 3 avg loss -1.397814 avg loss no lamb -1.397814 time 2019-02-21 18:53:33.093151
Model ind 685 epoch 1 head B batch: 4 avg loss -1.283877 avg loss no lamb -1.283877 time 2019-02-21 18:53:33.638162
Model ind 685 epoch 1 head B batch: 5 avg loss -1.430322 avg loss no lamb -1.430322 time 2019-02-21 18:53:34.163485
Model ind 685 epoch 1 head B batch: 6 avg loss -1.381606 avg loss no lamb -1.381606 time 2019-02-21 18:53:34.713648
Model ind 685 epoch 1 head B batch: 7 avg loss -1.354758 avg loss no lamb -1.354758 time 2019-02-21 18:53:35.226988
Model ind 685 epoch 1 head B batch: 8 avg loss -1.333115 avg loss no lamb -1.333115 time 2019-02-21 18:53:35.775475
Model ind 685 epoch 1 head B batch: 9 avg loss -1.305852 avg loss no lamb -1.305852 time 2019-02-21 18:53:36.335373
Model ind 685 epoch 1 head B batch: 10 avg loss -1.302001 avg loss no lamb -1.302001 time 2019-02-21 18:53:36.849155
Model ind 685 epoch 1 head B batch: 11 avg loss -1.434539 avg loss no lamb -1.434539 time 2019-02-21 18:53:37.346219
Model ind 685 epoch 1 head B batch: 12 avg loss -1.419143 avg loss no lamb -1.419143 time 2019-02-21 18:53:37.856235
Model ind 685 epoch 1 head B batch: 13 avg loss -1.420205 avg loss no lamb -1.420205 time 2019-02-21 18:53:38.378156
Model ind 685 epoch 1 head B batch: 14 avg loss -1.380439 avg loss no lamb -1.380439 time 2019-02-21 18:53:38.859479
Model ind 685 epoch 1 head B batch: 15 avg loss -1.460516 avg loss no lamb -1.460516 time 2019-02-21 18:53:39.380385
Model ind 685 epoch 1 head B batch: 16 avg loss -1.392072 avg loss no lamb -1.392072 time 2019-02-21 18:53:39.878264
Model ind 685 epoch 1 head B batch: 17 avg loss -1.407655 avg loss no lamb -1.407655 time 2019-02-21 18:53:40.449177
Model ind 685 epoch 1 head B batch: 18 avg loss -1.403927 avg loss no lamb -1.403927 time 2019-02-21 18:53:41.045202
Model ind 685 epoch 1 head B batch: 19 avg loss -1.468490 avg loss no lamb -1.468490 time 2019-02-21 18:53:41.607704
Model ind 685 epoch 1 head B batch: 20 avg loss -1.295277 avg loss no lamb -1.295277 time 2019-02-21 18:53:42.151621
Model ind 685 epoch 1 head B batch: 21 avg loss -1.466700 avg loss no lamb -1.466700 time 2019-02-21 18:53:42.726398
Model ind 685 epoch 1 head B batch: 22 avg loss -1.483688 avg loss no lamb -1.483688 time 2019-02-21 18:53:43.316282
Model ind 685 epoch 1 head B batch: 23 avg loss -1.383623 avg loss no lamb -1.383623 time 2019-02-21 18:53:43.907909
Model ind 685 epoch 1 head B batch: 24 avg loss -1.360939 avg loss no lamb -1.360939 time 2019-02-21 18:53:44.464166
Model ind 685 epoch 1 head B batch: 25 avg loss -1.467718 avg loss no lamb -1.467718 time 2019-02-21 18:53:44.977205
Model ind 685 epoch 1 head B batch: 26 avg loss -1.357976 avg loss no lamb -1.357976 time 2019-02-21 18:53:45.498617
Model ind 685 epoch 1 head B batch: 27 avg loss -1.410725 avg loss no lamb -1.410725 time 2019-02-21 18:53:46.004675
Model ind 685 epoch 1 head B batch: 28 avg loss -1.415733 avg loss no lamb -1.415733 time 2019-02-21 18:53:46.481137
Model ind 685 epoch 1 head B batch: 29 avg loss -1.410585 avg loss no lamb -1.410585 time 2019-02-21 18:53:46.974605
Model ind 685 epoch 1 head B batch: 30 avg loss -1.382265 avg loss no lamb -1.382265 time 2019-02-21 18:53:47.465180
Model ind 685 epoch 1 head B batch: 31 avg loss -1.416089 avg loss no lamb -1.416089 time 2019-02-21 18:53:48.005477
Model ind 685 epoch 1 head B batch: 32 avg loss -1.473354 avg loss no lamb -1.473354 time 2019-02-21 18:53:48.554520
Model ind 685 epoch 1 head B batch: 33 avg loss -1.309102 avg loss no lamb -1.309102 time 2019-02-21 18:53:49.089277
Model ind 685 epoch 1 head B batch: 34 avg loss -1.347890 avg loss no lamb -1.347890 time 2019-02-21 18:53:49.610893
Model ind 685 epoch 1 head B batch: 35 avg loss -1.411596 avg loss no lamb -1.411596 time 2019-02-21 18:53:50.160116
Model ind 685 epoch 1 head B batch: 36 avg loss -1.311178 avg loss no lamb -1.311178 time 2019-02-21 18:53:50.678637
Model ind 685 epoch 1 head B batch: 37 avg loss -1.368753 avg loss no lamb -1.368753 time 2019-02-21 18:53:51.158014
Model ind 685 epoch 1 head B batch: 38 avg loss -1.432443 avg loss no lamb -1.432443 time 2019-02-21 18:53:51.691749
Model ind 685 epoch 1 head B batch: 39 avg loss -1.418083 avg loss no lamb -1.418083 time 2019-02-21 18:53:52.238415
Model ind 685 epoch 1 head B batch: 40 avg loss -1.347549 avg loss no lamb -1.347549 time 2019-02-21 18:53:52.724132
Model ind 685 epoch 1 head B batch: 41 avg loss -1.385171 avg loss no lamb -1.385171 time 2019-02-21 18:53:53.287676
Model ind 685 epoch 1 head B batch: 42 avg loss -1.448726 avg loss no lamb -1.448726 time 2019-02-21 18:53:53.813397
Model ind 685 epoch 1 head B batch: 43 avg loss -1.535805 avg loss no lamb -1.535805 time 2019-02-21 18:53:54.314266
Model ind 685 epoch 1 head B batch: 44 avg loss -1.389797 avg loss no lamb -1.389797 time 2019-02-21 18:53:54.859383
Model ind 685 epoch 1 head B batch: 45 avg loss -1.471096 avg loss no lamb -1.471096 time 2019-02-21 18:53:55.414619
Model ind 685 epoch 1 head B batch: 46 avg loss -1.464402 avg loss no lamb -1.464402 time 2019-02-21 18:53:55.990778
Model ind 685 epoch 1 head B batch: 47 avg loss -1.446412 avg loss no lamb -1.446412 time 2019-02-21 18:53:56.559432
Model ind 685 epoch 1 head B batch: 48 avg loss -1.402065 avg loss no lamb -1.402065 time 2019-02-21 18:53:57.106607
Model ind 685 epoch 1 head B batch: 49 avg loss -1.383126 avg loss no lamb -1.383126 time 2019-02-21 18:53:57.688174
Model ind 685 epoch 1 head B batch: 50 avg loss -1.528872 avg loss no lamb -1.528872 time 2019-02-21 18:53:58.246484
Model ind 685 epoch 1 head B batch: 51 avg loss -1.403553 avg loss no lamb -1.403553 time 2019-02-21 18:53:58.824118
Model ind 685 epoch 1 head B batch: 52 avg loss -1.435905 avg loss no lamb -1.435905 time 2019-02-21 18:53:59.421981
Model ind 685 epoch 1 head B batch: 53 avg loss -1.399490 avg loss no lamb -1.399490 time 2019-02-21 18:54:00.014752
Model ind 685 epoch 1 head B batch: 54 avg loss -1.401235 avg loss no lamb -1.401235 time 2019-02-21 18:54:00.614700
Model ind 685 epoch 1 head B batch: 55 avg loss -1.451441 avg loss no lamb -1.451441 time 2019-02-21 18:54:01.186071
Model ind 685 epoch 1 head B batch: 56 avg loss -1.467279 avg loss no lamb -1.467279 time 2019-02-21 18:54:01.749509
Model ind 685 epoch 1 head B batch: 57 avg loss -1.386762 avg loss no lamb -1.386762 time 2019-02-21 18:54:02.294127
Model ind 685 epoch 1 head B batch: 58 avg loss -1.373438 avg loss no lamb -1.373438 time 2019-02-21 18:54:02.834261
Model ind 685 epoch 1 head B batch: 59 avg loss -1.408630 avg loss no lamb -1.408630 time 2019-02-21 18:54:03.389807
Model ind 685 epoch 1 head B batch: 60 avg loss -1.392877 avg loss no lamb -1.392877 time 2019-02-21 18:54:03.968360
Model ind 685 epoch 1 head B batch: 61 avg loss -1.402286 avg loss no lamb -1.402286 time 2019-02-21 18:54:04.606526
Model ind 685 epoch 1 head B batch: 62 avg loss -1.266106 avg loss no lamb -1.266106 time 2019-02-21 18:54:05.190723
Model ind 685 epoch 1 head B batch: 63 avg loss -1.265443 avg loss no lamb -1.265443 time 2019-02-21 18:54:05.702313
Model ind 685 epoch 1 head B batch: 64 avg loss -1.532503 avg loss no lamb -1.532503 time 2019-02-21 18:54:06.199765
Model ind 685 epoch 1 head B batch: 65 avg loss -1.392999 avg loss no lamb -1.392999 time 2019-02-21 18:54:06.680946
Model ind 685 epoch 1 head B batch: 66 avg loss -1.410689 avg loss no lamb -1.410689 time 2019-02-21 18:54:07.189794
Model ind 685 epoch 1 head B batch: 67 avg loss -1.230173 avg loss no lamb -1.230173 time 2019-02-21 18:54:07.698738
Model ind 685 epoch 1 head B batch: 68 avg loss -1.379795 avg loss no lamb -1.379795 time 2019-02-21 18:54:08.212939
Model ind 685 epoch 1 head B batch: 69 avg loss -1.421346 avg loss no lamb -1.421346 time 2019-02-21 18:54:08.740575
Model ind 685 epoch 1 head B batch: 70 avg loss -1.449747 avg loss no lamb -1.449747 time 2019-02-21 18:54:09.247547
Model ind 685 epoch 1 head B batch: 71 avg loss -1.476216 avg loss no lamb -1.476216 time 2019-02-21 18:54:09.761750
Model ind 685 epoch 1 head B batch: 72 avg loss -1.397545 avg loss no lamb -1.397545 time 2019-02-21 18:54:10.244980
Model ind 685 epoch 1 head B batch: 73 avg loss -1.227256 avg loss no lamb -1.227256 time 2019-02-21 18:54:10.784041
Model ind 685 epoch 1 head B batch: 74 avg loss -1.401707 avg loss no lamb -1.401707 time 2019-02-21 18:54:11.308874
Model ind 685 epoch 1 head B batch: 75 avg loss -1.434588 avg loss no lamb -1.434588 time 2019-02-21 18:54:11.840932
Model ind 685 epoch 1 head B batch: 76 avg loss -1.356935 avg loss no lamb -1.356935 time 2019-02-21 18:54:12.344583
Model ind 685 epoch 1 head B batch: 77 avg loss -1.494967 avg loss no lamb -1.494967 time 2019-02-21 18:54:12.864291
Model ind 685 epoch 1 head B batch: 78 avg loss -1.445351 avg loss no lamb -1.445351 time 2019-02-21 18:54:13.416002
Model ind 685 epoch 1 head B batch: 79 avg loss -1.445310 avg loss no lamb -1.445310 time 2019-02-21 18:54:13.928067
Model ind 685 epoch 1 head B batch: 80 avg loss -1.492493 avg loss no lamb -1.492493 time 2019-02-21 18:54:14.459991
Model ind 685 epoch 1 head B batch: 81 avg loss -1.495255 avg loss no lamb -1.495255 time 2019-02-21 18:54:14.955678
Model ind 685 epoch 1 head B batch: 82 avg loss -1.405724 avg loss no lamb -1.405724 time 2019-02-21 18:54:15.449291
Model ind 685 epoch 1 head B batch: 83 avg loss -1.268787 avg loss no lamb -1.268787 time 2019-02-21 18:54:16.011548
Model ind 685 epoch 1 head B batch: 84 avg loss -1.455062 avg loss no lamb -1.455062 time 2019-02-21 18:54:16.532529
Model ind 685 epoch 1 head B batch: 85 avg loss -1.366037 avg loss no lamb -1.366037 time 2019-02-21 18:54:17.041195
Model ind 685 epoch 1 head B batch: 86 avg loss -1.466715 avg loss no lamb -1.466715 time 2019-02-21 18:54:17.533511
Model ind 685 epoch 1 head B batch: 87 avg loss -1.309841 avg loss no lamb -1.309841 time 2019-02-21 18:54:18.079414
Model ind 685 epoch 1 head B batch: 88 avg loss -1.377636 avg loss no lamb -1.377636 time 2019-02-21 18:54:18.644922
Model ind 685 epoch 1 head B batch: 89 avg loss -1.274950 avg loss no lamb -1.274950 time 2019-02-21 18:54:19.177551
Model ind 685 epoch 1 head B batch: 90 avg loss -1.436104 avg loss no lamb -1.436104 time 2019-02-21 18:54:19.694349
Model ind 685 epoch 1 head B batch: 91 avg loss -1.372336 avg loss no lamb -1.372336 time 2019-02-21 18:54:20.218879
Model ind 685 epoch 1 head B batch: 92 avg loss -1.407812 avg loss no lamb -1.407812 time 2019-02-21 18:54:20.763293
Model ind 685 epoch 1 head B batch: 93 avg loss -1.288679 avg loss no lamb -1.288679 time 2019-02-21 18:54:21.381633
Model ind 685 epoch 1 head B batch: 94 avg loss -1.491019 avg loss no lamb -1.491019 time 2019-02-21 18:54:22.025823
Model ind 685 epoch 1 head B batch: 95 avg loss -1.412338 avg loss no lamb -1.412338 time 2019-02-21 18:54:22.639133
Model ind 685 epoch 1 head B batch: 96 avg loss -1.523520 avg loss no lamb -1.523520 time 2019-02-21 18:54:23.184994
Model ind 685 epoch 1 head B batch: 97 avg loss -1.323335 avg loss no lamb -1.323335 time 2019-02-21 18:54:23.749892
Model ind 685 epoch 1 head B batch: 98 avg loss -1.435995 avg loss no lamb -1.435995 time 2019-02-21 18:54:24.339883
Model ind 685 epoch 1 head B batch: 99 avg loss -1.402139 avg loss no lamb -1.402139 time 2019-02-21 18:54:24.941274
Model ind 685 epoch 1 head B batch: 100 avg loss -1.428039 avg loss no lamb -1.428039 time 2019-02-21 18:54:25.487616
Model ind 685 epoch 1 head B batch: 101 avg loss -1.430349 avg loss no lamb -1.430349 time 2019-02-21 18:54:26.005311
Model ind 685 epoch 1 head B batch: 102 avg loss -1.284688 avg loss no lamb -1.284688 time 2019-02-21 18:54:26.590347
Model ind 685 epoch 1 head B batch: 103 avg loss -1.338139 avg loss no lamb -1.338139 time 2019-02-21 18:54:27.131700
Model ind 685 epoch 1 head B batch: 104 avg loss -1.337046 avg loss no lamb -1.337046 time 2019-02-21 18:54:27.627218
Model ind 685 epoch 1 head B batch: 105 avg loss -1.360610 avg loss no lamb -1.360610 time 2019-02-21 18:54:28.179514
Model ind 685 epoch 1 head B batch: 106 avg loss -1.479645 avg loss no lamb -1.479645 time 2019-02-21 18:54:28.691660
Model ind 685 epoch 1 head B batch: 107 avg loss -1.548433 avg loss no lamb -1.548433 time 2019-02-21 18:54:29.205256
Model ind 685 epoch 1 head B batch: 108 avg loss -1.486107 avg loss no lamb -1.486107 time 2019-02-21 18:54:29.712657
Model ind 685 epoch 1 head B batch: 109 avg loss -1.570989 avg loss no lamb -1.570989 time 2019-02-21 18:54:30.287155
Model ind 685 epoch 1 head B batch: 110 avg loss -1.456657 avg loss no lamb -1.456657 time 2019-02-21 18:54:30.771562
Model ind 685 epoch 1 head B batch: 111 avg loss -1.460346 avg loss no lamb -1.460346 time 2019-02-21 18:54:31.294902
Model ind 685 epoch 1 head B batch: 112 avg loss -1.306357 avg loss no lamb -1.306357 time 2019-02-21 18:54:31.803029
Model ind 685 epoch 1 head B batch: 113 avg loss -1.340936 avg loss no lamb -1.340936 time 2019-02-21 18:54:32.365331
Model ind 685 epoch 1 head B batch: 114 avg loss -1.476905 avg loss no lamb -1.476905 time 2019-02-21 18:54:32.878941
Model ind 685 epoch 1 head B batch: 115 avg loss -1.548888 avg loss no lamb -1.548888 time 2019-02-21 18:54:33.397490
Model ind 685 epoch 1 head B batch: 116 avg loss -1.603517 avg loss no lamb -1.603517 time 2019-02-21 18:54:33.880307
Model ind 685 epoch 1 head B batch: 117 avg loss -1.548003 avg loss no lamb -1.548003 time 2019-02-21 18:54:34.417168
Model ind 685 epoch 1 head B batch: 118 avg loss -1.497354 avg loss no lamb -1.497354 time 2019-02-21 18:54:34.983035
Model ind 685 epoch 1 head B batch: 119 avg loss -1.396236 avg loss no lamb -1.396236 time 2019-02-21 18:54:35.547969
Model ind 685 epoch 1 head B batch: 120 avg loss -1.480681 avg loss no lamb -1.480681 time 2019-02-21 18:54:36.059922
Model ind 685 epoch 1 head B batch: 121 avg loss -1.389986 avg loss no lamb -1.389986 time 2019-02-21 18:54:36.549581
Model ind 685 epoch 1 head B batch: 122 avg loss -1.405763 avg loss no lamb -1.405763 time 2019-02-21 18:54:37.093213
Model ind 685 epoch 1 head B batch: 123 avg loss -1.525074 avg loss no lamb -1.525074 time 2019-02-21 18:54:37.634567
Model ind 685 epoch 1 head B batch: 124 avg loss -1.385916 avg loss no lamb -1.385916 time 2019-02-21 18:54:38.171601
Model ind 685 epoch 1 head B batch: 125 avg loss -1.408276 avg loss no lamb -1.408276 time 2019-02-21 18:54:38.696022
Model ind 685 epoch 1 head B batch: 126 avg loss -1.482810 avg loss no lamb -1.482810 time 2019-02-21 18:54:39.241054
Model ind 685 epoch 1 head B batch: 127 avg loss -1.489196 avg loss no lamb -1.489196 time 2019-02-21 18:54:39.776478
Model ind 685 epoch 1 head B batch: 128 avg loss -1.478604 avg loss no lamb -1.478604 time 2019-02-21 18:54:40.344238
Model ind 685 epoch 1 head B batch: 129 avg loss -1.606005 avg loss no lamb -1.606005 time 2019-02-21 18:54:40.886019
Model ind 685 epoch 1 head B batch: 130 avg loss -1.619269 avg loss no lamb -1.619269 time 2019-02-21 18:54:41.476514
Model ind 685 epoch 1 head B batch: 131 avg loss -1.425563 avg loss no lamb -1.425563 time 2019-02-21 18:54:42.070197
Model ind 685 epoch 1 head B batch: 132 avg loss -1.461658 avg loss no lamb -1.461658 time 2019-02-21 18:54:42.685225
Model ind 685 epoch 1 head B batch: 133 avg loss -1.493581 avg loss no lamb -1.493581 time 2019-02-21 18:54:43.264867
Model ind 685 epoch 1 head B batch: 134 avg loss -1.653587 avg loss no lamb -1.653587 time 2019-02-21 18:54:43.814823
Model ind 685 epoch 1 head B batch: 135 avg loss -1.453348 avg loss no lamb -1.453348 time 2019-02-21 18:54:44.309073
Model ind 685 epoch 1 head B batch: 136 avg loss -1.497736 avg loss no lamb -1.497736 time 2019-02-21 18:54:44.831265
Model ind 685 epoch 1 head B batch: 137 avg loss -1.496878 avg loss no lamb -1.496878 time 2019-02-21 18:54:45.390490
Model ind 685 epoch 1 head B batch: 138 avg loss -1.548173 avg loss no lamb -1.548173 time 2019-02-21 18:54:45.929797
Model ind 685 epoch 1 head B batch: 139 avg loss -1.462471 avg loss no lamb -1.462471 time 2019-02-21 18:54:46.475939
Model ind 685 epoch 1 head B batch: 140 avg loss -1.658479 avg loss no lamb -1.658479 time 2019-02-21 18:54:47.039414
Model ind 685 epoch 1 head B batch: 141 avg loss -1.535448 avg loss no lamb -1.535448 time 2019-02-21 18:54:47.569530
Model ind 685 epoch 1 head B batch: 142 avg loss -1.536076 avg loss no lamb -1.536076 time 2019-02-21 18:54:48.110865
Model ind 685 epoch 1 head B batch: 143 avg loss -1.520464 avg loss no lamb -1.520464 time 2019-02-21 18:54:48.630441
Model ind 685 epoch 1 head B batch: 144 avg loss -1.390689 avg loss no lamb -1.390689 time 2019-02-21 18:54:49.128210
Model ind 685 epoch 1 head B batch: 145 avg loss -1.507765 avg loss no lamb -1.507765 time 2019-02-21 18:54:49.615169
Model ind 685 epoch 1 head B batch: 146 avg loss -1.537549 avg loss no lamb -1.537549 time 2019-02-21 18:54:50.110257
Model ind 685 epoch 1 head B batch: 147 avg loss -1.517688 avg loss no lamb -1.517688 time 2019-02-21 18:54:50.622906
Model ind 685 epoch 1 head B batch: 148 avg loss -1.474558 avg loss no lamb -1.474558 time 2019-02-21 18:54:51.155301
Model ind 685 epoch 1 head B batch: 149 avg loss -1.413800 avg loss no lamb -1.413800 time 2019-02-21 18:54:51.712728
Model ind 685 epoch 1 head B batch: 150 avg loss -1.478327 avg loss no lamb -1.478327 time 2019-02-21 18:54:52.264043
Model ind 685 epoch 1 head B batch: 151 avg loss -1.502334 avg loss no lamb -1.502334 time 2019-02-21 18:54:52.757695
Model ind 685 epoch 1 head B batch: 152 avg loss -1.416160 avg loss no lamb -1.416160 time 2019-02-21 18:54:53.240098
Model ind 685 epoch 1 head B batch: 153 avg loss -1.451657 avg loss no lamb -1.451657 time 2019-02-21 18:54:53.771218
Model ind 685 epoch 1 head B batch: 154 avg loss -1.626239 avg loss no lamb -1.626239 time 2019-02-21 18:54:54.287567
Model ind 685 epoch 1 head B batch: 155 avg loss -1.636001 avg loss no lamb -1.636001 time 2019-02-21 18:54:54.782340
Model ind 685 epoch 1 head B batch: 156 avg loss -1.577566 avg loss no lamb -1.577566 time 2019-02-21 18:54:55.302719
Model ind 685 epoch 1 head B batch: 157 avg loss -1.484153 avg loss no lamb -1.484153 time 2019-02-21 18:54:55.891055
Model ind 685 epoch 1 head B batch: 158 avg loss -1.382900 avg loss no lamb -1.382900 time 2019-02-21 18:54:56.478057
Model ind 685 epoch 1 head B batch: 159 avg loss -1.526701 avg loss no lamb -1.526701 time 2019-02-21 18:54:57.002908
Model ind 685 epoch 1 head B batch: 160 avg loss -1.380580 avg loss no lamb -1.380580 time 2019-02-21 18:54:57.566454
Model ind 685 epoch 1 head B batch: 161 avg loss -1.311403 avg loss no lamb -1.311403 time 2019-02-21 18:54:58.101584
Model ind 685 epoch 1 head B batch: 162 avg loss -1.533402 avg loss no lamb -1.533402 time 2019-02-21 18:54:58.627604
Model ind 685 epoch 1 head B batch: 163 avg loss -1.491455 avg loss no lamb -1.491455 time 2019-02-21 18:54:59.183830
Model ind 685 epoch 1 head B batch: 164 avg loss -1.567663 avg loss no lamb -1.567663 time 2019-02-21 18:54:59.740687
Model ind 685 epoch 1 head B batch: 165 avg loss -1.495284 avg loss no lamb -1.495284 time 2019-02-21 18:55:00.250339
Model ind 685 epoch 1 head B batch: 166 avg loss -1.540676 avg loss no lamb -1.540676 time 2019-02-21 18:55:00.829974
Model ind 685 epoch 1 head B batch: 167 avg loss -1.479959 avg loss no lamb -1.479959 time 2019-02-21 18:55:01.402951
Model ind 685 epoch 1 head B batch: 168 avg loss -1.601256 avg loss no lamb -1.601256 time 2019-02-21 18:55:01.941720
Model ind 685 epoch 1 head B batch: 169 avg loss -1.509526 avg loss no lamb -1.509526 time 2019-02-21 18:55:02.471555
Model ind 685 epoch 1 head B batch: 170 avg loss -1.554630 avg loss no lamb -1.554630 time 2019-02-21 18:55:02.956792
Model ind 685 epoch 1 head B batch: 171 avg loss -1.521141 avg loss no lamb -1.521141 time 2019-02-21 18:55:03.459430
Model ind 685 epoch 1 head B batch: 172 avg loss -1.529238 avg loss no lamb -1.529238 time 2019-02-21 18:55:03.979744
Model ind 685 epoch 1 head B batch: 173 avg loss -1.502715 avg loss no lamb -1.502715 time 2019-02-21 18:55:04.479373
Model ind 685 epoch 1 head B batch: 174 avg loss -1.522360 avg loss no lamb -1.522360 time 2019-02-21 18:55:05.058438
Model ind 685 epoch 1 head B batch: 175 avg loss -1.401616 avg loss no lamb -1.401616 time 2019-02-21 18:55:05.604814
Model ind 685 epoch 1 head B batch: 176 avg loss -1.456908 avg loss no lamb -1.456908 time 2019-02-21 18:55:06.171285
Model ind 685 epoch 1 head B batch: 177 avg loss -1.537670 avg loss no lamb -1.537670 time 2019-02-21 18:55:06.723815
Model ind 685 epoch 1 head B batch: 178 avg loss -1.599959 avg loss no lamb -1.599959 time 2019-02-21 18:55:07.254832
Model ind 685 epoch 1 head B batch: 179 avg loss -1.599198 avg loss no lamb -1.599198 time 2019-02-21 18:55:07.850820
Model ind 685 epoch 1 head B batch: 180 avg loss -1.548816 avg loss no lamb -1.548816 time 2019-02-21 18:55:08.380047
Model ind 685 epoch 1 head B batch: 181 avg loss -1.566768 avg loss no lamb -1.566768 time 2019-02-21 18:55:08.897484
Model ind 685 epoch 1 head B batch: 182 avg loss -1.550798 avg loss no lamb -1.550798 time 2019-02-21 18:55:09.498962
Model ind 685 epoch 1 head B batch: 183 avg loss -1.650998 avg loss no lamb -1.650998 time 2019-02-21 18:55:09.997450
Model ind 685 epoch 1 head B batch: 184 avg loss -1.593865 avg loss no lamb -1.593865 time 2019-02-21 18:55:10.506795
Model ind 685 epoch 1 head B batch: 185 avg loss -1.504132 avg loss no lamb -1.504132 time 2019-02-21 18:55:11.098479
Model ind 685 epoch 1 head B batch: 186 avg loss -1.612525 avg loss no lamb -1.612525 time 2019-02-21 18:55:11.652774
Model ind 685 epoch 1 head B batch: 187 avg loss -1.594348 avg loss no lamb -1.594348 time 2019-02-21 18:55:12.197418
Model ind 685 epoch 1 head B batch: 188 avg loss -1.449857 avg loss no lamb -1.449857 time 2019-02-21 18:55:12.671241
Model ind 685 epoch 1 head B batch: 189 avg loss -1.519964 avg loss no lamb -1.519964 time 2019-02-21 18:55:13.205533
Model ind 685 epoch 1 head B batch: 190 avg loss -1.485371 avg loss no lamb -1.485371 time 2019-02-21 18:55:13.721528
Model ind 685 epoch 1 head B batch: 191 avg loss -1.547155 avg loss no lamb -1.547155 time 2019-02-21 18:55:14.247723
Model ind 685 epoch 1 head B batch: 192 avg loss -1.613669 avg loss no lamb -1.613669 time 2019-02-21 18:55:14.793585
Model ind 685 epoch 1 head B batch: 193 avg loss -1.477158 avg loss no lamb -1.477158 time 2019-02-21 18:55:15.364105
Model ind 685 epoch 1 head B batch: 194 avg loss -1.390768 avg loss no lamb -1.390768 time 2019-02-21 18:55:15.963174
Model ind 685 epoch 1 head B batch: 195 avg loss -1.480718 avg loss no lamb -1.480718 time 2019-02-21 18:55:16.517207
Model ind 685 epoch 1 head B batch: 196 avg loss -1.574669 avg loss no lamb -1.574669 time 2019-02-21 18:55:17.057534
Model ind 685 epoch 1 head B batch: 197 avg loss -1.586987 avg loss no lamb -1.586987 time 2019-02-21 18:55:17.659359
Model ind 685 epoch 1 head B batch: 198 avg loss -1.485586 avg loss no lamb -1.485586 time 2019-02-21 18:55:18.264585
Model ind 685 epoch 1 head B batch: 199 avg loss -1.634273 avg loss no lamb -1.634273 time 2019-02-21 18:55:18.879780
Model ind 685 epoch 1 head B batch: 200 avg loss -1.612209 avg loss no lamb -1.612209 time 2019-02-21 18:55:19.491245
Model ind 685 epoch 1 head B batch: 201 avg loss -1.514513 avg loss no lamb -1.514513 time 2019-02-21 18:55:20.099503
Model ind 685 epoch 1 head B batch: 202 avg loss -1.479562 avg loss no lamb -1.479562 time 2019-02-21 18:55:20.640623
Model ind 685 epoch 1 head B batch: 203 avg loss -1.552386 avg loss no lamb -1.552386 time 2019-02-21 18:55:21.180707
Model ind 685 epoch 1 head B batch: 204 avg loss -1.537495 avg loss no lamb -1.537495 time 2019-02-21 18:55:21.679297
Model ind 685 epoch 1 head B batch: 205 avg loss -1.432387 avg loss no lamb -1.432387 time 2019-02-21 18:55:22.207101
Model ind 685 epoch 1 head B batch: 206 avg loss -1.555880 avg loss no lamb -1.555880 time 2019-02-21 18:55:22.751102
Model ind 685 epoch 1 head B batch: 207 avg loss -1.530573 avg loss no lamb -1.530573 time 2019-02-21 18:55:23.268077
Model ind 685 epoch 1 head B batch: 208 avg loss -1.423552 avg loss no lamb -1.423552 time 2019-02-21 18:55:23.746187
Model ind 685 epoch 1 head B batch: 209 avg loss -1.574569 avg loss no lamb -1.574569 time 2019-02-21 18:55:24.244503
Model ind 685 epoch 1 head B batch: 210 avg loss -1.564498 avg loss no lamb -1.564498 time 2019-02-21 18:55:24.729699
Model ind 685 epoch 1 head B batch: 211 avg loss -1.593354 avg loss no lamb -1.593354 time 2019-02-21 18:55:25.278823
Model ind 685 epoch 1 head B batch: 212 avg loss -1.487341 avg loss no lamb -1.487341 time 2019-02-21 18:55:25.806463
Model ind 685 epoch 1 head B batch: 213 avg loss -1.375987 avg loss no lamb -1.375987 time 2019-02-21 18:55:26.343325
Model ind 685 epoch 1 head B batch: 214 avg loss -1.518464 avg loss no lamb -1.518464 time 2019-02-21 18:55:26.921252
Model ind 685 epoch 1 head B batch: 215 avg loss -1.468555 avg loss no lamb -1.468555 time 2019-02-21 18:55:27.509305
Model ind 685 epoch 1 head B batch: 216 avg loss -1.581188 avg loss no lamb -1.581188 time 2019-02-21 18:55:28.044041
Model ind 685 epoch 1 head B batch: 217 avg loss -1.565068 avg loss no lamb -1.565068 time 2019-02-21 18:55:28.583256
Model ind 685 epoch 1 head B batch: 218 avg loss -1.535746 avg loss no lamb -1.535746 time 2019-02-21 18:55:29.094782
Model ind 685 epoch 1 head B batch: 219 avg loss -1.490240 avg loss no lamb -1.490240 time 2019-02-21 18:55:29.585927
Model ind 685 epoch 1 head B batch: 220 avg loss -1.554519 avg loss no lamb -1.554519 time 2019-02-21 18:55:30.110135
Model ind 685 epoch 1 head B batch: 221 avg loss -1.523168 avg loss no lamb -1.523168 time 2019-02-21 18:55:30.639993
Model ind 685 epoch 1 head B batch: 222 avg loss -1.532397 avg loss no lamb -1.532397 time 2019-02-21 18:55:31.201846
Model ind 685 epoch 1 head B batch: 223 avg loss -1.535106 avg loss no lamb -1.535106 time 2019-02-21 18:55:31.735355
Model ind 685 epoch 1 head B batch: 224 avg loss -1.523672 avg loss no lamb -1.523672 time 2019-02-21 18:55:32.215117
Model ind 685 epoch 1 head B batch: 225 avg loss -1.524460 avg loss no lamb -1.524460 time 2019-02-21 18:55:32.733674
Model ind 685 epoch 1 head B batch: 226 avg loss -1.484265 avg loss no lamb -1.484265 time 2019-02-21 18:55:33.203096
Model ind 685 epoch 1 head B batch: 227 avg loss -1.615243 avg loss no lamb -1.615243 time 2019-02-21 18:55:33.718532
Model ind 685 epoch 1 head B batch: 228 avg loss -1.545555 avg loss no lamb -1.545555 time 2019-02-21 18:55:34.243450
Model ind 685 epoch 1 head B batch: 229 avg loss -1.543511 avg loss no lamb -1.543511 time 2019-02-21 18:55:34.782678
Model ind 685 epoch 1 head B batch: 230 avg loss -1.472639 avg loss no lamb -1.472639 time 2019-02-21 18:55:35.300411
Model ind 685 epoch 1 head B batch: 231 avg loss -1.371780 avg loss no lamb -1.371780 time 2019-02-21 18:55:35.807078
Model ind 685 epoch 1 head B batch: 232 avg loss -1.513697 avg loss no lamb -1.513697 time 2019-02-21 18:55:36.309998
Model ind 685 epoch 1 head B batch: 233 avg loss -1.532148 avg loss no lamb -1.532148 time 2019-02-21 18:55:36.839454
Model ind 685 epoch 1 head B batch: 234 avg loss -1.527445 avg loss no lamb -1.527445 time 2019-02-21 18:55:37.370331
Model ind 685 epoch 1 head B batch: 235 avg loss -1.534402 avg loss no lamb -1.534402 time 2019-02-21 18:55:37.936174
Model ind 685 epoch 1 head B batch: 236 avg loss -1.457072 avg loss no lamb -1.457072 time 2019-02-21 18:55:38.410412
Model ind 685 epoch 1 head B batch: 237 avg loss -1.484647 avg loss no lamb -1.484647 time 2019-02-21 18:55:39.011554
Model ind 685 epoch 1 head B batch: 238 avg loss -1.515680 avg loss no lamb -1.515680 time 2019-02-21 18:55:39.646057
Model ind 685 epoch 1 head B batch: 239 avg loss -1.551778 avg loss no lamb -1.551778 time 2019-02-21 18:55:40.199447
Model ind 685 epoch 1 head B batch: 240 avg loss -1.556738 avg loss no lamb -1.556738 time 2019-02-21 18:55:40.723347
Model ind 685 epoch 1 head B batch: 241 avg loss -1.691378 avg loss no lamb -1.691378 time 2019-02-21 18:55:41.268539
Model ind 685 epoch 1 head B batch: 242 avg loss -1.638567 avg loss no lamb -1.638567 time 2019-02-21 18:55:41.784817
Model ind 685 epoch 1 head B batch: 243 avg loss -1.603006 avg loss no lamb -1.603006 time 2019-02-21 18:55:42.282011
Model ind 685 epoch 1 head B batch: 244 avg loss -1.653668 avg loss no lamb -1.653668 time 2019-02-21 18:55:42.779242
Model ind 685 epoch 1 head B batch: 245 avg loss -1.486542 avg loss no lamb -1.486542 time 2019-02-21 18:55:43.273264
Model ind 685 epoch 1 head B batch: 246 avg loss -1.551759 avg loss no lamb -1.551759 time 2019-02-21 18:55:43.745164
Model ind 685 epoch 1 head B batch: 247 avg loss -1.447175 avg loss no lamb -1.447175 time 2019-02-21 18:55:44.292127
Model ind 685 epoch 1 head B batch: 248 avg loss -1.498808 avg loss no lamb -1.498808 time 2019-02-21 18:55:44.911288
Model ind 685 epoch 1 head B batch: 249 avg loss -1.590138 avg loss no lamb -1.590138 time 2019-02-21 18:55:45.468400
Model ind 685 epoch 1 head B batch: 250 avg loss -1.565990 avg loss no lamb -1.565990 time 2019-02-21 18:55:45.990000
Model ind 685 epoch 1 head B batch: 251 avg loss -1.563215 avg loss no lamb -1.563215 time 2019-02-21 18:55:46.521681
Model ind 685 epoch 1 head B batch: 252 avg loss -1.605116 avg loss no lamb -1.605116 time 2019-02-21 18:55:47.013078
Model ind 685 epoch 1 head B batch: 253 avg loss -1.613294 avg loss no lamb -1.613294 time 2019-02-21 18:55:47.537667
Model ind 685 epoch 1 head B batch: 254 avg loss -1.589917 avg loss no lamb -1.589917 time 2019-02-21 18:55:48.131709
Model ind 685 epoch 1 head B batch: 255 avg loss -1.616750 avg loss no lamb -1.616750 time 2019-02-21 18:55:48.770456
Model ind 685 epoch 1 head B batch: 256 avg loss -1.533458 avg loss no lamb -1.533458 time 2019-02-21 18:55:49.370704
Model ind 685 epoch 1 head B batch: 257 avg loss -1.484068 avg loss no lamb -1.484068 time 2019-02-21 18:55:49.863767
Model ind 685 epoch 1 head B batch: 258 avg loss -1.439708 avg loss no lamb -1.439708 time 2019-02-21 18:55:50.392130
Model ind 685 epoch 1 head B batch: 259 avg loss -1.550594 avg loss no lamb -1.550594 time 2019-02-21 18:55:50.897503
Model ind 685 epoch 1 head B batch: 260 avg loss -1.558479 avg loss no lamb -1.558479 time 2019-02-21 18:55:51.401909
Model ind 685 epoch 1 head B batch: 261 avg loss -1.622071 avg loss no lamb -1.622071 time 2019-02-21 18:55:51.974686
Model ind 685 epoch 1 head B batch: 262 avg loss -1.575767 avg loss no lamb -1.575767 time 2019-02-21 18:55:52.482677
Model ind 685 epoch 1 head B batch: 263 avg loss -1.581353 avg loss no lamb -1.581353 time 2019-02-21 18:55:53.015270
Model ind 685 epoch 1 head B batch: 264 avg loss -1.575142 avg loss no lamb -1.575142 time 2019-02-21 18:55:53.531826
Model ind 685 epoch 1 head B batch: 265 avg loss -1.579398 avg loss no lamb -1.579398 time 2019-02-21 18:55:54.033639
Model ind 685 epoch 1 head B batch: 266 avg loss -1.529362 avg loss no lamb -1.529362 time 2019-02-21 18:55:54.523778
Model ind 685 epoch 1 head B batch: 267 avg loss -1.532245 avg loss no lamb -1.532245 time 2019-02-21 18:55:55.058572
Model ind 685 epoch 1 head B batch: 268 avg loss -1.538332 avg loss no lamb -1.538332 time 2019-02-21 18:55:55.566144
Model ind 685 epoch 1 head B batch: 269 avg loss -1.517686 avg loss no lamb -1.517686 time 2019-02-21 18:55:56.063236
Model ind 685 epoch 1 head B batch: 270 avg loss -1.503208 avg loss no lamb -1.503208 time 2019-02-21 18:55:56.559291
Model ind 685 epoch 1 head B batch: 271 avg loss -1.586223 avg loss no lamb -1.586223 time 2019-02-21 18:55:57.070407
Model ind 685 epoch 1 head B batch: 272 avg loss -1.508323 avg loss no lamb -1.508323 time 2019-02-21 18:55:57.565503
Model ind 685 epoch 1 head B batch: 273 avg loss -1.433101 avg loss no lamb -1.433101 time 2019-02-21 18:55:58.090671
Model ind 685 epoch 1 head B batch: 274 avg loss -1.480524 avg loss no lamb -1.480524 time 2019-02-21 18:55:58.634596
Model ind 685 epoch 1 head B batch: 275 avg loss -1.542667 avg loss no lamb -1.542667 time 2019-02-21 18:55:59.225540
Model ind 685 epoch 1 head B batch: 276 avg loss -1.542196 avg loss no lamb -1.542196 time 2019-02-21 18:55:59.733669
Model ind 685 epoch 1 head B batch: 277 avg loss -1.636668 avg loss no lamb -1.636668 time 2019-02-21 18:56:00.217720
Model ind 685 epoch 1 head B batch: 278 avg loss -1.651529 avg loss no lamb -1.651529 time 2019-02-21 18:56:00.724858
Model ind 685 epoch 1 head B batch: 279 avg loss -1.665316 avg loss no lamb -1.665316 time 2019-02-21 18:56:01.264076
Model ind 685 epoch 1 head B batch: 280 avg loss -1.481663 avg loss no lamb -1.481663 time 2019-02-21 18:56:01.920008
Model ind 685 epoch 1 head B batch: 281 avg loss -1.501580 avg loss no lamb -1.501580 time 2019-02-21 18:56:02.523039
Model ind 685 epoch 1 head B batch: 282 avg loss -1.620415 avg loss no lamb -1.620415 time 2019-02-21 18:56:03.114936
Model ind 685 epoch 1 head B batch: 283 avg loss -1.540319 avg loss no lamb -1.540319 time 2019-02-21 18:56:03.654661
Model ind 685 epoch 1 head B batch: 284 avg loss -1.518452 avg loss no lamb -1.518452 time 2019-02-21 18:56:04.145778
Model ind 685 epoch 1 head B batch: 285 avg loss -1.576475 avg loss no lamb -1.576475 time 2019-02-21 18:56:04.667579
Model ind 685 epoch 1 head B batch: 286 avg loss -1.549428 avg loss no lamb -1.549428 time 2019-02-21 18:56:05.162932
Model ind 685 epoch 1 head B batch: 287 avg loss -1.580538 avg loss no lamb -1.580538 time 2019-02-21 18:56:05.710523
Model ind 685 epoch 1 head B batch: 288 avg loss -1.694427 avg loss no lamb -1.694427 time 2019-02-21 18:56:06.187004
Model ind 685 epoch 1 head B batch: 289 avg loss -1.614634 avg loss no lamb -1.614634 time 2019-02-21 18:56:06.669657
Model ind 685 epoch 1 head B batch: 290 avg loss -1.666012 avg loss no lamb -1.666012 time 2019-02-21 18:56:07.191876
Model ind 685 epoch 1 head B batch: 291 avg loss -1.535168 avg loss no lamb -1.535168 time 2019-02-21 18:56:07.715389
Model ind 685 epoch 1 head B batch: 292 avg loss -1.623718 avg loss no lamb -1.623718 time 2019-02-21 18:56:08.203991
Model ind 685 epoch 1 head B batch: 293 avg loss -1.582867 avg loss no lamb -1.582867 time 2019-02-21 18:56:08.721935
Model ind 685 epoch 1 head B batch: 294 avg loss -1.491234 avg loss no lamb -1.491234 time 2019-02-21 18:56:09.265671
Model ind 685 epoch 1 head B batch: 295 avg loss -1.435508 avg loss no lamb -1.435508 time 2019-02-21 18:56:09.816508
Model ind 685 epoch 1 head B batch: 296 avg loss -1.534292 avg loss no lamb -1.534292 time 2019-02-21 18:56:10.333058
Model ind 685 epoch 1 head B batch: 297 avg loss -1.600465 avg loss no lamb -1.600465 time 2019-02-21 18:56:10.854380
Model ind 685 epoch 1 head B batch: 298 avg loss -1.545863 avg loss no lamb -1.545863 time 2019-02-21 18:56:11.354669
Model ind 685 epoch 1 head B batch: 299 avg loss -1.652276 avg loss no lamb -1.652276 time 2019-02-21 18:56:11.873546
Model ind 685 epoch 1 head B batch: 300 avg loss -1.531865 avg loss no lamb -1.531865 time 2019-02-21 18:56:12.424911
Model ind 685 epoch 1 head B batch: 301 avg loss -1.548564 avg loss no lamb -1.548564 time 2019-02-21 18:56:12.922908
Model ind 685 epoch 1 head B batch: 302 avg loss -1.467321 avg loss no lamb -1.467321 time 2019-02-21 18:56:13.418071
Model ind 685 epoch 1 head B batch: 303 avg loss -1.523276 avg loss no lamb -1.523276 time 2019-02-21 18:56:13.911451
Model ind 685 epoch 1 head B batch: 304 avg loss -1.561301 avg loss no lamb -1.561301 time 2019-02-21 18:56:14.414722
Model ind 685 epoch 1 head B batch: 305 avg loss -1.503604 avg loss no lamb -1.503604 time 2019-02-21 18:56:15.003522
Model ind 685 epoch 1 head B batch: 306 avg loss -1.494703 avg loss no lamb -1.494703 time 2019-02-21 18:56:15.602089
Model ind 685 epoch 1 head B batch: 307 avg loss -1.535885 avg loss no lamb -1.535885 time 2019-02-21 18:56:16.149142
Model ind 685 epoch 1 head B batch: 308 avg loss -1.551587 avg loss no lamb -1.551587 time 2019-02-21 18:56:16.699278
Model ind 685 epoch 1 head B batch: 309 avg loss -1.689153 avg loss no lamb -1.689153 time 2019-02-21 18:56:17.186405
Model ind 685 epoch 1 head B batch: 310 avg loss -1.607610 avg loss no lamb -1.607610 time 2019-02-21 18:56:17.714742
Model ind 685 epoch 1 head B batch: 311 avg loss -1.545370 avg loss no lamb -1.545370 time 2019-02-21 18:56:18.230510
Model ind 685 epoch 1 head B batch: 312 avg loss -1.646269 avg loss no lamb -1.646269 time 2019-02-21 18:56:18.739871
Model ind 685 epoch 1 head B batch: 313 avg loss -1.586776 avg loss no lamb -1.586776 time 2019-02-21 18:56:19.271884
Model ind 685 epoch 1 head B batch: 314 avg loss -1.615625 avg loss no lamb -1.615625 time 2019-02-21 18:56:19.871413
Model ind 685 epoch 1 head B batch: 315 avg loss -1.540812 avg loss no lamb -1.540812 time 2019-02-21 18:56:20.478199
Model ind 685 epoch 1 head B batch: 316 avg loss -1.543664 avg loss no lamb -1.543664 time 2019-02-21 18:56:21.042902
Model ind 685 epoch 1 head B batch: 317 avg loss -1.571942 avg loss no lamb -1.571942 time 2019-02-21 18:56:21.584137
Model ind 685 epoch 1 head B batch: 318 avg loss -1.719499 avg loss no lamb -1.719499 time 2019-02-21 18:56:22.092593
Model ind 685 epoch 1 head B batch: 319 avg loss -1.547187 avg loss no lamb -1.547187 time 2019-02-21 18:56:22.625143
Model ind 685 epoch 1 head B batch: 320 avg loss -1.551399 avg loss no lamb -1.551399 time 2019-02-21 18:56:23.180286
Model ind 685 epoch 1 head B batch: 321 avg loss -1.548002 avg loss no lamb -1.548002 time 2019-02-21 18:56:23.736975
Model ind 685 epoch 1 head B batch: 322 avg loss -1.566884 avg loss no lamb -1.566884 time 2019-02-21 18:56:24.274225
Model ind 685 epoch 1 head B batch: 323 avg loss -1.590478 avg loss no lamb -1.590478 time 2019-02-21 18:56:24.803960
Model ind 685 epoch 1 head B batch: 324 avg loss -1.582126 avg loss no lamb -1.582126 time 2019-02-21 18:56:25.281471
Model ind 685 epoch 1 head B batch: 325 avg loss -1.550346 avg loss no lamb -1.550346 time 2019-02-21 18:56:25.767337
Model ind 685 epoch 1 head B batch: 326 avg loss -1.553499 avg loss no lamb -1.553499 time 2019-02-21 18:56:26.274410
Model ind 685 epoch 1 head B batch: 327 avg loss -1.519089 avg loss no lamb -1.519089 time 2019-02-21 18:56:26.788181
Model ind 685 epoch 1 head B batch: 328 avg loss -1.560878 avg loss no lamb -1.560878 time 2019-02-21 18:56:27.341143
Model ind 685 epoch 1 head B batch: 329 avg loss -1.638871 avg loss no lamb -1.638871 time 2019-02-21 18:56:27.866568
Model ind 685 epoch 1 head B batch: 330 avg loss -1.567879 avg loss no lamb -1.567879 time 2019-02-21 18:56:28.344480
Model ind 685 epoch 1 head B batch: 331 avg loss -1.531448 avg loss no lamb -1.531448 time 2019-02-21 18:56:28.870172
Model ind 685 epoch 1 head B batch: 332 avg loss -1.658661 avg loss no lamb -1.658661 time 2019-02-21 18:56:29.381700
Model ind 685 epoch 1 head B batch: 333 avg loss -1.553489 avg loss no lamb -1.553489 time 2019-02-21 18:56:29.902582
Model ind 685 epoch 1 head B batch: 334 avg loss -1.633003 avg loss no lamb -1.633003 time 2019-02-21 18:56:30.414497
Model ind 685 epoch 1 head B batch: 335 avg loss -1.663644 avg loss no lamb -1.663644 time 2019-02-21 18:56:30.933796
Model ind 685 epoch 1 head B batch: 336 avg loss -1.695179 avg loss no lamb -1.695179 time 2019-02-21 18:56:31.453801
Model ind 685 epoch 1 head B batch: 337 avg loss -1.469160 avg loss no lamb -1.469160 time 2019-02-21 18:56:31.983757
Model ind 685 epoch 1 head B batch: 338 avg loss -1.639977 avg loss no lamb -1.639977 time 2019-02-21 18:56:32.492218
Model ind 685 epoch 1 head B batch: 339 avg loss -1.461554 avg loss no lamb -1.461554 time 2019-02-21 18:56:33.019559
Model ind 685 epoch 1 head B batch: 340 avg loss -1.557099 avg loss no lamb -1.557099 time 2019-02-21 18:56:33.520459
Model ind 685 epoch 1 head B batch: 341 avg loss -1.633094 avg loss no lamb -1.633094 time 2019-02-21 18:56:34.045562
Model ind 685 epoch 1 head B batch: 342 avg loss -1.570643 avg loss no lamb -1.570643 time 2019-02-21 18:56:34.572540
Model ind 685 epoch 1 head B batch: 343 avg loss -1.496999 avg loss no lamb -1.496999 time 2019-02-21 18:56:35.059118
Model ind 685 epoch 1 head B batch: 344 avg loss -1.615006 avg loss no lamb -1.615006 time 2019-02-21 18:56:35.541069
Model ind 685 epoch 1 head B batch: 345 avg loss -1.549389 avg loss no lamb -1.549389 time 2019-02-21 18:56:36.075199
Model ind 685 epoch 1 head B batch: 346 avg loss -1.617695 avg loss no lamb -1.617695 time 2019-02-21 18:56:36.594310
Model ind 685 epoch 1 head B batch: 347 avg loss -1.495653 avg loss no lamb -1.495653 time 2019-02-21 18:56:37.118646
Model ind 685 epoch 1 head B batch: 348 avg loss -1.499540 avg loss no lamb -1.499540 time 2019-02-21 18:56:37.629062
Model ind 685 epoch 1 head B batch: 349 avg loss -1.497900 avg loss no lamb -1.497900 time 2019-02-21 18:56:38.157805
Model ind 685 epoch 1 head B batch: 350 avg loss -1.475335 avg loss no lamb -1.475335 time 2019-02-21 18:56:38.676816
Model ind 685 epoch 1 head B batch: 351 avg loss -1.550673 avg loss no lamb -1.550673 time 2019-02-21 18:56:39.225357
Model ind 685 epoch 1 head B batch: 352 avg loss -1.636187 avg loss no lamb -1.636187 time 2019-02-21 18:56:39.751134
Model ind 685 epoch 1 head B batch: 353 avg loss -1.386551 avg loss no lamb -1.386551 time 2019-02-21 18:56:40.277177
Model ind 685 epoch 1 head B batch: 354 avg loss -1.439414 avg loss no lamb -1.439414 time 2019-02-21 18:56:40.759327
Model ind 685 epoch 1 head B batch: 355 avg loss -1.573335 avg loss no lamb -1.573335 time 2019-02-21 18:56:41.280030
Model ind 685 epoch 1 head B batch: 356 avg loss -1.538455 avg loss no lamb -1.538455 time 2019-02-21 18:56:41.776653
Model ind 685 epoch 1 head B batch: 357 avg loss -1.570988 avg loss no lamb -1.570988 time 2019-02-21 18:56:42.299797
Model ind 685 epoch 1 head B batch: 358 avg loss -1.611619 avg loss no lamb -1.611619 time 2019-02-21 18:56:42.786062
Model ind 685 epoch 1 head B batch: 359 avg loss -1.512117 avg loss no lamb -1.512117 time 2019-02-21 18:56:43.309019
Model ind 685 epoch 1 head B batch: 360 avg loss -1.559059 avg loss no lamb -1.559059 time 2019-02-21 18:56:43.795608
Model ind 685 epoch 1 head B batch: 361 avg loss -1.600313 avg loss no lamb -1.600313 time 2019-02-21 18:56:44.289064
Model ind 685 epoch 1 head B batch: 362 avg loss -1.466801 avg loss no lamb -1.466801 time 2019-02-21 18:56:44.782290
Model ind 685 epoch 1 head B batch: 363 avg loss -1.541767 avg loss no lamb -1.541767 time 2019-02-21 18:56:45.298359
Model ind 685 epoch 1 head B batch: 364 avg loss -1.670755 avg loss no lamb -1.670755 time 2019-02-21 18:56:45.814993
Model ind 685 epoch 1 head B batch: 365 avg loss -1.597185 avg loss no lamb -1.597185 time 2019-02-21 18:56:46.395637
Model ind 685 epoch 1 head B batch: 366 avg loss -1.561338 avg loss no lamb -1.561338 time 2019-02-21 18:56:46.985100
Model ind 685 epoch 1 head B batch: 367 avg loss -1.585349 avg loss no lamb -1.585349 time 2019-02-21 18:56:47.489365
Model ind 685 epoch 1 head B batch: 368 avg loss -1.654703 avg loss no lamb -1.654703 time 2019-02-21 18:56:48.012015
Model ind 685 epoch 1 head B batch: 369 avg loss -1.605143 avg loss no lamb -1.605143 time 2019-02-21 18:56:48.523050
Model ind 685 epoch 1 head B batch: 370 avg loss -1.719314 avg loss no lamb -1.719314 time 2019-02-21 18:56:49.012957
Model ind 685 epoch 1 head B batch: 371 avg loss -1.567906 avg loss no lamb -1.567906 time 2019-02-21 18:56:49.585007
Model ind 685 epoch 1 head B batch: 372 avg loss -1.414437 avg loss no lamb -1.414437 time 2019-02-21 18:56:50.125261
Model ind 685 epoch 1 head B batch: 373 avg loss -1.602524 avg loss no lamb -1.602524 time 2019-02-21 18:56:50.664542
Model ind 685 epoch 1 head B batch: 374 avg loss -1.595913 avg loss no lamb -1.595913 time 2019-02-21 18:56:51.213587
Model ind 685 epoch 1 head B batch: 375 avg loss -1.586769 avg loss no lamb -1.586769 time 2019-02-21 18:56:51.741019
Model ind 685 epoch 1 head B batch: 376 avg loss -1.462115 avg loss no lamb -1.462115 time 2019-02-21 18:56:52.226539
Model ind 685 epoch 1 head B batch: 377 avg loss -1.479947 avg loss no lamb -1.479947 time 2019-02-21 18:56:52.780931
Model ind 685 epoch 1 head B batch: 378 avg loss -1.490924 avg loss no lamb -1.490924 time 2019-02-21 18:56:53.295754
Model ind 685 epoch 1 head B batch: 379 avg loss -1.587239 avg loss no lamb -1.587239 time 2019-02-21 18:56:53.787076
Model ind 685 epoch 1 head B batch: 380 avg loss -1.654394 avg loss no lamb -1.654394 time 2019-02-21 18:56:54.281552
Model ind 685 epoch 1 head B batch: 381 avg loss -1.605493 avg loss no lamb -1.605493 time 2019-02-21 18:56:54.818147
Model ind 685 epoch 1 head B batch: 382 avg loss -1.536122 avg loss no lamb -1.536122 time 2019-02-21 18:56:55.366876
Model ind 685 epoch 1 head B batch: 383 avg loss -1.552526 avg loss no lamb -1.552526 time 2019-02-21 18:56:55.909265
Model ind 685 epoch 1 head B batch: 384 avg loss -1.617843 avg loss no lamb -1.617843 time 2019-02-21 18:56:56.456737
Model ind 685 epoch 1 head B batch: 385 avg loss -1.459270 avg loss no lamb -1.459270 time 2019-02-21 18:56:56.977508
Model ind 685 epoch 1 head B batch: 386 avg loss -1.639733 avg loss no lamb -1.639733 time 2019-02-21 18:56:57.538424
Model ind 685 epoch 1 head B batch: 387 avg loss -1.565350 avg loss no lamb -1.565350 time 2019-02-21 18:56:58.037250
Model ind 685 epoch 1 head B batch: 388 avg loss -1.711182 avg loss no lamb -1.711182 time 2019-02-21 18:56:58.546081
Model ind 685 epoch 1 head B batch: 389 avg loss -1.624882 avg loss no lamb -1.624882 time 2019-02-21 18:56:59.055579
Model ind 685 epoch 1 head B batch: 390 avg loss -1.624475 avg loss no lamb -1.624475 time 2019-02-21 18:56:59.554083
Model ind 685 epoch 1 head B batch: 391 avg loss -1.613072 avg loss no lamb -1.613072 time 2019-02-21 18:57:00.108604
Model ind 685 epoch 1 head B batch: 392 avg loss -1.476335 avg loss no lamb -1.476335 time 2019-02-21 18:57:00.613631
Model ind 685 epoch 1 head B batch: 393 avg loss -1.679906 avg loss no lamb -1.679906 time 2019-02-21 18:57:01.157725
Model ind 685 epoch 1 head B batch: 394 avg loss -1.607398 avg loss no lamb -1.607398 time 2019-02-21 18:57:01.704687
Model ind 685 epoch 1 head B batch: 395 avg loss -1.620063 avg loss no lamb -1.620063 time 2019-02-21 18:57:02.305442
Model ind 685 epoch 1 head B batch: 396 avg loss -1.583883 avg loss no lamb -1.583883 time 2019-02-21 18:57:02.941726
Model ind 685 epoch 1 head B batch: 397 avg loss -1.628035 avg loss no lamb -1.628035 time 2019-02-21 18:57:03.541384
Model ind 685 epoch 1 head B batch: 398 avg loss -1.590505 avg loss no lamb -1.590505 time 2019-02-21 18:57:04.067443
Model ind 685 epoch 1 head B batch: 399 avg loss -1.603549 avg loss no lamb -1.603549 time 2019-02-21 18:57:04.577040
Model ind 685 epoch 1 head B batch: 400 avg loss -1.654670 avg loss no lamb -1.654670 time 2019-02-21 18:57:05.110098
Model ind 685 epoch 1 head B batch: 401 avg loss -1.489688 avg loss no lamb -1.489688 time 2019-02-21 18:57:05.634751
Model ind 685 epoch 1 head B batch: 402 avg loss -1.703562 avg loss no lamb -1.703562 time 2019-02-21 18:57:06.157793
Model ind 685 epoch 1 head B batch: 403 avg loss -1.547748 avg loss no lamb -1.547748 time 2019-02-21 18:57:06.656994
Model ind 685 epoch 1 head B batch: 404 avg loss -1.680262 avg loss no lamb -1.680262 time 2019-02-21 18:57:07.180512
Model ind 685 epoch 1 head B batch: 405 avg loss -1.584684 avg loss no lamb -1.584684 time 2019-02-21 18:57:07.724645
Model ind 685 epoch 1 head B batch: 406 avg loss -1.728543 avg loss no lamb -1.728543 time 2019-02-21 18:57:08.293198
Model ind 685 epoch 1 head B batch: 407 avg loss -1.660977 avg loss no lamb -1.660977 time 2019-02-21 18:57:08.818138
Model ind 685 epoch 1 head B batch: 408 avg loss -1.625976 avg loss no lamb -1.625976 time 2019-02-21 18:57:09.372759
Model ind 685 epoch 1 head B batch: 409 avg loss -1.605018 avg loss no lamb -1.605018 time 2019-02-21 18:57:09.882331
Model ind 685 epoch 1 head B batch: 410 avg loss -1.650520 avg loss no lamb -1.650520 time 2019-02-21 18:57:10.382095
Model ind 685 epoch 1 head B batch: 411 avg loss -1.623433 avg loss no lamb -1.623433 time 2019-02-21 18:57:10.944817
Model ind 685 epoch 1 head B batch: 412 avg loss -1.587597 avg loss no lamb -1.587597 time 2019-02-21 18:57:11.514130
Model ind 685 epoch 1 head B batch: 413 avg loss -1.574991 avg loss no lamb -1.574991 time 2019-02-21 18:57:12.040840
Model ind 685 epoch 1 head B batch: 414 avg loss -1.656734 avg loss no lamb -1.656734 time 2019-02-21 18:57:12.531283
Model ind 685 epoch 1 head B batch: 415 avg loss -1.783781 avg loss no lamb -1.783781 time 2019-02-21 18:57:13.131728
Model ind 685 epoch 1 head B batch: 416 avg loss -1.720879 avg loss no lamb -1.720879 time 2019-02-21 18:57:13.633618
Model ind 685 epoch 1 head B batch: 417 avg loss -1.625807 avg loss no lamb -1.625807 time 2019-02-21 18:57:14.139683
Model ind 685 epoch 1 head B batch: 418 avg loss -1.687721 avg loss no lamb -1.687721 time 2019-02-21 18:57:14.625021
Model ind 685 epoch 1 head B batch: 419 avg loss -1.787379 avg loss no lamb -1.787379 time 2019-02-21 18:57:15.131881
Model ind 685 epoch 1 head B batch: 420 avg loss -1.684453 avg loss no lamb -1.684453 time 2019-02-21 18:57:15.664898
Model ind 685 epoch 1 head B batch: 421 avg loss -1.795476 avg loss no lamb -1.795476 time 2019-02-21 18:57:16.172761
Model ind 685 epoch 1 head B batch: 422 avg loss -1.820285 avg loss no lamb -1.820285 time 2019-02-21 18:57:16.722892
Model ind 685 epoch 1 head B batch: 423 avg loss -1.435670 avg loss no lamb -1.435670 time 2019-02-21 18:57:17.247799
Model ind 685 epoch 1 head B batch: 424 avg loss -1.655073 avg loss no lamb -1.655073 time 2019-02-21 18:57:17.745154
Model ind 685 epoch 1 head B batch: 425 avg loss -1.727070 avg loss no lamb -1.727070 time 2019-02-21 18:57:18.237171
Model ind 685 epoch 1 head B batch: 426 avg loss -1.555157 avg loss no lamb -1.555157 time 2019-02-21 18:57:18.750625
Model ind 685 epoch 1 head B batch: 427 avg loss -1.696192 avg loss no lamb -1.696192 time 2019-02-21 18:57:19.318452
Model ind 685 epoch 1 head B batch: 428 avg loss -1.652770 avg loss no lamb -1.652770 time 2019-02-21 18:57:19.789038
Model ind 685 epoch 1 head B batch: 429 avg loss -1.589459 avg loss no lamb -1.589459 time 2019-02-21 18:57:20.312176
Model ind 685 epoch 1 head B batch: 430 avg loss -1.560913 avg loss no lamb -1.560913 time 2019-02-21 18:57:20.868630
Model ind 685 epoch 1 head B batch: 431 avg loss -1.613173 avg loss no lamb -1.613173 time 2019-02-21 18:57:21.410102
Model ind 685 epoch 1 head B batch: 432 avg loss -1.442582 avg loss no lamb -1.442582 time 2019-02-21 18:57:21.921226
Model ind 685 epoch 1 head B batch: 433 avg loss -1.582332 avg loss no lamb -1.582332 time 2019-02-21 18:57:22.405933
Model ind 685 epoch 1 head B batch: 434 avg loss -1.634423 avg loss no lamb -1.634423 time 2019-02-21 18:57:22.893355
Model ind 685 epoch 1 head B batch: 435 avg loss -1.654609 avg loss no lamb -1.654609 time 2019-02-21 18:57:23.417702
Model ind 685 epoch 1 head B batch: 436 avg loss -1.553874 avg loss no lamb -1.553874 time 2019-02-21 18:57:23.948112
Model ind 685 epoch 1 head B batch: 437 avg loss -1.565677 avg loss no lamb -1.565677 time 2019-02-21 18:57:24.498115
Model ind 685 epoch 1 head B batch: 438 avg loss -1.510995 avg loss no lamb -1.510995 time 2019-02-21 18:57:24.980347
Model ind 685 epoch 1 head B batch: 439 avg loss -1.521181 avg loss no lamb -1.521181 time 2019-02-21 18:57:25.533243
Model ind 685 epoch 1 head B batch: 440 avg loss -1.621280 avg loss no lamb -1.621280 time 2019-02-21 18:57:26.130402
Model ind 685 epoch 1 head B batch: 441 avg loss -1.605800 avg loss no lamb -1.605800 time 2019-02-21 18:57:26.707042
Model ind 685 epoch 1 head B batch: 442 avg loss -1.599704 avg loss no lamb -1.599704 time 2019-02-21 18:57:27.313347
Model ind 685 epoch 1 head B batch: 443 avg loss -1.513989 avg loss no lamb -1.513989 time 2019-02-21 18:57:27.985610
Model ind 685 epoch 1 head B batch: 444 avg loss -1.622245 avg loss no lamb -1.622245 time 2019-02-21 18:57:28.617136
Model ind 685 epoch 1 head B batch: 445 avg loss -1.532890 avg loss no lamb -1.532890 time 2019-02-21 18:57:29.184617
Model ind 685 epoch 1 head B batch: 446 avg loss -1.574569 avg loss no lamb -1.574569 time 2019-02-21 18:57:29.733440
Model ind 685 epoch 1 head B batch: 447 avg loss -1.588512 avg loss no lamb -1.588512 time 2019-02-21 18:57:30.293616
Model ind 685 epoch 1 head B batch: 448 avg loss -1.538125 avg loss no lamb -1.538125 time 2019-02-21 18:57:30.877561
Model ind 685 epoch 1 head B batch: 449 avg loss -1.528815 avg loss no lamb -1.528815 time 2019-02-21 18:57:31.412215
Model ind 685 epoch 1 head B batch: 450 avg loss -1.595779 avg loss no lamb -1.595779 time 2019-02-21 18:57:32.007350
Model ind 685 epoch 1 head B batch: 451 avg loss -1.615552 avg loss no lamb -1.615552 time 2019-02-21 18:57:32.574264
Model ind 685 epoch 1 head B batch: 452 avg loss -1.539390 avg loss no lamb -1.539390 time 2019-02-21 18:57:33.144115
Model ind 685 epoch 1 head B batch: 453 avg loss -1.591863 avg loss no lamb -1.591863 time 2019-02-21 18:57:33.680815
Model ind 685 epoch 1 head B batch: 454 avg loss -1.638476 avg loss no lamb -1.638476 time 2019-02-21 18:57:34.193843
Model ind 685 epoch 1 head B batch: 455 avg loss -1.534146 avg loss no lamb -1.534146 time 2019-02-21 18:57:34.685653
Model ind 685 epoch 1 head B batch: 456 avg loss -1.536886 avg loss no lamb -1.536886 time 2019-02-21 18:57:35.193399
Model ind 685 epoch 1 head B batch: 457 avg loss -1.627394 avg loss no lamb -1.627394 time 2019-02-21 18:57:35.746798
Model ind 685 epoch 1 head B batch: 458 avg loss -1.598483 avg loss no lamb -1.598483 time 2019-02-21 18:57:36.330771
Model ind 685 epoch 1 head B batch: 459 avg loss -1.497659 avg loss no lamb -1.497659 time 2019-02-21 18:57:36.926724
Model ind 685 epoch 1 head B batch: 460 avg loss -1.580718 avg loss no lamb -1.580718 time 2019-02-21 18:57:37.487520
Model ind 685 epoch 1 head B batch: 461 avg loss -1.612766 avg loss no lamb -1.612766 time 2019-02-21 18:57:38.017721
Model ind 685 epoch 1 head B batch: 462 avg loss -1.572951 avg loss no lamb -1.572951 time 2019-02-21 18:57:38.529524
Model ind 685 epoch 1 head B batch: 463 avg loss -1.492822 avg loss no lamb -1.492822 time 2019-02-21 18:57:39.014329
Model ind 685 epoch 1 head B batch: 464 avg loss -1.688545 avg loss no lamb -1.688545 time 2019-02-21 18:57:39.522103
Model ind 685 epoch 1 head B batch: 465 avg loss -1.695750 avg loss no lamb -1.695750 time 2019-02-21 18:57:40.011326
Model ind 685 epoch 1 head B batch: 466 avg loss -1.724963 avg loss no lamb -1.724963 time 2019-02-21 18:57:40.540129
Model ind 685 epoch 1 head B batch: 467 avg loss -1.786601 avg loss no lamb -1.786601 time 2019-02-21 18:57:41.061553
Model ind 685 epoch 1 head B batch: 468 avg loss -1.624422 avg loss no lamb -1.624422 time 2019-02-21 18:57:41.592693
Model ind 685 epoch 1 head B batch: 469 avg loss -1.697925 avg loss no lamb -1.697925 time 2019-02-21 18:57:42.104877
Model ind 685 epoch 1 head B batch: 470 avg loss -1.516975 avg loss no lamb -1.516975 time 2019-02-21 18:57:42.634568
Model ind 685 epoch 1 head B batch: 471 avg loss -1.650009 avg loss no lamb -1.650009 time 2019-02-21 18:57:43.235003
Model ind 685 epoch 1 head B batch: 472 avg loss -1.671635 avg loss no lamb -1.671635 time 2019-02-21 18:57:43.812730
Model ind 685 epoch 1 head B batch: 473 avg loss -1.840285 avg loss no lamb -1.840285 time 2019-02-21 18:57:44.398561
Model ind 685 epoch 1 head B batch: 474 avg loss -1.820043 avg loss no lamb -1.820043 time 2019-02-21 18:57:44.949931
Model ind 685 epoch 1 head B batch: 475 avg loss -1.521705 avg loss no lamb -1.521705 time 2019-02-21 18:57:45.475323
Model ind 685 epoch 1 head B batch: 476 avg loss -1.712163 avg loss no lamb -1.712163 time 2019-02-21 18:57:46.028226
Model ind 685 epoch 1 head B batch: 477 avg loss -1.653937 avg loss no lamb -1.653937 time 2019-02-21 18:57:46.537666
Model ind 685 epoch 1 head B batch: 478 avg loss -1.743962 avg loss no lamb -1.743962 time 2019-02-21 18:57:47.055603
Model ind 685 epoch 1 head B batch: 479 avg loss -1.798221 avg loss no lamb -1.798221 time 2019-02-21 18:57:47.547992
Model ind 685 epoch 1 head B batch: 480 avg loss -1.740339 avg loss no lamb -1.740339 time 2019-02-21 18:57:48.082497
Model ind 685 epoch 1 head B batch: 481 avg loss -1.721027 avg loss no lamb -1.721027 time 2019-02-21 18:57:48.589394
Model ind 685 epoch 1 head B batch: 482 avg loss -1.686552 avg loss no lamb -1.686552 time 2019-02-21 18:57:49.192119
Model ind 685 epoch 1 head B batch: 483 avg loss -1.740525 avg loss no lamb -1.740525 time 2019-02-21 18:57:49.707964
Model ind 685 epoch 1 head B batch: 484 avg loss -1.710464 avg loss no lamb -1.710464 time 2019-02-21 18:57:50.217035
Model ind 685 epoch 1 head B batch: 485 avg loss -1.688078 avg loss no lamb -1.688078 time 2019-02-21 18:57:50.760580
Model ind 685 epoch 1 head B batch: 486 avg loss -1.630883 avg loss no lamb -1.630883 time 2019-02-21 18:57:51.279311
Model ind 685 epoch 1 head B batch: 487 avg loss -1.606251 avg loss no lamb -1.606251 time 2019-02-21 18:57:51.767202
Model ind 685 epoch 1 head B batch: 488 avg loss -1.642328 avg loss no lamb -1.642328 time 2019-02-21 18:57:52.273094
Model ind 685 epoch 1 head B batch: 489 avg loss -1.726409 avg loss no lamb -1.726409 time 2019-02-21 18:57:52.784807
Model ind 685 epoch 1 head B batch: 490 avg loss -1.816474 avg loss no lamb -1.816474 time 2019-02-21 18:57:53.328286
Model ind 685 epoch 1 head B batch: 491 avg loss -1.796276 avg loss no lamb -1.796276 time 2019-02-21 18:57:53.917921
Model ind 685 epoch 1 head B batch: 492 avg loss -1.782692 avg loss no lamb -1.782692 time 2019-02-21 18:57:54.476478
Model ind 685 epoch 1 head B batch: 493 avg loss -1.687838 avg loss no lamb -1.687838 time 2019-02-21 18:57:55.034526
Model ind 685 epoch 1 head B batch: 494 avg loss -1.675767 avg loss no lamb -1.675767 time 2019-02-21 18:57:55.579829
Model ind 685 epoch 1 head B batch: 495 avg loss -1.783194 avg loss no lamb -1.783194 time 2019-02-21 18:57:56.094250
Model ind 685 epoch 1 head B batch: 496 avg loss -1.718221 avg loss no lamb -1.718221 time 2019-02-21 18:57:56.635541
Model ind 685 epoch 1 head B batch: 497 avg loss -1.616988 avg loss no lamb -1.616988 time 2019-02-21 18:57:57.183424
Model ind 685 epoch 1 head B batch: 498 avg loss -1.526586 avg loss no lamb -1.526586 time 2019-02-21 18:57:57.736658
Model ind 685 epoch 1 head B batch: 499 avg loss -1.606606 avg loss no lamb -1.606606 time 2019-02-21 18:57:58.314079
Model ind 685 epoch 1 head A batch: 0 avg loss -1.652104 avg loss no lamb -1.652104 time 2019-02-21 18:57:58.853596
Model ind 685 epoch 1 head A batch: 1 avg loss -1.657043 avg loss no lamb -1.657043 time 2019-02-21 18:57:59.382754
Model ind 685 epoch 1 head A batch: 2 avg loss -1.723703 avg loss no lamb -1.723703 time 2019-02-21 18:57:59.982872
Model ind 685 epoch 1 head A batch: 3 avg loss -1.571519 avg loss no lamb -1.571519 time 2019-02-21 18:58:00.565869
Model ind 685 epoch 1 head A batch: 4 avg loss -1.619350 avg loss no lamb -1.619350 time 2019-02-21 18:58:01.186713
Model ind 685 epoch 1 head A batch: 5 avg loss -1.605735 avg loss no lamb -1.605735 time 2019-02-21 18:58:01.806855
Model ind 685 epoch 1 head A batch: 6 avg loss -1.551093 avg loss no lamb -1.551093 time 2019-02-21 18:58:02.428081
Model ind 685 epoch 1 head A batch: 7 avg loss -1.633963 avg loss no lamb -1.633963 time 2019-02-21 18:58:03.026845
Model ind 685 epoch 1 head A batch: 8 avg loss -1.625663 avg loss no lamb -1.625663 time 2019-02-21 18:58:03.590882
Model ind 685 epoch 1 head A batch: 9 avg loss -1.499596 avg loss no lamb -1.499596 time 2019-02-21 18:58:04.108888
Model ind 685 epoch 1 head A batch: 10 avg loss -1.616124 avg loss no lamb -1.616124 time 2019-02-21 18:58:04.603263
Model ind 685 epoch 1 head A batch: 11 avg loss -1.648312 avg loss no lamb -1.648312 time 2019-02-21 18:58:05.149237
Model ind 685 epoch 1 head A batch: 12 avg loss -1.691038 avg loss no lamb -1.691038 time 2019-02-21 18:58:05.691976
Model ind 685 epoch 1 head A batch: 13 avg loss -1.702298 avg loss no lamb -1.702298 time 2019-02-21 18:58:06.172087
Model ind 685 epoch 1 head A batch: 14 avg loss -1.615825 avg loss no lamb -1.615825 time 2019-02-21 18:58:06.764915
Model ind 685 epoch 1 head A batch: 15 avg loss -1.659670 avg loss no lamb -1.659670 time 2019-02-21 18:58:07.326257
Model ind 685 epoch 1 head A batch: 16 avg loss -1.607552 avg loss no lamb -1.607552 time 2019-02-21 18:58:07.888094
Model ind 685 epoch 1 head A batch: 17 avg loss -1.674350 avg loss no lamb -1.674350 time 2019-02-21 18:58:08.447734
Model ind 685 epoch 1 head A batch: 18 avg loss -1.594005 avg loss no lamb -1.594005 time 2019-02-21 18:58:09.011726
Model ind 685 epoch 1 head A batch: 19 avg loss -1.698371 avg loss no lamb -1.698371 time 2019-02-21 18:58:09.553748
Model ind 685 epoch 1 head A batch: 20 avg loss -1.527511 avg loss no lamb -1.527511 time 2019-02-21 18:58:10.096614
Model ind 685 epoch 1 head A batch: 21 avg loss -1.650946 avg loss no lamb -1.650946 time 2019-02-21 18:58:10.619763
Model ind 685 epoch 1 head A batch: 22 avg loss -1.679804 avg loss no lamb -1.679804 time 2019-02-21 18:58:11.125371
Model ind 685 epoch 1 head A batch: 23 avg loss -1.671396 avg loss no lamb -1.671396 time 2019-02-21 18:58:11.681125
Model ind 685 epoch 1 head A batch: 24 avg loss -1.559366 avg loss no lamb -1.559366 time 2019-02-21 18:58:12.293817
Model ind 685 epoch 1 head A batch: 25 avg loss -1.661643 avg loss no lamb -1.661643 time 2019-02-21 18:58:12.856369
Model ind 685 epoch 1 head A batch: 26 avg loss -1.600002 avg loss no lamb -1.600002 time 2019-02-21 18:58:13.442429
Model ind 685 epoch 1 head A batch: 27 avg loss -1.666837 avg loss no lamb -1.666837 time 2019-02-21 18:58:13.991988
Model ind 685 epoch 1 head A batch: 28 avg loss -1.687643 avg loss no lamb -1.687643 time 2019-02-21 18:58:14.546643
Model ind 685 epoch 1 head A batch: 29 avg loss -1.638204 avg loss no lamb -1.638204 time 2019-02-21 18:58:15.070198
Model ind 685 epoch 1 head A batch: 30 avg loss -1.605830 avg loss no lamb -1.605830 time 2019-02-21 18:58:15.633755
Model ind 685 epoch 1 head A batch: 31 avg loss -1.729784 avg loss no lamb -1.729784 time 2019-02-21 18:58:16.149158
Model ind 685 epoch 1 head A batch: 32 avg loss -1.697873 avg loss no lamb -1.697873 time 2019-02-21 18:58:16.709069
Model ind 685 epoch 1 head A batch: 33 avg loss -1.571120 avg loss no lamb -1.571120 time 2019-02-21 18:58:17.233032
Model ind 685 epoch 1 head A batch: 34 avg loss -1.631419 avg loss no lamb -1.631419 time 2019-02-21 18:58:17.716311
Model ind 685 epoch 1 head A batch: 35 avg loss -1.637138 avg loss no lamb -1.637138 time 2019-02-21 18:58:18.200129
Model ind 685 epoch 1 head A batch: 36 avg loss -1.519681 avg loss no lamb -1.519681 time 2019-02-21 18:58:18.696660
Model ind 685 epoch 1 head A batch: 37 avg loss -1.567531 avg loss no lamb -1.567531 time 2019-02-21 18:58:19.237492
Model ind 685 epoch 1 head A batch: 38 avg loss -1.596420 avg loss no lamb -1.596420 time 2019-02-21 18:58:19.767701
Model ind 685 epoch 1 head A batch: 39 avg loss -1.639433 avg loss no lamb -1.639433 time 2019-02-21 18:58:20.303717
Model ind 685 epoch 1 head A batch: 40 avg loss -1.541956 avg loss no lamb -1.541956 time 2019-02-21 18:58:20.866282
Model ind 685 epoch 1 head A batch: 41 avg loss -1.639497 avg loss no lamb -1.639497 time 2019-02-21 18:58:21.422544
Model ind 685 epoch 1 head A batch: 42 avg loss -1.617178 avg loss no lamb -1.617178 time 2019-02-21 18:58:22.001630
Model ind 685 epoch 1 head A batch: 43 avg loss -1.735394 avg loss no lamb -1.735394 time 2019-02-21 18:58:22.606824
Model ind 685 epoch 1 head A batch: 44 avg loss -1.627901 avg loss no lamb -1.627901 time 2019-02-21 18:58:23.221111
Model ind 685 epoch 1 head A batch: 45 avg loss -1.619500 avg loss no lamb -1.619500 time 2019-02-21 18:58:23.812013
Model ind 685 epoch 1 head A batch: 46 avg loss -1.723547 avg loss no lamb -1.723547 time 2019-02-21 18:58:24.352191
Model ind 685 epoch 1 head A batch: 47 avg loss -1.745717 avg loss no lamb -1.745717 time 2019-02-21 18:58:24.885213
Model ind 685 epoch 1 head A batch: 48 avg loss -1.559574 avg loss no lamb -1.559574 time 2019-02-21 18:58:25.386933
Model ind 685 epoch 1 head A batch: 49 avg loss -1.592949 avg loss no lamb -1.592949 time 2019-02-21 18:58:25.900920
Model ind 685 epoch 1 head A batch: 50 avg loss -1.668159 avg loss no lamb -1.668159 time 2019-02-21 18:58:26.426341
Model ind 685 epoch 1 head A batch: 51 avg loss -1.616133 avg loss no lamb -1.616133 time 2019-02-21 18:58:26.920156
Model ind 685 epoch 1 head A batch: 52 avg loss -1.596964 avg loss no lamb -1.596964 time 2019-02-21 18:58:27.405337
Model ind 685 epoch 1 head A batch: 53 avg loss -1.669775 avg loss no lamb -1.669775 time 2019-02-21 18:58:27.936601
Model ind 685 epoch 1 head A batch: 54 avg loss -1.623486 avg loss no lamb -1.623486 time 2019-02-21 18:58:28.498216
Model ind 685 epoch 1 head A batch: 55 avg loss -1.667394 avg loss no lamb -1.667394 time 2019-02-21 18:58:29.059636
Model ind 685 epoch 1 head A batch: 56 avg loss -1.590908 avg loss no lamb -1.590908 time 2019-02-21 18:58:29.588149
Model ind 685 epoch 1 head A batch: 57 avg loss -1.600741 avg loss no lamb -1.600741 time 2019-02-21 18:58:30.110453
Model ind 685 epoch 1 head A batch: 58 avg loss -1.584870 avg loss no lamb -1.584870 time 2019-02-21 18:58:30.643726
Model ind 685 epoch 1 head A batch: 59 avg loss -1.670871 avg loss no lamb -1.670871 time 2019-02-21 18:58:31.169696
Model ind 685 epoch 1 head A batch: 60 avg loss -1.665344 avg loss no lamb -1.665344 time 2019-02-21 18:58:31.658222
Model ind 685 epoch 1 head A batch: 61 avg loss -1.621037 avg loss no lamb -1.621037 time 2019-02-21 18:58:32.173112
Model ind 685 epoch 1 head A batch: 62 avg loss -1.467178 avg loss no lamb -1.467178 time 2019-02-21 18:58:32.707548
Model ind 685 epoch 1 head A batch: 63 avg loss -1.485605 avg loss no lamb -1.485605 time 2019-02-21 18:58:33.283073
Model ind 685 epoch 1 head A batch: 64 avg loss -1.740205 avg loss no lamb -1.740205 time 2019-02-21 18:58:33.789305
Model ind 685 epoch 1 head A batch: 65 avg loss -1.592730 avg loss no lamb -1.592730 time 2019-02-21 18:58:34.288585
Model ind 685 epoch 1 head A batch: 66 avg loss -1.637534 avg loss no lamb -1.637534 time 2019-02-21 18:58:34.808255
Model ind 685 epoch 1 head A batch: 67 avg loss -1.419914 avg loss no lamb -1.419914 time 2019-02-21 18:58:35.368502
Model ind 685 epoch 1 head A batch: 68 avg loss -1.568669 avg loss no lamb -1.568669 time 2019-02-21 18:58:35.981425
Model ind 685 epoch 1 head A batch: 69 avg loss -1.630018 avg loss no lamb -1.630018 time 2019-02-21 18:58:36.538477
Model ind 685 epoch 1 head A batch: 70 avg loss -1.702392 avg loss no lamb -1.702392 time 2019-02-21 18:58:37.105809
Model ind 685 epoch 1 head A batch: 71 avg loss -1.609307 avg loss no lamb -1.609307 time 2019-02-21 18:58:37.598907
Model ind 685 epoch 1 head A batch: 72 avg loss -1.568033 avg loss no lamb -1.568033 time 2019-02-21 18:58:38.144820
Model ind 685 epoch 1 head A batch: 73 avg loss -1.426556 avg loss no lamb -1.426556 time 2019-02-21 18:58:38.669510
Model ind 685 epoch 1 head A batch: 74 avg loss -1.619958 avg loss no lamb -1.619958 time 2019-02-21 18:58:39.180257
Model ind 685 epoch 1 head A batch: 75 avg loss -1.687439 avg loss no lamb -1.687439 time 2019-02-21 18:58:39.694416
Model ind 685 epoch 1 head A batch: 76 avg loss -1.569655 avg loss no lamb -1.569655 time 2019-02-21 18:58:40.218911
Model ind 685 epoch 1 head A batch: 77 avg loss -1.663374 avg loss no lamb -1.663374 time 2019-02-21 18:58:40.744577
Model ind 685 epoch 1 head A batch: 78 avg loss -1.610587 avg loss no lamb -1.610587 time 2019-02-21 18:58:41.275396
Model ind 685 epoch 1 head A batch: 79 avg loss -1.689834 avg loss no lamb -1.689834 time 2019-02-21 18:58:41.799998
Model ind 685 epoch 1 head A batch: 80 avg loss -1.692807 avg loss no lamb -1.692807 time 2019-02-21 18:58:42.321096
Model ind 685 epoch 1 head A batch: 81 avg loss -1.775315 avg loss no lamb -1.775315 time 2019-02-21 18:58:42.848834
Model ind 685 epoch 1 head A batch: 82 avg loss -1.526442 avg loss no lamb -1.526442 time 2019-02-21 18:58:43.344617
Model ind 685 epoch 1 head A batch: 83 avg loss -1.558855 avg loss no lamb -1.558855 time 2019-02-21 18:58:43.875171
Model ind 685 epoch 1 head A batch: 84 avg loss -1.614421 avg loss no lamb -1.614421 time 2019-02-21 18:58:44.437390
Model ind 685 epoch 1 head A batch: 85 avg loss -1.565556 avg loss no lamb -1.565556 time 2019-02-21 18:58:44.995400
Model ind 685 epoch 1 head A batch: 86 avg loss -1.681643 avg loss no lamb -1.681643 time 2019-02-21 18:58:45.507903
Model ind 685 epoch 1 head A batch: 87 avg loss -1.591637 avg loss no lamb -1.591637 time 2019-02-21 18:58:46.007895
Model ind 685 epoch 1 head A batch: 88 avg loss -1.621584 avg loss no lamb -1.621584 time 2019-02-21 18:58:46.485404
Model ind 685 epoch 1 head A batch: 89 avg loss -1.548433 avg loss no lamb -1.548433 time 2019-02-21 18:58:46.995273
Model ind 685 epoch 1 head A batch: 90 avg loss -1.619103 avg loss no lamb -1.619103 time 2019-02-21 18:58:47.526648
Model ind 685 epoch 1 head A batch: 91 avg loss -1.564848 avg loss no lamb -1.564848 time 2019-02-21 18:58:48.016569
Model ind 685 epoch 1 head A batch: 92 avg loss -1.570904 avg loss no lamb -1.570904 time 2019-02-21 18:58:48.491798
Model ind 685 epoch 1 head A batch: 93 avg loss -1.515658 avg loss no lamb -1.515658 time 2019-02-21 18:58:48.962584
Model ind 685 epoch 1 head A batch: 94 avg loss -1.624430 avg loss no lamb -1.624430 time 2019-02-21 18:58:49.550025
Model ind 685 epoch 1 head A batch: 95 avg loss -1.651430 avg loss no lamb -1.651430 time 2019-02-21 18:58:50.048036
Model ind 685 epoch 1 head A batch: 96 avg loss -1.674321 avg loss no lamb -1.674321 time 2019-02-21 18:58:50.666697
Model ind 685 epoch 1 head A batch: 97 avg loss -1.536797 avg loss no lamb -1.536797 time 2019-02-21 18:58:51.264004
Model ind 685 epoch 1 head A batch: 98 avg loss -1.499684 avg loss no lamb -1.499684 time 2019-02-21 18:58:51.853903
Model ind 685 epoch 1 head A batch: 99 avg loss -1.600661 avg loss no lamb -1.600661 time 2019-02-21 18:58:52.442698
Model ind 685 epoch 1 head A batch: 100 avg loss -1.658849 avg loss no lamb -1.658849 time 2019-02-21 18:58:53.028563
Model ind 685 epoch 1 head A batch: 101 avg loss -1.607247 avg loss no lamb -1.607247 time 2019-02-21 18:58:53.611523
Model ind 685 epoch 1 head A batch: 102 avg loss -1.532927 avg loss no lamb -1.532927 time 2019-02-21 18:58:54.133896
Model ind 685 epoch 1 head A batch: 103 avg loss -1.576514 avg loss no lamb -1.576514 time 2019-02-21 18:58:54.703835
Model ind 685 epoch 1 head A batch: 104 avg loss -1.532952 avg loss no lamb -1.532952 time 2019-02-21 18:58:55.237057
Model ind 685 epoch 1 head A batch: 105 avg loss -1.541167 avg loss no lamb -1.541167 time 2019-02-21 18:58:55.731096
Model ind 685 epoch 1 head A batch: 106 avg loss -1.632491 avg loss no lamb -1.632491 time 2019-02-21 18:58:56.293799
Model ind 685 epoch 1 head A batch: 107 avg loss -1.678737 avg loss no lamb -1.678737 time 2019-02-21 18:58:56.861092
Model ind 685 epoch 1 head A batch: 108 avg loss -1.665366 avg loss no lamb -1.665366 time 2019-02-21 18:58:57.377669
Model ind 685 epoch 1 head A batch: 109 avg loss -1.631994 avg loss no lamb -1.631994 time 2019-02-21 18:58:57.881076
Model ind 685 epoch 1 head A batch: 110 avg loss -1.584829 avg loss no lamb -1.584829 time 2019-02-21 18:58:58.393159
Model ind 685 epoch 1 head A batch: 111 avg loss -1.634315 avg loss no lamb -1.634315 time 2019-02-21 18:58:58.958846
Model ind 685 epoch 1 head A batch: 112 avg loss -1.513594 avg loss no lamb -1.513594 time 2019-02-21 18:58:59.484064
Model ind 685 epoch 1 head A batch: 113 avg loss -1.583184 avg loss no lamb -1.583184 time 2019-02-21 18:58:59.996935
Model ind 685 epoch 1 head A batch: 114 avg loss -1.535108 avg loss no lamb -1.535108 time 2019-02-21 18:59:00.508319
Model ind 685 epoch 1 head A batch: 115 avg loss -1.677824 avg loss no lamb -1.677824 time 2019-02-21 18:59:01.016180
Model ind 685 epoch 1 head A batch: 116 avg loss -1.701225 avg loss no lamb -1.701225 time 2019-02-21 18:59:01.508584
Model ind 685 epoch 1 head A batch: 117 avg loss -1.720387 avg loss no lamb -1.720387 time 2019-02-21 18:59:02.030473
Model ind 685 epoch 1 head A batch: 118 avg loss -1.669762 avg loss no lamb -1.669762 time 2019-02-21 18:59:02.510867
Model ind 685 epoch 1 head A batch: 119 avg loss -1.631424 avg loss no lamb -1.631424 time 2019-02-21 18:59:03.025480
Model ind 685 epoch 1 head A batch: 120 avg loss -1.622016 avg loss no lamb -1.622016 time 2019-02-21 18:59:03.569222
Model ind 685 epoch 1 head A batch: 121 avg loss -1.479498 avg loss no lamb -1.479498 time 2019-02-21 18:59:04.086055
Model ind 685 epoch 1 head A batch: 122 avg loss -1.501688 avg loss no lamb -1.501688 time 2019-02-21 18:59:04.609995
Model ind 685 epoch 1 head A batch: 123 avg loss -1.607405 avg loss no lamb -1.607405 time 2019-02-21 18:59:05.116736
Model ind 685 epoch 1 head A batch: 124 avg loss -1.528974 avg loss no lamb -1.528974 time 2019-02-21 18:59:05.622000
Model ind 685 epoch 1 head A batch: 125 avg loss -1.584070 avg loss no lamb -1.584070 time 2019-02-21 18:59:06.125322
Model ind 685 epoch 1 head A batch: 126 avg loss -1.625955 avg loss no lamb -1.625955 time 2019-02-21 18:59:06.641168
Model ind 685 epoch 1 head A batch: 127 avg loss -1.646438 avg loss no lamb -1.646438 time 2019-02-21 18:59:07.256573
Model ind 685 epoch 1 head A batch: 128 avg loss -1.690611 avg loss no lamb -1.690611 time 2019-02-21 18:59:07.844463
Model ind 685 epoch 1 head A batch: 129 avg loss -1.680166 avg loss no lamb -1.680166 time 2019-02-21 18:59:08.376718
Model ind 685 epoch 1 head A batch: 130 avg loss -1.650832 avg loss no lamb -1.650832 time 2019-02-21 18:59:08.880902
Model ind 685 epoch 1 head A batch: 131 avg loss -1.633147 avg loss no lamb -1.633147 time 2019-02-21 18:59:09.449174
Model ind 685 epoch 1 head A batch: 132 avg loss -1.632317 avg loss no lamb -1.632317 time 2019-02-21 18:59:10.000036
Model ind 685 epoch 1 head A batch: 133 avg loss -1.618955 avg loss no lamb -1.618955 time 2019-02-21 18:59:10.526571
Model ind 685 epoch 1 head A batch: 134 avg loss -1.747978 avg loss no lamb -1.747978 time 2019-02-21 18:59:11.027125
Model ind 685 epoch 1 head A batch: 135 avg loss -1.650729 avg loss no lamb -1.650729 time 2019-02-21 18:59:11.545610
Model ind 685 epoch 1 head A batch: 136 avg loss -1.676674 avg loss no lamb -1.676674 time 2019-02-21 18:59:12.070968
Model ind 685 epoch 1 head A batch: 137 avg loss -1.646883 avg loss no lamb -1.646883 time 2019-02-21 18:59:12.611779
Model ind 685 epoch 1 head A batch: 138 avg loss -1.669670 avg loss no lamb -1.669670 time 2019-02-21 18:59:13.174941
Model ind 685 epoch 1 head A batch: 139 avg loss -1.554801 avg loss no lamb -1.554801 time 2019-02-21 18:59:13.706948
Model ind 685 epoch 1 head A batch: 140 avg loss -1.689537 avg loss no lamb -1.689537 time 2019-02-21 18:59:14.291598
Model ind 685 epoch 1 head A batch: 141 avg loss -1.611306 avg loss no lamb -1.611306 time 2019-02-21 18:59:14.858057
Model ind 685 epoch 1 head A batch: 142 avg loss -1.620247 avg loss no lamb -1.620247 time 2019-02-21 18:59:15.358571
Model ind 685 epoch 1 head A batch: 143 avg loss -1.646011 avg loss no lamb -1.646011 time 2019-02-21 18:59:15.858410
Model ind 685 epoch 1 head A batch: 144 avg loss -1.536329 avg loss no lamb -1.536329 time 2019-02-21 18:59:16.456229
Model ind 685 epoch 1 head A batch: 145 avg loss -1.698644 avg loss no lamb -1.698644 time 2019-02-21 18:59:16.996545
Model ind 685 epoch 1 head A batch: 146 avg loss -1.710604 avg loss no lamb -1.710604 time 2019-02-21 18:59:17.568923
Model ind 685 epoch 1 head A batch: 147 avg loss -1.651621 avg loss no lamb -1.651621 time 2019-02-21 18:59:18.084464
Model ind 685 epoch 1 head A batch: 148 avg loss -1.632315 avg loss no lamb -1.632315 time 2019-02-21 18:59:18.603558
Model ind 685 epoch 1 head A batch: 149 avg loss -1.669205 avg loss no lamb -1.669205 time 2019-02-21 18:59:19.120606
Model ind 685 epoch 1 head A batch: 150 avg loss -1.651866 avg loss no lamb -1.651866 time 2019-02-21 18:59:19.606538
Model ind 685 epoch 1 head A batch: 151 avg loss -1.625243 avg loss no lamb -1.625243 time 2019-02-21 18:59:20.145610
Model ind 685 epoch 1 head A batch: 152 avg loss -1.546779 avg loss no lamb -1.546779 time 2019-02-21 18:59:20.661873
Model ind 685 epoch 1 head A batch: 153 avg loss -1.648907 avg loss no lamb -1.648907 time 2019-02-21 18:59:21.209321
Model ind 685 epoch 1 head A batch: 154 avg loss -1.686611 avg loss no lamb -1.686611 time 2019-02-21 18:59:21.693754
Model ind 685 epoch 1 head A batch: 155 avg loss -1.654631 avg loss no lamb -1.654631 time 2019-02-21 18:59:22.207533
Model ind 685 epoch 1 head A batch: 156 avg loss -1.654266 avg loss no lamb -1.654266 time 2019-02-21 18:59:22.744622
Model ind 685 epoch 1 head A batch: 157 avg loss -1.648068 avg loss no lamb -1.648068 time 2019-02-21 18:59:23.259358
Model ind 685 epoch 1 head A batch: 158 avg loss -1.539572 avg loss no lamb -1.539572 time 2019-02-21 18:59:23.768672
Model ind 685 epoch 1 head A batch: 159 avg loss -1.750247 avg loss no lamb -1.750247 time 2019-02-21 18:59:24.286350
Model ind 685 epoch 1 head A batch: 160 avg loss -1.555759 avg loss no lamb -1.555759 time 2019-02-21 18:59:24.810407
Model ind 685 epoch 1 head A batch: 161 avg loss -1.537762 avg loss no lamb -1.537762 time 2019-02-21 18:59:25.330507
Model ind 685 epoch 1 head A batch: 162 avg loss -1.598311 avg loss no lamb -1.598311 time 2019-02-21 18:59:25.840953
Model ind 685 epoch 1 head A batch: 163 avg loss -1.701672 avg loss no lamb -1.701672 time 2019-02-21 18:59:26.374024
Model ind 685 epoch 1 head A batch: 164 avg loss -1.626527 avg loss no lamb -1.626527 time 2019-02-21 18:59:26.878711
Model ind 685 epoch 1 head A batch: 165 avg loss -1.652767 avg loss no lamb -1.652767 time 2019-02-21 18:59:27.395662
Model ind 685 epoch 1 head A batch: 166 avg loss -1.705940 avg loss no lamb -1.705940 time 2019-02-21 18:59:27.877450
Model ind 685 epoch 1 head A batch: 167 avg loss -1.699785 avg loss no lamb -1.699785 time 2019-02-21 18:59:28.357764
Model ind 685 epoch 1 head A batch: 168 avg loss -1.767361 avg loss no lamb -1.767361 time 2019-02-21 18:59:28.945725
Model ind 685 epoch 1 head A batch: 169 avg loss -1.677138 avg loss no lamb -1.677138 time 2019-02-21 18:59:29.527582
Model ind 685 epoch 1 head A batch: 170 avg loss -1.603849 avg loss no lamb -1.603849 time 2019-02-21 18:59:30.111226
Model ind 685 epoch 1 head A batch: 171 avg loss -1.594035 avg loss no lamb -1.594035 time 2019-02-21 18:59:30.646817
Model ind 685 epoch 1 head A batch: 172 avg loss -1.685126 avg loss no lamb -1.685126 time 2019-02-21 18:59:31.164293
Model ind 685 epoch 1 head A batch: 173 avg loss -1.670321 avg loss no lamb -1.670321 time 2019-02-21 18:59:31.664146
Model ind 685 epoch 1 head A batch: 174 avg loss -1.625955 avg loss no lamb -1.625955 time 2019-02-21 18:59:32.165110
Model ind 685 epoch 1 head A batch: 175 avg loss -1.550912 avg loss no lamb -1.550912 time 2019-02-21 18:59:32.699591
Model ind 685 epoch 1 head A batch: 176 avg loss -1.641259 avg loss no lamb -1.641259 time 2019-02-21 18:59:33.242512
Model ind 685 epoch 1 head A batch: 177 avg loss -1.601648 avg loss no lamb -1.601648 time 2019-02-21 18:59:33.789126
Model ind 685 epoch 1 head A batch: 178 avg loss -1.710733 avg loss no lamb -1.710733 time 2019-02-21 18:59:34.354370
Model ind 685 epoch 1 head A batch: 179 avg loss -1.632313 avg loss no lamb -1.632313 time 2019-02-21 18:59:34.961820
Model ind 685 epoch 1 head A batch: 180 avg loss -1.647276 avg loss no lamb -1.647276 time 2019-02-21 18:59:35.552728
Model ind 685 epoch 1 head A batch: 181 avg loss -1.747330 avg loss no lamb -1.747330 time 2019-02-21 18:59:36.141333
Model ind 685 epoch 1 head A batch: 182 avg loss -1.674558 avg loss no lamb -1.674558 time 2019-02-21 18:59:36.679091
Model ind 685 epoch 1 head A batch: 183 avg loss -1.683107 avg loss no lamb -1.683107 time 2019-02-21 18:59:37.231517
Model ind 685 epoch 1 head A batch: 184 avg loss -1.708819 avg loss no lamb -1.708819 time 2019-02-21 18:59:37.794087
Model ind 685 epoch 1 head A batch: 185 avg loss -1.661889 avg loss no lamb -1.661889 time 2019-02-21 18:59:38.328504
Model ind 685 epoch 1 head A batch: 186 avg loss -1.752281 avg loss no lamb -1.752281 time 2019-02-21 18:59:38.870318
Model ind 685 epoch 1 head A batch: 187 avg loss -1.696846 avg loss no lamb -1.696846 time 2019-02-21 18:59:39.396342
Model ind 685 epoch 1 head A batch: 188 avg loss -1.564540 avg loss no lamb -1.564540 time 2019-02-21 18:59:39.923600
Model ind 685 epoch 1 head A batch: 189 avg loss -1.629314 avg loss no lamb -1.629314 time 2019-02-21 18:59:40.421909
Model ind 685 epoch 1 head A batch: 190 avg loss -1.572828 avg loss no lamb -1.572828 time 2019-02-21 18:59:40.938840
Model ind 685 epoch 1 head A batch: 191 avg loss -1.648163 avg loss no lamb -1.648163 time 2019-02-21 18:59:41.413474
Model ind 685 epoch 1 head A batch: 192 avg loss -1.683745 avg loss no lamb -1.683745 time 2019-02-21 18:59:41.998475
Model ind 685 epoch 1 head A batch: 193 avg loss -1.577178 avg loss no lamb -1.577178 time 2019-02-21 18:59:42.590775
Model ind 685 epoch 1 head A batch: 194 avg loss -1.496165 avg loss no lamb -1.496165 time 2019-02-21 18:59:43.142816
Model ind 685 epoch 1 head A batch: 195 avg loss -1.645514 avg loss no lamb -1.645514 time 2019-02-21 18:59:43.649009
Model ind 685 epoch 1 head A batch: 196 avg loss -1.682833 avg loss no lamb -1.682833 time 2019-02-21 18:59:44.130355
Model ind 685 epoch 1 head A batch: 197 avg loss -1.660634 avg loss no lamb -1.660634 time 2019-02-21 18:59:44.693292
Model ind 685 epoch 1 head A batch: 198 avg loss -1.637849 avg loss no lamb -1.637849 time 2019-02-21 18:59:45.272311
Model ind 685 epoch 1 head A batch: 199 avg loss -1.685189 avg loss no lamb -1.685189 time 2019-02-21 18:59:45.831106
Model ind 685 epoch 1 head A batch: 200 avg loss -1.719072 avg loss no lamb -1.719072 time 2019-02-21 18:59:46.430240
Model ind 685 epoch 1 head A batch: 201 avg loss -1.640459 avg loss no lamb -1.640459 time 2019-02-21 18:59:47.008132
Model ind 685 epoch 1 head A batch: 202 avg loss -1.623368 avg loss no lamb -1.623368 time 2019-02-21 18:59:47.598458
Model ind 685 epoch 1 head A batch: 203 avg loss -1.598318 avg loss no lamb -1.598318 time 2019-02-21 18:59:48.161285
Model ind 685 epoch 1 head A batch: 204 avg loss -1.618270 avg loss no lamb -1.618270 time 2019-02-21 18:59:48.682821
Model ind 685 epoch 1 head A batch: 205 avg loss -1.600502 avg loss no lamb -1.600502 time 2019-02-21 18:59:49.163250
Model ind 685 epoch 1 head A batch: 206 avg loss -1.667292 avg loss no lamb -1.667292 time 2019-02-21 18:59:49.686787
Model ind 685 epoch 1 head A batch: 207 avg loss -1.583799 avg loss no lamb -1.583799 time 2019-02-21 18:59:50.201860
Model ind 685 epoch 1 head A batch: 208 avg loss -1.589964 avg loss no lamb -1.589964 time 2019-02-21 18:59:50.715808
Model ind 685 epoch 1 head A batch: 209 avg loss -1.519829 avg loss no lamb -1.519829 time 2019-02-21 18:59:51.206401
Model ind 685 epoch 1 head A batch: 210 avg loss -1.660396 avg loss no lamb -1.660396 time 2019-02-21 18:59:51.741314
Model ind 685 epoch 1 head A batch: 211 avg loss -1.642959 avg loss no lamb -1.642959 time 2019-02-21 18:59:52.251075
Model ind 685 epoch 1 head A batch: 212 avg loss -1.635470 avg loss no lamb -1.635470 time 2019-02-21 18:59:52.781070
Model ind 685 epoch 1 head A batch: 213 avg loss -1.544799 avg loss no lamb -1.544799 time 2019-02-21 18:59:53.326206
Model ind 685 epoch 1 head A batch: 214 avg loss -1.550536 avg loss no lamb -1.550536 time 2019-02-21 18:59:53.847651
Model ind 685 epoch 1 head A batch: 215 avg loss -1.554573 avg loss no lamb -1.554573 time 2019-02-21 18:59:54.373206
Model ind 685 epoch 1 head A batch: 216 avg loss -1.652765 avg loss no lamb -1.652765 time 2019-02-21 18:59:54.880216
Model ind 685 epoch 1 head A batch: 217 avg loss -1.562778 avg loss no lamb -1.562778 time 2019-02-21 18:59:55.420101
Model ind 685 epoch 1 head A batch: 218 avg loss -1.557986 avg loss no lamb -1.557986 time 2019-02-21 18:59:55.980590
Model ind 685 epoch 1 head A batch: 219 avg loss -1.592111 avg loss no lamb -1.592111 time 2019-02-21 18:59:56.522239
Model ind 685 epoch 1 head A batch: 220 avg loss -1.702064 avg loss no lamb -1.702064 time 2019-02-21 18:59:57.020170
Model ind 685 epoch 1 head A batch: 221 avg loss -1.611967 avg loss no lamb -1.611967 time 2019-02-21 18:59:57.524149
Model ind 685 epoch 1 head A batch: 222 avg loss -1.542943 avg loss no lamb -1.542943 time 2019-02-21 18:59:58.051431
Model ind 685 epoch 1 head A batch: 223 avg loss -1.627351 avg loss no lamb -1.627351 time 2019-02-21 18:59:58.595558
Model ind 685 epoch 1 head A batch: 224 avg loss -1.612713 avg loss no lamb -1.612713 time 2019-02-21 18:59:59.165551
Model ind 685 epoch 1 head A batch: 225 avg loss -1.595026 avg loss no lamb -1.595026 time 2019-02-21 18:59:59.711369
Model ind 685 epoch 1 head A batch: 226 avg loss -1.595867 avg loss no lamb -1.595867 time 2019-02-21 19:00:00.221123
Model ind 685 epoch 1 head A batch: 227 avg loss -1.633732 avg loss no lamb -1.633732 time 2019-02-21 19:00:00.692084
Model ind 685 epoch 1 head A batch: 228 avg loss -1.678672 avg loss no lamb -1.678672 time 2019-02-21 19:00:01.227857
Model ind 685 epoch 1 head A batch: 229 avg loss -1.676786 avg loss no lamb -1.676786 time 2019-02-21 19:00:01.701921
Model ind 685 epoch 1 head A batch: 230 avg loss -1.594900 avg loss no lamb -1.594900 time 2019-02-21 19:00:02.216285
Model ind 685 epoch 1 head A batch: 231 avg loss -1.456151 avg loss no lamb -1.456151 time 2019-02-21 19:00:02.704466
Model ind 685 epoch 1 head A batch: 232 avg loss -1.620776 avg loss no lamb -1.620776 time 2019-02-21 19:00:03.194395
Model ind 685 epoch 1 head A batch: 233 avg loss -1.685443 avg loss no lamb -1.685443 time 2019-02-21 19:00:03.755223
Model ind 685 epoch 1 head A batch: 234 avg loss -1.718289 avg loss no lamb -1.718289 time 2019-02-21 19:00:04.263208
Model ind 685 epoch 1 head A batch: 235 avg loss -1.627040 avg loss no lamb -1.627040 time 2019-02-21 19:00:04.765091
Model ind 685 epoch 1 head A batch: 236 avg loss -1.572357 avg loss no lamb -1.572357 time 2019-02-21 19:00:05.243929
Model ind 685 epoch 1 head A batch: 237 avg loss -1.605033 avg loss no lamb -1.605033 time 2019-02-21 19:00:05.779902
Model ind 685 epoch 1 head A batch: 238 avg loss -1.563821 avg loss no lamb -1.563821 time 2019-02-21 19:00:06.317923
Model ind 685 epoch 1 head A batch: 239 avg loss -1.624030 avg loss no lamb -1.624030 time 2019-02-21 19:00:06.834757
Model ind 685 epoch 1 head A batch: 240 avg loss -1.643880 avg loss no lamb -1.643880 time 2019-02-21 19:00:07.412189
Model ind 685 epoch 1 head A batch: 241 avg loss -1.772024 avg loss no lamb -1.772024 time 2019-02-21 19:00:07.952204
Model ind 685 epoch 1 head A batch: 242 avg loss -1.702741 avg loss no lamb -1.702741 time 2019-02-21 19:00:08.520623
Model ind 685 epoch 1 head A batch: 243 avg loss -1.661059 avg loss no lamb -1.661059 time 2019-02-21 19:00:09.104223
Model ind 685 epoch 1 head A batch: 244 avg loss -1.822749 avg loss no lamb -1.822749 time 2019-02-21 19:00:09.664258
Model ind 685 epoch 1 head A batch: 245 avg loss -1.608233 avg loss no lamb -1.608233 time 2019-02-21 19:00:10.154129
Model ind 685 epoch 1 head A batch: 246 avg loss -1.697054 avg loss no lamb -1.697054 time 2019-02-21 19:00:10.659067
Model ind 685 epoch 1 head A batch: 247 avg loss -1.548022 avg loss no lamb -1.548022 time 2019-02-21 19:00:11.165969
Model ind 685 epoch 1 head A batch: 248 avg loss -1.605347 avg loss no lamb -1.605347 time 2019-02-21 19:00:11.689548
Model ind 685 epoch 1 head A batch: 249 avg loss -1.677713 avg loss no lamb -1.677713 time 2019-02-21 19:00:12.200812
Model ind 685 epoch 1 head A batch: 250 avg loss -1.684549 avg loss no lamb -1.684549 time 2019-02-21 19:00:12.698950
Model ind 685 epoch 1 head A batch: 251 avg loss -1.652909 avg loss no lamb -1.652909 time 2019-02-21 19:00:13.276832
Model ind 685 epoch 1 head A batch: 252 avg loss -1.681026 avg loss no lamb -1.681026 time 2019-02-21 19:00:13.805211
Model ind 685 epoch 1 head A batch: 253 avg loss -1.742074 avg loss no lamb -1.742074 time 2019-02-21 19:00:14.320986
Model ind 685 epoch 1 head A batch: 254 avg loss -1.595773 avg loss no lamb -1.595773 time 2019-02-21 19:00:14.797995
Model ind 685 epoch 1 head A batch: 255 avg loss -1.614389 avg loss no lamb -1.614389 time 2019-02-21 19:00:15.322647
Model ind 685 epoch 1 head A batch: 256 avg loss -1.633908 avg loss no lamb -1.633908 time 2019-02-21 19:00:15.865457
Model ind 685 epoch 1 head A batch: 257 avg loss -1.576884 avg loss no lamb -1.576884 time 2019-02-21 19:00:16.425581
Model ind 685 epoch 1 head A batch: 258 avg loss -1.634407 avg loss no lamb -1.634407 time 2019-02-21 19:00:16.947565
Model ind 685 epoch 1 head A batch: 259 avg loss -1.607226 avg loss no lamb -1.607226 time 2019-02-21 19:00:17.446070
Model ind 685 epoch 1 head A batch: 260 avg loss -1.639699 avg loss no lamb -1.639699 time 2019-02-21 19:00:17.996342
Model ind 685 epoch 1 head A batch: 261 avg loss -1.734496 avg loss no lamb -1.734496 time 2019-02-21 19:00:18.535916
Model ind 685 epoch 1 head A batch: 262 avg loss -1.680278 avg loss no lamb -1.680278 time 2019-02-21 19:00:19.083770
Model ind 685 epoch 1 head A batch: 263 avg loss -1.592751 avg loss no lamb -1.592751 time 2019-02-21 19:00:19.602866
Model ind 685 epoch 1 head A batch: 264 avg loss -1.734107 avg loss no lamb -1.734107 time 2019-02-21 19:00:20.108513
Model ind 685 epoch 1 head A batch: 265 avg loss -1.668938 avg loss no lamb -1.668938 time 2019-02-21 19:00:20.623582
Model ind 685 epoch 1 head A batch: 266 avg loss -1.653108 avg loss no lamb -1.653108 time 2019-02-21 19:00:21.150346
Model ind 685 epoch 1 head A batch: 267 avg loss -1.592370 avg loss no lamb -1.592370 time 2019-02-21 19:00:21.656458
Model ind 685 epoch 1 head A batch: 268 avg loss -1.603972 avg loss no lamb -1.603972 time 2019-02-21 19:00:22.130488
Model ind 685 epoch 1 head A batch: 269 avg loss -1.574508 avg loss no lamb -1.574508 time 2019-02-21 19:00:22.658434
Model ind 685 epoch 1 head A batch: 270 avg loss -1.673271 avg loss no lamb -1.673271 time 2019-02-21 19:00:23.207280
Model ind 685 epoch 1 head A batch: 271 avg loss -1.680701 avg loss no lamb -1.680701 time 2019-02-21 19:00:23.779508
Model ind 685 epoch 1 head A batch: 272 avg loss -1.646104 avg loss no lamb -1.646104 time 2019-02-21 19:00:24.361616
Model ind 685 epoch 1 head A batch: 273 avg loss -1.526877 avg loss no lamb -1.526877 time 2019-02-21 19:00:24.886410
Model ind 685 epoch 1 head A batch: 274 avg loss -1.593097 avg loss no lamb -1.593097 time 2019-02-21 19:00:25.386398
Model ind 685 epoch 1 head A batch: 275 avg loss -1.646307 avg loss no lamb -1.646307 time 2019-02-21 19:00:25.869721
Model ind 685 epoch 1 head A batch: 276 avg loss -1.621281 avg loss no lamb -1.621281 time 2019-02-21 19:00:26.448293
Model ind 685 epoch 1 head A batch: 277 avg loss -1.721156 avg loss no lamb -1.721156 time 2019-02-21 19:00:27.043656
Model ind 685 epoch 1 head A batch: 278 avg loss -1.731179 avg loss no lamb -1.731179 time 2019-02-21 19:00:27.620057
Model ind 685 epoch 1 head A batch: 279 avg loss -1.688626 avg loss no lamb -1.688626 time 2019-02-21 19:00:28.139705
Model ind 685 epoch 1 head A batch: 280 avg loss -1.537423 avg loss no lamb -1.537423 time 2019-02-21 19:00:28.663325
Model ind 685 epoch 1 head A batch: 281 avg loss -1.502932 avg loss no lamb -1.502932 time 2019-02-21 19:00:29.183263
Model ind 685 epoch 1 head A batch: 282 avg loss -1.730342 avg loss no lamb -1.730342 time 2019-02-21 19:00:29.664275
Model ind 685 epoch 1 head A batch: 283 avg loss -1.656694 avg loss no lamb -1.656694 time 2019-02-21 19:00:30.139346
Model ind 685 epoch 1 head A batch: 284 avg loss -1.565998 avg loss no lamb -1.565998 time 2019-02-21 19:00:30.668787
Model ind 685 epoch 1 head A batch: 285 avg loss -1.634930 avg loss no lamb -1.634930 time 2019-02-21 19:00:31.179582
Model ind 685 epoch 1 head A batch: 286 avg loss -1.767585 avg loss no lamb -1.767585 time 2019-02-21 19:00:31.694452
Model ind 685 epoch 1 head A batch: 287 avg loss -1.657710 avg loss no lamb -1.657710 time 2019-02-21 19:00:32.228835
Model ind 685 epoch 1 head A batch: 288 avg loss -1.758443 avg loss no lamb -1.758443 time 2019-02-21 19:00:32.746635
Model ind 685 epoch 1 head A batch: 289 avg loss -1.659375 avg loss no lamb -1.659375 time 2019-02-21 19:00:33.344237
Model ind 685 epoch 1 head A batch: 290 avg loss -1.727389 avg loss no lamb -1.727389 time 2019-02-21 19:00:33.879197
Model ind 685 epoch 1 head A batch: 291 avg loss -1.662952 avg loss no lamb -1.662952 time 2019-02-21 19:00:34.399674
Model ind 685 epoch 1 head A batch: 292 avg loss -1.694914 avg loss no lamb -1.694914 time 2019-02-21 19:00:34.912513
Model ind 685 epoch 1 head A batch: 293 avg loss -1.681364 avg loss no lamb -1.681364 time 2019-02-21 19:00:35.442228
Model ind 685 epoch 1 head A batch: 294 avg loss -1.571346 avg loss no lamb -1.571346 time 2019-02-21 19:00:35.958784
Model ind 685 epoch 1 head A batch: 295 avg loss -1.515424 avg loss no lamb -1.515424 time 2019-02-21 19:00:36.467860
Model ind 685 epoch 1 head A batch: 296 avg loss -1.573028 avg loss no lamb -1.573028 time 2019-02-21 19:00:36.978780
Model ind 685 epoch 1 head A batch: 297 avg loss -1.663470 avg loss no lamb -1.663470 time 2019-02-21 19:00:37.468322
Model ind 685 epoch 1 head A batch: 298 avg loss -1.658837 avg loss no lamb -1.658837 time 2019-02-21 19:00:37.968658
Model ind 685 epoch 1 head A batch: 299 avg loss -1.678215 avg loss no lamb -1.678215 time 2019-02-21 19:00:38.517871
Model ind 685 epoch 1 head A batch: 300 avg loss -1.683772 avg loss no lamb -1.683772 time 2019-02-21 19:00:39.128706
Model ind 685 epoch 1 head A batch: 301 avg loss -1.618162 avg loss no lamb -1.618162 time 2019-02-21 19:00:39.683406
Model ind 685 epoch 1 head A batch: 302 avg loss -1.622177 avg loss no lamb -1.622177 time 2019-02-21 19:00:40.265076
Model ind 685 epoch 1 head A batch: 303 avg loss -1.595972 avg loss no lamb -1.595972 time 2019-02-21 19:00:40.834272
Model ind 685 epoch 1 head A batch: 304 avg loss -1.652836 avg loss no lamb -1.652836 time 2019-02-21 19:00:41.383078
Model ind 685 epoch 1 head A batch: 305 avg loss -1.615312 avg loss no lamb -1.615312 time 2019-02-21 19:00:41.864426
Model ind 685 epoch 1 head A batch: 306 avg loss -1.614241 avg loss no lamb -1.614241 time 2019-02-21 19:00:42.395140
Model ind 685 epoch 1 head A batch: 307 avg loss -1.621827 avg loss no lamb -1.621827 time 2019-02-21 19:00:42.891970
Model ind 685 epoch 1 head A batch: 308 avg loss -1.703018 avg loss no lamb -1.703018 time 2019-02-21 19:00:43.393717
Model ind 685 epoch 1 head A batch: 309 avg loss -1.745282 avg loss no lamb -1.745282 time 2019-02-21 19:00:43.915080
Model ind 685 epoch 1 head A batch: 310 avg loss -1.695462 avg loss no lamb -1.695462 time 2019-02-21 19:00:44.422885
Model ind 685 epoch 1 head A batch: 311 avg loss -1.584608 avg loss no lamb -1.584608 time 2019-02-21 19:00:44.951625
Model ind 685 epoch 1 head A batch: 312 avg loss -1.735646 avg loss no lamb -1.735646 time 2019-02-21 19:00:45.483894
Model ind 685 epoch 1 head A batch: 313 avg loss -1.667969 avg loss no lamb -1.667969 time 2019-02-21 19:00:45.996859
Model ind 685 epoch 1 head A batch: 314 avg loss -1.695622 avg loss no lamb -1.695622 time 2019-02-21 19:00:46.518331
Model ind 685 epoch 1 head A batch: 315 avg loss -1.605595 avg loss no lamb -1.605595 time 2019-02-21 19:00:47.055781
Model ind 685 epoch 1 head A batch: 316 avg loss -1.595309 avg loss no lamb -1.595309 time 2019-02-21 19:00:47.616609
Model ind 685 epoch 1 head A batch: 317 avg loss -1.673595 avg loss no lamb -1.673595 time 2019-02-21 19:00:48.146105
Model ind 685 epoch 1 head A batch: 318 avg loss -1.782874 avg loss no lamb -1.782874 time 2019-02-21 19:00:48.650404
Model ind 685 epoch 1 head A batch: 319 avg loss -1.649920 avg loss no lamb -1.649920 time 2019-02-21 19:00:49.135173
Model ind 685 epoch 1 head A batch: 320 avg loss -1.602051 avg loss no lamb -1.602051 time 2019-02-21 19:00:49.667540
Model ind 685 epoch 1 head A batch: 321 avg loss -1.582344 avg loss no lamb -1.582344 time 2019-02-21 19:00:50.152717
Model ind 685 epoch 1 head A batch: 322 avg loss -1.742359 avg loss no lamb -1.742359 time 2019-02-21 19:00:50.701787
Model ind 685 epoch 1 head A batch: 323 avg loss -1.745970 avg loss no lamb -1.745970 time 2019-02-21 19:00:51.221433
Model ind 685 epoch 1 head A batch: 324 avg loss -1.668258 avg loss no lamb -1.668258 time 2019-02-21 19:00:51.749144
Model ind 685 epoch 1 head A batch: 325 avg loss -1.678967 avg loss no lamb -1.678967 time 2019-02-21 19:00:52.242843
Model ind 685 epoch 1 head A batch: 326 avg loss -1.702607 avg loss no lamb -1.702607 time 2019-02-21 19:00:52.720944
Model ind 685 epoch 1 head A batch: 327 avg loss -1.579724 avg loss no lamb -1.579724 time 2019-02-21 19:00:53.215798
Model ind 685 epoch 1 head A batch: 328 avg loss -1.585774 avg loss no lamb -1.585774 time 2019-02-21 19:00:53.728489
Model ind 685 epoch 1 head A batch: 329 avg loss -1.678776 avg loss no lamb -1.678776 time 2019-02-21 19:00:54.269344
Model ind 685 epoch 1 head A batch: 330 avg loss -1.587490 avg loss no lamb -1.587490 time 2019-02-21 19:00:54.828873
Model ind 685 epoch 1 head A batch: 331 avg loss -1.693907 avg loss no lamb -1.693907 time 2019-02-21 19:00:55.382850
Model ind 685 epoch 1 head A batch: 332 avg loss -1.660573 avg loss no lamb -1.660573 time 2019-02-21 19:00:55.917251
Model ind 685 epoch 1 head A batch: 333 avg loss -1.643959 avg loss no lamb -1.643959 time 2019-02-21 19:00:56.437614
Model ind 685 epoch 1 head A batch: 334 avg loss -1.658723 avg loss no lamb -1.658723 time 2019-02-21 19:00:56.939498
Model ind 685 epoch 1 head A batch: 335 avg loss -1.697609 avg loss no lamb -1.697609 time 2019-02-21 19:00:57.444715
Model ind 685 epoch 1 head A batch: 336 avg loss -1.644389 avg loss no lamb -1.644389 time 2019-02-21 19:00:57.936458
Model ind 685 epoch 1 head A batch: 337 avg loss -1.622770 avg loss no lamb -1.622770 time 2019-02-21 19:00:58.447594
Model ind 685 epoch 1 head A batch: 338 avg loss -1.650466 avg loss no lamb -1.650466 time 2019-02-21 19:00:58.961930
Model ind 685 epoch 1 head A batch: 339 avg loss -1.586686 avg loss no lamb -1.586686 time 2019-02-21 19:00:59.499044
Model ind 685 epoch 1 head A batch: 340 avg loss -1.677451 avg loss no lamb -1.677451 time 2019-02-21 19:01:00.010836
Model ind 685 epoch 1 head A batch: 341 avg loss -1.636921 avg loss no lamb -1.636921 time 2019-02-21 19:01:00.607959
Model ind 685 epoch 1 head A batch: 342 avg loss -1.598215 avg loss no lamb -1.598215 time 2019-02-21 19:01:01.106176
Model ind 685 epoch 1 head A batch: 343 avg loss -1.689405 avg loss no lamb -1.689405 time 2019-02-21 19:01:01.616860
Model ind 685 epoch 1 head A batch: 344 avg loss -1.651029 avg loss no lamb -1.651029 time 2019-02-21 19:01:02.169808
Model ind 685 epoch 1 head A batch: 345 avg loss -1.655936 avg loss no lamb -1.655936 time 2019-02-21 19:01:02.708363
Model ind 685 epoch 1 head A batch: 346 avg loss -1.663621 avg loss no lamb -1.663621 time 2019-02-21 19:01:03.241022
Model ind 685 epoch 1 head A batch: 347 avg loss -1.644802 avg loss no lamb -1.644802 time 2019-02-21 19:01:03.716510
Model ind 685 epoch 1 head A batch: 348 avg loss -1.622935 avg loss no lamb -1.622935 time 2019-02-21 19:01:04.220132
Model ind 685 epoch 1 head A batch: 349 avg loss -1.507294 avg loss no lamb -1.507294 time 2019-02-21 19:01:04.733222
Model ind 685 epoch 1 head A batch: 350 avg loss -1.580789 avg loss no lamb -1.580789 time 2019-02-21 19:01:05.262865
Model ind 685 epoch 1 head A batch: 351 avg loss -1.705884 avg loss no lamb -1.705884 time 2019-02-21 19:01:05.826859
Model ind 685 epoch 1 head A batch: 352 avg loss -1.663138 avg loss no lamb -1.663138 time 2019-02-21 19:01:06.309344
Model ind 685 epoch 1 head A batch: 353 avg loss -1.568009 avg loss no lamb -1.568009 time 2019-02-21 19:01:06.834634
Model ind 685 epoch 1 head A batch: 354 avg loss -1.494073 avg loss no lamb -1.494073 time 2019-02-21 19:01:07.340605
Model ind 685 epoch 1 head A batch: 355 avg loss -1.725828 avg loss no lamb -1.725828 time 2019-02-21 19:01:07.854582
Model ind 685 epoch 1 head A batch: 356 avg loss -1.579518 avg loss no lamb -1.579518 time 2019-02-21 19:01:08.453129
Model ind 685 epoch 1 head A batch: 357 avg loss -1.636132 avg loss no lamb -1.636132 time 2019-02-21 19:01:08.989679
Model ind 685 epoch 1 head A batch: 358 avg loss -1.715129 avg loss no lamb -1.715129 time 2019-02-21 19:01:09.501572
Model ind 685 epoch 1 head A batch: 359 avg loss -1.610955 avg loss no lamb -1.610955 time 2019-02-21 19:01:10.035691
Model ind 685 epoch 1 head A batch: 360 avg loss -1.545111 avg loss no lamb -1.545111 time 2019-02-21 19:01:10.560656
Model ind 685 epoch 1 head A batch: 361 avg loss -1.671963 avg loss no lamb -1.671963 time 2019-02-21 19:01:11.055709
Model ind 685 epoch 1 head A batch: 362 avg loss -1.500786 avg loss no lamb -1.500786 time 2019-02-21 19:01:11.673782
Model ind 685 epoch 1 head A batch: 363 avg loss -1.635706 avg loss no lamb -1.635706 time 2019-02-21 19:01:12.220784
Model ind 685 epoch 1 head A batch: 364 avg loss -1.711718 avg loss no lamb -1.711718 time 2019-02-21 19:01:12.749678
Model ind 685 epoch 1 head A batch: 365 avg loss -1.630563 avg loss no lamb -1.630563 time 2019-02-21 19:01:13.304341
Model ind 685 epoch 1 head A batch: 366 avg loss -1.706642 avg loss no lamb -1.706642 time 2019-02-21 19:01:13.808761
Model ind 685 epoch 1 head A batch: 367 avg loss -1.670864 avg loss no lamb -1.670864 time 2019-02-21 19:01:14.331801
Model ind 685 epoch 1 head A batch: 368 avg loss -1.683437 avg loss no lamb -1.683437 time 2019-02-21 19:01:14.837040
Model ind 685 epoch 1 head A batch: 369 avg loss -1.706081 avg loss no lamb -1.706081 time 2019-02-21 19:01:15.397176
Model ind 685 epoch 1 head A batch: 370 avg loss -1.811896 avg loss no lamb -1.811896 time 2019-02-21 19:01:15.938630
Model ind 685 epoch 1 head A batch: 371 avg loss -1.692627 avg loss no lamb -1.692627 time 2019-02-21 19:01:16.490813
Model ind 685 epoch 1 head A batch: 372 avg loss -1.503513 avg loss no lamb -1.503513 time 2019-02-21 19:01:17.033702
Model ind 685 epoch 1 head A batch: 373 avg loss -1.747322 avg loss no lamb -1.747322 time 2019-02-21 19:01:17.566812
Model ind 685 epoch 1 head A batch: 374 avg loss -1.754946 avg loss no lamb -1.754946 time 2019-02-21 19:01:18.130931
Model ind 685 epoch 1 head A batch: 375 avg loss -1.671964 avg loss no lamb -1.671964 time 2019-02-21 19:01:18.639234
Model ind 685 epoch 1 head A batch: 376 avg loss -1.612916 avg loss no lamb -1.612916 time 2019-02-21 19:01:19.161828
Model ind 685 epoch 1 head A batch: 377 avg loss -1.592860 avg loss no lamb -1.592860 time 2019-02-21 19:01:19.659201
Model ind 685 epoch 1 head A batch: 378 avg loss -1.545980 avg loss no lamb -1.545980 time 2019-02-21 19:01:20.159999
Model ind 685 epoch 1 head A batch: 379 avg loss -1.692989 avg loss no lamb -1.692989 time 2019-02-21 19:01:20.776891
Model ind 685 epoch 1 head A batch: 380 avg loss -1.689381 avg loss no lamb -1.689381 time 2019-02-21 19:01:21.331280
Model ind 685 epoch 1 head A batch: 381 avg loss -1.708891 avg loss no lamb -1.708891 time 2019-02-21 19:01:21.823938
Model ind 685 epoch 1 head A batch: 382 avg loss -1.625712 avg loss no lamb -1.625712 time 2019-02-21 19:01:22.326568
Model ind 685 epoch 1 head A batch: 383 avg loss -1.618175 avg loss no lamb -1.618175 time 2019-02-21 19:01:22.826151
Model ind 685 epoch 1 head A batch: 384 avg loss -1.702622 avg loss no lamb -1.702622 time 2019-02-21 19:01:23.347284
Model ind 685 epoch 1 head A batch: 385 avg loss -1.499224 avg loss no lamb -1.499224 time 2019-02-21 19:01:23.901716
Model ind 685 epoch 1 head A batch: 386 avg loss -1.753032 avg loss no lamb -1.753032 time 2019-02-21 19:01:24.424431
Model ind 685 epoch 1 head A batch: 387 avg loss -1.673979 avg loss no lamb -1.673979 time 2019-02-21 19:01:24.933604
Model ind 685 epoch 1 head A batch: 388 avg loss -1.712849 avg loss no lamb -1.712849 time 2019-02-21 19:01:25.417985
Model ind 685 epoch 1 head A batch: 389 avg loss -1.680646 avg loss no lamb -1.680646 time 2019-02-21 19:01:25.949167
Model ind 685 epoch 1 head A batch: 390 avg loss -1.749032 avg loss no lamb -1.749032 time 2019-02-21 19:01:26.468259
Model ind 685 epoch 1 head A batch: 391 avg loss -1.630240 avg loss no lamb -1.630240 time 2019-02-21 19:01:26.990441
Model ind 685 epoch 1 head A batch: 392 avg loss -1.607233 avg loss no lamb -1.607233 time 2019-02-21 19:01:27.489496
Model ind 685 epoch 1 head A batch: 393 avg loss -1.746458 avg loss no lamb -1.746458 time 2019-02-21 19:01:28.061817
Model ind 685 epoch 1 head A batch: 394 avg loss -1.723915 avg loss no lamb -1.723915 time 2019-02-21 19:01:28.604591
Model ind 685 epoch 1 head A batch: 395 avg loss -1.795443 avg loss no lamb -1.795443 time 2019-02-21 19:01:29.182482
Model ind 685 epoch 1 head A batch: 396 avg loss -1.638108 avg loss no lamb -1.638108 time 2019-02-21 19:01:29.662925
Model ind 685 epoch 1 head A batch: 397 avg loss -1.719001 avg loss no lamb -1.719001 time 2019-02-21 19:01:30.220184
Model ind 685 epoch 1 head A batch: 398 avg loss -1.717167 avg loss no lamb -1.717167 time 2019-02-21 19:01:30.724272
Model ind 685 epoch 1 head A batch: 399 avg loss -1.607223 avg loss no lamb -1.607223 time 2019-02-21 19:01:31.258757
Model ind 685 epoch 1 head A batch: 400 avg loss -1.760920 avg loss no lamb -1.760920 time 2019-02-21 19:01:31.807697
Model ind 685 epoch 1 head A batch: 401 avg loss -1.655280 avg loss no lamb -1.655280 time 2019-02-21 19:01:32.332990
Model ind 685 epoch 1 head A batch: 402 avg loss -1.677972 avg loss no lamb -1.677972 time 2019-02-21 19:01:32.835943
Model ind 685 epoch 1 head A batch: 403 avg loss -1.665807 avg loss no lamb -1.665807 time 2019-02-21 19:01:33.380557
Model ind 685 epoch 1 head A batch: 404 avg loss -1.701100 avg loss no lamb -1.701100 time 2019-02-21 19:01:33.925684
Model ind 685 epoch 1 head A batch: 405 avg loss -1.588187 avg loss no lamb -1.588187 time 2019-02-21 19:01:34.416671
Model ind 685 epoch 1 head A batch: 406 avg loss -1.757935 avg loss no lamb -1.757935 time 2019-02-21 19:01:34.925866
Model ind 685 epoch 1 head A batch: 407 avg loss -1.690164 avg loss no lamb -1.690164 time 2019-02-21 19:01:35.458904
Model ind 685 epoch 1 head A batch: 408 avg loss -1.665219 avg loss no lamb -1.665219 time 2019-02-21 19:01:35.989080
Model ind 685 epoch 1 head A batch: 409 avg loss -1.703009 avg loss no lamb -1.703009 time 2019-02-21 19:01:36.570990
Model ind 685 epoch 1 head A batch: 410 avg loss -1.762436 avg loss no lamb -1.762436 time 2019-02-21 19:01:37.115410
Model ind 685 epoch 1 head A batch: 411 avg loss -1.689098 avg loss no lamb -1.689098 time 2019-02-21 19:01:37.657348
Model ind 685 epoch 1 head A batch: 412 avg loss -1.551920 avg loss no lamb -1.551920 time 2019-02-21 19:01:38.135199
Model ind 685 epoch 1 head A batch: 413 avg loss -1.628407 avg loss no lamb -1.628407 time 2019-02-21 19:01:38.640054
Model ind 685 epoch 1 head A batch: 414 avg loss -1.748872 avg loss no lamb -1.748872 time 2019-02-21 19:01:39.162170
Model ind 685 epoch 1 head A batch: 415 avg loss -1.804299 avg loss no lamb -1.804299 time 2019-02-21 19:01:39.667693
Model ind 685 epoch 1 head A batch: 416 avg loss -1.852735 avg loss no lamb -1.852735 time 2019-02-21 19:01:40.179071
Model ind 685 epoch 1 head A batch: 417 avg loss -1.670020 avg loss no lamb -1.670020 time 2019-02-21 19:01:40.707597
Model ind 685 epoch 1 head A batch: 418 avg loss -1.803210 avg loss no lamb -1.803210 time 2019-02-21 19:01:41.224146
Model ind 685 epoch 1 head A batch: 419 avg loss -1.894411 avg loss no lamb -1.894411 time 2019-02-21 19:01:41.691302
Model ind 685 epoch 1 head A batch: 420 avg loss -1.763060 avg loss no lamb -1.763060 time 2019-02-21 19:01:42.185272
Model ind 685 epoch 1 head A batch: 421 avg loss -1.858891 avg loss no lamb -1.858891 time 2019-02-21 19:01:42.663790
Model ind 685 epoch 1 head A batch: 422 avg loss -1.836033 avg loss no lamb -1.836033 time 2019-02-21 19:01:43.156924
Model ind 685 epoch 1 head A batch: 423 avg loss -1.469807 avg loss no lamb -1.469807 time 2019-02-21 19:01:43.654901
Model ind 685 epoch 1 head A batch: 424 avg loss -1.638718 avg loss no lamb -1.638718 time 2019-02-21 19:01:44.192610
Model ind 685 epoch 1 head A batch: 425 avg loss -1.830407 avg loss no lamb -1.830407 time 2019-02-21 19:01:44.737820
Model ind 685 epoch 1 head A batch: 426 avg loss -1.620085 avg loss no lamb -1.620085 time 2019-02-21 19:01:45.315505
Model ind 685 epoch 1 head A batch: 427 avg loss -1.841650 avg loss no lamb -1.841650 time 2019-02-21 19:01:45.833261
Model ind 685 epoch 1 head A batch: 428 avg loss -1.762123 avg loss no lamb -1.762123 time 2019-02-21 19:01:46.345654
Model ind 685 epoch 1 head A batch: 429 avg loss -1.718526 avg loss no lamb -1.718526 time 2019-02-21 19:01:46.882490
Model ind 685 epoch 1 head A batch: 430 avg loss -1.634311 avg loss no lamb -1.634311 time 2019-02-21 19:01:47.395936
Model ind 685 epoch 1 head A batch: 431 avg loss -1.627369 avg loss no lamb -1.627369 time 2019-02-21 19:01:47.909135
Model ind 685 epoch 1 head A batch: 432 avg loss -1.539688 avg loss no lamb -1.539688 time 2019-02-21 19:01:48.409702
Model ind 685 epoch 1 head A batch: 433 avg loss -1.641532 avg loss no lamb -1.641532 time 2019-02-21 19:01:48.935998
Model ind 685 epoch 1 head A batch: 434 avg loss -1.626105 avg loss no lamb -1.626105 time 2019-02-21 19:01:49.460008
Model ind 685 epoch 1 head A batch: 435 avg loss -1.714171 avg loss no lamb -1.714171 time 2019-02-21 19:01:49.953472
Model ind 685 epoch 1 head A batch: 436 avg loss -1.603724 avg loss no lamb -1.603724 time 2019-02-21 19:01:50.514608
Model ind 685 epoch 1 head A batch: 437 avg loss -1.675863 avg loss no lamb -1.675863 time 2019-02-21 19:01:51.027566
Model ind 685 epoch 1 head A batch: 438 avg loss -1.618364 avg loss no lamb -1.618364 time 2019-02-21 19:01:51.557725
Model ind 685 epoch 1 head A batch: 439 avg loss -1.651307 avg loss no lamb -1.651307 time 2019-02-21 19:01:52.058329
Model ind 685 epoch 1 head A batch: 440 avg loss -1.623251 avg loss no lamb -1.623251 time 2019-02-21 19:01:52.597534
Model ind 685 epoch 1 head A batch: 441 avg loss -1.624421 avg loss no lamb -1.624421 time 2019-02-21 19:01:53.144013
Model ind 685 epoch 1 head A batch: 442 avg loss -1.663533 avg loss no lamb -1.663533 time 2019-02-21 19:01:53.629237
Model ind 685 epoch 1 head A batch: 443 avg loss -1.564590 avg loss no lamb -1.564590 time 2019-02-21 19:01:54.144989
Model ind 685 epoch 1 head A batch: 444 avg loss -1.587739 avg loss no lamb -1.587739 time 2019-02-21 19:01:54.696117
Model ind 685 epoch 1 head A batch: 445 avg loss -1.634852 avg loss no lamb -1.634852 time 2019-02-21 19:01:55.195584
Model ind 685 epoch 1 head A batch: 446 avg loss -1.567743 avg loss no lamb -1.567743 time 2019-02-21 19:01:55.711984
Model ind 685 epoch 1 head A batch: 447 avg loss -1.670061 avg loss no lamb -1.670061 time 2019-02-21 19:01:56.246323
Model ind 685 epoch 1 head A batch: 448 avg loss -1.689488 avg loss no lamb -1.689488 time 2019-02-21 19:01:56.834404
Model ind 685 epoch 1 head A batch: 449 avg loss -1.598616 avg loss no lamb -1.598616 time 2019-02-21 19:01:57.363506
Model ind 685 epoch 1 head A batch: 450 avg loss -1.630527 avg loss no lamb -1.630527 time 2019-02-21 19:01:57.888775
Model ind 685 epoch 1 head A batch: 451 avg loss -1.695902 avg loss no lamb -1.695902 time 2019-02-21 19:01:58.428825
Model ind 685 epoch 1 head A batch: 452 avg loss -1.600675 avg loss no lamb -1.600675 time 2019-02-21 19:01:58.957290
Model ind 685 epoch 1 head A batch: 453 avg loss -1.639570 avg loss no lamb -1.639570 time 2019-02-21 19:01:59.454890
Model ind 685 epoch 1 head A batch: 454 avg loss -1.680797 avg loss no lamb -1.680797 time 2019-02-21 19:01:59.962572
Model ind 685 epoch 1 head A batch: 455 avg loss -1.503563 avg loss no lamb -1.503563 time 2019-02-21 19:02:00.458681
Model ind 685 epoch 1 head A batch: 456 avg loss -1.593974 avg loss no lamb -1.593974 time 2019-02-21 19:02:00.942349
Model ind 685 epoch 1 head A batch: 457 avg loss -1.624900 avg loss no lamb -1.624900 time 2019-02-21 19:02:01.519306
Model ind 685 epoch 1 head A batch: 458 avg loss -1.666085 avg loss no lamb -1.666085 time 2019-02-21 19:02:02.111182
Model ind 685 epoch 1 head A batch: 459 avg loss -1.644424 avg loss no lamb -1.644424 time 2019-02-21 19:02:02.706425
Model ind 685 epoch 1 head A batch: 460 avg loss -1.602549 avg loss no lamb -1.602549 time 2019-02-21 19:02:03.296173
Model ind 685 epoch 1 head A batch: 461 avg loss -1.645839 avg loss no lamb -1.645839 time 2019-02-21 19:02:03.896637
Model ind 685 epoch 1 head A batch: 462 avg loss -1.673630 avg loss no lamb -1.673630 time 2019-02-21 19:02:04.491960
Model ind 685 epoch 1 head A batch: 463 avg loss -1.672029 avg loss no lamb -1.672029 time 2019-02-21 19:02:05.084735
Model ind 685 epoch 1 head A batch: 464 avg loss -1.698979 avg loss no lamb -1.698979 time 2019-02-21 19:02:05.690119
Model ind 685 epoch 1 head A batch: 465 avg loss -1.725423 avg loss no lamb -1.725423 time 2019-02-21 19:02:06.297381
Model ind 685 epoch 1 head A batch: 466 avg loss -1.770542 avg loss no lamb -1.770542 time 2019-02-21 19:02:06.896706
Model ind 685 epoch 1 head A batch: 467 avg loss -1.820620 avg loss no lamb -1.820620 time 2019-02-21 19:02:07.510026
Model ind 685 epoch 1 head A batch: 468 avg loss -1.748884 avg loss no lamb -1.748884 time 2019-02-21 19:02:08.131481
Model ind 685 epoch 1 head A batch: 469 avg loss -1.780759 avg loss no lamb -1.780759 time 2019-02-21 19:02:08.691468
Model ind 685 epoch 1 head A batch: 470 avg loss -1.710727 avg loss no lamb -1.710727 time 2019-02-21 19:02:09.304722
Model ind 685 epoch 1 head A batch: 471 avg loss -1.736139 avg loss no lamb -1.736139 time 2019-02-21 19:02:09.859504
Model ind 685 epoch 1 head A batch: 472 avg loss -1.738506 avg loss no lamb -1.738506 time 2019-02-21 19:02:10.385679
Model ind 685 epoch 1 head A batch: 473 avg loss -1.894672 avg loss no lamb -1.894672 time 2019-02-21 19:02:10.885996
Model ind 685 epoch 1 head A batch: 474 avg loss -1.870780 avg loss no lamb -1.870780 time 2019-02-21 19:02:11.409858
Model ind 685 epoch 1 head A batch: 475 avg loss -1.546268 avg loss no lamb -1.546268 time 2019-02-21 19:02:11.925022
Model ind 685 epoch 1 head A batch: 476 avg loss -1.691592 avg loss no lamb -1.691592 time 2019-02-21 19:02:12.457440
Model ind 685 epoch 1 head A batch: 477 avg loss -1.750558 avg loss no lamb -1.750558 time 2019-02-21 19:02:12.940694
Model ind 685 epoch 1 head A batch: 478 avg loss -1.841103 avg loss no lamb -1.841103 time 2019-02-21 19:02:13.467857
Model ind 685 epoch 1 head A batch: 479 avg loss -1.887424 avg loss no lamb -1.887424 time 2019-02-21 19:02:14.007839
Model ind 685 epoch 1 head A batch: 480 avg loss -1.828087 avg loss no lamb -1.828087 time 2019-02-21 19:02:14.540857
Model ind 685 epoch 1 head A batch: 481 avg loss -1.709911 avg loss no lamb -1.709911 time 2019-02-21 19:02:15.051551
Model ind 685 epoch 1 head A batch: 482 avg loss -1.791567 avg loss no lamb -1.791567 time 2019-02-21 19:02:15.591553
Model ind 685 epoch 1 head A batch: 483 avg loss -1.813998 avg loss no lamb -1.813998 time 2019-02-21 19:02:16.223324
Model ind 685 epoch 1 head A batch: 484 avg loss -1.821573 avg loss no lamb -1.821573 time 2019-02-21 19:02:16.878341
Model ind 685 epoch 1 head A batch: 485 avg loss -1.766412 avg loss no lamb -1.766412 time 2019-02-21 19:02:17.490481
Model ind 685 epoch 1 head A batch: 486 avg loss -1.741530 avg loss no lamb -1.741530 time 2019-02-21 19:02:18.035966
Model ind 685 epoch 1 head A batch: 487 avg loss -1.722957 avg loss no lamb -1.722957 time 2019-02-21 19:02:18.642532
Model ind 685 epoch 1 head A batch: 488 avg loss -1.767641 avg loss no lamb -1.767641 time 2019-02-21 19:02:19.214282
Model ind 685 epoch 1 head A batch: 489 avg loss -1.827265 avg loss no lamb -1.827265 time 2019-02-21 19:02:19.753107
Model ind 685 epoch 1 head A batch: 490 avg loss -1.915551 avg loss no lamb -1.915551 time 2019-02-21 19:02:20.301672
Model ind 685 epoch 1 head A batch: 491 avg loss -1.868649 avg loss no lamb -1.868649 time 2019-02-21 19:02:20.859935
Model ind 685 epoch 1 head A batch: 492 avg loss -1.788216 avg loss no lamb -1.788216 time 2019-02-21 19:02:21.410686
Model ind 685 epoch 1 head A batch: 493 avg loss -1.755991 avg loss no lamb -1.755991 time 2019-02-21 19:02:21.948931
Model ind 685 epoch 1 head A batch: 494 avg loss -1.736955 avg loss no lamb -1.736955 time 2019-02-21 19:02:22.509039
Model ind 685 epoch 1 head A batch: 495 avg loss -1.797065 avg loss no lamb -1.797065 time 2019-02-21 19:02:23.037062
Model ind 685 epoch 1 head A batch: 496 avg loss -1.797936 avg loss no lamb -1.797936 time 2019-02-21 19:02:23.543448
Model ind 685 epoch 1 head A batch: 497 avg loss -1.711390 avg loss no lamb -1.711390 time 2019-02-21 19:02:24.071191
Model ind 685 epoch 1 head A batch: 498 avg loss -1.609727 avg loss no lamb -1.609727 time 2019-02-21 19:02:24.578797
Model ind 685 epoch 1 head A batch: 499 avg loss -1.701139 avg loss no lamb -1.701139 time 2019-02-21 19:02:25.085440
Pre: time 2019-02-21 19:02:39.488210: 
 	std: 0.031156577
	best_train_sub_head_match: [(0, 4), (1, 8), (2, 0), (3, 7), (4, 2), (5, 5), (6, 3), (7, 9), (8, 1), (9, 6)]
	test_accs: [0.8442, 0.85611427, 0.77262855, 0.82614285, 0.85585713]
	train_accs: [0.8442, 0.85611427, 0.77262855, 0.82614285, 0.85585713]
	best_train_sub_head: 1
	worst: 0.77262855
	avg: 0.8309885
	best: 0.85611427

Starting e_i: 2
Model ind 685 epoch 2 head B batch: 0 avg loss -1.666652 avg loss no lamb -1.666652 time 2019-02-21 19:02:42.591448
Model ind 685 epoch 2 head B batch: 100 avg loss -1.726856 avg loss no lamb -1.726856 time 2019-02-21 19:03:37.711833
Model ind 685 epoch 2 head B batch: 200 avg loss -1.821963 avg loss no lamb -1.821963 time 2019-02-21 19:04:31.107658
Model ind 685 epoch 2 head B batch: 300 avg loss -1.824378 avg loss no lamb -1.824378 time 2019-02-21 19:05:23.698547
Model ind 685 epoch 2 head B batch: 400 avg loss -1.830038 avg loss no lamb -1.830038 time 2019-02-21 19:06:16.785945
Model ind 685 epoch 2 head B batch: 0 avg loss -1.964873 avg loss no lamb -1.964873 time 2019-02-21 19:07:10.817797
Model ind 685 epoch 2 head B batch: 100 avg loss -1.814148 avg loss no lamb -1.814148 time 2019-02-21 19:08:05.177651
Model ind 685 epoch 2 head B batch: 200 avg loss -1.876819 avg loss no lamb -1.876819 time 2019-02-21 19:08:59.183949
Model ind 685 epoch 2 head B batch: 300 avg loss -1.885916 avg loss no lamb -1.885916 time 2019-02-21 19:09:53.248646
Model ind 685 epoch 2 head B batch: 400 avg loss -1.895813 avg loss no lamb -1.895813 time 2019-02-21 19:10:47.120738
Model ind 685 epoch 2 head A batch: 0 avg loss -1.889085 avg loss no lamb -1.889085 time 2019-02-21 19:11:39.905666
Model ind 685 epoch 2 head A batch: 100 avg loss -1.860861 avg loss no lamb -1.860861 time 2019-02-21 19:12:32.792132
Model ind 685 epoch 2 head A batch: 200 avg loss -1.921417 avg loss no lamb -1.921417 time 2019-02-21 19:13:26.620317
Model ind 685 epoch 2 head A batch: 300 avg loss -1.939585 avg loss no lamb -1.939585 time 2019-02-21 19:14:19.854800
Model ind 685 epoch 2 head A batch: 400 avg loss -1.849624 avg loss no lamb -1.849624 time 2019-02-21 19:15:14.427536
Pre: time 2019-02-21 19:16:21.814894: 
 	std: 0.03349444
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.8991714, 0.8889, 0.8082143, 0.8869143, 0.8901714]
	train_accs: [0.8991714, 0.8889, 0.8082143, 0.8869143, 0.8901714]
	best_train_sub_head: 0
	worst: 0.8082143
	avg: 0.8746743
	best: 0.8991714

Starting e_i: 3
Model ind 685 epoch 3 head B batch: 0 avg loss -1.874414 avg loss no lamb -1.874414 time 2019-02-21 19:16:24.462850
Model ind 685 epoch 3 head B batch: 100 avg loss -1.892920 avg loss no lamb -1.892920 time 2019-02-21 19:17:17.490572
Model ind 685 epoch 3 head B batch: 200 avg loss -1.960484 avg loss no lamb -1.960484 time 2019-02-21 19:18:09.677231
Model ind 685 epoch 3 head B batch: 300 avg loss -1.917816 avg loss no lamb -1.917816 time 2019-02-21 19:19:01.757097
Model ind 685 epoch 3 head B batch: 400 avg loss -1.881238 avg loss no lamb -1.881238 time 2019-02-21 19:19:55.027611
Model ind 685 epoch 3 head B batch: 0 avg loss -1.943208 avg loss no lamb -1.943208 time 2019-02-21 19:20:47.384449
Model ind 685 epoch 3 head B batch: 100 avg loss -1.969676 avg loss no lamb -1.969676 time 2019-02-21 19:21:39.394656
Model ind 685 epoch 3 head B batch: 200 avg loss -1.925860 avg loss no lamb -1.925860 time 2019-02-21 19:22:32.270417
Model ind 685 epoch 3 head B batch: 300 avg loss -1.994442 avg loss no lamb -1.994442 time 2019-02-21 19:23:25.428768
Model ind 685 epoch 3 head B batch: 400 avg loss -1.992481 avg loss no lamb -1.992481 time 2019-02-21 19:24:17.906858
Model ind 685 epoch 3 head A batch: 0 avg loss -2.026794 avg loss no lamb -2.026794 time 2019-02-21 19:25:11.479709
Model ind 685 epoch 3 head A batch: 100 avg loss -2.022707 avg loss no lamb -2.022707 time 2019-02-21 19:26:05.288548
Model ind 685 epoch 3 head A batch: 200 avg loss -2.023185 avg loss no lamb -2.023185 time 2019-02-21 19:26:58.523488
Model ind 685 epoch 3 head A batch: 300 avg loss -2.009746 avg loss no lamb -2.009746 time 2019-02-21 19:27:53.603978
Model ind 685 epoch 3 head A batch: 400 avg loss -2.019760 avg loss no lamb -2.019760 time 2019-02-21 19:28:48.428944
Pre: time 2019-02-21 19:29:56.580484: 
 	std: 0.03292106
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.98742855, 0.9749857, 0.89664286, 0.97487146, 0.9749143]
	train_accs: [0.98742855, 0.9749857, 0.89664286, 0.97487146, 0.9749143]
	best_train_sub_head: 0
	worst: 0.89664286
	avg: 0.9617685
	best: 0.98742855

Starting e_i: 4
Model ind 685 epoch 4 head B batch: 0 avg loss -1.979326 avg loss no lamb -1.979326 time 2019-02-21 19:29:59.467070
Model ind 685 epoch 4 head B batch: 100 avg loss -2.061324 avg loss no lamb -2.061324 time 2019-02-21 19:30:53.496143
Model ind 685 epoch 4 head B batch: 200 avg loss -2.016182 avg loss no lamb -2.016182 time 2019-02-21 19:31:45.703442
Model ind 685 epoch 4 head B batch: 300 avg loss -2.011168 avg loss no lamb -2.011168 time 2019-02-21 19:32:40.306496
Model ind 685 epoch 4 head B batch: 400 avg loss -1.956449 avg loss no lamb -1.956449 time 2019-02-21 19:33:35.790830
Model ind 685 epoch 4 head B batch: 0 avg loss -2.062052 avg loss no lamb -2.062052 time 2019-02-21 19:34:29.013464
Model ind 685 epoch 4 head B batch: 100 avg loss -2.021793 avg loss no lamb -2.021793 time 2019-02-21 19:35:21.711797
Model ind 685 epoch 4 head B batch: 200 avg loss -1.985878 avg loss no lamb -1.985878 time 2019-02-21 19:36:18.174038
Model ind 685 epoch 4 head B batch: 300 avg loss -2.030735 avg loss no lamb -2.030735 time 2019-02-21 19:37:12.742318
Model ind 685 epoch 4 head B batch: 400 avg loss -2.010091 avg loss no lamb -2.010091 time 2019-02-21 19:38:07.575935
Model ind 685 epoch 4 head A batch: 0 avg loss -2.050649 avg loss no lamb -2.050649 time 2019-02-21 19:39:00.640283
Model ind 685 epoch 4 head A batch: 100 avg loss -2.033211 avg loss no lamb -2.033211 time 2019-02-21 19:39:56.392405
Model ind 685 epoch 4 head A batch: 200 avg loss -1.995852 avg loss no lamb -1.995852 time 2019-02-21 19:40:50.077089
Model ind 685 epoch 4 head A batch: 300 avg loss -2.061083 avg loss no lamb -2.061083 time 2019-02-21 19:41:45.447057
Model ind 685 epoch 4 head A batch: 400 avg loss -1.995179 avg loss no lamb -1.995179 time 2019-02-21 19:42:38.718214
Pre: time 2019-02-21 19:43:47.337506: 
 	std: 0.032891206
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9887428, 0.97601426, 0.89787143, 0.9759143, 0.9759857]
	train_accs: [0.9887428, 0.97601426, 0.89787143, 0.9759143, 0.9759857]
	best_train_sub_head: 0
	worst: 0.89787143
	avg: 0.9629057
	best: 0.9887428

Starting e_i: 5
Model ind 685 epoch 5 head B batch: 0 avg loss -2.065184 avg loss no lamb -2.065184 time 2019-02-21 19:43:49.946209
Model ind 685 epoch 5 head B batch: 100 avg loss -1.999329 avg loss no lamb -1.999329 time 2019-02-21 19:44:44.084645
Model ind 685 epoch 5 head B batch: 200 avg loss -2.058873 avg loss no lamb -2.058873 time 2019-02-21 19:45:36.867973
Model ind 685 epoch 5 head B batch: 300 avg loss -2.052032 avg loss no lamb -2.052032 time 2019-02-21 19:46:30.138201
Model ind 685 epoch 5 head B batch: 400 avg loss -1.989902 avg loss no lamb -1.989902 time 2019-02-21 19:47:23.561589
Model ind 685 epoch 5 head B batch: 0 avg loss -2.049214 avg loss no lamb -2.049214 time 2019-02-21 19:48:16.641863
Model ind 685 epoch 5 head B batch: 100 avg loss -2.107646 avg loss no lamb -2.107646 time 2019-02-21 19:49:10.576207
Model ind 685 epoch 5 head B batch: 200 avg loss -2.021342 avg loss no lamb -2.021342 time 2019-02-21 19:50:04.408485
Model ind 685 epoch 5 head B batch: 300 avg loss -2.078611 avg loss no lamb -2.078611 time 2019-02-21 19:50:57.683822
Model ind 685 epoch 5 head B batch: 400 avg loss -1.992876 avg loss no lamb -1.992876 time 2019-02-21 19:51:50.604996
Model ind 685 epoch 5 head A batch: 0 avg loss -2.081164 avg loss no lamb -2.081164 time 2019-02-21 19:52:43.903393
Model ind 685 epoch 5 head A batch: 100 avg loss -2.071276 avg loss no lamb -2.071276 time 2019-02-21 19:53:36.805792
Model ind 685 epoch 5 head A batch: 200 avg loss -2.021582 avg loss no lamb -2.021582 time 2019-02-21 19:54:30.414331
Model ind 685 epoch 5 head A batch: 300 avg loss -2.113280 avg loss no lamb -2.113280 time 2019-02-21 19:55:23.684457
Model ind 685 epoch 5 head A batch: 400 avg loss -2.066720 avg loss no lamb -2.066720 time 2019-02-21 19:56:16.843108
Pre: time 2019-02-21 19:57:25.039804: 
 	std: 0.032928426
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9896, 0.9765, 0.89842856, 0.9763857, 0.97657144]
	train_accs: [0.9896, 0.9765, 0.89842856, 0.9763857, 0.97657144]
	best_train_sub_head: 0
	worst: 0.89842856
	avg: 0.96349716
	best: 0.9896

Starting e_i: 6
Model ind 685 epoch 6 head B batch: 0 avg loss -2.091863 avg loss no lamb -2.091863 time 2019-02-21 19:57:27.718594
Model ind 685 epoch 6 head B batch: 100 avg loss -2.075319 avg loss no lamb -2.075319 time 2019-02-21 19:58:22.499530
Model ind 685 epoch 6 head B batch: 200 avg loss -2.037982 avg loss no lamb -2.037982 time 2019-02-21 19:59:15.889346
Model ind 685 epoch 6 head B batch: 300 avg loss -2.049608 avg loss no lamb -2.049608 time 2019-02-21 20:00:08.819493
Model ind 685 epoch 6 head B batch: 400 avg loss -2.014923 avg loss no lamb -2.014923 time 2019-02-21 20:01:02.829118
Model ind 685 epoch 6 head B batch: 0 avg loss -2.094959 avg loss no lamb -2.094959 time 2019-02-21 20:01:57.540455
Model ind 685 epoch 6 head B batch: 100 avg loss -2.056124 avg loss no lamb -2.056124 time 2019-02-21 20:02:50.584849
Model ind 685 epoch 6 head B batch: 200 avg loss -2.069616 avg loss no lamb -2.069616 time 2019-02-21 20:03:44.688032
Model ind 685 epoch 6 head B batch: 300 avg loss -2.069500 avg loss no lamb -2.069500 time 2019-02-21 20:04:37.202339
Model ind 685 epoch 6 head B batch: 400 avg loss -2.031060 avg loss no lamb -2.031060 time 2019-02-21 20:05:31.574610
Model ind 685 epoch 6 head A batch: 0 avg loss -2.065126 avg loss no lamb -2.065126 time 2019-02-21 20:06:26.898265
Model ind 685 epoch 6 head A batch: 100 avg loss -1.999657 avg loss no lamb -1.999657 time 2019-02-21 20:07:20.443040
Model ind 685 epoch 6 head A batch: 200 avg loss -2.055047 avg loss no lamb -2.055047 time 2019-02-21 20:08:13.681425
Model ind 685 epoch 6 head A batch: 300 avg loss -2.059448 avg loss no lamb -2.059448 time 2019-02-21 20:09:06.395745
Model ind 685 epoch 6 head A batch: 400 avg loss -2.048283 avg loss no lamb -2.048283 time 2019-02-21 20:09:58.701220
Pre: time 2019-02-21 20:11:05.382308: 
 	std: 0.032763463
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9900572, 0.9771714, 0.8994143, 0.97711426, 0.9771]
	train_accs: [0.9900572, 0.9771714, 0.8994143, 0.97711426, 0.9771]
	best_train_sub_head: 0
	worst: 0.8994143
	avg: 0.9641714
	best: 0.9900572

Starting e_i: 7
Model ind 685 epoch 7 head B batch: 0 avg loss -2.050105 avg loss no lamb -2.050105 time 2019-02-21 20:11:07.858254
Model ind 685 epoch 7 head B batch: 100 avg loss -2.046974 avg loss no lamb -2.046974 time 2019-02-21 20:12:02.417634
Model ind 685 epoch 7 head B batch: 200 avg loss -2.050218 avg loss no lamb -2.050218 time 2019-02-21 20:12:56.844973
Model ind 685 epoch 7 head B batch: 300 avg loss -2.105471 avg loss no lamb -2.105471 time 2019-02-21 20:13:51.692693
Model ind 685 epoch 7 head B batch: 400 avg loss -2.017873 avg loss no lamb -2.017873 time 2019-02-21 20:14:45.997118
Model ind 685 epoch 7 head B batch: 0 avg loss -2.067690 avg loss no lamb -2.067690 time 2019-02-21 20:15:39.061222
Model ind 685 epoch 7 head B batch: 100 avg loss -2.089512 avg loss no lamb -2.089512 time 2019-02-21 20:16:32.509305
Model ind 685 epoch 7 head B batch: 200 avg loss -2.068452 avg loss no lamb -2.068452 time 2019-02-21 20:17:25.422270
Model ind 685 epoch 7 head B batch: 300 avg loss -2.083475 avg loss no lamb -2.083475 time 2019-02-21 20:18:18.810030
Model ind 685 epoch 7 head B batch: 400 avg loss -2.012073 avg loss no lamb -2.012073 time 2019-02-21 20:19:11.488540
Model ind 685 epoch 7 head A batch: 0 avg loss -2.036118 avg loss no lamb -2.036118 time 2019-02-21 20:20:05.867411
Model ind 685 epoch 7 head A batch: 100 avg loss -1.988343 avg loss no lamb -1.988343 time 2019-02-21 20:20:59.221527
Model ind 685 epoch 7 head A batch: 200 avg loss -2.109108 avg loss no lamb -2.109108 time 2019-02-21 20:21:54.411543
Model ind 685 epoch 7 head A batch: 300 avg loss -2.093118 avg loss no lamb -2.093118 time 2019-02-21 20:22:47.515840
Model ind 685 epoch 7 head A batch: 400 avg loss -2.018020 avg loss no lamb -2.018020 time 2019-02-21 20:23:42.259693
Pre: time 2019-02-21 20:24:50.742638: 
 	std: 0.032953218
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9904, 0.97727144, 0.89915717, 0.97728574, 0.97725713]
	train_accs: [0.9904, 0.97727144, 0.89915717, 0.97728574, 0.97725713]
	best_train_sub_head: 0
	worst: 0.89915717
	avg: 0.9642743
	best: 0.9904

Starting e_i: 8
Model ind 685 epoch 8 head B batch: 0 avg loss -2.089020 avg loss no lamb -2.089020 time 2019-02-21 20:24:53.687612
Model ind 685 epoch 8 head B batch: 100 avg loss -2.074886 avg loss no lamb -2.074886 time 2019-02-21 20:25:47.527517
Model ind 685 epoch 8 head B batch: 200 avg loss -2.030569 avg loss no lamb -2.030569 time 2019-02-21 20:26:40.738366
Model ind 685 epoch 8 head B batch: 300 avg loss -2.162868 avg loss no lamb -2.162868 time 2019-02-21 20:27:33.433535
Model ind 685 epoch 8 head B batch: 400 avg loss -2.051987 avg loss no lamb -2.051987 time 2019-02-21 20:28:27.789086
Model ind 685 epoch 8 head B batch: 0 avg loss -2.084326 avg loss no lamb -2.084326 time 2019-02-21 20:29:22.260482
Model ind 685 epoch 8 head B batch: 100 avg loss -2.043813 avg loss no lamb -2.043813 time 2019-02-21 20:30:16.248965
Model ind 685 epoch 8 head B batch: 200 avg loss -1.986696 avg loss no lamb -1.986696 time 2019-02-21 20:31:08.696531
Model ind 685 epoch 8 head B batch: 300 avg loss -2.066292 avg loss no lamb -2.066292 time 2019-02-21 20:32:01.591716
Model ind 685 epoch 8 head B batch: 400 avg loss -2.080589 avg loss no lamb -2.080589 time 2019-02-21 20:32:55.841603
Model ind 685 epoch 8 head A batch: 0 avg loss -2.094412 avg loss no lamb -2.094412 time 2019-02-21 20:33:49.097956
Model ind 685 epoch 8 head A batch: 100 avg loss -2.066252 avg loss no lamb -2.066252 time 2019-02-21 20:34:43.457979
Model ind 685 epoch 8 head A batch: 200 avg loss -2.058932 avg loss no lamb -2.058932 time 2019-02-21 20:35:36.297125
Model ind 685 epoch 8 head A batch: 300 avg loss -2.084138 avg loss no lamb -2.084138 time 2019-02-21 20:36:29.679492
Model ind 685 epoch 8 head A batch: 400 avg loss -2.117860 avg loss no lamb -2.117860 time 2019-02-21 20:37:23.429733
Pre: time 2019-02-21 20:38:30.786888: 
 	std: 0.032813158
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99074286, 0.9778714, 0.89998573, 0.97782856, 0.9778]
	train_accs: [0.99074286, 0.9778714, 0.89998573, 0.97782856, 0.9778]
	best_train_sub_head: 0
	worst: 0.89998573
	avg: 0.9648458
	best: 0.99074286

Starting e_i: 9
Model ind 685 epoch 9 head B batch: 0 avg loss -2.031727 avg loss no lamb -2.031727 time 2019-02-21 20:38:34.323877
Model ind 685 epoch 9 head B batch: 100 avg loss -2.052251 avg loss no lamb -2.052251 time 2019-02-21 20:39:27.754698
Model ind 685 epoch 9 head B batch: 200 avg loss -2.049468 avg loss no lamb -2.049468 time 2019-02-21 20:40:21.251254
Model ind 685 epoch 9 head B batch: 300 avg loss -2.103043 avg loss no lamb -2.103043 time 2019-02-21 20:41:15.121544
Model ind 685 epoch 9 head B batch: 400 avg loss -2.066777 avg loss no lamb -2.066777 time 2019-02-21 20:42:08.718718
Model ind 685 epoch 9 head B batch: 0 avg loss -2.114640 avg loss no lamb -2.114640 time 2019-02-21 20:43:01.800646
Model ind 685 epoch 9 head B batch: 100 avg loss -2.088024 avg loss no lamb -2.088024 time 2019-02-21 20:43:54.605120
Model ind 685 epoch 9 head B batch: 200 avg loss -2.077339 avg loss no lamb -2.077339 time 2019-02-21 20:44:47.966680
Model ind 685 epoch 9 head B batch: 300 avg loss -2.102518 avg loss no lamb -2.102518 time 2019-02-21 20:45:40.380000
Model ind 685 epoch 9 head B batch: 400 avg loss -2.042748 avg loss no lamb -2.042748 time 2019-02-21 20:46:33.876913
Model ind 685 epoch 9 head A batch: 0 avg loss -2.054895 avg loss no lamb -2.054895 time 2019-02-21 20:47:28.448032
Model ind 685 epoch 9 head A batch: 100 avg loss -2.088016 avg loss no lamb -2.088016 time 2019-02-21 20:48:24.725991
Model ind 685 epoch 9 head A batch: 200 avg loss -2.050457 avg loss no lamb -2.050457 time 2019-02-21 20:49:19.575646
Model ind 685 epoch 9 head A batch: 300 avg loss -2.068429 avg loss no lamb -2.068429 time 2019-02-21 20:50:12.724554
Model ind 685 epoch 9 head A batch: 400 avg loss -2.048311 avg loss no lamb -2.048311 time 2019-02-21 20:51:06.031151
Pre: time 2019-02-21 20:52:12.999865: 
 	std: 0.033033416
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9908429, 0.97772855, 0.8993857, 0.97765714, 0.9777]
	train_accs: [0.9908429, 0.97772855, 0.8993857, 0.97765714, 0.9777]
	best_train_sub_head: 0
	worst: 0.8993857
	avg: 0.96466285
	best: 0.9908429

Starting e_i: 10
Model ind 685 epoch 10 head B batch: 0 avg loss -2.090630 avg loss no lamb -2.090630 time 2019-02-21 20:52:15.692600
Model ind 685 epoch 10 head B batch: 100 avg loss -2.105689 avg loss no lamb -2.105689 time 2019-02-21 20:53:10.958155
Model ind 685 epoch 10 head B batch: 200 avg loss -2.051636 avg loss no lamb -2.051636 time 2019-02-21 20:54:03.927205
Model ind 685 epoch 10 head B batch: 300 avg loss -2.080500 avg loss no lamb -2.080500 time 2019-02-21 20:54:57.103021
Model ind 685 epoch 10 head B batch: 400 avg loss -2.038917 avg loss no lamb -2.038917 time 2019-02-21 20:55:50.318834
Model ind 685 epoch 10 head B batch: 0 avg loss -2.079932 avg loss no lamb -2.079932 time 2019-02-21 20:56:43.530050
Model ind 685 epoch 10 head B batch: 100 avg loss -2.072898 avg loss no lamb -2.072898 time 2019-02-21 20:57:36.316296
Model ind 685 epoch 10 head B batch: 200 avg loss -2.119024 avg loss no lamb -2.119024 time 2019-02-21 20:58:29.022111
Model ind 685 epoch 10 head B batch: 300 avg loss -2.096713 avg loss no lamb -2.096713 time 2019-02-21 20:59:21.572879
Model ind 685 epoch 10 head B batch: 400 avg loss -2.073793 avg loss no lamb -2.073793 time 2019-02-21 21:00:14.167666
Model ind 685 epoch 10 head A batch: 0 avg loss -2.113465 avg loss no lamb -2.113465 time 2019-02-21 21:01:07.778759
Model ind 685 epoch 10 head A batch: 100 avg loss -2.046040 avg loss no lamb -2.046040 time 2019-02-21 21:02:02.145622
Model ind 685 epoch 10 head A batch: 200 avg loss -2.112483 avg loss no lamb -2.112483 time 2019-02-21 21:02:54.544730
Model ind 685 epoch 10 head A batch: 300 avg loss -2.120426 avg loss no lamb -2.120426 time 2019-02-21 21:03:47.552174
Model ind 685 epoch 10 head A batch: 400 avg loss -2.077189 avg loss no lamb -2.077189 time 2019-02-21 21:04:40.932369
Pre: time 2019-02-21 21:05:46.778180: 
 	std: 0.03291951
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99125713, 0.9779286, 0.9, 0.978, 0.9779571]
	train_accs: [0.99125713, 0.9779286, 0.9, 0.978, 0.9779571]
	best_train_sub_head: 0
	worst: 0.9
	avg: 0.9650286
	best: 0.99125713

Starting e_i: 11
Model ind 685 epoch 11 head B batch: 0 avg loss -2.154118 avg loss no lamb -2.154118 time 2019-02-21 21:05:49.900073
Model ind 685 epoch 11 head B batch: 100 avg loss -2.114987 avg loss no lamb -2.114987 time 2019-02-21 21:06:44.494484
Model ind 685 epoch 11 head B batch: 200 avg loss -2.069804 avg loss no lamb -2.069804 time 2019-02-21 21:07:37.891849
Model ind 685 epoch 11 head B batch: 300 avg loss -2.146219 avg loss no lamb -2.146219 time 2019-02-21 21:08:31.227597
Model ind 685 epoch 11 head B batch: 400 avg loss -2.059336 avg loss no lamb -2.059336 time 2019-02-21 21:09:24.957721
Model ind 685 epoch 11 head B batch: 0 avg loss -2.150860 avg loss no lamb -2.150860 time 2019-02-21 21:10:18.620803
Model ind 685 epoch 11 head B batch: 100 avg loss -2.043738 avg loss no lamb -2.043738 time 2019-02-21 21:11:12.130409
Model ind 685 epoch 11 head B batch: 200 avg loss -2.112117 avg loss no lamb -2.112117 time 2019-02-21 21:12:05.248483
Model ind 685 epoch 11 head B batch: 300 avg loss -2.119656 avg loss no lamb -2.119656 time 2019-02-21 21:12:58.307647
Model ind 685 epoch 11 head B batch: 400 avg loss -2.088468 avg loss no lamb -2.088468 time 2019-02-21 21:13:50.564339
Model ind 685 epoch 11 head A batch: 0 avg loss -2.088738 avg loss no lamb -2.088738 time 2019-02-21 21:14:43.204915
Model ind 685 epoch 11 head A batch: 100 avg loss -2.024478 avg loss no lamb -2.024478 time 2019-02-21 21:15:35.731023
Model ind 685 epoch 11 head A batch: 200 avg loss -2.071340 avg loss no lamb -2.071340 time 2019-02-21 21:16:28.482013
Model ind 685 epoch 11 head A batch: 300 avg loss -2.059426 avg loss no lamb -2.059426 time 2019-02-21 21:17:21.703907
Model ind 685 epoch 11 head A batch: 400 avg loss -2.035978 avg loss no lamb -2.035978 time 2019-02-21 21:18:17.417947
Pre: time 2019-02-21 21:19:25.895988: 
 	std: 0.03286073
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9910857, 0.9778572, 0.9000143, 0.97781426, 0.9778857]
	train_accs: [0.9910857, 0.9778572, 0.9000143, 0.97781426, 0.9778857]
	best_train_sub_head: 0
	worst: 0.9000143
	avg: 0.96493137
	best: 0.9910857

Starting e_i: 12
Model ind 685 epoch 12 head B batch: 0 avg loss -2.113393 avg loss no lamb -2.113393 time 2019-02-21 21:19:27.469371
Model ind 685 epoch 12 head B batch: 100 avg loss -2.084343 avg loss no lamb -2.084343 time 2019-02-21 21:20:23.152521
Model ind 685 epoch 12 head B batch: 200 avg loss -2.125108 avg loss no lamb -2.125108 time 2019-02-21 21:21:16.872464
Model ind 685 epoch 12 head B batch: 300 avg loss -2.079201 avg loss no lamb -2.079201 time 2019-02-21 21:22:09.391302
Model ind 685 epoch 12 head B batch: 400 avg loss -2.066831 avg loss no lamb -2.066831 time 2019-02-21 21:23:03.172444
Model ind 685 epoch 12 head B batch: 0 avg loss -2.106731 avg loss no lamb -2.106731 time 2019-02-21 21:23:55.856649
Model ind 685 epoch 12 head B batch: 100 avg loss -2.083206 avg loss no lamb -2.083206 time 2019-02-21 21:24:49.656057
Model ind 685 epoch 12 head B batch: 200 avg loss -2.061071 avg loss no lamb -2.061071 time 2019-02-21 21:25:44.308633
Model ind 685 epoch 12 head B batch: 300 avg loss -2.154336 avg loss no lamb -2.154336 time 2019-02-21 21:26:36.620287
Model ind 685 epoch 12 head B batch: 400 avg loss -2.062896 avg loss no lamb -2.062896 time 2019-02-21 21:27:31.576918
Model ind 685 epoch 12 head A batch: 0 avg loss -2.129464 avg loss no lamb -2.129464 time 2019-02-21 21:28:25.947460
Model ind 685 epoch 12 head A batch: 100 avg loss -2.073935 avg loss no lamb -2.073935 time 2019-02-21 21:29:20.087420
Model ind 685 epoch 12 head A batch: 200 avg loss -2.059721 avg loss no lamb -2.059721 time 2019-02-21 21:30:13.933194
Model ind 685 epoch 12 head A batch: 300 avg loss -2.099554 avg loss no lamb -2.099554 time 2019-02-21 21:31:08.257790
Model ind 685 epoch 12 head A batch: 400 avg loss -2.118672 avg loss no lamb -2.118672 time 2019-02-21 21:32:02.054680
Pre: time 2019-02-21 21:33:08.711568: 
 	std: 0.0329759
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99067146, 0.9774143, 0.8993, 0.97744286, 0.9774143]
	train_accs: [0.99067146, 0.9774143, 0.8993, 0.97744286, 0.9774143]
	best_train_sub_head: 0
	worst: 0.8993
	avg: 0.9644486
	best: 0.99067146

Starting e_i: 13
Model ind 685 epoch 13 head B batch: 0 avg loss -2.118876 avg loss no lamb -2.118876 time 2019-02-21 21:33:10.136914
Model ind 685 epoch 13 head B batch: 100 avg loss -2.072643 avg loss no lamb -2.072643 time 2019-02-21 21:34:04.150430
Model ind 685 epoch 13 head B batch: 200 avg loss -2.068666 avg loss no lamb -2.068666 time 2019-02-21 21:34:57.778540
Model ind 685 epoch 13 head B batch: 300 avg loss -2.131054 avg loss no lamb -2.131054 time 2019-02-21 21:35:49.918881
Model ind 685 epoch 13 head B batch: 400 avg loss -2.101280 avg loss no lamb -2.101280 time 2019-02-21 21:36:42.853635
Model ind 685 epoch 13 head B batch: 0 avg loss -2.108885 avg loss no lamb -2.108885 time 2019-02-21 21:37:37.044992
Model ind 685 epoch 13 head B batch: 100 avg loss -2.045146 avg loss no lamb -2.045146 time 2019-02-21 21:38:30.112814
Model ind 685 epoch 13 head B batch: 200 avg loss -2.047679 avg loss no lamb -2.047679 time 2019-02-21 21:39:22.661795
Model ind 685 epoch 13 head B batch: 300 avg loss -2.104089 avg loss no lamb -2.104089 time 2019-02-21 21:40:16.564068
Model ind 685 epoch 13 head B batch: 400 avg loss -2.085961 avg loss no lamb -2.085961 time 2019-02-21 21:41:10.427008
Model ind 685 epoch 13 head A batch: 0 avg loss -2.068004 avg loss no lamb -2.068004 time 2019-02-21 21:42:04.925010
Model ind 685 epoch 13 head A batch: 100 avg loss -2.102127 avg loss no lamb -2.102127 time 2019-02-21 21:43:00.053231
Model ind 685 epoch 13 head A batch: 200 avg loss -2.032908 avg loss no lamb -2.032908 time 2019-02-21 21:43:54.281241
Model ind 685 epoch 13 head A batch: 300 avg loss -2.123721 avg loss no lamb -2.123721 time 2019-02-21 21:44:47.344219
Model ind 685 epoch 13 head A batch: 400 avg loss -2.123725 avg loss no lamb -2.123725 time 2019-02-21 21:45:41.601124
Pre: time 2019-02-21 21:46:50.745749: 
 	std: 0.0329284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99114287, 0.9779571, 0.8999429, 0.9780286, 0.9779571]
	train_accs: [0.99114287, 0.9779571, 0.8999429, 0.9780286, 0.9779571]
	best_train_sub_head: 0
	worst: 0.8999429
	avg: 0.9650057
	best: 0.99114287

Starting e_i: 14
Model ind 685 epoch 14 head B batch: 0 avg loss -2.150671 avg loss no lamb -2.150671 time 2019-02-21 21:46:52.367232
Model ind 685 epoch 14 head B batch: 100 avg loss -2.048662 avg loss no lamb -2.048662 time 2019-02-21 21:47:46.901917
Model ind 685 epoch 14 head B batch: 200 avg loss -2.063501 avg loss no lamb -2.063501 time 2019-02-21 21:48:38.689977
Model ind 685 epoch 14 head B batch: 300 avg loss -2.072377 avg loss no lamb -2.072377 time 2019-02-21 21:49:32.490183
Model ind 685 epoch 14 head B batch: 400 avg loss -2.090061 avg loss no lamb -2.090061 time 2019-02-21 21:50:26.433344
Model ind 685 epoch 14 head B batch: 0 avg loss -2.073662 avg loss no lamb -2.073662 time 2019-02-21 21:51:20.492681
Model ind 685 epoch 14 head B batch: 100 avg loss -2.072976 avg loss no lamb -2.072976 time 2019-02-21 21:52:14.818109
Model ind 685 epoch 14 head B batch: 200 avg loss -2.082114 avg loss no lamb -2.082114 time 2019-02-21 21:53:09.208596
Model ind 685 epoch 14 head B batch: 300 avg loss -2.082681 avg loss no lamb -2.082681 time 2019-02-21 21:54:03.139813
Model ind 685 epoch 14 head B batch: 400 avg loss -2.096575 avg loss no lamb -2.096575 time 2019-02-21 21:54:56.702838
Model ind 685 epoch 14 head A batch: 0 avg loss -2.110335 avg loss no lamb -2.110335 time 2019-02-21 21:55:49.949116
Model ind 685 epoch 14 head A batch: 100 avg loss -2.150415 avg loss no lamb -2.150415 time 2019-02-21 21:56:42.814165
Model ind 685 epoch 14 head A batch: 200 avg loss -2.072874 avg loss no lamb -2.072874 time 2019-02-21 21:57:35.933404
Model ind 685 epoch 14 head A batch: 300 avg loss -2.122780 avg loss no lamb -2.122780 time 2019-02-21 21:58:30.567560
Model ind 685 epoch 14 head A batch: 400 avg loss -2.104570 avg loss no lamb -2.104570 time 2019-02-21 21:59:24.147169
Pre: time 2019-02-21 22:00:32.853357: 
 	std: 0.032791983
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99102855, 0.9777143, 0.9000857, 0.97774285, 0.9777]
	train_accs: [0.99102855, 0.9777143, 0.9000857, 0.97774285, 0.9777]
	best_train_sub_head: 0
	worst: 0.9000857
	avg: 0.96485424
	best: 0.99102855

Starting e_i: 15
Model ind 685 epoch 15 head B batch: 0 avg loss -2.098256 avg loss no lamb -2.098256 time 2019-02-21 22:00:34.473010
Model ind 685 epoch 15 head B batch: 100 avg loss -2.055408 avg loss no lamb -2.055408 time 2019-02-21 22:01:29.878511
Model ind 685 epoch 15 head B batch: 200 avg loss -2.070826 avg loss no lamb -2.070826 time 2019-02-21 22:02:22.662154
Model ind 685 epoch 15 head B batch: 300 avg loss -2.103474 avg loss no lamb -2.103474 time 2019-02-21 22:03:16.186909
Model ind 685 epoch 15 head B batch: 400 avg loss -2.130710 avg loss no lamb -2.130710 time 2019-02-21 22:04:10.330459
Model ind 685 epoch 15 head B batch: 0 avg loss -2.100473 avg loss no lamb -2.100473 time 2019-02-21 22:05:04.493127
Model ind 685 epoch 15 head B batch: 100 avg loss -2.034725 avg loss no lamb -2.034725 time 2019-02-21 22:06:01.395418
Model ind 685 epoch 15 head B batch: 200 avg loss -2.087494 avg loss no lamb -2.087494 time 2019-02-21 22:06:56.196816
Model ind 685 epoch 15 head B batch: 300 avg loss -2.091443 avg loss no lamb -2.091443 time 2019-02-21 22:07:50.007092
Model ind 685 epoch 15 head B batch: 400 avg loss -2.103876 avg loss no lamb -2.103876 time 2019-02-21 22:08:44.957483
Model ind 685 epoch 15 head A batch: 0 avg loss -2.149750 avg loss no lamb -2.149750 time 2019-02-21 22:09:39.930922
Model ind 685 epoch 15 head A batch: 100 avg loss -2.151913 avg loss no lamb -2.151913 time 2019-02-21 22:10:35.369367
Model ind 685 epoch 15 head A batch: 200 avg loss -2.102052 avg loss no lamb -2.102052 time 2019-02-21 22:11:30.267568
Model ind 685 epoch 15 head A batch: 300 avg loss -2.144861 avg loss no lamb -2.144861 time 2019-02-21 22:12:23.967705
Model ind 685 epoch 15 head A batch: 400 avg loss -2.096328 avg loss no lamb -2.096328 time 2019-02-21 22:13:19.481761
Pre: time 2019-02-21 22:14:27.193194: 
 	std: 0.03273216
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99127144, 0.97798574, 0.9004857, 0.97798574, 0.9779429]
	train_accs: [0.99127144, 0.97798574, 0.9004857, 0.97798574, 0.9779429]
	best_train_sub_head: 0
	worst: 0.9004857
	avg: 0.9651343
	best: 0.99127144

Starting e_i: 16
Model ind 685 epoch 16 head B batch: 0 avg loss -2.105394 avg loss no lamb -2.105394 time 2019-02-21 22:14:29.835884
Model ind 685 epoch 16 head B batch: 100 avg loss -2.131484 avg loss no lamb -2.131484 time 2019-02-21 22:15:24.118745
Model ind 685 epoch 16 head B batch: 200 avg loss -2.089497 avg loss no lamb -2.089497 time 2019-02-21 22:16:17.880628
Model ind 685 epoch 16 head B batch: 300 avg loss -2.131421 avg loss no lamb -2.131421 time 2019-02-21 22:17:12.243009
Model ind 685 epoch 16 head B batch: 400 avg loss -2.056039 avg loss no lamb -2.056039 time 2019-02-21 22:18:05.291847
Model ind 685 epoch 16 head B batch: 0 avg loss -2.071970 avg loss no lamb -2.071970 time 2019-02-21 22:18:58.349052
Model ind 685 epoch 16 head B batch: 100 avg loss -2.132950 avg loss no lamb -2.132950 time 2019-02-21 22:19:51.459464
Model ind 685 epoch 16 head B batch: 200 avg loss -2.124542 avg loss no lamb -2.124542 time 2019-02-21 22:20:46.281348
Model ind 685 epoch 16 head B batch: 300 avg loss -2.132067 avg loss no lamb -2.132067 time 2019-02-21 22:21:41.410291
Model ind 685 epoch 16 head B batch: 400 avg loss -2.088725 avg loss no lamb -2.088725 time 2019-02-21 22:22:35.431072
Model ind 685 epoch 16 head A batch: 0 avg loss -2.111215 avg loss no lamb -2.111215 time 2019-02-21 22:23:27.923102
Model ind 685 epoch 16 head A batch: 100 avg loss -2.119734 avg loss no lamb -2.119734 time 2019-02-21 22:24:21.986945
Model ind 685 epoch 16 head A batch: 200 avg loss -2.063018 avg loss no lamb -2.063018 time 2019-02-21 22:25:14.707230
Model ind 685 epoch 16 head A batch: 300 avg loss -2.090303 avg loss no lamb -2.090303 time 2019-02-21 22:26:08.959853
Model ind 685 epoch 16 head A batch: 400 avg loss -2.112678 avg loss no lamb -2.112678 time 2019-02-21 22:27:01.852828
Pre: time 2019-02-21 22:28:07.982334: 
 	std: 0.03278167
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9914143, 0.97821426, 0.90055716, 0.9782, 0.9782]
	train_accs: [0.9914143, 0.97821426, 0.90055716, 0.9782, 0.9782]
	best_train_sub_head: 0
	worst: 0.90055716
	avg: 0.96531713
	best: 0.9914143

Starting e_i: 17
Model ind 685 epoch 17 head B batch: 0 avg loss -2.117103 avg loss no lamb -2.117103 time 2019-02-21 22:28:11.088690
Model ind 685 epoch 17 head B batch: 100 avg loss -2.104585 avg loss no lamb -2.104585 time 2019-02-21 22:29:05.972645
Model ind 685 epoch 17 head B batch: 200 avg loss -2.006305 avg loss no lamb -2.006305 time 2019-02-21 22:30:01.994157
Model ind 685 epoch 17 head B batch: 300 avg loss -2.104447 avg loss no lamb -2.104447 time 2019-02-21 22:30:55.139796
Model ind 685 epoch 17 head B batch: 400 avg loss -2.072780 avg loss no lamb -2.072780 time 2019-02-21 22:31:50.833646
Model ind 685 epoch 17 head B batch: 0 avg loss -2.122017 avg loss no lamb -2.122017 time 2019-02-21 22:32:44.497259
Model ind 685 epoch 17 head B batch: 100 avg loss -2.086464 avg loss no lamb -2.086464 time 2019-02-21 22:33:39.225953
Model ind 685 epoch 17 head B batch: 200 avg loss -2.049968 avg loss no lamb -2.049968 time 2019-02-21 22:34:34.300776
Model ind 685 epoch 17 head B batch: 300 avg loss -2.137523 avg loss no lamb -2.137523 time 2019-02-21 22:35:28.339448
Model ind 685 epoch 17 head B batch: 400 avg loss -2.083025 avg loss no lamb -2.083025 time 2019-02-21 22:36:21.639352
Model ind 685 epoch 17 head A batch: 0 avg loss -2.106224 avg loss no lamb -2.106224 time 2019-02-21 22:37:15.447821
Model ind 685 epoch 17 head A batch: 100 avg loss -2.099422 avg loss no lamb -2.099422 time 2019-02-21 22:38:08.563710
Model ind 685 epoch 17 head A batch: 200 avg loss -2.098188 avg loss no lamb -2.098188 time 2019-02-21 22:39:04.973061
Model ind 685 epoch 17 head A batch: 300 avg loss -2.139876 avg loss no lamb -2.139876 time 2019-02-21 22:39:58.847313
Model ind 685 epoch 17 head A batch: 400 avg loss -2.101544 avg loss no lamb -2.101544 time 2019-02-21 22:40:52.748063
Pre: time 2019-02-21 22:41:58.792666: 
 	std: 0.032912582
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9915714, 0.97842854, 0.9004143, 0.97842854, 0.9783857]
	train_accs: [0.9915714, 0.97842854, 0.9004143, 0.97842854, 0.9783857]
	best_train_sub_head: 0
	worst: 0.9004143
	avg: 0.9654457
	best: 0.9915714

Starting e_i: 18
Model ind 685 epoch 18 head B batch: 0 avg loss -2.131733 avg loss no lamb -2.131733 time 2019-02-21 22:42:01.340134
Model ind 685 epoch 18 head B batch: 100 avg loss -2.107916 avg loss no lamb -2.107916 time 2019-02-21 22:42:55.244796
Model ind 685 epoch 18 head B batch: 200 avg loss -2.102249 avg loss no lamb -2.102249 time 2019-02-21 22:43:50.393921
Model ind 685 epoch 18 head B batch: 300 avg loss -2.148325 avg loss no lamb -2.148325 time 2019-02-21 22:44:47.561212
Model ind 685 epoch 18 head B batch: 400 avg loss -2.029052 avg loss no lamb -2.029052 time 2019-02-21 22:45:41.384056
Model ind 685 epoch 18 head B batch: 0 avg loss -2.139613 avg loss no lamb -2.139613 time 2019-02-21 22:46:35.831893
Model ind 685 epoch 18 head B batch: 100 avg loss -2.136566 avg loss no lamb -2.136566 time 2019-02-21 22:47:29.782124
Model ind 685 epoch 18 head B batch: 200 avg loss -2.087452 avg loss no lamb -2.087452 time 2019-02-21 22:48:25.295179
Model ind 685 epoch 18 head B batch: 300 avg loss -2.157445 avg loss no lamb -2.157445 time 2019-02-21 22:49:18.827847
Model ind 685 epoch 18 head B batch: 400 avg loss -2.067668 avg loss no lamb -2.067668 time 2019-02-21 22:50:13.166036
Model ind 685 epoch 18 head A batch: 0 avg loss -2.101828 avg loss no lamb -2.101828 time 2019-02-21 22:51:09.624257
Model ind 685 epoch 18 head A batch: 100 avg loss -2.152232 avg loss no lamb -2.152232 time 2019-02-21 22:52:03.397763
Model ind 685 epoch 18 head A batch: 200 avg loss -2.070574 avg loss no lamb -2.070574 time 2019-02-21 22:52:57.060192
Model ind 685 epoch 18 head A batch: 300 avg loss -2.196984 avg loss no lamb -2.196984 time 2019-02-21 22:53:49.273423
Model ind 685 epoch 18 head A batch: 400 avg loss -2.080736 avg loss no lamb -2.080736 time 2019-02-21 22:54:41.854306
Pre: time 2019-02-21 22:55:49.500318: 
 	std: 0.032709338
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9914143, 0.9783143, 0.9008, 0.9782714, 0.9783286]
	train_accs: [0.9914143, 0.9783143, 0.9008, 0.9782714, 0.9783286]
	best_train_sub_head: 0
	worst: 0.9008
	avg: 0.9654258
	best: 0.9914143

Starting e_i: 19
Model ind 685 epoch 19 head B batch: 0 avg loss -2.187210 avg loss no lamb -2.187210 time 2019-02-21 22:55:51.062708
Model ind 685 epoch 19 head B batch: 100 avg loss -2.184023 avg loss no lamb -2.184023 time 2019-02-21 22:56:46.410030
Model ind 685 epoch 19 head B batch: 200 avg loss -2.110298 avg loss no lamb -2.110298 time 2019-02-21 22:57:40.394523
Model ind 685 epoch 19 head B batch: 300 avg loss -2.123283 avg loss no lamb -2.123283 time 2019-02-21 22:58:35.143618
Model ind 685 epoch 19 head B batch: 400 avg loss -2.058160 avg loss no lamb -2.058160 time 2019-02-21 22:59:30.331900
Model ind 685 epoch 19 head B batch: 0 avg loss -2.132531 avg loss no lamb -2.132531 time 2019-02-21 23:00:25.179871
Model ind 685 epoch 19 head B batch: 100 avg loss -2.094675 avg loss no lamb -2.094675 time 2019-02-21 23:01:18.452558
Model ind 685 epoch 19 head B batch: 200 avg loss -2.066751 avg loss no lamb -2.066751 time 2019-02-21 23:02:14.514407
Model ind 685 epoch 19 head B batch: 300 avg loss -2.116808 avg loss no lamb -2.116808 time 2019-02-21 23:03:08.516212
Model ind 685 epoch 19 head B batch: 400 avg loss -2.101518 avg loss no lamb -2.101518 time 2019-02-21 23:04:01.511485
Model ind 685 epoch 19 head A batch: 0 avg loss -2.129129 avg loss no lamb -2.129129 time 2019-02-21 23:04:54.488562
Model ind 685 epoch 19 head A batch: 100 avg loss -2.092044 avg loss no lamb -2.092044 time 2019-02-21 23:05:47.740484
Model ind 685 epoch 19 head A batch: 200 avg loss -2.115073 avg loss no lamb -2.115073 time 2019-02-21 23:06:41.313326
Model ind 685 epoch 19 head A batch: 300 avg loss -2.086264 avg loss no lamb -2.086264 time 2019-02-21 23:07:35.049580
Model ind 685 epoch 19 head A batch: 400 avg loss -2.097553 avg loss no lamb -2.097553 time 2019-02-21 23:08:28.354084
Pre: time 2019-02-21 23:09:35.904656: 
 	std: 0.032873087
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99145716, 0.97828573, 0.90038574, 0.9782429, 0.9783]
	train_accs: [0.99145716, 0.97828573, 0.90038574, 0.9782429, 0.9783]
	best_train_sub_head: 0
	worst: 0.90038574
	avg: 0.9653343
	best: 0.99145716

Starting e_i: 20
Model ind 685 epoch 20 head B batch: 0 avg loss -2.136175 avg loss no lamb -2.136175 time 2019-02-21 23:09:37.330983
Model ind 685 epoch 20 head B batch: 100 avg loss -2.133240 avg loss no lamb -2.133240 time 2019-02-21 23:10:30.874327
Model ind 685 epoch 20 head B batch: 200 avg loss -2.095888 avg loss no lamb -2.095888 time 2019-02-21 23:11:24.224739
Model ind 685 epoch 20 head B batch: 300 avg loss -2.113657 avg loss no lamb -2.113657 time 2019-02-21 23:12:16.202496
Model ind 685 epoch 20 head B batch: 400 avg loss -2.053585 avg loss no lamb -2.053585 time 2019-02-21 23:13:09.139039
Model ind 685 epoch 20 head B batch: 0 avg loss -2.146649 avg loss no lamb -2.146649 time 2019-02-21 23:14:04.560246
Model ind 685 epoch 20 head B batch: 100 avg loss -2.108742 avg loss no lamb -2.108742 time 2019-02-21 23:14:57.776952
Model ind 685 epoch 20 head B batch: 200 avg loss -2.124269 avg loss no lamb -2.124269 time 2019-02-21 23:15:50.805221
Model ind 685 epoch 20 head B batch: 300 avg loss -2.083749 avg loss no lamb -2.083749 time 2019-02-21 23:16:45.005423
Model ind 685 epoch 20 head B batch: 400 avg loss -2.131044 avg loss no lamb -2.131044 time 2019-02-21 23:17:39.062073
Model ind 685 epoch 20 head A batch: 0 avg loss -2.146239 avg loss no lamb -2.146239 time 2019-02-21 23:18:33.164723
Model ind 685 epoch 20 head A batch: 100 avg loss -2.067063 avg loss no lamb -2.067063 time 2019-02-21 23:19:26.568834
Model ind 685 epoch 20 head A batch: 200 avg loss -2.063880 avg loss no lamb -2.063880 time 2019-02-21 23:20:21.611985
Model ind 685 epoch 20 head A batch: 300 avg loss -2.189211 avg loss no lamb -2.189211 time 2019-02-21 23:21:14.460141
Model ind 685 epoch 20 head A batch: 400 avg loss -2.079829 avg loss no lamb -2.079829 time 2019-02-21 23:22:08.683109
Pre: time 2019-02-21 23:23:15.554143: 
 	std: 0.03283721
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9916143, 0.9782571, 0.90052855, 0.9782429, 0.9782714]
	train_accs: [0.9916143, 0.9782571, 0.90052855, 0.9782429, 0.9782714]
	best_train_sub_head: 0
	worst: 0.90052855
	avg: 0.9653829
	best: 0.9916143

Starting e_i: 21
Model ind 685 epoch 21 head B batch: 0 avg loss -2.090046 avg loss no lamb -2.090046 time 2019-02-21 23:23:21.232185
Model ind 685 epoch 21 head B batch: 100 avg loss -2.146402 avg loss no lamb -2.146402 time 2019-02-21 23:24:16.097142
Model ind 685 epoch 21 head B batch: 200 avg loss -2.011271 avg loss no lamb -2.011271 time 2019-02-21 23:25:10.437331
Model ind 685 epoch 21 head B batch: 300 avg loss -2.137495 avg loss no lamb -2.137495 time 2019-02-21 23:26:05.203039
Model ind 685 epoch 21 head B batch: 400 avg loss -2.143430 avg loss no lamb -2.143430 time 2019-02-21 23:26:58.161664
Model ind 685 epoch 21 head B batch: 0 avg loss -2.112676 avg loss no lamb -2.112676 time 2019-02-21 23:27:52.753541
Model ind 685 epoch 21 head B batch: 100 avg loss -2.105411 avg loss no lamb -2.105411 time 2019-02-21 23:28:45.117296
Model ind 685 epoch 21 head B batch: 200 avg loss -2.074013 avg loss no lamb -2.074013 time 2019-02-21 23:29:38.630583
Model ind 685 epoch 21 head B batch: 300 avg loss -2.160926 avg loss no lamb -2.160926 time 2019-02-21 23:30:32.484577
Model ind 685 epoch 21 head B batch: 400 avg loss -2.122540 avg loss no lamb -2.122540 time 2019-02-21 23:31:24.859686
Model ind 685 epoch 21 head A batch: 0 avg loss -2.149178 avg loss no lamb -2.149178 time 2019-02-21 23:32:18.834045
Model ind 685 epoch 21 head A batch: 100 avg loss -2.038528 avg loss no lamb -2.038528 time 2019-02-21 23:33:12.649789
Model ind 685 epoch 21 head A batch: 200 avg loss -2.075209 avg loss no lamb -2.075209 time 2019-02-21 23:34:06.666039
Model ind 685 epoch 21 head A batch: 300 avg loss -2.165128 avg loss no lamb -2.165128 time 2019-02-21 23:35:02.174567
Model ind 685 epoch 21 head A batch: 400 avg loss -2.092070 avg loss no lamb -2.092070 time 2019-02-21 23:35:55.081934
Pre: time 2019-02-21 23:37:03.600096: 
 	std: 0.032890134
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99175715, 0.97847146, 0.9005857, 0.97847146, 0.9785]
	train_accs: [0.99175715, 0.97847146, 0.9005857, 0.97847146, 0.9785]
	best_train_sub_head: 0
	worst: 0.9005857
	avg: 0.9655572
	best: 0.99175715

Starting e_i: 22
Model ind 685 epoch 22 head B batch: 0 avg loss -2.148636 avg loss no lamb -2.148636 time 2019-02-21 23:37:06.418750
Model ind 685 epoch 22 head B batch: 100 avg loss -2.069879 avg loss no lamb -2.069879 time 2019-02-21 23:38:03.245883
Model ind 685 epoch 22 head B batch: 200 avg loss -2.142138 avg loss no lamb -2.142138 time 2019-02-21 23:38:57.142733
Model ind 685 epoch 22 head B batch: 300 avg loss -2.199669 avg loss no lamb -2.199669 time 2019-02-21 23:39:53.865731
Model ind 685 epoch 22 head B batch: 400 avg loss -2.128196 avg loss no lamb -2.128196 time 2019-02-21 23:40:47.965426
Model ind 685 epoch 22 head B batch: 0 avg loss -2.103225 avg loss no lamb -2.103225 time 2019-02-21 23:41:41.144030
Model ind 685 epoch 22 head B batch: 100 avg loss -2.130558 avg loss no lamb -2.130558 time 2019-02-21 23:42:34.658415
Model ind 685 epoch 22 head B batch: 200 avg loss -2.086613 avg loss no lamb -2.086613 time 2019-02-21 23:43:29.222269
Model ind 685 epoch 22 head B batch: 300 avg loss -2.186099 avg loss no lamb -2.186099 time 2019-02-21 23:44:22.388588
Model ind 685 epoch 22 head B batch: 400 avg loss -2.084681 avg loss no lamb -2.084681 time 2019-02-21 23:45:16.082285
Model ind 685 epoch 22 head A batch: 0 avg loss -2.105343 avg loss no lamb -2.105343 time 2019-02-21 23:46:11.415592
Model ind 685 epoch 22 head A batch: 100 avg loss -2.080285 avg loss no lamb -2.080285 time 2019-02-21 23:47:06.252952
Model ind 685 epoch 22 head A batch: 200 avg loss -2.139576 avg loss no lamb -2.139576 time 2019-02-21 23:48:00.467099
Model ind 685 epoch 22 head A batch: 300 avg loss -2.086303 avg loss no lamb -2.086303 time 2019-02-21 23:48:53.666042
Model ind 685 epoch 22 head A batch: 400 avg loss -2.101768 avg loss no lamb -2.101768 time 2019-02-21 23:49:47.101244
Pre: time 2019-02-21 23:50:54.904973: 
 	std: 0.03292297
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9918857, 0.97852856, 0.9005857, 0.97854286, 0.97852856]
	train_accs: [0.9918857, 0.97852856, 0.9005857, 0.97854286, 0.97852856]
	best_train_sub_head: 0
	worst: 0.9005857
	avg: 0.9656143
	best: 0.9918857

Starting e_i: 23
Model ind 685 epoch 23 head B batch: 0 avg loss -2.089902 avg loss no lamb -2.089902 time 2019-02-21 23:50:58.183249
Model ind 685 epoch 23 head B batch: 100 avg loss -2.139547 avg loss no lamb -2.139547 time 2019-02-21 23:51:53.255213
Model ind 685 epoch 23 head B batch: 200 avg loss -2.092878 avg loss no lamb -2.092878 time 2019-02-21 23:52:47.707649
Model ind 685 epoch 23 head B batch: 300 avg loss -2.135501 avg loss no lamb -2.135501 time 2019-02-21 23:53:41.088331
Model ind 685 epoch 23 head B batch: 400 avg loss -2.132705 avg loss no lamb -2.132705 time 2019-02-21 23:54:36.670317
Model ind 685 epoch 23 head B batch: 0 avg loss -2.145050 avg loss no lamb -2.145050 time 2019-02-21 23:55:30.847658
Model ind 685 epoch 23 head B batch: 100 avg loss -2.119253 avg loss no lamb -2.119253 time 2019-02-21 23:56:25.094680
Model ind 685 epoch 23 head B batch: 200 avg loss -2.113791 avg loss no lamb -2.113791 time 2019-02-21 23:57:17.799306
Model ind 685 epoch 23 head B batch: 300 avg loss -2.118828 avg loss no lamb -2.118828 time 2019-02-21 23:58:10.652417
Model ind 685 epoch 23 head B batch: 400 avg loss -2.136159 avg loss no lamb -2.136159 time 2019-02-21 23:59:02.687775
Model ind 685 epoch 23 head A batch: 0 avg loss -2.105575 avg loss no lamb -2.105575 time 2019-02-21 23:59:54.586874
Model ind 685 epoch 23 head A batch: 100 avg loss -2.100355 avg loss no lamb -2.100355 time 2019-02-22 00:00:46.501327
Model ind 685 epoch 23 head A batch: 200 avg loss -2.103547 avg loss no lamb -2.103547 time 2019-02-22 00:01:40.392320
Model ind 685 epoch 23 head A batch: 300 avg loss -2.139292 avg loss no lamb -2.139292 time 2019-02-22 00:02:32.764559
Model ind 685 epoch 23 head A batch: 400 avg loss -2.096909 avg loss no lamb -2.096909 time 2019-02-22 00:03:26.486369
Pre: time 2019-02-22 00:04:34.683966: 
 	std: 0.032820016
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9917714, 0.9786143, 0.90084285, 0.9786, 0.9786]
	train_accs: [0.9917714, 0.9786143, 0.90084285, 0.9786, 0.9786]
	best_train_sub_head: 0
	worst: 0.90084285
	avg: 0.96568567
	best: 0.9917714

Starting e_i: 24
Model ind 685 epoch 24 head B batch: 0 avg loss -2.122814 avg loss no lamb -2.122814 time 2019-02-22 00:04:36.111951
Model ind 685 epoch 24 head B batch: 100 avg loss -2.098485 avg loss no lamb -2.098485 time 2019-02-22 00:05:28.308601
Model ind 685 epoch 24 head B batch: 200 avg loss -2.106078 avg loss no lamb -2.106078 time 2019-02-22 00:06:21.266278
Model ind 685 epoch 24 head B batch: 300 avg loss -2.165092 avg loss no lamb -2.165092 time 2019-02-22 00:07:15.834889
Model ind 685 epoch 24 head B batch: 400 avg loss -2.107695 avg loss no lamb -2.107695 time 2019-02-22 00:08:09.242739
Model ind 685 epoch 24 head B batch: 0 avg loss -2.109551 avg loss no lamb -2.109551 time 2019-02-22 00:09:03.222833
Model ind 685 epoch 24 head B batch: 100 avg loss -2.133687 avg loss no lamb -2.133687 time 2019-02-22 00:09:56.010661
Model ind 685 epoch 24 head B batch: 200 avg loss -2.101890 avg loss no lamb -2.101890 time 2019-02-22 00:10:50.996402
Model ind 685 epoch 24 head B batch: 300 avg loss -2.140787 avg loss no lamb -2.140787 time 2019-02-22 00:11:43.829761
Model ind 685 epoch 24 head B batch: 400 avg loss -2.124042 avg loss no lamb -2.124042 time 2019-02-22 00:12:36.413983
Model ind 685 epoch 24 head A batch: 0 avg loss -2.121591 avg loss no lamb -2.121591 time 2019-02-22 00:13:30.337028
Model ind 685 epoch 24 head A batch: 100 avg loss -2.105302 avg loss no lamb -2.105302 time 2019-02-22 00:14:23.311043
Model ind 685 epoch 24 head A batch: 200 avg loss -2.054258 avg loss no lamb -2.054258 time 2019-02-22 00:15:17.104942
Model ind 685 epoch 24 head A batch: 300 avg loss -2.150992 avg loss no lamb -2.150992 time 2019-02-22 00:16:09.622146
Model ind 685 epoch 24 head A batch: 400 avg loss -2.098137 avg loss no lamb -2.098137 time 2019-02-22 00:17:02.677292
Pre: time 2019-02-22 00:18:09.889880: 
 	std: 0.03297591
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9919, 0.9786429, 0.9005143, 0.9786143, 0.9786286]
	train_accs: [0.9919, 0.9786429, 0.9005143, 0.9786143, 0.9786286]
	best_train_sub_head: 0
	worst: 0.9005143
	avg: 0.96566
	best: 0.9919

Starting e_i: 25
Model ind 685 epoch 25 head B batch: 0 avg loss -2.113128 avg loss no lamb -2.113128 time 2019-02-22 00:18:12.326497
Model ind 685 epoch 25 head B batch: 100 avg loss -2.164493 avg loss no lamb -2.164493 time 2019-02-22 00:19:05.683972
Model ind 685 epoch 25 head B batch: 200 avg loss -2.085681 avg loss no lamb -2.085681 time 2019-02-22 00:19:59.108046
Model ind 685 epoch 25 head B batch: 300 avg loss -2.187521 avg loss no lamb -2.187521 time 2019-02-22 00:20:52.539508
Model ind 685 epoch 25 head B batch: 400 avg loss -2.091733 avg loss no lamb -2.091733 time 2019-02-22 00:21:47.525938
Model ind 685 epoch 25 head B batch: 0 avg loss -2.148598 avg loss no lamb -2.148598 time 2019-02-22 00:22:41.080198
Model ind 685 epoch 25 head B batch: 100 avg loss -2.099823 avg loss no lamb -2.099823 time 2019-02-22 00:23:34.523493
Model ind 685 epoch 25 head B batch: 200 avg loss -2.110445 avg loss no lamb -2.110445 time 2019-02-22 00:24:28.808380
Model ind 685 epoch 25 head B batch: 300 avg loss -2.131569 avg loss no lamb -2.131569 time 2019-02-22 00:25:24.166331
Model ind 685 epoch 25 head B batch: 400 avg loss -2.107216 avg loss no lamb -2.107216 time 2019-02-22 00:26:17.627876
Model ind 685 epoch 25 head A batch: 0 avg loss -2.143615 avg loss no lamb -2.143615 time 2019-02-22 00:27:13.389161
Model ind 685 epoch 25 head A batch: 100 avg loss -2.121311 avg loss no lamb -2.121311 time 2019-02-22 00:28:07.027458
Model ind 685 epoch 25 head A batch: 200 avg loss -2.139246 avg loss no lamb -2.139246 time 2019-02-22 00:29:02.277189
Model ind 685 epoch 25 head A batch: 300 avg loss -2.178773 avg loss no lamb -2.178773 time 2019-02-22 00:29:55.394567
Model ind 685 epoch 25 head A batch: 400 avg loss -2.101606 avg loss no lamb -2.101606 time 2019-02-22 00:30:47.939391
Pre: time 2019-02-22 00:31:55.394580: 
 	std: 0.03290822
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9919, 0.9786143, 0.9006714, 0.9786, 0.9786]
	train_accs: [0.9919, 0.9786143, 0.9006714, 0.9786, 0.9786]
	best_train_sub_head: 0
	worst: 0.9006714
	avg: 0.96567714
	best: 0.9919

Starting e_i: 26
Model ind 685 epoch 26 head B batch: 0 avg loss -2.119141 avg loss no lamb -2.119141 time 2019-02-22 00:31:56.883806
Model ind 685 epoch 26 head B batch: 100 avg loss -2.089864 avg loss no lamb -2.089864 time 2019-02-22 00:32:51.561819
Model ind 685 epoch 26 head B batch: 200 avg loss -2.094115 avg loss no lamb -2.094115 time 2019-02-22 00:33:45.518515
Model ind 685 epoch 26 head B batch: 300 avg loss -2.126448 avg loss no lamb -2.126448 time 2019-02-22 00:34:40.355469
Model ind 685 epoch 26 head B batch: 400 avg loss -2.133375 avg loss no lamb -2.133375 time 2019-02-22 00:35:33.779648
Model ind 685 epoch 26 head B batch: 0 avg loss -2.181221 avg loss no lamb -2.181221 time 2019-02-22 00:36:28.145529
Model ind 685 epoch 26 head B batch: 100 avg loss -2.113216 avg loss no lamb -2.113216 time 2019-02-22 00:37:21.990745
Model ind 685 epoch 26 head B batch: 200 avg loss -2.143692 avg loss no lamb -2.143692 time 2019-02-22 00:38:17.517679
Model ind 685 epoch 26 head B batch: 300 avg loss -2.133656 avg loss no lamb -2.133656 time 2019-02-22 00:39:11.737423
Model ind 685 epoch 26 head B batch: 400 avg loss -2.118461 avg loss no lamb -2.118461 time 2019-02-22 00:40:04.870584
Model ind 685 epoch 26 head A batch: 0 avg loss -2.158288 avg loss no lamb -2.158288 time 2019-02-22 00:41:00.114605
Model ind 685 epoch 26 head A batch: 100 avg loss -2.110586 avg loss no lamb -2.110586 time 2019-02-22 00:41:53.904996
Model ind 685 epoch 26 head A batch: 200 avg loss -2.101300 avg loss no lamb -2.101300 time 2019-02-22 00:42:46.237196
Model ind 685 epoch 26 head A batch: 300 avg loss -2.106683 avg loss no lamb -2.106683 time 2019-02-22 00:43:40.235907
Model ind 685 epoch 26 head A batch: 400 avg loss -2.131809 avg loss no lamb -2.131809 time 2019-02-22 00:44:33.831233
Pre: time 2019-02-22 00:45:42.326783: 
 	std: 0.032859735
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9919429, 0.9786286, 0.9008143, 0.9785857, 0.9786143]
	train_accs: [0.9919429, 0.9786286, 0.9008143, 0.9785857, 0.9786143]
	best_train_sub_head: 0
	worst: 0.9008143
	avg: 0.96571714
	best: 0.9919429

Starting e_i: 27
Model ind 685 epoch 27 head B batch: 0 avg loss -2.118425 avg loss no lamb -2.118425 time 2019-02-22 00:45:44.961838
Model ind 685 epoch 27 head B batch: 100 avg loss -2.084786 avg loss no lamb -2.084786 time 2019-02-22 00:46:40.172696
Model ind 685 epoch 27 head B batch: 200 avg loss -2.055019 avg loss no lamb -2.055019 time 2019-02-22 00:47:36.628176
Model ind 685 epoch 27 head B batch: 300 avg loss -2.154836 avg loss no lamb -2.154836 time 2019-02-22 00:48:31.501415
Model ind 685 epoch 27 head B batch: 400 avg loss -2.103351 avg loss no lamb -2.103351 time 2019-02-22 00:49:24.938000
Model ind 685 epoch 27 head B batch: 0 avg loss -2.141869 avg loss no lamb -2.141869 time 2019-02-22 00:50:18.683404
Model ind 685 epoch 27 head B batch: 100 avg loss -2.107790 avg loss no lamb -2.107790 time 2019-02-22 00:51:12.520146
Model ind 685 epoch 27 head B batch: 200 avg loss -2.085003 avg loss no lamb -2.085003 time 2019-02-22 00:52:05.843003
Model ind 685 epoch 27 head B batch: 300 avg loss -2.145584 avg loss no lamb -2.145584 time 2019-02-22 00:52:59.117537
Model ind 685 epoch 27 head B batch: 400 avg loss -2.102574 avg loss no lamb -2.102574 time 2019-02-22 00:53:54.209956
Model ind 685 epoch 27 head A batch: 0 avg loss -2.165151 avg loss no lamb -2.165151 time 2019-02-22 00:54:50.530298
Model ind 685 epoch 27 head A batch: 100 avg loss -2.130243 avg loss no lamb -2.130243 time 2019-02-22 00:55:45.118058
Model ind 685 epoch 27 head A batch: 200 avg loss -2.081204 avg loss no lamb -2.081204 time 2019-02-22 00:56:41.015995
Model ind 685 epoch 27 head A batch: 300 avg loss -2.143663 avg loss no lamb -2.143663 time 2019-02-22 00:57:34.953647
Model ind 685 epoch 27 head A batch: 400 avg loss -2.081596 avg loss no lamb -2.081596 time 2019-02-22 00:58:28.190015
Pre: time 2019-02-22 00:59:35.203026: 
 	std: 0.03300629
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9916143, 0.9785143, 0.90024287, 0.97845715, 0.9785143]
	train_accs: [0.9916143, 0.9785143, 0.90024287, 0.97845715, 0.9785143]
	best_train_sub_head: 0
	worst: 0.90024287
	avg: 0.9654686
	best: 0.9916143

Starting e_i: 28
Model ind 685 epoch 28 head B batch: 0 avg loss -2.139282 avg loss no lamb -2.139282 time 2019-02-22 00:59:36.766186
Model ind 685 epoch 28 head B batch: 100 avg loss -2.105256 avg loss no lamb -2.105256 time 2019-02-22 01:00:30.641889
Model ind 685 epoch 28 head B batch: 200 avg loss -2.122411 avg loss no lamb -2.122411 time 2019-02-22 01:01:24.395349
Model ind 685 epoch 28 head B batch: 300 avg loss -2.111454 avg loss no lamb -2.111454 time 2019-02-22 01:02:18.746247
Model ind 685 epoch 28 head B batch: 400 avg loss -2.136065 avg loss no lamb -2.136065 time 2019-02-22 01:03:11.898990
Model ind 685 epoch 28 head B batch: 0 avg loss -2.128619 avg loss no lamb -2.128619 time 2019-02-22 01:04:05.505455
Model ind 685 epoch 28 head B batch: 100 avg loss -2.119875 avg loss no lamb -2.119875 time 2019-02-22 01:04:59.774613
Model ind 685 epoch 28 head B batch: 200 avg loss -2.083234 avg loss no lamb -2.083234 time 2019-02-22 01:05:52.089347
Model ind 685 epoch 28 head B batch: 300 avg loss -2.136243 avg loss no lamb -2.136243 time 2019-02-22 01:06:46.196036
Model ind 685 epoch 28 head B batch: 400 avg loss -2.078682 avg loss no lamb -2.078682 time 2019-02-22 01:07:39.374238
Model ind 685 epoch 28 head A batch: 0 avg loss -2.146003 avg loss no lamb -2.146003 time 2019-02-22 01:08:32.890929
Model ind 685 epoch 28 head A batch: 100 avg loss -2.112715 avg loss no lamb -2.112715 time 2019-02-22 01:09:28.006009
Model ind 685 epoch 28 head A batch: 200 avg loss -2.106487 avg loss no lamb -2.106487 time 2019-02-22 01:10:22.522888
Model ind 685 epoch 28 head A batch: 300 avg loss -2.162473 avg loss no lamb -2.162473 time 2019-02-22 01:11:17.452641
Model ind 685 epoch 28 head A batch: 400 avg loss -2.133324 avg loss no lamb -2.133324 time 2019-02-22 01:12:12.010122
Pre: time 2019-02-22 01:13:19.758575: 
 	std: 0.032767158
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99182856, 0.9784857, 0.90094286, 0.97854286, 0.9785]
	train_accs: [0.99182856, 0.9784857, 0.90094286, 0.97854286, 0.9785]
	best_train_sub_head: 0
	worst: 0.90094286
	avg: 0.96566
	best: 0.99182856

Starting e_i: 29
Model ind 685 epoch 29 head B batch: 0 avg loss -2.110290 avg loss no lamb -2.110290 time 2019-02-22 01:13:21.330824
Model ind 685 epoch 29 head B batch: 100 avg loss -2.103116 avg loss no lamb -2.103116 time 2019-02-22 01:14:15.392809
Model ind 685 epoch 29 head B batch: 200 avg loss -2.138795 avg loss no lamb -2.138795 time 2019-02-22 01:15:09.557967
Model ind 685 epoch 29 head B batch: 300 avg loss -2.152919 avg loss no lamb -2.152919 time 2019-02-22 01:16:03.268278
Model ind 685 epoch 29 head B batch: 400 avg loss -2.098901 avg loss no lamb -2.098901 time 2019-02-22 01:16:57.034079
Model ind 685 epoch 29 head B batch: 0 avg loss -2.147772 avg loss no lamb -2.147772 time 2019-02-22 01:17:51.665463
Model ind 685 epoch 29 head B batch: 100 avg loss -2.140461 avg loss no lamb -2.140461 time 2019-02-22 01:18:44.463163
Model ind 685 epoch 29 head B batch: 200 avg loss -2.110673 avg loss no lamb -2.110673 time 2019-02-22 01:19:38.110090
Model ind 685 epoch 29 head B batch: 300 avg loss -2.154319 avg loss no lamb -2.154319 time 2019-02-22 01:20:32.617543
Model ind 685 epoch 29 head B batch: 400 avg loss -2.107860 avg loss no lamb -2.107860 time 2019-02-22 01:21:26.484932
Model ind 685 epoch 29 head A batch: 0 avg loss -2.133723 avg loss no lamb -2.133723 time 2019-02-22 01:22:21.733894
Model ind 685 epoch 29 head A batch: 100 avg loss -2.100775 avg loss no lamb -2.100775 time 2019-02-22 01:23:16.744808
Model ind 685 epoch 29 head A batch: 200 avg loss -2.130309 avg loss no lamb -2.130309 time 2019-02-22 01:24:10.642998
Model ind 685 epoch 29 head A batch: 300 avg loss -2.135474 avg loss no lamb -2.135474 time 2019-02-22 01:25:04.751965
Model ind 685 epoch 29 head A batch: 400 avg loss -2.066145 avg loss no lamb -2.066145 time 2019-02-22 01:25:57.387519
Pre: time 2019-02-22 01:27:07.522971: 
 	std: 0.032963753
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.992, 0.9785714, 0.90054286, 0.97852856, 0.9785714]
	train_accs: [0.992, 0.9785714, 0.90054286, 0.97852856, 0.9785714]
	best_train_sub_head: 0
	worst: 0.90054286
	avg: 0.9656428
	best: 0.992

Starting e_i: 30
Model ind 685 epoch 30 head B batch: 0 avg loss -2.189300 avg loss no lamb -2.189300 time 2019-02-22 01:27:10.747366
Model ind 685 epoch 30 head B batch: 100 avg loss -2.107124 avg loss no lamb -2.107124 time 2019-02-22 01:28:04.949549
Model ind 685 epoch 30 head B batch: 200 avg loss -2.050869 avg loss no lamb -2.050869 time 2019-02-22 01:28:58.228993
Model ind 685 epoch 30 head B batch: 300 avg loss -2.152004 avg loss no lamb -2.152004 time 2019-02-22 01:29:52.263514
Model ind 685 epoch 30 head B batch: 400 avg loss -2.086746 avg loss no lamb -2.086746 time 2019-02-22 01:30:45.550866
Model ind 685 epoch 30 head B batch: 0 avg loss -2.139401 avg loss no lamb -2.139401 time 2019-02-22 01:31:39.718108
Model ind 685 epoch 30 head B batch: 100 avg loss -2.126216 avg loss no lamb -2.126216 time 2019-02-22 01:32:33.144084
Model ind 685 epoch 30 head B batch: 200 avg loss -2.126721 avg loss no lamb -2.126721 time 2019-02-22 01:33:27.001457
Model ind 685 epoch 30 head B batch: 300 avg loss -2.182135 avg loss no lamb -2.182135 time 2019-02-22 01:34:20.238544
Model ind 685 epoch 30 head B batch: 400 avg loss -2.085497 avg loss no lamb -2.085497 time 2019-02-22 01:35:14.144793
Model ind 685 epoch 30 head A batch: 0 avg loss -2.145842 avg loss no lamb -2.145842 time 2019-02-22 01:36:09.716572
Model ind 685 epoch 30 head A batch: 100 avg loss -2.138872 avg loss no lamb -2.138872 time 2019-02-22 01:37:03.782889
Model ind 685 epoch 30 head A batch: 200 avg loss -2.130459 avg loss no lamb -2.130459 time 2019-02-22 01:37:58.129220
Model ind 685 epoch 30 head A batch: 300 avg loss -2.121009 avg loss no lamb -2.121009 time 2019-02-22 01:38:51.081461
Model ind 685 epoch 30 head A batch: 400 avg loss -2.143555 avg loss no lamb -2.143555 time 2019-02-22 01:39:44.254122
Pre: time 2019-02-22 01:40:52.685884: 
 	std: 0.03281599
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99212855, 0.97867143, 0.9010286, 0.9786571, 0.9786429]
	train_accs: [0.99212855, 0.97867143, 0.9010286, 0.9786571, 0.9786429]
	best_train_sub_head: 0
	worst: 0.9010286
	avg: 0.96582574
	best: 0.99212855

Starting e_i: 31
Model ind 685 epoch 31 head B batch: 0 avg loss -2.147770 avg loss no lamb -2.147770 time 2019-02-22 01:40:55.235278
Model ind 685 epoch 31 head B batch: 100 avg loss -2.034211 avg loss no lamb -2.034211 time 2019-02-22 01:41:49.095655
Model ind 685 epoch 31 head B batch: 200 avg loss -2.131984 avg loss no lamb -2.131984 time 2019-02-22 01:42:44.011131
Model ind 685 epoch 31 head B batch: 300 avg loss -2.136407 avg loss no lamb -2.136407 time 2019-02-22 01:43:37.396056
Model ind 685 epoch 31 head B batch: 400 avg loss -2.142462 avg loss no lamb -2.142462 time 2019-02-22 01:44:30.996826
Model ind 685 epoch 31 head B batch: 0 avg loss -2.113603 avg loss no lamb -2.113603 time 2019-02-22 01:45:26.327519
Model ind 685 epoch 31 head B batch: 100 avg loss -2.148559 avg loss no lamb -2.148559 time 2019-02-22 01:46:20.163013
Model ind 685 epoch 31 head B batch: 200 avg loss -2.137809 avg loss no lamb -2.137809 time 2019-02-22 01:47:14.871162
Model ind 685 epoch 31 head B batch: 300 avg loss -2.134753 avg loss no lamb -2.134753 time 2019-02-22 01:48:08.883771
Model ind 685 epoch 31 head B batch: 400 avg loss -2.142302 avg loss no lamb -2.142302 time 2019-02-22 01:49:02.722888
Model ind 685 epoch 31 head A batch: 0 avg loss -2.171395 avg loss no lamb -2.171395 time 2019-02-22 01:49:56.913545
Model ind 685 epoch 31 head A batch: 100 avg loss -2.088060 avg loss no lamb -2.088060 time 2019-02-22 01:50:50.902160
Model ind 685 epoch 31 head A batch: 200 avg loss -2.102375 avg loss no lamb -2.102375 time 2019-02-22 01:51:43.717923
Model ind 685 epoch 31 head A batch: 300 avg loss -2.106854 avg loss no lamb -2.106854 time 2019-02-22 01:52:37.825269
Model ind 685 epoch 31 head A batch: 400 avg loss -2.093908 avg loss no lamb -2.093908 time 2019-02-22 01:53:32.020371
Pre: time 2019-02-22 01:54:41.776321: 
 	std: 0.03275143
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99212855, 0.97877145, 0.90125716, 0.9787857, 0.97874284]
	train_accs: [0.99212855, 0.97877145, 0.90125716, 0.9787857, 0.97874284]
	best_train_sub_head: 0
	worst: 0.90125716
	avg: 0.96593714
	best: 0.99212855

Starting e_i: 32
Model ind 685 epoch 32 head B batch: 0 avg loss -2.160707 avg loss no lamb -2.160707 time 2019-02-22 01:54:43.253521
Model ind 685 epoch 32 head B batch: 100 avg loss -2.137356 avg loss no lamb -2.137356 time 2019-02-22 01:55:37.801198
Model ind 685 epoch 32 head B batch: 200 avg loss -2.077079 avg loss no lamb -2.077079 time 2019-02-22 01:56:33.045182
Model ind 685 epoch 32 head B batch: 300 avg loss -2.153829 avg loss no lamb -2.153829 time 2019-02-22 01:57:25.974629
Model ind 685 epoch 32 head B batch: 400 avg loss -2.112900 avg loss no lamb -2.112900 time 2019-02-22 01:58:19.011856
Model ind 685 epoch 32 head B batch: 0 avg loss -2.122904 avg loss no lamb -2.122904 time 2019-02-22 01:59:12.221755
Model ind 685 epoch 32 head B batch: 100 avg loss -2.180979 avg loss no lamb -2.180979 time 2019-02-22 02:00:06.876871
Model ind 685 epoch 32 head B batch: 200 avg loss -2.142350 avg loss no lamb -2.142350 time 2019-02-22 02:01:02.176213
Model ind 685 epoch 32 head B batch: 300 avg loss -2.161087 avg loss no lamb -2.161087 time 2019-02-22 02:01:56.681657
Model ind 685 epoch 32 head B batch: 400 avg loss -2.138550 avg loss no lamb -2.138550 time 2019-02-22 02:02:49.470819
Model ind 685 epoch 32 head A batch: 0 avg loss -2.148929 avg loss no lamb -2.148929 time 2019-02-22 02:03:44.322738
Model ind 685 epoch 32 head A batch: 100 avg loss -2.032247 avg loss no lamb -2.032247 time 2019-02-22 02:04:37.919158
Model ind 685 epoch 32 head A batch: 200 avg loss -2.137647 avg loss no lamb -2.137647 time 2019-02-22 02:05:31.980224
Model ind 685 epoch 32 head A batch: 300 avg loss -2.189318 avg loss no lamb -2.189318 time 2019-02-22 02:06:23.746768
Model ind 685 epoch 32 head A batch: 400 avg loss -2.087631 avg loss no lamb -2.087631 time 2019-02-22 02:07:17.459099
Pre: time 2019-02-22 02:08:24.934957: 
 	std: 0.03285637
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9918, 0.97844285, 0.9006714, 0.97845715, 0.97845715]
	train_accs: [0.9918, 0.97844285, 0.9006714, 0.97845715, 0.97845715]
	best_train_sub_head: 0
	worst: 0.9006714
	avg: 0.9655657
	best: 0.9918

Starting e_i: 33
Model ind 685 epoch 33 head B batch: 0 avg loss -2.153199 avg loss no lamb -2.153199 time 2019-02-22 02:08:26.677291
Model ind 685 epoch 33 head B batch: 100 avg loss -2.140873 avg loss no lamb -2.140873 time 2019-02-22 02:09:20.485276
Model ind 685 epoch 33 head B batch: 200 avg loss -2.086078 avg loss no lamb -2.086078 time 2019-02-22 02:10:14.970071
Model ind 685 epoch 33 head B batch: 300 avg loss -2.138070 avg loss no lamb -2.138070 time 2019-02-22 02:11:08.798561
Model ind 685 epoch 33 head B batch: 400 avg loss -2.147434 avg loss no lamb -2.147434 time 2019-02-22 02:12:02.188175
Model ind 685 epoch 33 head B batch: 0 avg loss -2.202690 avg loss no lamb -2.202690 time 2019-02-22 02:12:55.326574
Model ind 685 epoch 33 head B batch: 100 avg loss -2.136543 avg loss no lamb -2.136543 time 2019-02-22 02:13:49.166689
Model ind 685 epoch 33 head B batch: 200 avg loss -2.143422 avg loss no lamb -2.143422 time 2019-02-22 02:14:43.787545
Model ind 685 epoch 33 head B batch: 300 avg loss -2.121938 avg loss no lamb -2.121938 time 2019-02-22 02:15:37.466459
Model ind 685 epoch 33 head B batch: 400 avg loss -2.155119 avg loss no lamb -2.155119 time 2019-02-22 02:16:32.026970
Model ind 685 epoch 33 head A batch: 0 avg loss -2.166322 avg loss no lamb -2.166322 time 2019-02-22 02:17:26.800163
Model ind 685 epoch 33 head A batch: 100 avg loss -2.122224 avg loss no lamb -2.122224 time 2019-02-22 02:18:20.785241
Model ind 685 epoch 33 head A batch: 200 avg loss -2.088699 avg loss no lamb -2.088699 time 2019-02-22 02:19:15.426129
Model ind 685 epoch 33 head A batch: 300 avg loss -2.168600 avg loss no lamb -2.168600 time 2019-02-22 02:20:10.594023
Model ind 685 epoch 33 head A batch: 400 avg loss -2.166696 avg loss no lamb -2.166696 time 2019-02-22 02:21:04.616145
Pre: time 2019-02-22 02:22:11.959150: 
 	std: 0.03291728
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9920857, 0.97877145, 0.9008143, 0.97874284, 0.97875714]
	train_accs: [0.9920857, 0.97877145, 0.9008143, 0.97874284, 0.97875714]
	best_train_sub_head: 0
	worst: 0.9008143
	avg: 0.96583426
	best: 0.9920857

Starting e_i: 34
Model ind 685 epoch 34 head B batch: 0 avg loss -2.113979 avg loss no lamb -2.113979 time 2019-02-22 02:22:13.678308
Model ind 685 epoch 34 head B batch: 100 avg loss -2.139313 avg loss no lamb -2.139313 time 2019-02-22 02:23:08.067552
Model ind 685 epoch 34 head B batch: 200 avg loss -2.134777 avg loss no lamb -2.134777 time 2019-02-22 02:24:01.383540
Model ind 685 epoch 34 head B batch: 300 avg loss -2.122875 avg loss no lamb -2.122875 time 2019-02-22 02:24:56.625964
Model ind 685 epoch 34 head B batch: 400 avg loss -2.108271 avg loss no lamb -2.108271 time 2019-02-22 02:25:49.311126
Model ind 685 epoch 34 head B batch: 0 avg loss -2.144274 avg loss no lamb -2.144274 time 2019-02-22 02:26:42.414239
Model ind 685 epoch 34 head B batch: 100 avg loss -2.147758 avg loss no lamb -2.147758 time 2019-02-22 02:27:37.523887
Model ind 685 epoch 34 head B batch: 200 avg loss -2.119053 avg loss no lamb -2.119053 time 2019-02-22 02:28:30.323124
Model ind 685 epoch 34 head B batch: 300 avg loss -2.139087 avg loss no lamb -2.139087 time 2019-02-22 02:29:23.113218
Model ind 685 epoch 34 head B batch: 400 avg loss -2.112534 avg loss no lamb -2.112534 time 2019-02-22 02:30:17.015257
Model ind 685 epoch 34 head A batch: 0 avg loss -2.121634 avg loss no lamb -2.121634 time 2019-02-22 02:31:10.757898
Model ind 685 epoch 34 head A batch: 100 avg loss -2.075248 avg loss no lamb -2.075248 time 2019-02-22 02:32:04.367160
Model ind 685 epoch 34 head A batch: 200 avg loss -2.120538 avg loss no lamb -2.120538 time 2019-02-22 02:32:58.763489
Model ind 685 epoch 34 head A batch: 300 avg loss -2.164463 avg loss no lamb -2.164463 time 2019-02-22 02:33:52.593553
Model ind 685 epoch 34 head A batch: 400 avg loss -2.108758 avg loss no lamb -2.108758 time 2019-02-22 02:34:45.562859
Pre: time 2019-02-22 02:35:54.069341: 
 	std: 0.032805588
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99205714, 0.9787, 0.9010571, 0.9787, 0.9787286]
	train_accs: [0.99205714, 0.9787, 0.9010571, 0.9787, 0.9787286]
	best_train_sub_head: 0
	worst: 0.9010571
	avg: 0.96584857
	best: 0.99205714

Starting e_i: 35
Model ind 685 epoch 35 head B batch: 0 avg loss -2.158193 avg loss no lamb -2.158193 time 2019-02-22 02:35:55.625494
Model ind 685 epoch 35 head B batch: 100 avg loss -2.122949 avg loss no lamb -2.122949 time 2019-02-22 02:36:50.913169
Model ind 685 epoch 35 head B batch: 200 avg loss -2.084719 avg loss no lamb -2.084719 time 2019-02-22 02:37:45.816689
Model ind 685 epoch 35 head B batch: 300 avg loss -2.126337 avg loss no lamb -2.126337 time 2019-02-22 02:38:40.249102
Model ind 685 epoch 35 head B batch: 400 avg loss -2.087843 avg loss no lamb -2.087843 time 2019-02-22 02:39:34.266207
Model ind 685 epoch 35 head B batch: 0 avg loss -2.134223 avg loss no lamb -2.134223 time 2019-02-22 02:40:26.729773
Model ind 685 epoch 35 head B batch: 100 avg loss -2.108604 avg loss no lamb -2.108604 time 2019-02-22 02:41:21.265140
Model ind 685 epoch 35 head B batch: 200 avg loss -2.114890 avg loss no lamb -2.114890 time 2019-02-22 02:42:15.373089
Model ind 685 epoch 35 head B batch: 300 avg loss -2.129096 avg loss no lamb -2.129096 time 2019-02-22 02:43:08.380572
Model ind 685 epoch 35 head B batch: 400 avg loss -2.125443 avg loss no lamb -2.125443 time 2019-02-22 02:44:03.662668
Model ind 685 epoch 35 head A batch: 0 avg loss -2.115326 avg loss no lamb -2.115326 time 2019-02-22 02:44:57.862506
Model ind 685 epoch 35 head A batch: 100 avg loss -2.147129 avg loss no lamb -2.147129 time 2019-02-22 02:45:52.355673
Model ind 685 epoch 35 head A batch: 200 avg loss -2.120931 avg loss no lamb -2.120931 time 2019-02-22 02:46:45.164364
Model ind 685 epoch 35 head A batch: 300 avg loss -2.122420 avg loss no lamb -2.122420 time 2019-02-22 02:47:38.621474
Model ind 685 epoch 35 head A batch: 400 avg loss -2.133024 avg loss no lamb -2.133024 time 2019-02-22 02:48:32.309651
Pre: time 2019-02-22 02:49:40.486764: 
 	std: 0.032516513
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921143, 0.9788714, 0.9019, 0.97885716, 0.97884285]
	train_accs: [0.9921143, 0.9788714, 0.9019, 0.97885716, 0.97884285]
	best_train_sub_head: 0
	worst: 0.9019
	avg: 0.9661171
	best: 0.9921143

Starting e_i: 36
Model ind 685 epoch 36 head B batch: 0 avg loss -2.151759 avg loss no lamb -2.151759 time 2019-02-22 02:49:42.116604
Model ind 685 epoch 36 head B batch: 100 avg loss -2.135039 avg loss no lamb -2.135039 time 2019-02-22 02:50:36.275988
Model ind 685 epoch 36 head B batch: 200 avg loss -2.094546 avg loss no lamb -2.094546 time 2019-02-22 02:51:32.312735
Model ind 685 epoch 36 head B batch: 300 avg loss -2.057214 avg loss no lamb -2.057214 time 2019-02-22 02:52:26.480969
Model ind 685 epoch 36 head B batch: 400 avg loss -2.139449 avg loss no lamb -2.139449 time 2019-02-22 02:53:20.604018
Model ind 685 epoch 36 head B batch: 0 avg loss -2.112477 avg loss no lamb -2.112477 time 2019-02-22 02:54:14.353783
Model ind 685 epoch 36 head B batch: 100 avg loss -2.135646 avg loss no lamb -2.135646 time 2019-02-22 02:55:08.649506
Model ind 685 epoch 36 head B batch: 200 avg loss -2.116231 avg loss no lamb -2.116231 time 2019-02-22 02:56:03.027034
Model ind 685 epoch 36 head B batch: 300 avg loss -2.152139 avg loss no lamb -2.152139 time 2019-02-22 02:56:58.438118
Model ind 685 epoch 36 head B batch: 400 avg loss -2.098216 avg loss no lamb -2.098216 time 2019-02-22 02:57:51.996665
Model ind 685 epoch 36 head A batch: 0 avg loss -2.130872 avg loss no lamb -2.130872 time 2019-02-22 02:58:47.554557
Model ind 685 epoch 36 head A batch: 100 avg loss -2.069238 avg loss no lamb -2.069238 time 2019-02-22 02:59:40.480689
Model ind 685 epoch 36 head A batch: 200 avg loss -2.132660 avg loss no lamb -2.132660 time 2019-02-22 03:00:32.899534
Model ind 685 epoch 36 head A batch: 300 avg loss -2.196826 avg loss no lamb -2.196826 time 2019-02-22 03:01:27.080806
Model ind 685 epoch 36 head A batch: 400 avg loss -2.119586 avg loss no lamb -2.119586 time 2019-02-22 03:02:21.971605
Pre: time 2019-02-22 03:03:29.751828: 
 	std: 0.0327063
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99205714, 0.97867143, 0.9013, 0.97868574, 0.9787286]
	train_accs: [0.99205714, 0.97867143, 0.9013, 0.97868574, 0.9787286]
	best_train_sub_head: 0
	worst: 0.9013
	avg: 0.9658886
	best: 0.99205714

Starting e_i: 37
Model ind 685 epoch 37 head B batch: 0 avg loss -2.155027 avg loss no lamb -2.155027 time 2019-02-22 03:03:31.465564
Model ind 685 epoch 37 head B batch: 100 avg loss -2.113832 avg loss no lamb -2.113832 time 2019-02-22 03:04:25.233458
Model ind 685 epoch 37 head B batch: 200 avg loss -2.159704 avg loss no lamb -2.159704 time 2019-02-22 03:05:19.567610
Model ind 685 epoch 37 head B batch: 300 avg loss -2.105743 avg loss no lamb -2.105743 time 2019-02-22 03:06:13.632646
Model ind 685 epoch 37 head B batch: 400 avg loss -2.111211 avg loss no lamb -2.111211 time 2019-02-22 03:07:07.898822
Model ind 685 epoch 37 head B batch: 0 avg loss -2.164799 avg loss no lamb -2.164799 time 2019-02-22 03:08:01.382927
Model ind 685 epoch 37 head B batch: 100 avg loss -2.110712 avg loss no lamb -2.110712 time 2019-02-22 03:08:54.417357
Model ind 685 epoch 37 head B batch: 200 avg loss -2.038739 avg loss no lamb -2.038739 time 2019-02-22 03:09:48.549976
Model ind 685 epoch 37 head B batch: 300 avg loss -2.110803 avg loss no lamb -2.110803 time 2019-02-22 03:10:43.451704
Model ind 685 epoch 37 head B batch: 400 avg loss -2.080722 avg loss no lamb -2.080722 time 2019-02-22 03:11:37.322335
Model ind 685 epoch 37 head A batch: 0 avg loss -2.155626 avg loss no lamb -2.155626 time 2019-02-22 03:12:31.305088
Model ind 685 epoch 37 head A batch: 100 avg loss -2.178920 avg loss no lamb -2.178920 time 2019-02-22 03:13:24.445048
Model ind 685 epoch 37 head A batch: 200 avg loss -2.192716 avg loss no lamb -2.192716 time 2019-02-22 03:14:18.985583
Model ind 685 epoch 37 head A batch: 300 avg loss -2.165140 avg loss no lamb -2.165140 time 2019-02-22 03:15:13.268708
Model ind 685 epoch 37 head A batch: 400 avg loss -2.111850 avg loss no lamb -2.111850 time 2019-02-22 03:16:07.847703
Pre: time 2019-02-22 03:17:15.638871: 
 	std: 0.032753833
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9788857, 0.9013714, 0.97885716, 0.97884285]
	train_accs: [0.9922857, 0.9788857, 0.9013714, 0.97885716, 0.97884285]
	best_train_sub_head: 0
	worst: 0.9013714
	avg: 0.96604854
	best: 0.9922857

Starting e_i: 38
Model ind 685 epoch 38 head B batch: 0 avg loss -2.179911 avg loss no lamb -2.179911 time 2019-02-22 03:17:18.575859
Model ind 685 epoch 38 head B batch: 100 avg loss -2.150661 avg loss no lamb -2.150661 time 2019-02-22 03:18:11.989157
Model ind 685 epoch 38 head B batch: 200 avg loss -2.109746 avg loss no lamb -2.109746 time 2019-02-22 03:19:05.203945
Model ind 685 epoch 38 head B batch: 300 avg loss -2.125611 avg loss no lamb -2.125611 time 2019-02-22 03:19:58.741116
Model ind 685 epoch 38 head B batch: 400 avg loss -2.104729 avg loss no lamb -2.104729 time 2019-02-22 03:20:51.634961
Model ind 685 epoch 38 head B batch: 0 avg loss -2.137789 avg loss no lamb -2.137789 time 2019-02-22 03:21:45.928259
Model ind 685 epoch 38 head B batch: 100 avg loss -2.121492 avg loss no lamb -2.121492 time 2019-02-22 03:22:40.506306
Model ind 685 epoch 38 head B batch: 200 avg loss -2.124784 avg loss no lamb -2.124784 time 2019-02-22 03:23:33.877633
Model ind 685 epoch 38 head B batch: 300 avg loss -2.116140 avg loss no lamb -2.116140 time 2019-02-22 03:24:28.215705
Model ind 685 epoch 38 head B batch: 400 avg loss -2.109360 avg loss no lamb -2.109360 time 2019-02-22 03:25:21.213470
Model ind 685 epoch 38 head A batch: 0 avg loss -2.168419 avg loss no lamb -2.168419 time 2019-02-22 03:26:13.000783
Model ind 685 epoch 38 head A batch: 100 avg loss -2.122229 avg loss no lamb -2.122229 time 2019-02-22 03:27:06.969553
Model ind 685 epoch 38 head A batch: 200 avg loss -2.140068 avg loss no lamb -2.140068 time 2019-02-22 03:28:01.109590
Model ind 685 epoch 38 head A batch: 300 avg loss -2.159405 avg loss no lamb -2.159405 time 2019-02-22 03:28:57.033364
Model ind 685 epoch 38 head A batch: 400 avg loss -2.087193 avg loss no lamb -2.087193 time 2019-02-22 03:29:51.660148
Pre: time 2019-02-22 03:31:01.660288: 
 	std: 0.032832522
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9919143, 0.9786571, 0.9009, 0.9786571, 0.9786571]
	train_accs: [0.9919143, 0.9786571, 0.9009, 0.9786571, 0.9786571]
	best_train_sub_head: 0
	worst: 0.9009
	avg: 0.9657572
	best: 0.9919143

Starting e_i: 39
Model ind 685 epoch 39 head B batch: 0 avg loss -2.122772 avg loss no lamb -2.122772 time 2019-02-22 03:31:03.227878
Model ind 685 epoch 39 head B batch: 100 avg loss -2.091295 avg loss no lamb -2.091295 time 2019-02-22 03:31:56.281077
Model ind 685 epoch 39 head B batch: 200 avg loss -2.131784 avg loss no lamb -2.131784 time 2019-02-22 03:32:51.235977
Model ind 685 epoch 39 head B batch: 300 avg loss -2.153686 avg loss no lamb -2.153686 time 2019-02-22 03:33:46.452314
Model ind 685 epoch 39 head B batch: 400 avg loss -2.102496 avg loss no lamb -2.102496 time 2019-02-22 03:34:42.704508
Model ind 685 epoch 39 head B batch: 0 avg loss -2.119388 avg loss no lamb -2.119388 time 2019-02-22 03:35:36.901014
Model ind 685 epoch 39 head B batch: 100 avg loss -2.137669 avg loss no lamb -2.137669 time 2019-02-22 03:36:31.974278
Model ind 685 epoch 39 head B batch: 200 avg loss -2.123946 avg loss no lamb -2.123946 time 2019-02-22 03:37:27.262925
Model ind 685 epoch 39 head B batch: 300 avg loss -2.145110 avg loss no lamb -2.145110 time 2019-02-22 03:38:22.803232
Model ind 685 epoch 39 head B batch: 400 avg loss -2.155857 avg loss no lamb -2.155857 time 2019-02-22 03:39:17.225019
Model ind 685 epoch 39 head A batch: 0 avg loss -2.149771 avg loss no lamb -2.149771 time 2019-02-22 03:40:12.292889
Model ind 685 epoch 39 head A batch: 100 avg loss -2.095985 avg loss no lamb -2.095985 time 2019-02-22 03:41:05.852520
Model ind 685 epoch 39 head A batch: 200 avg loss -2.096699 avg loss no lamb -2.096699 time 2019-02-22 03:42:02.123163
Model ind 685 epoch 39 head A batch: 300 avg loss -2.152865 avg loss no lamb -2.152865 time 2019-02-22 03:42:57.452379
Model ind 685 epoch 39 head A batch: 400 avg loss -2.151005 avg loss no lamb -2.151005 time 2019-02-22 03:43:53.896526
Pre: time 2019-02-22 03:45:04.759680: 
 	std: 0.03281575
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9920143, 0.97867143, 0.9009857, 0.9786429, 0.97867143]
	train_accs: [0.9920143, 0.97867143, 0.9009857, 0.9786429, 0.97867143]
	best_train_sub_head: 0
	worst: 0.9009857
	avg: 0.9657971
	best: 0.9920143

Starting e_i: 40
Model ind 685 epoch 40 head B batch: 0 avg loss -2.201130 avg loss no lamb -2.201130 time 2019-02-22 03:45:06.731487
Model ind 685 epoch 40 head B batch: 100 avg loss -2.170319 avg loss no lamb -2.170319 time 2019-02-22 03:46:00.995508
Model ind 685 epoch 40 head B batch: 200 avg loss -2.118196 avg loss no lamb -2.118196 time 2019-02-22 03:46:55.886230
Model ind 685 epoch 40 head B batch: 300 avg loss -2.168272 avg loss no lamb -2.168272 time 2019-02-22 03:47:51.004595
Model ind 685 epoch 40 head B batch: 400 avg loss -2.129554 avg loss no lamb -2.129554 time 2019-02-22 03:48:45.194622
Model ind 685 epoch 40 head B batch: 0 avg loss -2.174796 avg loss no lamb -2.174796 time 2019-02-22 03:49:39.708042
Model ind 685 epoch 40 head B batch: 100 avg loss -2.127860 avg loss no lamb -2.127860 time 2019-02-22 03:50:33.885148
Model ind 685 epoch 40 head B batch: 200 avg loss -2.112226 avg loss no lamb -2.112226 time 2019-02-22 03:51:27.498867
Model ind 685 epoch 40 head B batch: 300 avg loss -2.172250 avg loss no lamb -2.172250 time 2019-02-22 03:52:21.339463
Model ind 685 epoch 40 head B batch: 400 avg loss -2.119619 avg loss no lamb -2.119619 time 2019-02-22 03:53:15.549165
Model ind 685 epoch 40 head A batch: 0 avg loss -2.155933 avg loss no lamb -2.155933 time 2019-02-22 03:54:08.371780
Model ind 685 epoch 40 head A batch: 100 avg loss -2.141926 avg loss no lamb -2.141926 time 2019-02-22 03:55:02.277689
Model ind 685 epoch 40 head A batch: 200 avg loss -2.135985 avg loss no lamb -2.135985 time 2019-02-22 03:55:55.751058
Model ind 685 epoch 40 head A batch: 300 avg loss -2.187824 avg loss no lamb -2.187824 time 2019-02-22 03:56:48.566707
Model ind 685 epoch 40 head A batch: 400 avg loss -2.132698 avg loss no lamb -2.132698 time 2019-02-22 03:57:42.181205
Pre: time 2019-02-22 03:58:51.276542: 
 	std: 0.032760642
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.97891426, 0.9014143, 0.97892857, 0.9789]
	train_accs: [0.99235713, 0.97891426, 0.9014143, 0.97892857, 0.9789]
	best_train_sub_head: 0
	worst: 0.9014143
	avg: 0.9661029
	best: 0.99235713

Starting e_i: 41
Model ind 685 epoch 41 head B batch: 0 avg loss -2.166788 avg loss no lamb -2.166788 time 2019-02-22 03:58:56.824173
Model ind 685 epoch 41 head B batch: 100 avg loss -2.091838 avg loss no lamb -2.091838 time 2019-02-22 03:59:52.954714
Model ind 685 epoch 41 head B batch: 200 avg loss -2.107549 avg loss no lamb -2.107549 time 2019-02-22 04:00:47.603184
Model ind 685 epoch 41 head B batch: 300 avg loss -2.156798 avg loss no lamb -2.156798 time 2019-02-22 04:01:41.217458
Model ind 685 epoch 41 head B batch: 400 avg loss -2.129049 avg loss no lamb -2.129049 time 2019-02-22 04:02:35.440473
Model ind 685 epoch 41 head B batch: 0 avg loss -2.141912 avg loss no lamb -2.141912 time 2019-02-22 04:03:29.427110
Model ind 685 epoch 41 head B batch: 100 avg loss -2.149595 avg loss no lamb -2.149595 time 2019-02-22 04:04:23.710435
Model ind 685 epoch 41 head B batch: 200 avg loss -2.138057 avg loss no lamb -2.138057 time 2019-02-22 04:05:17.768730
Model ind 685 epoch 41 head B batch: 300 avg loss -2.169087 avg loss no lamb -2.169087 time 2019-02-22 04:06:11.511288
Model ind 685 epoch 41 head B batch: 400 avg loss -2.115061 avg loss no lamb -2.115061 time 2019-02-22 04:07:04.643766
Model ind 685 epoch 41 head A batch: 0 avg loss -2.134468 avg loss no lamb -2.134468 time 2019-02-22 04:07:59.368228
Model ind 685 epoch 41 head A batch: 100 avg loss -2.137124 avg loss no lamb -2.137124 time 2019-02-22 04:08:54.487130
Model ind 685 epoch 41 head A batch: 200 avg loss -2.083215 avg loss no lamb -2.083215 time 2019-02-22 04:09:48.711037
Model ind 685 epoch 41 head A batch: 300 avg loss -2.168361 avg loss no lamb -2.168361 time 2019-02-22 04:10:40.946798
Model ind 685 epoch 41 head A batch: 400 avg loss -2.130126 avg loss no lamb -2.130126 time 2019-02-22 04:11:36.187024
Pre: time 2019-02-22 04:12:44.992093: 
 	std: 0.032740053
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.9789429, 0.9014429, 0.9789429, 0.9789429]
	train_accs: [0.9922571, 0.9789429, 0.9014429, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9014429
	avg: 0.96610576
	best: 0.9922571

Starting e_i: 42
Model ind 685 epoch 42 head B batch: 0 avg loss -2.178342 avg loss no lamb -2.178342 time 2019-02-22 04:12:46.898198
Model ind 685 epoch 42 head B batch: 100 avg loss -2.103724 avg loss no lamb -2.103724 time 2019-02-22 04:13:41.288135
Model ind 685 epoch 42 head B batch: 200 avg loss -2.129146 avg loss no lamb -2.129146 time 2019-02-22 04:14:36.452078
Model ind 685 epoch 42 head B batch: 300 avg loss -2.184871 avg loss no lamb -2.184871 time 2019-02-22 04:15:29.008784
Model ind 685 epoch 42 head B batch: 400 avg loss -2.105635 avg loss no lamb -2.105635 time 2019-02-22 04:16:22.963444
Model ind 685 epoch 42 head B batch: 0 avg loss -2.093708 avg loss no lamb -2.093708 time 2019-02-22 04:17:16.235395
Model ind 685 epoch 42 head B batch: 100 avg loss -2.111155 avg loss no lamb -2.111155 time 2019-02-22 04:18:09.522386
Model ind 685 epoch 42 head B batch: 200 avg loss -2.124474 avg loss no lamb -2.124474 time 2019-02-22 04:19:03.250217
Model ind 685 epoch 42 head B batch: 300 avg loss -2.185229 avg loss no lamb -2.185229 time 2019-02-22 04:19:58.170156
Model ind 685 epoch 42 head B batch: 400 avg loss -2.077388 avg loss no lamb -2.077388 time 2019-02-22 04:20:51.844104
Model ind 685 epoch 42 head A batch: 0 avg loss -2.129247 avg loss no lamb -2.129247 time 2019-02-22 04:21:46.513958
Model ind 685 epoch 42 head A batch: 100 avg loss -2.119035 avg loss no lamb -2.119035 time 2019-02-22 04:22:41.529072
Model ind 685 epoch 42 head A batch: 200 avg loss -2.121082 avg loss no lamb -2.121082 time 2019-02-22 04:23:35.117450
Model ind 685 epoch 42 head A batch: 300 avg loss -2.132337 avg loss no lamb -2.132337 time 2019-02-22 04:24:28.722223
Model ind 685 epoch 42 head A batch: 400 avg loss -2.174256 avg loss no lamb -2.174256 time 2019-02-22 04:25:23.019380
Pre: time 2019-02-22 04:26:31.032657: 
 	std: 0.032611616
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.9788, 0.9016714, 0.97882855, 0.9788]
	train_accs: [0.99221426, 0.9788, 0.9016714, 0.97882855, 0.9788]
	best_train_sub_head: 0
	worst: 0.9016714
	avg: 0.96606284
	best: 0.99221426

Starting e_i: 43
Model ind 685 epoch 43 head B batch: 0 avg loss -2.152759 avg loss no lamb -2.152759 time 2019-02-22 04:26:32.742438
Model ind 685 epoch 43 head B batch: 100 avg loss -2.182074 avg loss no lamb -2.182074 time 2019-02-22 04:27:25.716483
Model ind 685 epoch 43 head B batch: 200 avg loss -2.159687 avg loss no lamb -2.159687 time 2019-02-22 04:28:19.631747
Model ind 685 epoch 43 head B batch: 300 avg loss -2.151479 avg loss no lamb -2.151479 time 2019-02-22 04:29:13.030355
Model ind 685 epoch 43 head B batch: 400 avg loss -2.147975 avg loss no lamb -2.147975 time 2019-02-22 04:30:07.381314
Model ind 685 epoch 43 head B batch: 0 avg loss -2.155590 avg loss no lamb -2.155590 time 2019-02-22 04:31:01.251320
Model ind 685 epoch 43 head B batch: 100 avg loss -2.132973 avg loss no lamb -2.132973 time 2019-02-22 04:31:54.918071
Model ind 685 epoch 43 head B batch: 200 avg loss -2.110272 avg loss no lamb -2.110272 time 2019-02-22 04:32:48.312720
Model ind 685 epoch 43 head B batch: 300 avg loss -2.157286 avg loss no lamb -2.157286 time 2019-02-22 04:33:42.520607
Model ind 685 epoch 43 head B batch: 400 avg loss -2.134135 avg loss no lamb -2.134135 time 2019-02-22 04:34:36.203296
Model ind 685 epoch 43 head A batch: 0 avg loss -2.133974 avg loss no lamb -2.133974 time 2019-02-22 04:35:29.170153
Model ind 685 epoch 43 head A batch: 100 avg loss -2.137565 avg loss no lamb -2.137565 time 2019-02-22 04:36:22.341713
Model ind 685 epoch 43 head A batch: 200 avg loss -2.142032 avg loss no lamb -2.142032 time 2019-02-22 04:37:15.313032
Model ind 685 epoch 43 head A batch: 300 avg loss -2.064956 avg loss no lamb -2.064956 time 2019-02-22 04:38:10.099100
Model ind 685 epoch 43 head A batch: 400 avg loss -2.113189 avg loss no lamb -2.113189 time 2019-02-22 04:39:02.694359
Pre: time 2019-02-22 04:40:10.685960: 
 	std: 0.03261347
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.97907144, 0.90187144, 0.9791143, 0.9791]
	train_accs: [0.9923, 0.97907144, 0.90187144, 0.9791143, 0.9791]
	best_train_sub_head: 0
	worst: 0.90187144
	avg: 0.9662914
	best: 0.9923

Starting e_i: 44
Model ind 685 epoch 44 head B batch: 0 avg loss -2.144722 avg loss no lamb -2.144722 time 2019-02-22 04:40:12.111198
Model ind 685 epoch 44 head B batch: 100 avg loss -2.167278 avg loss no lamb -2.167278 time 2019-02-22 04:41:07.791279
Model ind 685 epoch 44 head B batch: 200 avg loss -2.117420 avg loss no lamb -2.117420 time 2019-02-22 04:42:02.888383
Model ind 685 epoch 44 head B batch: 300 avg loss -2.153783 avg loss no lamb -2.153783 time 2019-02-22 04:42:57.217173
Model ind 685 epoch 44 head B batch: 400 avg loss -2.109208 avg loss no lamb -2.109208 time 2019-02-22 04:43:52.660902
Model ind 685 epoch 44 head B batch: 0 avg loss -2.158769 avg loss no lamb -2.158769 time 2019-02-22 04:44:47.348172
Model ind 685 epoch 44 head B batch: 100 avg loss -2.149554 avg loss no lamb -2.149554 time 2019-02-22 04:45:40.585726
Model ind 685 epoch 44 head B batch: 200 avg loss -2.093623 avg loss no lamb -2.093623 time 2019-02-22 04:46:35.584411
Model ind 685 epoch 44 head B batch: 300 avg loss -2.178469 avg loss no lamb -2.178469 time 2019-02-22 04:47:31.182610
Model ind 685 epoch 44 head B batch: 400 avg loss -2.118932 avg loss no lamb -2.118932 time 2019-02-22 04:48:24.507867
Model ind 685 epoch 44 head A batch: 0 avg loss -2.141584 avg loss no lamb -2.141584 time 2019-02-22 04:49:17.631588
Model ind 685 epoch 44 head A batch: 100 avg loss -2.092777 avg loss no lamb -2.092777 time 2019-02-22 04:50:10.658075
Model ind 685 epoch 44 head A batch: 200 avg loss -2.167769 avg loss no lamb -2.167769 time 2019-02-22 04:51:05.494542
Model ind 685 epoch 44 head A batch: 300 avg loss -2.185884 avg loss no lamb -2.185884 time 2019-02-22 04:51:59.625205
Model ind 685 epoch 44 head A batch: 400 avg loss -2.120493 avg loss no lamb -2.120493 time 2019-02-22 04:52:53.846482
Pre: time 2019-02-22 04:54:02.274643: 
 	std: 0.032926343
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9920857, 0.9787286, 0.9007857, 0.97875714, 0.97875714]
	train_accs: [0.9920857, 0.9787286, 0.9007857, 0.97875714, 0.97875714]
	best_train_sub_head: 0
	worst: 0.9007857
	avg: 0.9658228
	best: 0.9920857

Starting e_i: 45
Model ind 685 epoch 45 head B batch: 0 avg loss -2.169521 avg loss no lamb -2.169521 time 2019-02-22 04:54:03.897642
Model ind 685 epoch 45 head B batch: 100 avg loss -2.110083 avg loss no lamb -2.110083 time 2019-02-22 04:54:56.907130
Model ind 685 epoch 45 head B batch: 200 avg loss -2.108794 avg loss no lamb -2.108794 time 2019-02-22 04:55:51.081772
Model ind 685 epoch 45 head B batch: 300 avg loss -2.154815 avg loss no lamb -2.154815 time 2019-02-22 04:56:44.796856
Model ind 685 epoch 45 head B batch: 400 avg loss -2.140474 avg loss no lamb -2.140474 time 2019-02-22 04:57:39.835268
Model ind 685 epoch 45 head B batch: 0 avg loss -2.166872 avg loss no lamb -2.166872 time 2019-02-22 04:58:34.694016
Model ind 685 epoch 45 head B batch: 100 avg loss -2.139554 avg loss no lamb -2.139554 time 2019-02-22 04:59:29.221385
Model ind 685 epoch 45 head B batch: 200 avg loss -2.104473 avg loss no lamb -2.104473 time 2019-02-22 05:00:25.834097
Model ind 685 epoch 45 head B batch: 300 avg loss -2.151873 avg loss no lamb -2.151873 time 2019-02-22 05:01:21.330134
Model ind 685 epoch 45 head B batch: 400 avg loss -2.113986 avg loss no lamb -2.113986 time 2019-02-22 05:02:15.321127
Model ind 685 epoch 45 head A batch: 0 avg loss -2.162805 avg loss no lamb -2.162805 time 2019-02-22 05:03:10.171198
Model ind 685 epoch 45 head A batch: 100 avg loss -2.112607 avg loss no lamb -2.112607 time 2019-02-22 05:04:03.111615
Model ind 685 epoch 45 head A batch: 200 avg loss -2.074261 avg loss no lamb -2.074261 time 2019-02-22 05:04:58.234047
Model ind 685 epoch 45 head A batch: 300 avg loss -2.127061 avg loss no lamb -2.127061 time 2019-02-22 05:05:51.994114
Model ind 685 epoch 45 head A batch: 400 avg loss -2.131910 avg loss no lamb -2.131910 time 2019-02-22 05:06:45.692844
Pre: time 2019-02-22 05:07:55.435141: 
 	std: 0.03281911
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99215716, 0.9787857, 0.9011286, 0.9788, 0.9788714]
	train_accs: [0.99215716, 0.9787857, 0.9011286, 0.9788, 0.9788714]
	best_train_sub_head: 0
	worst: 0.9011286
	avg: 0.9659486
	best: 0.99215716

Starting e_i: 46
Model ind 685 epoch 46 head B batch: 0 avg loss -2.153409 avg loss no lamb -2.153409 time 2019-02-22 05:07:57.105587
Model ind 685 epoch 46 head B batch: 100 avg loss -2.094752 avg loss no lamb -2.094752 time 2019-02-22 05:08:50.641199
Model ind 685 epoch 46 head B batch: 200 avg loss -2.130966 avg loss no lamb -2.130966 time 2019-02-22 05:09:46.401290
Model ind 685 epoch 46 head B batch: 300 avg loss -2.169826 avg loss no lamb -2.169826 time 2019-02-22 05:10:39.721343
Model ind 685 epoch 46 head B batch: 400 avg loss -2.130796 avg loss no lamb -2.130796 time 2019-02-22 05:11:33.506616
Model ind 685 epoch 46 head B batch: 0 avg loss -2.152571 avg loss no lamb -2.152571 time 2019-02-22 05:12:26.512845
Model ind 685 epoch 46 head B batch: 100 avg loss -2.137600 avg loss no lamb -2.137600 time 2019-02-22 05:13:19.481633
Model ind 685 epoch 46 head B batch: 200 avg loss -2.136895 avg loss no lamb -2.136895 time 2019-02-22 05:14:13.773884
Model ind 685 epoch 46 head B batch: 300 avg loss -2.180852 avg loss no lamb -2.180852 time 2019-02-22 05:15:07.541859
Model ind 685 epoch 46 head B batch: 400 avg loss -2.152938 avg loss no lamb -2.152938 time 2019-02-22 05:16:02.072372
Model ind 685 epoch 46 head A batch: 0 avg loss -2.168105 avg loss no lamb -2.168105 time 2019-02-22 05:16:56.209148
Model ind 685 epoch 46 head A batch: 100 avg loss -2.159207 avg loss no lamb -2.159207 time 2019-02-22 05:17:51.044347
Model ind 685 epoch 46 head A batch: 200 avg loss -2.174589 avg loss no lamb -2.174589 time 2019-02-22 05:18:44.466925
Model ind 685 epoch 46 head A batch: 300 avg loss -2.189213 avg loss no lamb -2.189213 time 2019-02-22 05:19:38.597909
Model ind 685 epoch 46 head A batch: 400 avg loss -2.128700 avg loss no lamb -2.128700 time 2019-02-22 05:20:32.202332
Pre: time 2019-02-22 05:21:40.336907: 
 	std: 0.032963477
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99175715, 0.9785, 0.9004143, 0.9785143, 0.9785]
	train_accs: [0.99175715, 0.9785, 0.9004143, 0.9785143, 0.9785]
	best_train_sub_head: 0
	worst: 0.9004143
	avg: 0.9655372
	best: 0.99175715

Starting e_i: 47
Model ind 685 epoch 47 head B batch: 0 avg loss -2.174031 avg loss no lamb -2.174031 time 2019-02-22 05:21:41.870790
Model ind 685 epoch 47 head B batch: 100 avg loss -2.108865 avg loss no lamb -2.108865 time 2019-02-22 05:22:36.731316
Model ind 685 epoch 47 head B batch: 200 avg loss -2.129852 avg loss no lamb -2.129852 time 2019-02-22 05:23:33.700696
Model ind 685 epoch 47 head B batch: 300 avg loss -2.165785 avg loss no lamb -2.165785 time 2019-02-22 05:24:27.743581
Model ind 685 epoch 47 head B batch: 400 avg loss -2.125124 avg loss no lamb -2.125124 time 2019-02-22 05:25:21.127942
Model ind 685 epoch 47 head B batch: 0 avg loss -2.185280 avg loss no lamb -2.185280 time 2019-02-22 05:26:15.936082
Model ind 685 epoch 47 head B batch: 100 avg loss -2.178124 avg loss no lamb -2.178124 time 2019-02-22 05:27:10.253337
Model ind 685 epoch 47 head B batch: 200 avg loss -2.124589 avg loss no lamb -2.124589 time 2019-02-22 05:28:03.949842
Model ind 685 epoch 47 head B batch: 300 avg loss -2.161557 avg loss no lamb -2.161557 time 2019-02-22 05:28:58.149809
Model ind 685 epoch 47 head B batch: 400 avg loss -2.142773 avg loss no lamb -2.142773 time 2019-02-22 05:29:52.246217
Model ind 685 epoch 47 head A batch: 0 avg loss -2.153780 avg loss no lamb -2.153780 time 2019-02-22 05:30:47.536677
Model ind 685 epoch 47 head A batch: 100 avg loss -2.203067 avg loss no lamb -2.203067 time 2019-02-22 05:31:42.386628
Model ind 685 epoch 47 head A batch: 200 avg loss -2.150774 avg loss no lamb -2.150774 time 2019-02-22 05:32:37.060477
Model ind 685 epoch 47 head A batch: 300 avg loss -2.180669 avg loss no lamb -2.180669 time 2019-02-22 05:33:32.513253
Model ind 685 epoch 47 head A batch: 400 avg loss -2.128381 avg loss no lamb -2.128381 time 2019-02-22 05:34:26.937805
Pre: time 2019-02-22 05:35:35.476704: 
 	std: 0.032839425
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.97891426, 0.90117145, 0.97891426, 0.9789]
	train_accs: [0.9922571, 0.97891426, 0.90117145, 0.97891426, 0.9789]
	best_train_sub_head: 0
	worst: 0.90117145
	avg: 0.96603143
	best: 0.9922571

Starting e_i: 48
Model ind 685 epoch 48 head B batch: 0 avg loss -2.179949 avg loss no lamb -2.179949 time 2019-02-22 05:35:36.938967
Model ind 685 epoch 48 head B batch: 100 avg loss -2.140287 avg loss no lamb -2.140287 time 2019-02-22 05:36:30.591128
Model ind 685 epoch 48 head B batch: 200 avg loss -2.118639 avg loss no lamb -2.118639 time 2019-02-22 05:37:26.140395
Model ind 685 epoch 48 head B batch: 300 avg loss -2.160708 avg loss no lamb -2.160708 time 2019-02-22 05:38:20.781754
Model ind 685 epoch 48 head B batch: 400 avg loss -2.138638 avg loss no lamb -2.138638 time 2019-02-22 05:39:14.321194
Model ind 685 epoch 48 head B batch: 0 avg loss -2.194078 avg loss no lamb -2.194078 time 2019-02-22 05:40:08.825309
Model ind 685 epoch 48 head B batch: 100 avg loss -2.091585 avg loss no lamb -2.091585 time 2019-02-22 05:41:01.631649
Model ind 685 epoch 48 head B batch: 200 avg loss -2.125627 avg loss no lamb -2.125627 time 2019-02-22 05:41:55.130410
Model ind 685 epoch 48 head B batch: 300 avg loss -2.138028 avg loss no lamb -2.138028 time 2019-02-22 05:42:48.166924
Model ind 685 epoch 48 head B batch: 400 avg loss -2.117820 avg loss no lamb -2.117820 time 2019-02-22 05:43:40.685632
Model ind 685 epoch 48 head A batch: 0 avg loss -2.137745 avg loss no lamb -2.137745 time 2019-02-22 05:44:35.461355
Model ind 685 epoch 48 head A batch: 100 avg loss -2.118027 avg loss no lamb -2.118027 time 2019-02-22 05:45:28.801910
Model ind 685 epoch 48 head A batch: 200 avg loss -2.124914 avg loss no lamb -2.124914 time 2019-02-22 05:46:23.867564
Model ind 685 epoch 48 head A batch: 300 avg loss -2.152801 avg loss no lamb -2.152801 time 2019-02-22 05:47:18.523980
Model ind 685 epoch 48 head A batch: 400 avg loss -2.093783 avg loss no lamb -2.093783 time 2019-02-22 05:48:12.758498
Pre: time 2019-02-22 05:49:23.213929: 
 	std: 0.032759283
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99215716, 0.97884285, 0.9012857, 0.9788, 0.97884285]
	train_accs: [0.99215716, 0.97884285, 0.9012857, 0.9788, 0.97884285]
	best_train_sub_head: 0
	worst: 0.9012857
	avg: 0.96598566
	best: 0.99215716

Starting e_i: 49
Model ind 685 epoch 49 head B batch: 0 avg loss -2.143044 avg loss no lamb -2.143044 time 2019-02-22 05:49:24.714648
Model ind 685 epoch 49 head B batch: 100 avg loss -2.127159 avg loss no lamb -2.127159 time 2019-02-22 05:50:19.208672
Model ind 685 epoch 49 head B batch: 200 avg loss -2.121626 avg loss no lamb -2.121626 time 2019-02-22 05:51:13.734855
Model ind 685 epoch 49 head B batch: 300 avg loss -2.132685 avg loss no lamb -2.132685 time 2019-02-22 05:52:07.941739
Model ind 685 epoch 49 head B batch: 400 avg loss -2.127821 avg loss no lamb -2.127821 time 2019-02-22 05:53:03.266732
Model ind 685 epoch 49 head B batch: 0 avg loss -2.167156 avg loss no lamb -2.167156 time 2019-02-22 05:53:57.207563
Model ind 685 epoch 49 head B batch: 100 avg loss -2.127559 avg loss no lamb -2.127559 time 2019-02-22 05:54:52.142160
Model ind 685 epoch 49 head B batch: 200 avg loss -2.097182 avg loss no lamb -2.097182 time 2019-02-22 05:55:45.376977
Model ind 685 epoch 49 head B batch: 300 avg loss -2.167976 avg loss no lamb -2.167976 time 2019-02-22 05:56:38.747219
Model ind 685 epoch 49 head B batch: 400 avg loss -2.066676 avg loss no lamb -2.066676 time 2019-02-22 05:57:32.065116
Model ind 685 epoch 49 head A batch: 0 avg loss -2.127626 avg loss no lamb -2.127626 time 2019-02-22 05:58:25.500462
Model ind 685 epoch 49 head A batch: 100 avg loss -2.127459 avg loss no lamb -2.127459 time 2019-02-22 05:59:19.782028
Model ind 685 epoch 49 head A batch: 200 avg loss -2.089522 avg loss no lamb -2.089522 time 2019-02-22 06:00:13.522893
Model ind 685 epoch 49 head A batch: 300 avg loss -2.135547 avg loss no lamb -2.135547 time 2019-02-22 06:01:07.575444
Model ind 685 epoch 49 head A batch: 400 avg loss -2.139208 avg loss no lamb -2.139208 time 2019-02-22 06:02:01.034998
Pre: time 2019-02-22 06:03:09.206666: 
 	std: 0.032698464
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9788857, 0.9015286, 0.9788857, 0.9789]
	train_accs: [0.9922857, 0.9788857, 0.9015286, 0.9788857, 0.9789]
	best_train_sub_head: 0
	worst: 0.9015286
	avg: 0.9660972
	best: 0.9922857

Starting e_i: 50
Model ind 685 epoch 50 head B batch: 0 avg loss -2.149935 avg loss no lamb -2.149935 time 2019-02-22 06:03:10.876153
Model ind 685 epoch 50 head B batch: 100 avg loss -2.156422 avg loss no lamb -2.156422 time 2019-02-22 06:04:05.289622
Model ind 685 epoch 50 head B batch: 200 avg loss -2.129320 avg loss no lamb -2.129320 time 2019-02-22 06:04:59.755811
Model ind 685 epoch 50 head B batch: 300 avg loss -2.133476 avg loss no lamb -2.133476 time 2019-02-22 06:05:53.159046
Model ind 685 epoch 50 head B batch: 400 avg loss -2.122637 avg loss no lamb -2.122637 time 2019-02-22 06:06:46.555969
Model ind 685 epoch 50 head B batch: 0 avg loss -2.171818 avg loss no lamb -2.171818 time 2019-02-22 06:07:40.904947
Model ind 685 epoch 50 head B batch: 100 avg loss -2.187249 avg loss no lamb -2.187249 time 2019-02-22 06:08:34.872488
Model ind 685 epoch 50 head B batch: 200 avg loss -2.130130 avg loss no lamb -2.130130 time 2019-02-22 06:09:27.525355
Model ind 685 epoch 50 head B batch: 300 avg loss -2.175313 avg loss no lamb -2.175313 time 2019-02-22 06:10:22.814489
Model ind 685 epoch 50 head B batch: 400 avg loss -2.130801 avg loss no lamb -2.130801 time 2019-02-22 06:11:16.540922
Model ind 685 epoch 50 head A batch: 0 avg loss -2.120052 avg loss no lamb -2.120052 time 2019-02-22 06:12:10.994729
Model ind 685 epoch 50 head A batch: 100 avg loss -2.124045 avg loss no lamb -2.124045 time 2019-02-22 06:13:05.623009
Model ind 685 epoch 50 head A batch: 200 avg loss -2.069555 avg loss no lamb -2.069555 time 2019-02-22 06:14:00.800765
Model ind 685 epoch 50 head A batch: 300 avg loss -2.174683 avg loss no lamb -2.174683 time 2019-02-22 06:14:54.256471
Model ind 685 epoch 50 head A batch: 400 avg loss -2.162393 avg loss no lamb -2.162393 time 2019-02-22 06:15:48.430085
Pre: time 2019-02-22 06:16:56.986562: 
 	std: 0.032596055
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9787857, 0.9017286, 0.9788, 0.9787857]
	train_accs: [0.9922857, 0.9787857, 0.9017286, 0.9788, 0.9787857]
	best_train_sub_head: 0
	worst: 0.9017286
	avg: 0.96607715
	best: 0.9922857

Starting e_i: 51
Model ind 685 epoch 51 head B batch: 0 avg loss -2.139204 avg loss no lamb -2.139204 time 2019-02-22 06:16:58.559094
Model ind 685 epoch 51 head B batch: 100 avg loss -2.145767 avg loss no lamb -2.145767 time 2019-02-22 06:17:51.547194
Model ind 685 epoch 51 head B batch: 200 avg loss -2.074535 avg loss no lamb -2.074535 time 2019-02-22 06:18:46.801543
Model ind 685 epoch 51 head B batch: 300 avg loss -2.154607 avg loss no lamb -2.154607 time 2019-02-22 06:19:41.971643
Model ind 685 epoch 51 head B batch: 400 avg loss -2.131760 avg loss no lamb -2.131760 time 2019-02-22 06:20:36.337945
Model ind 685 epoch 51 head B batch: 0 avg loss -2.158525 avg loss no lamb -2.158525 time 2019-02-22 06:21:29.799663
Model ind 685 epoch 51 head B batch: 100 avg loss -2.128698 avg loss no lamb -2.128698 time 2019-02-22 06:22:22.090186
Model ind 685 epoch 51 head B batch: 200 avg loss -2.087569 avg loss no lamb -2.087569 time 2019-02-22 06:23:14.911518
Model ind 685 epoch 51 head B batch: 300 avg loss -2.170088 avg loss no lamb -2.170088 time 2019-02-22 06:24:08.084091
Model ind 685 epoch 51 head B batch: 400 avg loss -2.155482 avg loss no lamb -2.155482 time 2019-02-22 06:25:02.565432
Model ind 685 epoch 51 head A batch: 0 avg loss -2.125696 avg loss no lamb -2.125696 time 2019-02-22 06:25:56.702980
Model ind 685 epoch 51 head A batch: 100 avg loss -2.134972 avg loss no lamb -2.134972 time 2019-02-22 06:26:49.404433
Model ind 685 epoch 51 head A batch: 200 avg loss -2.099569 avg loss no lamb -2.099569 time 2019-02-22 06:27:44.498688
Model ind 685 epoch 51 head A batch: 300 avg loss -2.161547 avg loss no lamb -2.161547 time 2019-02-22 06:28:39.870180
Model ind 685 epoch 51 head A batch: 400 avg loss -2.184627 avg loss no lamb -2.184627 time 2019-02-22 06:29:35.458942
Pre: time 2019-02-22 06:30:45.446408: 
 	std: 0.032603562
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789571, 0.9018286, 0.9789714, 0.9789857]
	train_accs: [0.9923143, 0.9789571, 0.9018286, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9018286
	avg: 0.96621144
	best: 0.9923143

Starting e_i: 52
Model ind 685 epoch 52 head B batch: 0 avg loss -2.162578 avg loss no lamb -2.162578 time 2019-02-22 06:30:47.139994
Model ind 685 epoch 52 head B batch: 100 avg loss -2.136946 avg loss no lamb -2.136946 time 2019-02-22 06:31:42.057331
Model ind 685 epoch 52 head B batch: 200 avg loss -2.099706 avg loss no lamb -2.099706 time 2019-02-22 06:32:36.665538
Model ind 685 epoch 52 head B batch: 300 avg loss -2.191254 avg loss no lamb -2.191254 time 2019-02-22 06:33:31.398085
Model ind 685 epoch 52 head B batch: 400 avg loss -2.096371 avg loss no lamb -2.096371 time 2019-02-22 06:34:26.219649
Model ind 685 epoch 52 head B batch: 0 avg loss -2.168919 avg loss no lamb -2.168919 time 2019-02-22 06:35:19.993041
Model ind 685 epoch 52 head B batch: 100 avg loss -2.171307 avg loss no lamb -2.171307 time 2019-02-22 06:36:14.037812
Model ind 685 epoch 52 head B batch: 200 avg loss -2.123154 avg loss no lamb -2.123154 time 2019-02-22 06:37:06.411780
Model ind 685 epoch 52 head B batch: 300 avg loss -2.163681 avg loss no lamb -2.163681 time 2019-02-22 06:38:00.298735
Model ind 685 epoch 52 head B batch: 400 avg loss -2.137084 avg loss no lamb -2.137084 time 2019-02-22 06:38:53.759419
Model ind 685 epoch 52 head A batch: 0 avg loss -2.168353 avg loss no lamb -2.168353 time 2019-02-22 06:39:47.567250
Model ind 685 epoch 52 head A batch: 100 avg loss -2.155049 avg loss no lamb -2.155049 time 2019-02-22 06:40:41.996379
Model ind 685 epoch 52 head A batch: 200 avg loss -2.125017 avg loss no lamb -2.125017 time 2019-02-22 06:41:36.705208
Model ind 685 epoch 52 head A batch: 300 avg loss -2.158743 avg loss no lamb -2.158743 time 2019-02-22 06:42:30.296598
Model ind 685 epoch 52 head A batch: 400 avg loss -2.190545 avg loss no lamb -2.190545 time 2019-02-22 06:43:25.242589
Pre: time 2019-02-22 06:44:33.236165: 
 	std: 0.032571074
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.97875714, 0.9017571, 0.9787857, 0.9788]
	train_accs: [0.99221426, 0.97875714, 0.9017571, 0.9787857, 0.9788]
	best_train_sub_head: 0
	worst: 0.9017571
	avg: 0.96606284
	best: 0.99221426

Starting e_i: 53
Model ind 685 epoch 53 head B batch: 0 avg loss -2.182646 avg loss no lamb -2.182646 time 2019-02-22 06:44:34.901953
Model ind 685 epoch 53 head B batch: 100 avg loss -2.175871 avg loss no lamb -2.175871 time 2019-02-22 06:45:30.608182
Model ind 685 epoch 53 head B batch: 200 avg loss -2.123594 avg loss no lamb -2.123594 time 2019-02-22 06:46:25.398683
Model ind 685 epoch 53 head B batch: 300 avg loss -2.179497 avg loss no lamb -2.179497 time 2019-02-22 06:47:21.213276
Model ind 685 epoch 53 head B batch: 400 avg loss -2.128494 avg loss no lamb -2.128494 time 2019-02-22 06:48:15.501490
Model ind 685 epoch 53 head B batch: 0 avg loss -2.171791 avg loss no lamb -2.171791 time 2019-02-22 06:49:09.605765
Model ind 685 epoch 53 head B batch: 100 avg loss -2.129779 avg loss no lamb -2.129779 time 2019-02-22 06:50:03.969476
Model ind 685 epoch 53 head B batch: 200 avg loss -2.120126 avg loss no lamb -2.120126 time 2019-02-22 06:50:57.303691
Model ind 685 epoch 53 head B batch: 300 avg loss -2.162142 avg loss no lamb -2.162142 time 2019-02-22 06:51:50.703433
Model ind 685 epoch 53 head B batch: 400 avg loss -2.141255 avg loss no lamb -2.141255 time 2019-02-22 06:52:44.552096
Model ind 685 epoch 53 head A batch: 0 avg loss -2.150307 avg loss no lamb -2.150307 time 2019-02-22 06:53:38.419615
Model ind 685 epoch 53 head A batch: 100 avg loss -2.141017 avg loss no lamb -2.141017 time 2019-02-22 06:54:31.502917
Model ind 685 epoch 53 head A batch: 200 avg loss -2.157617 avg loss no lamb -2.157617 time 2019-02-22 06:55:24.909692
Model ind 685 epoch 53 head A batch: 300 avg loss -2.182635 avg loss no lamb -2.182635 time 2019-02-22 06:56:19.149724
Model ind 685 epoch 53 head A batch: 400 avg loss -2.125483 avg loss no lamb -2.125483 time 2019-02-22 06:57:12.682795
Pre: time 2019-02-22 06:58:20.449029: 
 	std: 0.032557316
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921857, 0.97882855, 0.9018143, 0.97884285, 0.97884285]
	train_accs: [0.9921857, 0.97882855, 0.9018143, 0.97884285, 0.97884285]
	best_train_sub_head: 0
	worst: 0.9018143
	avg: 0.9661028
	best: 0.9921857

Starting e_i: 54
Model ind 685 epoch 54 head B batch: 0 avg loss -2.160304 avg loss no lamb -2.160304 time 2019-02-22 06:58:22.067109
Model ind 685 epoch 54 head B batch: 100 avg loss -2.193262 avg loss no lamb -2.193262 time 2019-02-22 06:59:14.644421
Model ind 685 epoch 54 head B batch: 200 avg loss -2.146793 avg loss no lamb -2.146793 time 2019-02-22 07:00:08.184280
Model ind 685 epoch 54 head B batch: 300 avg loss -2.165186 avg loss no lamb -2.165186 time 2019-02-22 07:00:59.973463
Model ind 685 epoch 54 head B batch: 400 avg loss -2.174664 avg loss no lamb -2.174664 time 2019-02-22 07:01:53.349732
Model ind 685 epoch 54 head B batch: 0 avg loss -2.211630 avg loss no lamb -2.211630 time 2019-02-22 07:02:46.288318
Model ind 685 epoch 54 head B batch: 100 avg loss -2.077724 avg loss no lamb -2.077724 time 2019-02-22 07:03:38.994910
Model ind 685 epoch 54 head B batch: 200 avg loss -2.115101 avg loss no lamb -2.115101 time 2019-02-22 07:04:32.752798
Model ind 685 epoch 54 head B batch: 300 avg loss -2.158433 avg loss no lamb -2.158433 time 2019-02-22 07:05:26.552502
Model ind 685 epoch 54 head B batch: 400 avg loss -2.151304 avg loss no lamb -2.151304 time 2019-02-22 07:06:20.013158
Model ind 685 epoch 54 head A batch: 0 avg loss -2.159808 avg loss no lamb -2.159808 time 2019-02-22 07:07:13.033935
Model ind 685 epoch 54 head A batch: 100 avg loss -2.118418 avg loss no lamb -2.118418 time 2019-02-22 07:08:05.550827
Model ind 685 epoch 54 head A batch: 200 avg loss -2.128264 avg loss no lamb -2.128264 time 2019-02-22 07:08:59.729456
Model ind 685 epoch 54 head A batch: 300 avg loss -2.157765 avg loss no lamb -2.157765 time 2019-02-22 07:09:55.402015
Model ind 685 epoch 54 head A batch: 400 avg loss -2.160540 avg loss no lamb -2.160540 time 2019-02-22 07:10:50.585216
Pre: time 2019-02-22 07:11:59.779300: 
 	std: 0.032697413
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99215716, 0.97875714, 0.9013857, 0.9787143, 0.9787286]
	train_accs: [0.99215716, 0.97875714, 0.9013857, 0.9787143, 0.9787286]
	best_train_sub_head: 0
	worst: 0.9013857
	avg: 0.9659486
	best: 0.99215716

Starting e_i: 55
Model ind 685 epoch 55 head B batch: 0 avg loss -2.193268 avg loss no lamb -2.193268 time 2019-02-22 07:12:01.312654
Model ind 685 epoch 55 head B batch: 100 avg loss -2.182315 avg loss no lamb -2.182315 time 2019-02-22 07:12:57.184677
Model ind 685 epoch 55 head B batch: 200 avg loss -2.114795 avg loss no lamb -2.114795 time 2019-02-22 07:13:52.169107
Model ind 685 epoch 55 head B batch: 300 avg loss -2.177623 avg loss no lamb -2.177623 time 2019-02-22 07:14:47.523111
Model ind 685 epoch 55 head B batch: 400 avg loss -2.135378 avg loss no lamb -2.135378 time 2019-02-22 07:15:41.463024
Model ind 685 epoch 55 head B batch: 0 avg loss -2.170152 avg loss no lamb -2.170152 time 2019-02-22 07:16:36.851729
Model ind 685 epoch 55 head B batch: 100 avg loss -2.104939 avg loss no lamb -2.104939 time 2019-02-22 07:17:31.209454
Model ind 685 epoch 55 head B batch: 200 avg loss -2.128729 avg loss no lamb -2.128729 time 2019-02-22 07:18:24.931886
Model ind 685 epoch 55 head B batch: 300 avg loss -2.176938 avg loss no lamb -2.176938 time 2019-02-22 07:19:19.062470
Model ind 685 epoch 55 head B batch: 400 avg loss -2.124223 avg loss no lamb -2.124223 time 2019-02-22 07:20:14.329871
Model ind 685 epoch 55 head A batch: 0 avg loss -2.167899 avg loss no lamb -2.167899 time 2019-02-22 07:21:08.276383
Model ind 685 epoch 55 head A batch: 100 avg loss -2.142168 avg loss no lamb -2.142168 time 2019-02-22 07:22:01.928700
Model ind 685 epoch 55 head A batch: 200 avg loss -2.115129 avg loss no lamb -2.115129 time 2019-02-22 07:22:57.187258
Model ind 685 epoch 55 head A batch: 300 avg loss -2.146771 avg loss no lamb -2.146771 time 2019-02-22 07:23:52.477493
Model ind 685 epoch 55 head A batch: 400 avg loss -2.148220 avg loss no lamb -2.148220 time 2019-02-22 07:24:46.881527
Pre: time 2019-02-22 07:25:54.081574: 
 	std: 0.03297608
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.9788143, 0.90075713, 0.9788143, 0.97884285]
	train_accs: [0.99221426, 0.9788143, 0.90075713, 0.9788143, 0.97884285]
	best_train_sub_head: 0
	worst: 0.90075713
	avg: 0.9658886
	best: 0.99221426

Starting e_i: 56
Model ind 685 epoch 56 head B batch: 0 avg loss -2.134294 avg loss no lamb -2.134294 time 2019-02-22 07:25:55.744073
Model ind 685 epoch 56 head B batch: 100 avg loss -2.152690 avg loss no lamb -2.152690 time 2019-02-22 07:26:50.112509
Model ind 685 epoch 56 head B batch: 200 avg loss -2.135263 avg loss no lamb -2.135263 time 2019-02-22 07:27:44.081240
Model ind 685 epoch 56 head B batch: 300 avg loss -2.203907 avg loss no lamb -2.203907 time 2019-02-22 07:28:37.247530
Model ind 685 epoch 56 head B batch: 400 avg loss -2.158595 avg loss no lamb -2.158595 time 2019-02-22 07:29:31.286906
Model ind 685 epoch 56 head B batch: 0 avg loss -2.134268 avg loss no lamb -2.134268 time 2019-02-22 07:30:24.029978
Model ind 685 epoch 56 head B batch: 100 avg loss -2.135479 avg loss no lamb -2.135479 time 2019-02-22 07:31:17.657959
Model ind 685 epoch 56 head B batch: 200 avg loss -2.147886 avg loss no lamb -2.147886 time 2019-02-22 07:32:10.357939
Model ind 685 epoch 56 head B batch: 300 avg loss -2.169535 avg loss no lamb -2.169535 time 2019-02-22 07:33:03.443842
Model ind 685 epoch 56 head B batch: 400 avg loss -2.181005 avg loss no lamb -2.181005 time 2019-02-22 07:33:57.000096
Model ind 685 epoch 56 head A batch: 0 avg loss -2.196075 avg loss no lamb -2.196075 time 2019-02-22 07:34:50.106822
Model ind 685 epoch 56 head A batch: 100 avg loss -2.157645 avg loss no lamb -2.157645 time 2019-02-22 07:35:41.791450
Model ind 685 epoch 56 head A batch: 200 avg loss -2.103659 avg loss no lamb -2.103659 time 2019-02-22 07:36:34.286801
Model ind 685 epoch 56 head A batch: 300 avg loss -2.193856 avg loss no lamb -2.193856 time 2019-02-22 07:37:28.310469
Model ind 685 epoch 56 head A batch: 400 avg loss -2.097681 avg loss no lamb -2.097681 time 2019-02-22 07:38:22.680355
Pre: time 2019-02-22 07:39:30.466652: 
 	std: 0.03266782
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9920286, 0.9787286, 0.9014, 0.9787143, 0.9787143]
	train_accs: [0.9920286, 0.9787286, 0.9014, 0.9787143, 0.9787143]
	best_train_sub_head: 0
	worst: 0.9014
	avg: 0.9659172
	best: 0.9920286

Starting e_i: 57
Model ind 685 epoch 57 head B batch: 0 avg loss -2.118745 avg loss no lamb -2.118745 time 2019-02-22 07:39:31.959041
Model ind 685 epoch 57 head B batch: 100 avg loss -2.154265 avg loss no lamb -2.154265 time 2019-02-22 07:40:25.353859
Model ind 685 epoch 57 head B batch: 200 avg loss -2.128982 avg loss no lamb -2.128982 time 2019-02-22 07:41:20.647284
Model ind 685 epoch 57 head B batch: 300 avg loss -2.165417 avg loss no lamb -2.165417 time 2019-02-22 07:42:14.291863
Model ind 685 epoch 57 head B batch: 400 avg loss -2.133265 avg loss no lamb -2.133265 time 2019-02-22 07:43:08.973901
Model ind 685 epoch 57 head B batch: 0 avg loss -2.162504 avg loss no lamb -2.162504 time 2019-02-22 07:44:03.437603
Model ind 685 epoch 57 head B batch: 100 avg loss -2.119035 avg loss no lamb -2.119035 time 2019-02-22 07:44:58.161561
Model ind 685 epoch 57 head B batch: 200 avg loss -2.075553 avg loss no lamb -2.075553 time 2019-02-22 07:45:52.615691
Model ind 685 epoch 57 head B batch: 300 avg loss -2.173986 avg loss no lamb -2.173986 time 2019-02-22 07:46:46.236515
Model ind 685 epoch 57 head B batch: 400 avg loss -2.130612 avg loss no lamb -2.130612 time 2019-02-22 07:47:40.084599
Model ind 685 epoch 57 head A batch: 0 avg loss -2.152255 avg loss no lamb -2.152255 time 2019-02-22 07:48:33.810126
Model ind 685 epoch 57 head A batch: 100 avg loss -2.120139 avg loss no lamb -2.120139 time 2019-02-22 07:49:27.221591
Model ind 685 epoch 57 head A batch: 200 avg loss -2.113680 avg loss no lamb -2.113680 time 2019-02-22 07:50:22.283967
Model ind 685 epoch 57 head A batch: 300 avg loss -2.208820 avg loss no lamb -2.208820 time 2019-02-22 07:51:17.878268
Model ind 685 epoch 57 head A batch: 400 avg loss -2.123451 avg loss no lamb -2.123451 time 2019-02-22 07:52:13.003651
Pre: time 2019-02-22 07:53:21.243970: 
 	std: 0.032772977
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.9788143, 0.90125716, 0.9787857, 0.9788]
	train_accs: [0.99221426, 0.9788143, 0.90125716, 0.9787857, 0.9788]
	best_train_sub_head: 0
	worst: 0.90125716
	avg: 0.96597433
	best: 0.99221426

Starting e_i: 58
Model ind 685 epoch 58 head B batch: 0 avg loss -2.127867 avg loss no lamb -2.127867 time 2019-02-22 07:53:22.690205
Model ind 685 epoch 58 head B batch: 100 avg loss -2.104756 avg loss no lamb -2.104756 time 2019-02-22 07:54:17.467658
Model ind 685 epoch 58 head B batch: 200 avg loss -2.078556 avg loss no lamb -2.078556 time 2019-02-22 07:55:12.388189
Model ind 685 epoch 58 head B batch: 300 avg loss -2.157984 avg loss no lamb -2.157984 time 2019-02-22 07:56:07.212926
Model ind 685 epoch 58 head B batch: 400 avg loss -2.077665 avg loss no lamb -2.077665 time 2019-02-22 07:57:02.118924
Model ind 685 epoch 58 head B batch: 0 avg loss -2.161222 avg loss no lamb -2.161222 time 2019-02-22 07:57:56.986367
Model ind 685 epoch 58 head B batch: 100 avg loss -2.132876 avg loss no lamb -2.132876 time 2019-02-22 07:58:52.410352
Model ind 685 epoch 58 head B batch: 200 avg loss -2.108256 avg loss no lamb -2.108256 time 2019-02-22 07:59:46.210406
Model ind 685 epoch 58 head B batch: 300 avg loss -2.151125 avg loss no lamb -2.151125 time 2019-02-22 08:00:39.693463
Model ind 685 epoch 58 head B batch: 400 avg loss -2.164901 avg loss no lamb -2.164901 time 2019-02-22 08:01:34.533570
Model ind 685 epoch 58 head A batch: 0 avg loss -2.181516 avg loss no lamb -2.181516 time 2019-02-22 08:02:28.178816
Model ind 685 epoch 58 head A batch: 100 avg loss -2.148872 avg loss no lamb -2.148872 time 2019-02-22 08:03:22.639155
Model ind 685 epoch 58 head A batch: 200 avg loss -2.121962 avg loss no lamb -2.121962 time 2019-02-22 08:04:16.810988
Model ind 685 epoch 58 head A batch: 300 avg loss -2.113030 avg loss no lamb -2.113030 time 2019-02-22 08:05:11.530135
Model ind 685 epoch 58 head A batch: 400 avg loss -2.187771 avg loss no lamb -2.187771 time 2019-02-22 08:06:06.250203
Pre: time 2019-02-22 08:07:15.211421: 
 	std: 0.03251459
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789571, 0.9020857, 0.9789857, 0.9789857]
	train_accs: [0.9923857, 0.9789571, 0.9020857, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9020857
	avg: 0.96628
	best: 0.9923857

Starting e_i: 59
Model ind 685 epoch 59 head B batch: 0 avg loss -2.180124 avg loss no lamb -2.180124 time 2019-02-22 08:07:17.928614
Model ind 685 epoch 59 head B batch: 100 avg loss -2.122635 avg loss no lamb -2.122635 time 2019-02-22 08:08:11.914433
Model ind 685 epoch 59 head B batch: 200 avg loss -2.137393 avg loss no lamb -2.137393 time 2019-02-22 08:09:07.156084
Model ind 685 epoch 59 head B batch: 300 avg loss -2.154219 avg loss no lamb -2.154219 time 2019-02-22 08:10:01.436486
Model ind 685 epoch 59 head B batch: 400 avg loss -2.141579 avg loss no lamb -2.141579 time 2019-02-22 08:10:56.603946
Model ind 685 epoch 59 head B batch: 0 avg loss -2.165339 avg loss no lamb -2.165339 time 2019-02-22 08:11:51.126436
Model ind 685 epoch 59 head B batch: 100 avg loss -2.159608 avg loss no lamb -2.159608 time 2019-02-22 08:12:45.252947
Model ind 685 epoch 59 head B batch: 200 avg loss -2.145071 avg loss no lamb -2.145071 time 2019-02-22 08:13:38.867278
Model ind 685 epoch 59 head B batch: 300 avg loss -2.160567 avg loss no lamb -2.160567 time 2019-02-22 08:14:32.278113
Model ind 685 epoch 59 head B batch: 400 avg loss -2.123233 avg loss no lamb -2.123233 time 2019-02-22 08:15:25.888105
Model ind 685 epoch 59 head A batch: 0 avg loss -2.166960 avg loss no lamb -2.166960 time 2019-02-22 08:16:21.941619
Model ind 685 epoch 59 head A batch: 100 avg loss -2.142359 avg loss no lamb -2.142359 time 2019-02-22 08:17:16.346004
Model ind 685 epoch 59 head A batch: 200 avg loss -2.139806 avg loss no lamb -2.139806 time 2019-02-22 08:18:09.959847
Model ind 685 epoch 59 head A batch: 300 avg loss -2.162551 avg loss no lamb -2.162551 time 2019-02-22 08:19:04.194646
Model ind 685 epoch 59 head A batch: 400 avg loss -2.171808 avg loss no lamb -2.171808 time 2019-02-22 08:19:58.680781
Pre: time 2019-02-22 08:21:05.996564: 
 	std: 0.032775003
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.97892857, 0.90132856, 0.9789429, 0.97891426]
	train_accs: [0.99221426, 0.97892857, 0.90132856, 0.9789429, 0.97891426]
	best_train_sub_head: 0
	worst: 0.90132856
	avg: 0.9660657
	best: 0.99221426

Starting e_i: 60
Model ind 685 epoch 60 head B batch: 0 avg loss -2.158431 avg loss no lamb -2.158431 time 2019-02-22 08:21:07.559787
Model ind 685 epoch 60 head B batch: 100 avg loss -2.130731 avg loss no lamb -2.130731 time 2019-02-22 08:22:01.544073
Model ind 685 epoch 60 head B batch: 200 avg loss -2.137439 avg loss no lamb -2.137439 time 2019-02-22 08:22:55.530205
Model ind 685 epoch 60 head B batch: 300 avg loss -2.174798 avg loss no lamb -2.174798 time 2019-02-22 08:23:49.514837
Model ind 685 epoch 60 head B batch: 400 avg loss -2.114175 avg loss no lamb -2.114175 time 2019-02-22 08:24:43.194527
Model ind 685 epoch 60 head B batch: 0 avg loss -2.128435 avg loss no lamb -2.128435 time 2019-02-22 08:25:37.100797
Model ind 685 epoch 60 head B batch: 100 avg loss -2.200538 avg loss no lamb -2.200538 time 2019-02-22 08:26:31.361731
Model ind 685 epoch 60 head B batch: 200 avg loss -2.137380 avg loss no lamb -2.137380 time 2019-02-22 08:27:28.293146
Model ind 685 epoch 60 head B batch: 300 avg loss -2.171740 avg loss no lamb -2.171740 time 2019-02-22 08:28:21.546055
Model ind 685 epoch 60 head B batch: 400 avg loss -2.146571 avg loss no lamb -2.146571 time 2019-02-22 08:29:16.272258
Model ind 685 epoch 60 head A batch: 0 avg loss -2.186018 avg loss no lamb -2.186018 time 2019-02-22 08:30:08.516088
Model ind 685 epoch 60 head A batch: 100 avg loss -2.116899 avg loss no lamb -2.116899 time 2019-02-22 08:31:02.116490
Model ind 685 epoch 60 head A batch: 200 avg loss -2.131184 avg loss no lamb -2.131184 time 2019-02-22 08:31:58.202727
Model ind 685 epoch 60 head A batch: 300 avg loss -2.138843 avg loss no lamb -2.138843 time 2019-02-22 08:32:52.114508
Model ind 685 epoch 60 head A batch: 400 avg loss -2.122818 avg loss no lamb -2.122818 time 2019-02-22 08:33:46.526314
Pre: time 2019-02-22 08:34:55.005210: 
 	std: 0.032898158
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.9789, 0.90101427, 0.9789, 0.9788857]
	train_accs: [0.9922571, 0.9789, 0.90101427, 0.9789, 0.9788857]
	best_train_sub_head: 0
	worst: 0.90101427
	avg: 0.9659914
	best: 0.9922571

Starting e_i: 61
Model ind 685 epoch 61 head B batch: 0 avg loss -2.146482 avg loss no lamb -2.146482 time 2019-02-22 08:34:57.966189
Model ind 685 epoch 61 head B batch: 100 avg loss -2.156519 avg loss no lamb -2.156519 time 2019-02-22 08:35:53.600518
Model ind 685 epoch 61 head B batch: 200 avg loss -2.145789 avg loss no lamb -2.145789 time 2019-02-22 08:36:50.394650
Model ind 685 epoch 61 head B batch: 300 avg loss -2.153811 avg loss no lamb -2.153811 time 2019-02-22 08:37:44.520884
Model ind 685 epoch 61 head B batch: 400 avg loss -2.119972 avg loss no lamb -2.119972 time 2019-02-22 08:38:37.821531
Model ind 685 epoch 61 head B batch: 0 avg loss -2.159026 avg loss no lamb -2.159026 time 2019-02-22 08:39:31.023142
Model ind 685 epoch 61 head B batch: 100 avg loss -2.143589 avg loss no lamb -2.143589 time 2019-02-22 08:40:24.091936
Model ind 685 epoch 61 head B batch: 200 avg loss -2.111173 avg loss no lamb -2.111173 time 2019-02-22 08:41:17.727410
Model ind 685 epoch 61 head B batch: 300 avg loss -2.186063 avg loss no lamb -2.186063 time 2019-02-22 08:42:10.229143
Model ind 685 epoch 61 head B batch: 400 avg loss -2.167181 avg loss no lamb -2.167181 time 2019-02-22 08:43:03.520059
Model ind 685 epoch 61 head A batch: 0 avg loss -2.184699 avg loss no lamb -2.184699 time 2019-02-22 08:43:57.849961
Model ind 685 epoch 61 head A batch: 100 avg loss -2.135630 avg loss no lamb -2.135630 time 2019-02-22 08:44:50.563390
Model ind 685 epoch 61 head A batch: 200 avg loss -2.168385 avg loss no lamb -2.168385 time 2019-02-22 08:45:43.464517
Model ind 685 epoch 61 head A batch: 300 avg loss -2.204686 avg loss no lamb -2.204686 time 2019-02-22 08:46:37.934929
Model ind 685 epoch 61 head A batch: 400 avg loss -2.139222 avg loss no lamb -2.139222 time 2019-02-22 08:47:31.216741
Pre: time 2019-02-22 08:48:40.079845: 
 	std: 0.03271651
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922429, 0.97885716, 0.9014429, 0.97885716, 0.97884285]
	train_accs: [0.9922429, 0.97885716, 0.9014429, 0.97885716, 0.97884285]
	best_train_sub_head: 0
	worst: 0.9014429
	avg: 0.96604854
	best: 0.9922429

Starting e_i: 62
Model ind 685 epoch 62 head B batch: 0 avg loss -2.183821 avg loss no lamb -2.183821 time 2019-02-22 08:48:41.687245
Model ind 685 epoch 62 head B batch: 100 avg loss -2.177249 avg loss no lamb -2.177249 time 2019-02-22 08:49:35.660662
Model ind 685 epoch 62 head B batch: 200 avg loss -2.145543 avg loss no lamb -2.145543 time 2019-02-22 08:50:29.778999
Model ind 685 epoch 62 head B batch: 300 avg loss -2.198565 avg loss no lamb -2.198565 time 2019-02-22 08:51:22.804137
Model ind 685 epoch 62 head B batch: 400 avg loss -2.156077 avg loss no lamb -2.156077 time 2019-02-22 08:52:15.665141
Model ind 685 epoch 62 head B batch: 0 avg loss -2.152456 avg loss no lamb -2.152456 time 2019-02-22 08:53:09.040290
Model ind 685 epoch 62 head B batch: 100 avg loss -2.176272 avg loss no lamb -2.176272 time 2019-02-22 08:54:01.224055
Model ind 685 epoch 62 head B batch: 200 avg loss -2.104241 avg loss no lamb -2.104241 time 2019-02-22 08:54:53.881227
Model ind 685 epoch 62 head B batch: 300 avg loss -2.193380 avg loss no lamb -2.193380 time 2019-02-22 08:55:48.388014
Model ind 685 epoch 62 head B batch: 400 avg loss -2.152222 avg loss no lamb -2.152222 time 2019-02-22 08:56:40.975182
Model ind 685 epoch 62 head A batch: 0 avg loss -2.169182 avg loss no lamb -2.169182 time 2019-02-22 08:57:34.733999
Model ind 685 epoch 62 head A batch: 100 avg loss -2.108046 avg loss no lamb -2.108046 time 2019-02-22 08:58:29.069295
Model ind 685 epoch 62 head A batch: 200 avg loss -2.081485 avg loss no lamb -2.081485 time 2019-02-22 08:59:22.847871
Model ind 685 epoch 62 head A batch: 300 avg loss -2.182163 avg loss no lamb -2.182163 time 2019-02-22 09:00:15.829405
Model ind 685 epoch 62 head A batch: 400 avg loss -2.196748 avg loss no lamb -2.196748 time 2019-02-22 09:01:08.545271
Pre: time 2019-02-22 09:02:15.055286: 
 	std: 0.03236886
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99222857, 0.9788857, 0.90234286, 0.9789, 0.9789]
	train_accs: [0.99222857, 0.9788857, 0.90234286, 0.9789, 0.9789]
	best_train_sub_head: 0
	worst: 0.90234286
	avg: 0.9662515
	best: 0.99222857

Starting e_i: 63
Model ind 685 epoch 63 head B batch: 0 avg loss -2.204530 avg loss no lamb -2.204530 time 2019-02-22 09:02:16.586048
Model ind 685 epoch 63 head B batch: 100 avg loss -2.160735 avg loss no lamb -2.160735 time 2019-02-22 09:03:10.991373
Model ind 685 epoch 63 head B batch: 200 avg loss -2.128265 avg loss no lamb -2.128265 time 2019-02-22 09:04:05.842491
Model ind 685 epoch 63 head B batch: 300 avg loss -2.190714 avg loss no lamb -2.190714 time 2019-02-22 09:05:00.450533
Model ind 685 epoch 63 head B batch: 400 avg loss -2.156117 avg loss no lamb -2.156117 time 2019-02-22 09:05:55.364271
Model ind 685 epoch 63 head B batch: 0 avg loss -2.147367 avg loss no lamb -2.147367 time 2019-02-22 09:06:49.348542
Model ind 685 epoch 63 head B batch: 100 avg loss -2.157140 avg loss no lamb -2.157140 time 2019-02-22 09:07:43.585370
Model ind 685 epoch 63 head B batch: 200 avg loss -2.078158 avg loss no lamb -2.078158 time 2019-02-22 09:08:37.095195
Model ind 685 epoch 63 head B batch: 300 avg loss -2.173532 avg loss no lamb -2.173532 time 2019-02-22 09:09:32.920196
Model ind 685 epoch 63 head B batch: 400 avg loss -2.189213 avg loss no lamb -2.189213 time 2019-02-22 09:10:27.858297
Model ind 685 epoch 63 head A batch: 0 avg loss -2.190562 avg loss no lamb -2.190562 time 2019-02-22 09:11:22.376295
Model ind 685 epoch 63 head A batch: 100 avg loss -2.094587 avg loss no lamb -2.094587 time 2019-02-22 09:12:15.613423
Model ind 685 epoch 63 head A batch: 200 avg loss -2.144227 avg loss no lamb -2.144227 time 2019-02-22 09:13:09.250443
Model ind 685 epoch 63 head A batch: 300 avg loss -2.149070 avg loss no lamb -2.149070 time 2019-02-22 09:14:02.084158
Model ind 685 epoch 63 head A batch: 400 avg loss -2.161801 avg loss no lamb -2.161801 time 2019-02-22 09:14:54.929968
Pre: time 2019-02-22 09:16:02.242004: 
 	std: 0.032831643
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789714, 0.9012857, 0.9789857, 0.9789857]
	train_accs: [0.9923857, 0.9789714, 0.9012857, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9012857
	avg: 0.9661228
	best: 0.9923857

Starting e_i: 64
Model ind 685 epoch 64 head B batch: 0 avg loss -2.220093 avg loss no lamb -2.220093 time 2019-02-22 09:16:03.948168
Model ind 685 epoch 64 head B batch: 100 avg loss -2.167267 avg loss no lamb -2.167267 time 2019-02-22 09:16:56.465765
Model ind 685 epoch 64 head B batch: 200 avg loss -2.102401 avg loss no lamb -2.102401 time 2019-02-22 09:17:50.397605
Model ind 685 epoch 64 head B batch: 300 avg loss -2.201012 avg loss no lamb -2.201012 time 2019-02-22 09:18:42.744658
Model ind 685 epoch 64 head B batch: 400 avg loss -2.165830 avg loss no lamb -2.165830 time 2019-02-22 09:19:36.080266
Model ind 685 epoch 64 head B batch: 0 avg loss -2.106388 avg loss no lamb -2.106388 time 2019-02-22 09:20:28.029860
Model ind 685 epoch 64 head B batch: 100 avg loss -2.149255 avg loss no lamb -2.149255 time 2019-02-22 09:21:23.751088
Model ind 685 epoch 64 head B batch: 200 avg loss -2.081429 avg loss no lamb -2.081429 time 2019-02-22 09:22:16.816702
Model ind 685 epoch 64 head B batch: 300 avg loss -2.199925 avg loss no lamb -2.199925 time 2019-02-22 09:23:10.763433
Model ind 685 epoch 64 head B batch: 400 avg loss -2.138490 avg loss no lamb -2.138490 time 2019-02-22 09:24:05.192367
Model ind 685 epoch 64 head A batch: 0 avg loss -2.161264 avg loss no lamb -2.161264 time 2019-02-22 09:24:59.389177
Model ind 685 epoch 64 head A batch: 100 avg loss -2.156697 avg loss no lamb -2.156697 time 2019-02-22 09:25:53.954180
Model ind 685 epoch 64 head A batch: 200 avg loss -2.162409 avg loss no lamb -2.162409 time 2019-02-22 09:26:48.734942
Model ind 685 epoch 64 head A batch: 300 avg loss -2.213371 avg loss no lamb -2.213371 time 2019-02-22 09:27:42.268089
Model ind 685 epoch 64 head A batch: 400 avg loss -2.152911 avg loss no lamb -2.152911 time 2019-02-22 09:28:37.194747
Pre: time 2019-02-22 09:29:44.783943: 
 	std: 0.032685872
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.97907144, 0.9017, 0.9790428, 0.97908574]
	train_accs: [0.99237144, 0.97907144, 0.9017, 0.9790428, 0.97908574]
	best_train_sub_head: 0
	worst: 0.9017
	avg: 0.96625435
	best: 0.99237144

Starting e_i: 65
Model ind 685 epoch 65 head B batch: 0 avg loss -2.147445 avg loss no lamb -2.147445 time 2019-02-22 09:29:46.254445
Model ind 685 epoch 65 head B batch: 100 avg loss -2.160243 avg loss no lamb -2.160243 time 2019-02-22 09:30:41.310874
Model ind 685 epoch 65 head B batch: 200 avg loss -2.106061 avg loss no lamb -2.106061 time 2019-02-22 09:31:38.406102
Model ind 685 epoch 65 head B batch: 300 avg loss -2.171260 avg loss no lamb -2.171260 time 2019-02-22 09:32:33.129847
Model ind 685 epoch 65 head B batch: 400 avg loss -2.148985 avg loss no lamb -2.148985 time 2019-02-22 09:33:28.804738
Model ind 685 epoch 65 head B batch: 0 avg loss -2.117714 avg loss no lamb -2.117714 time 2019-02-22 09:34:23.450501
Model ind 685 epoch 65 head B batch: 100 avg loss -2.118872 avg loss no lamb -2.118872 time 2019-02-22 09:35:16.426337
Model ind 685 epoch 65 head B batch: 200 avg loss -2.150976 avg loss no lamb -2.150976 time 2019-02-22 09:36:11.521865
Model ind 685 epoch 65 head B batch: 300 avg loss -2.164502 avg loss no lamb -2.164502 time 2019-02-22 09:37:05.266882
Model ind 685 epoch 65 head B batch: 400 avg loss -2.098288 avg loss no lamb -2.098288 time 2019-02-22 09:37:59.359526
Model ind 685 epoch 65 head A batch: 0 avg loss -2.183500 avg loss no lamb -2.183500 time 2019-02-22 09:38:52.171163
Model ind 685 epoch 65 head A batch: 100 avg loss -2.155859 avg loss no lamb -2.155859 time 2019-02-22 09:39:45.661311
Model ind 685 epoch 65 head A batch: 200 avg loss -2.134844 avg loss no lamb -2.134844 time 2019-02-22 09:40:39.117330
Model ind 685 epoch 65 head A batch: 300 avg loss -2.213288 avg loss no lamb -2.213288 time 2019-02-22 09:41:33.643630
Model ind 685 epoch 65 head A batch: 400 avg loss -2.145729 avg loss no lamb -2.145729 time 2019-02-22 09:42:27.539416
Pre: time 2019-02-22 09:43:35.357797: 
 	std: 0.03264875
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921, 0.97874284, 0.90148574, 0.9787286, 0.9787286]
	train_accs: [0.9921, 0.97874284, 0.90148574, 0.9787286, 0.9787286]
	best_train_sub_head: 0
	worst: 0.90148574
	avg: 0.96595716
	best: 0.9921

Starting e_i: 66
Model ind 685 epoch 66 head B batch: 0 avg loss -2.155147 avg loss no lamb -2.155147 time 2019-02-22 09:43:37.046787
Model ind 685 epoch 66 head B batch: 100 avg loss -2.144390 avg loss no lamb -2.144390 time 2019-02-22 09:44:30.845705
Model ind 685 epoch 66 head B batch: 200 avg loss -2.114532 avg loss no lamb -2.114532 time 2019-02-22 09:45:25.083721
Model ind 685 epoch 66 head B batch: 300 avg loss -2.157305 avg loss no lamb -2.157305 time 2019-02-22 09:46:19.736336
Model ind 685 epoch 66 head B batch: 400 avg loss -2.133715 avg loss no lamb -2.133715 time 2019-02-22 09:47:15.232408
Model ind 685 epoch 66 head B batch: 0 avg loss -2.167869 avg loss no lamb -2.167869 time 2019-02-22 09:48:11.101673
Model ind 685 epoch 66 head B batch: 100 avg loss -2.170454 avg loss no lamb -2.170454 time 2019-02-22 09:49:05.108828
Model ind 685 epoch 66 head B batch: 200 avg loss -2.178970 avg loss no lamb -2.178970 time 2019-02-22 09:49:59.381685
Model ind 685 epoch 66 head B batch: 300 avg loss -2.169311 avg loss no lamb -2.169311 time 2019-02-22 09:50:53.703062
Model ind 685 epoch 66 head B batch: 400 avg loss -2.133941 avg loss no lamb -2.133941 time 2019-02-22 09:51:48.933490
Model ind 685 epoch 66 head A batch: 0 avg loss -2.125533 avg loss no lamb -2.125533 time 2019-02-22 09:52:44.752123
Model ind 685 epoch 66 head A batch: 100 avg loss -2.155385 avg loss no lamb -2.155385 time 2019-02-22 09:53:39.508910
Model ind 685 epoch 66 head A batch: 200 avg loss -2.134997 avg loss no lamb -2.134997 time 2019-02-22 09:54:36.133702
Model ind 685 epoch 66 head A batch: 300 avg loss -2.190153 avg loss no lamb -2.190153 time 2019-02-22 09:55:30.578193
Model ind 685 epoch 66 head A batch: 400 avg loss -2.186063 avg loss no lamb -2.186063 time 2019-02-22 09:56:25.394372
Pre: time 2019-02-22 09:57:35.511699: 
 	std: 0.032606024
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922, 0.97875714, 0.90165716, 0.97877145, 0.9787857]
	train_accs: [0.9922, 0.97875714, 0.90165716, 0.97877145, 0.9787857]
	best_train_sub_head: 0
	worst: 0.90165716
	avg: 0.9660343
	best: 0.9922

Starting e_i: 67
Model ind 685 epoch 67 head B batch: 0 avg loss -2.195220 avg loss no lamb -2.195220 time 2019-02-22 09:57:37.088079
Model ind 685 epoch 67 head B batch: 100 avg loss -2.172235 avg loss no lamb -2.172235 time 2019-02-22 09:58:32.199371
Model ind 685 epoch 67 head B batch: 200 avg loss -2.140564 avg loss no lamb -2.140564 time 2019-02-22 09:59:27.478676
Model ind 685 epoch 67 head B batch: 300 avg loss -2.151140 avg loss no lamb -2.151140 time 2019-02-22 10:00:21.269079
Model ind 685 epoch 67 head B batch: 400 avg loss -2.152572 avg loss no lamb -2.152572 time 2019-02-22 10:01:15.619860
Model ind 685 epoch 67 head B batch: 0 avg loss -2.161407 avg loss no lamb -2.161407 time 2019-02-22 10:02:10.290625
Model ind 685 epoch 67 head B batch: 100 avg loss -2.153725 avg loss no lamb -2.153725 time 2019-02-22 10:03:04.819340
Model ind 685 epoch 67 head B batch: 200 avg loss -2.115903 avg loss no lamb -2.115903 time 2019-02-22 10:03:58.527024
Model ind 685 epoch 67 head B batch: 300 avg loss -2.178779 avg loss no lamb -2.178779 time 2019-02-22 10:04:52.543476
Model ind 685 epoch 67 head B batch: 400 avg loss -2.171601 avg loss no lamb -2.171601 time 2019-02-22 10:05:45.878267
Model ind 685 epoch 67 head A batch: 0 avg loss -2.183266 avg loss no lamb -2.183266 time 2019-02-22 10:06:39.548318
Model ind 685 epoch 67 head A batch: 100 avg loss -2.116160 avg loss no lamb -2.116160 time 2019-02-22 10:07:32.143123
Model ind 685 epoch 67 head A batch: 200 avg loss -2.096375 avg loss no lamb -2.096375 time 2019-02-22 10:08:26.358800
Model ind 685 epoch 67 head A batch: 300 avg loss -2.172959 avg loss no lamb -2.172959 time 2019-02-22 10:09:20.039046
Model ind 685 epoch 67 head A batch: 400 avg loss -2.132462 avg loss no lamb -2.132462 time 2019-02-22 10:10:13.199812
Pre: time 2019-02-22 10:11:21.049526: 
 	std: 0.03278646
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921714, 0.9788, 0.9012, 0.9788, 0.97877145]
	train_accs: [0.9921714, 0.9788, 0.9012, 0.9788, 0.97877145]
	best_train_sub_head: 0
	worst: 0.9012
	avg: 0.9659486
	best: 0.9921714

Starting e_i: 68
Model ind 685 epoch 68 head B batch: 0 avg loss -2.195836 avg loss no lamb -2.195836 time 2019-02-22 10:11:22.693078
Model ind 685 epoch 68 head B batch: 100 avg loss -2.153542 avg loss no lamb -2.153542 time 2019-02-22 10:12:15.178525
Model ind 685 epoch 68 head B batch: 200 avg loss -2.129976 avg loss no lamb -2.129976 time 2019-02-22 10:13:10.339752
Model ind 685 epoch 68 head B batch: 300 avg loss -2.194356 avg loss no lamb -2.194356 time 2019-02-22 10:14:04.457715
Model ind 685 epoch 68 head B batch: 400 avg loss -2.128206 avg loss no lamb -2.128206 time 2019-02-22 10:14:57.617983
Model ind 685 epoch 68 head B batch: 0 avg loss -2.162206 avg loss no lamb -2.162206 time 2019-02-22 10:15:51.327148
Model ind 685 epoch 68 head B batch: 100 avg loss -2.133933 avg loss no lamb -2.133933 time 2019-02-22 10:16:45.499594
Model ind 685 epoch 68 head B batch: 200 avg loss -2.129733 avg loss no lamb -2.129733 time 2019-02-22 10:17:38.886038
Model ind 685 epoch 68 head B batch: 300 avg loss -2.175087 avg loss no lamb -2.175087 time 2019-02-22 10:18:32.616755
Model ind 685 epoch 68 head B batch: 400 avg loss -2.129364 avg loss no lamb -2.129364 time 2019-02-22 10:19:27.894679
Model ind 685 epoch 68 head A batch: 0 avg loss -2.187694 avg loss no lamb -2.187694 time 2019-02-22 10:20:22.361356
Model ind 685 epoch 68 head A batch: 100 avg loss -2.130584 avg loss no lamb -2.130584 time 2019-02-22 10:21:17.203084
Model ind 685 epoch 68 head A batch: 200 avg loss -2.115922 avg loss no lamb -2.115922 time 2019-02-22 10:22:10.540585
Model ind 685 epoch 68 head A batch: 300 avg loss -2.149369 avg loss no lamb -2.149369 time 2019-02-22 10:23:05.852216
Model ind 685 epoch 68 head A batch: 400 avg loss -2.174721 avg loss no lamb -2.174721 time 2019-02-22 10:23:58.832621
Pre: time 2019-02-22 10:25:05.997101: 
 	std: 0.03256747
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921857, 0.97884285, 0.90178573, 0.97882855, 0.97882855]
	train_accs: [0.9921857, 0.97884285, 0.90178573, 0.97882855, 0.97882855]
	best_train_sub_head: 0
	worst: 0.90178573
	avg: 0.9660942
	best: 0.9921857

Starting e_i: 69
Model ind 685 epoch 69 head B batch: 0 avg loss -2.174932 avg loss no lamb -2.174932 time 2019-02-22 10:25:07.529253
Model ind 685 epoch 69 head B batch: 100 avg loss -2.162948 avg loss no lamb -2.162948 time 2019-02-22 10:25:59.802991
Model ind 685 epoch 69 head B batch: 200 avg loss -2.139828 avg loss no lamb -2.139828 time 2019-02-22 10:26:53.247832
Model ind 685 epoch 69 head B batch: 300 avg loss -2.183536 avg loss no lamb -2.183536 time 2019-02-22 10:27:46.094214
Model ind 685 epoch 69 head B batch: 400 avg loss -2.173980 avg loss no lamb -2.173980 time 2019-02-22 10:28:40.972154
Model ind 685 epoch 69 head B batch: 0 avg loss -2.156089 avg loss no lamb -2.156089 time 2019-02-22 10:29:34.310134
Model ind 685 epoch 69 head B batch: 100 avg loss -2.148584 avg loss no lamb -2.148584 time 2019-02-22 10:30:27.654100
Model ind 685 epoch 69 head B batch: 200 avg loss -2.132685 avg loss no lamb -2.132685 time 2019-02-22 10:31:21.078842
Model ind 685 epoch 69 head B batch: 300 avg loss -2.133072 avg loss no lamb -2.133072 time 2019-02-22 10:32:14.883967
Model ind 685 epoch 69 head B batch: 400 avg loss -2.178740 avg loss no lamb -2.178740 time 2019-02-22 10:33:07.381853
Model ind 685 epoch 69 head A batch: 0 avg loss -2.188444 avg loss no lamb -2.188444 time 2019-02-22 10:34:01.105786
Model ind 685 epoch 69 head A batch: 100 avg loss -2.122098 avg loss no lamb -2.122098 time 2019-02-22 10:34:54.695492
Model ind 685 epoch 69 head A batch: 200 avg loss -2.121447 avg loss no lamb -2.121447 time 2019-02-22 10:35:50.737361
Model ind 685 epoch 69 head A batch: 300 avg loss -2.165693 avg loss no lamb -2.165693 time 2019-02-22 10:36:44.608060
Model ind 685 epoch 69 head A batch: 400 avg loss -2.139388 avg loss no lamb -2.139388 time 2019-02-22 10:37:37.537594
Pre: time 2019-02-22 10:38:46.718442: 
 	std: 0.032772813
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.97891426, 0.90131426, 0.97885716, 0.97891426]
	train_accs: [0.99221426, 0.97891426, 0.90131426, 0.97885716, 0.97891426]
	best_train_sub_head: 0
	worst: 0.90131426
	avg: 0.9660428
	best: 0.99221426

Starting e_i: 70
Model ind 685 epoch 70 head B batch: 0 avg loss -2.172667 avg loss no lamb -2.172667 time 2019-02-22 10:38:48.328751
Model ind 685 epoch 70 head B batch: 100 avg loss -2.122629 avg loss no lamb -2.122629 time 2019-02-22 10:39:40.381002
Model ind 685 epoch 70 head B batch: 200 avg loss -2.158702 avg loss no lamb -2.158702 time 2019-02-22 10:40:33.875509
Model ind 685 epoch 70 head B batch: 300 avg loss -2.174592 avg loss no lamb -2.174592 time 2019-02-22 10:41:27.587036
Model ind 685 epoch 70 head B batch: 400 avg loss -2.177052 avg loss no lamb -2.177052 time 2019-02-22 10:42:22.707894
Model ind 685 epoch 70 head B batch: 0 avg loss -2.173382 avg loss no lamb -2.173382 time 2019-02-22 10:43:17.451960
Model ind 685 epoch 70 head B batch: 100 avg loss -2.113564 avg loss no lamb -2.113564 time 2019-02-22 10:44:12.211635
Model ind 685 epoch 70 head B batch: 200 avg loss -2.109329 avg loss no lamb -2.109329 time 2019-02-22 10:45:06.077824
Model ind 685 epoch 70 head B batch: 300 avg loss -2.175400 avg loss no lamb -2.175400 time 2019-02-22 10:45:59.766032
Model ind 685 epoch 70 head B batch: 400 avg loss -2.149741 avg loss no lamb -2.149741 time 2019-02-22 10:46:55.174833
Model ind 685 epoch 70 head A batch: 0 avg loss -2.165981 avg loss no lamb -2.165981 time 2019-02-22 10:47:48.580079
Model ind 685 epoch 70 head A batch: 100 avg loss -2.134259 avg loss no lamb -2.134259 time 2019-02-22 10:48:42.005253
Model ind 685 epoch 70 head A batch: 200 avg loss -2.153230 avg loss no lamb -2.153230 time 2019-02-22 10:49:36.089223
Model ind 685 epoch 70 head A batch: 300 avg loss -2.154640 avg loss no lamb -2.154640 time 2019-02-22 10:50:30.580408
Model ind 685 epoch 70 head A batch: 400 avg loss -2.149221 avg loss no lamb -2.149221 time 2019-02-22 10:51:26.543187
Pre: time 2019-02-22 10:52:36.841107: 
 	std: 0.0326023
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921, 0.97882855, 0.90165716, 0.9788143, 0.97882855]
	train_accs: [0.9921, 0.97882855, 0.90165716, 0.9788143, 0.97882855]
	best_train_sub_head: 0
	worst: 0.90165716
	avg: 0.96604574
	best: 0.9921

Starting e_i: 71
Model ind 685 epoch 71 head B batch: 0 avg loss -2.160851 avg loss no lamb -2.160851 time 2019-02-22 10:52:38.544846
Model ind 685 epoch 71 head B batch: 100 avg loss -2.155511 avg loss no lamb -2.155511 time 2019-02-22 10:53:34.107946
Model ind 685 epoch 71 head B batch: 200 avg loss -2.129719 avg loss no lamb -2.129719 time 2019-02-22 10:54:29.093649
Model ind 685 epoch 71 head B batch: 300 avg loss -2.193727 avg loss no lamb -2.193727 time 2019-02-22 10:55:23.768605
Model ind 685 epoch 71 head B batch: 400 avg loss -2.093122 avg loss no lamb -2.093122 time 2019-02-22 10:56:16.494144
Model ind 685 epoch 71 head B batch: 0 avg loss -2.113586 avg loss no lamb -2.113586 time 2019-02-22 10:57:10.983599
Model ind 685 epoch 71 head B batch: 100 avg loss -2.175735 avg loss no lamb -2.175735 time 2019-02-22 10:58:04.246144
Model ind 685 epoch 71 head B batch: 200 avg loss -2.175004 avg loss no lamb -2.175004 time 2019-02-22 10:58:57.299645
Model ind 685 epoch 71 head B batch: 300 avg loss -2.134672 avg loss no lamb -2.134672 time 2019-02-22 10:59:49.707749
Model ind 685 epoch 71 head B batch: 400 avg loss -2.184833 avg loss no lamb -2.184833 time 2019-02-22 11:00:43.562000
Model ind 685 epoch 71 head A batch: 0 avg loss -2.123578 avg loss no lamb -2.123578 time 2019-02-22 11:01:37.653577
Model ind 685 epoch 71 head A batch: 100 avg loss -2.180430 avg loss no lamb -2.180430 time 2019-02-22 11:02:31.368218
Model ind 685 epoch 71 head A batch: 200 avg loss -2.136346 avg loss no lamb -2.136346 time 2019-02-22 11:03:24.112598
Model ind 685 epoch 71 head A batch: 300 avg loss -2.189819 avg loss no lamb -2.189819 time 2019-02-22 11:04:17.534540
Model ind 685 epoch 71 head A batch: 400 avg loss -2.150769 avg loss no lamb -2.150769 time 2019-02-22 11:05:10.782551
Pre: time 2019-02-22 11:06:18.517685: 
 	std: 0.032635417
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.9788143, 0.90162855, 0.9788143, 0.9788]
	train_accs: [0.9922571, 0.9788143, 0.90162855, 0.9788143, 0.9788]
	best_train_sub_head: 0
	worst: 0.90162855
	avg: 0.96606284
	best: 0.9922571

Starting e_i: 72
Model ind 685 epoch 72 head B batch: 0 avg loss -2.167526 avg loss no lamb -2.167526 time 2019-02-22 11:06:20.119861
Model ind 685 epoch 72 head B batch: 100 avg loss -2.140193 avg loss no lamb -2.140193 time 2019-02-22 11:07:13.502796
Model ind 685 epoch 72 head B batch: 200 avg loss -2.181912 avg loss no lamb -2.181912 time 2019-02-22 11:08:06.384779
Model ind 685 epoch 72 head B batch: 300 avg loss -2.133687 avg loss no lamb -2.133687 time 2019-02-22 11:08:59.170448
Model ind 685 epoch 72 head B batch: 400 avg loss -2.150765 avg loss no lamb -2.150765 time 2019-02-22 11:09:51.551015
Model ind 685 epoch 72 head B batch: 0 avg loss -2.164368 avg loss no lamb -2.164368 time 2019-02-22 11:10:44.902305
Model ind 685 epoch 72 head B batch: 100 avg loss -2.141169 avg loss no lamb -2.141169 time 2019-02-22 11:11:38.345944
Model ind 685 epoch 72 head B batch: 200 avg loss -2.153055 avg loss no lamb -2.153055 time 2019-02-22 11:12:31.422071
Model ind 685 epoch 72 head B batch: 300 avg loss -2.177623 avg loss no lamb -2.177623 time 2019-02-22 11:13:25.503575
Model ind 685 epoch 72 head B batch: 400 avg loss -2.165800 avg loss no lamb -2.165800 time 2019-02-22 11:14:18.535389
Model ind 685 epoch 72 head A batch: 0 avg loss -2.179192 avg loss no lamb -2.179192 time 2019-02-22 11:15:12.459326
Model ind 685 epoch 72 head A batch: 100 avg loss -2.145996 avg loss no lamb -2.145996 time 2019-02-22 11:16:05.742572
Model ind 685 epoch 72 head A batch: 200 avg loss -2.162019 avg loss no lamb -2.162019 time 2019-02-22 11:16:59.793031
Model ind 685 epoch 72 head A batch: 300 avg loss -2.187279 avg loss no lamb -2.187279 time 2019-02-22 11:17:55.253681
Model ind 685 epoch 72 head A batch: 400 avg loss -2.167565 avg loss no lamb -2.167565 time 2019-02-22 11:18:49.219598
Pre: time 2019-02-22 11:19:57.902359: 
 	std: 0.032760452
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921143, 0.97875714, 0.90122855, 0.9787857, 0.97875714]
	train_accs: [0.9921143, 0.97875714, 0.90122855, 0.9787857, 0.97875714]
	best_train_sub_head: 0
	worst: 0.90122855
	avg: 0.96592855
	best: 0.9921143

Starting e_i: 73
Model ind 685 epoch 73 head B batch: 0 avg loss -2.172190 avg loss no lamb -2.172190 time 2019-02-22 11:19:59.600675
Model ind 685 epoch 73 head B batch: 100 avg loss -2.161706 avg loss no lamb -2.161706 time 2019-02-22 11:20:54.160889
Model ind 685 epoch 73 head B batch: 200 avg loss -2.145172 avg loss no lamb -2.145172 time 2019-02-22 11:21:47.204736
Model ind 685 epoch 73 head B batch: 300 avg loss -2.200994 avg loss no lamb -2.200994 time 2019-02-22 11:22:41.418983
Model ind 685 epoch 73 head B batch: 400 avg loss -2.153370 avg loss no lamb -2.153370 time 2019-02-22 11:23:34.349485
Model ind 685 epoch 73 head B batch: 0 avg loss -2.173898 avg loss no lamb -2.173898 time 2019-02-22 11:24:27.143382
Model ind 685 epoch 73 head B batch: 100 avg loss -2.124915 avg loss no lamb -2.124915 time 2019-02-22 11:25:19.277515
Model ind 685 epoch 73 head B batch: 200 avg loss -2.162434 avg loss no lamb -2.162434 time 2019-02-22 11:26:13.252002
Model ind 685 epoch 73 head B batch: 300 avg loss -2.208432 avg loss no lamb -2.208432 time 2019-02-22 11:27:08.239853
Model ind 685 epoch 73 head B batch: 400 avg loss -2.138958 avg loss no lamb -2.138958 time 2019-02-22 11:28:01.940092
Model ind 685 epoch 73 head A batch: 0 avg loss -2.197524 avg loss no lamb -2.197524 time 2019-02-22 11:28:55.859426
Model ind 685 epoch 73 head A batch: 100 avg loss -2.171129 avg loss no lamb -2.171129 time 2019-02-22 11:29:49.375044
Model ind 685 epoch 73 head A batch: 200 avg loss -2.150392 avg loss no lamb -2.150392 time 2019-02-22 11:30:44.140526
Model ind 685 epoch 73 head A batch: 300 avg loss -2.187136 avg loss no lamb -2.187136 time 2019-02-22 11:31:38.154961
Model ind 685 epoch 73 head A batch: 400 avg loss -2.169481 avg loss no lamb -2.169481 time 2019-02-22 11:32:32.366413
Pre: time 2019-02-22 11:33:41.032822: 
 	std: 0.032677013
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99212855, 0.97874284, 0.9014286, 0.97874284, 0.9787286]
	train_accs: [0.99212855, 0.97874284, 0.9014286, 0.97874284, 0.9787286]
	best_train_sub_head: 0
	worst: 0.9014286
	avg: 0.9659543
	best: 0.99212855

Starting e_i: 74
Model ind 685 epoch 74 head B batch: 0 avg loss -2.175735 avg loss no lamb -2.175735 time 2019-02-22 11:33:42.634257
Model ind 685 epoch 74 head B batch: 100 avg loss -2.160901 avg loss no lamb -2.160901 time 2019-02-22 11:34:36.895917
Model ind 685 epoch 74 head B batch: 200 avg loss -2.122505 avg loss no lamb -2.122505 time 2019-02-22 11:35:30.973850
Model ind 685 epoch 74 head B batch: 300 avg loss -2.164707 avg loss no lamb -2.164707 time 2019-02-22 11:36:24.139976
Model ind 685 epoch 74 head B batch: 400 avg loss -2.117283 avg loss no lamb -2.117283 time 2019-02-22 11:37:18.909477
Model ind 685 epoch 74 head B batch: 0 avg loss -2.174754 avg loss no lamb -2.174754 time 2019-02-22 11:38:12.139038
Model ind 685 epoch 74 head B batch: 100 avg loss -2.172517 avg loss no lamb -2.172517 time 2019-02-22 11:39:05.860867
Model ind 685 epoch 74 head B batch: 200 avg loss -2.143681 avg loss no lamb -2.143681 time 2019-02-22 11:39:58.583266
Model ind 685 epoch 74 head B batch: 300 avg loss -2.187695 avg loss no lamb -2.187695 time 2019-02-22 11:40:51.632625
Model ind 685 epoch 74 head B batch: 400 avg loss -2.141516 avg loss no lamb -2.141516 time 2019-02-22 11:41:47.270259
Model ind 685 epoch 74 head A batch: 0 avg loss -2.133814 avg loss no lamb -2.133814 time 2019-02-22 11:42:40.426056
Model ind 685 epoch 74 head A batch: 100 avg loss -2.113857 avg loss no lamb -2.113857 time 2019-02-22 11:43:33.912929
Model ind 685 epoch 74 head A batch: 200 avg loss -2.148855 avg loss no lamb -2.148855 time 2019-02-22 11:44:27.366469
Model ind 685 epoch 74 head A batch: 300 avg loss -2.182520 avg loss no lamb -2.182520 time 2019-02-22 11:45:19.521770
Model ind 685 epoch 74 head A batch: 400 avg loss -2.194688 avg loss no lamb -2.194688 time 2019-02-22 11:46:13.501155
Pre: time 2019-02-22 11:47:21.451764: 
 	std: 0.03286761
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.9789, 0.9010714, 0.9789, 0.9788714]
	train_accs: [0.99221426, 0.9789, 0.9010714, 0.9789, 0.9788714]
	best_train_sub_head: 0
	worst: 0.9010714
	avg: 0.9659914
	best: 0.99221426

Starting e_i: 75
Model ind 685 epoch 75 head B batch: 0 avg loss -2.183040 avg loss no lamb -2.183040 time 2019-02-22 11:47:23.390330
Model ind 685 epoch 75 head B batch: 100 avg loss -2.153013 avg loss no lamb -2.153013 time 2019-02-22 11:48:17.166893
Model ind 685 epoch 75 head B batch: 200 avg loss -2.120801 avg loss no lamb -2.120801 time 2019-02-22 11:49:11.612445
Model ind 685 epoch 75 head B batch: 300 avg loss -2.187556 avg loss no lamb -2.187556 time 2019-02-22 11:50:06.432341
Model ind 685 epoch 75 head B batch: 400 avg loss -2.123524 avg loss no lamb -2.123524 time 2019-02-22 11:51:00.872123
Model ind 685 epoch 75 head B batch: 0 avg loss -2.136819 avg loss no lamb -2.136819 time 2019-02-22 11:51:54.199998
Model ind 685 epoch 75 head B batch: 100 avg loss -2.131821 avg loss no lamb -2.131821 time 2019-02-22 11:52:48.982779
Model ind 685 epoch 75 head B batch: 200 avg loss -2.066697 avg loss no lamb -2.066697 time 2019-02-22 11:53:45.588584
Model ind 685 epoch 75 head B batch: 300 avg loss -2.195184 avg loss no lamb -2.195184 time 2019-02-22 11:54:40.165141
Model ind 685 epoch 75 head B batch: 400 avg loss -2.139512 avg loss no lamb -2.139512 time 2019-02-22 11:55:35.165755
Model ind 685 epoch 75 head A batch: 0 avg loss -2.172232 avg loss no lamb -2.172232 time 2019-02-22 11:56:28.285081
Model ind 685 epoch 75 head A batch: 100 avg loss -2.172210 avg loss no lamb -2.172210 time 2019-02-22 11:57:23.647022
Model ind 685 epoch 75 head A batch: 200 avg loss -2.118766 avg loss no lamb -2.118766 time 2019-02-22 11:58:16.742462
Model ind 685 epoch 75 head A batch: 300 avg loss -2.166422 avg loss no lamb -2.166422 time 2019-02-22 11:59:11.560302
Model ind 685 epoch 75 head A batch: 400 avg loss -2.146678 avg loss no lamb -2.146678 time 2019-02-22 12:00:05.410255
Pre: time 2019-02-22 12:01:14.674653: 
 	std: 0.03255063
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.9788714, 0.90192854, 0.9789571, 0.9789429]
	train_accs: [0.9923, 0.9788714, 0.90192854, 0.9789571, 0.9789429]
	best_train_sub_head: 0
	worst: 0.90192854
	avg: 0.9662
	best: 0.9923

Starting e_i: 76
Model ind 685 epoch 76 head B batch: 0 avg loss -2.132263 avg loss no lamb -2.132263 time 2019-02-22 12:01:16.386309
Model ind 685 epoch 76 head B batch: 100 avg loss -2.155490 avg loss no lamb -2.155490 time 2019-02-22 12:02:10.684935
Model ind 685 epoch 76 head B batch: 200 avg loss -2.137853 avg loss no lamb -2.137853 time 2019-02-22 12:03:04.727738
Model ind 685 epoch 76 head B batch: 300 avg loss -2.185983 avg loss no lamb -2.185983 time 2019-02-22 12:04:01.905765
Model ind 685 epoch 76 head B batch: 400 avg loss -2.130717 avg loss no lamb -2.130717 time 2019-02-22 12:04:56.601736
Model ind 685 epoch 76 head B batch: 0 avg loss -2.133556 avg loss no lamb -2.133556 time 2019-02-22 12:05:51.393249
Model ind 685 epoch 76 head B batch: 100 avg loss -2.146628 avg loss no lamb -2.146628 time 2019-02-22 12:06:43.898487
Model ind 685 epoch 76 head B batch: 200 avg loss -2.133031 avg loss no lamb -2.133031 time 2019-02-22 12:07:38.117265
Model ind 685 epoch 76 head B batch: 300 avg loss -2.178927 avg loss no lamb -2.178927 time 2019-02-22 12:08:32.210023
Model ind 685 epoch 76 head B batch: 400 avg loss -2.174020 avg loss no lamb -2.174020 time 2019-02-22 12:09:26.998549
Model ind 685 epoch 76 head A batch: 0 avg loss -2.202390 avg loss no lamb -2.202390 time 2019-02-22 12:10:22.811603
Model ind 685 epoch 76 head A batch: 100 avg loss -2.194984 avg loss no lamb -2.194984 time 2019-02-22 12:11:17.447592
Model ind 685 epoch 76 head A batch: 200 avg loss -2.135728 avg loss no lamb -2.135728 time 2019-02-22 12:12:10.443943
Model ind 685 epoch 76 head A batch: 300 avg loss -2.199148 avg loss no lamb -2.199148 time 2019-02-22 12:13:03.492984
Model ind 685 epoch 76 head A batch: 400 avg loss -2.145235 avg loss no lamb -2.145235 time 2019-02-22 12:13:57.297514
Pre: time 2019-02-22 12:15:05.171361: 
 	std: 0.032664683
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922714, 0.97884285, 0.9015857, 0.97885716, 0.97885716]
	train_accs: [0.9922714, 0.97884285, 0.9015857, 0.97885716, 0.97885716]
	best_train_sub_head: 0
	worst: 0.9015857
	avg: 0.9660829
	best: 0.9922714

Starting e_i: 77
Model ind 685 epoch 77 head B batch: 0 avg loss -2.140463 avg loss no lamb -2.140463 time 2019-02-22 12:15:06.799510
Model ind 685 epoch 77 head B batch: 100 avg loss -2.187083 avg loss no lamb -2.187083 time 2019-02-22 12:16:01.798306
Model ind 685 epoch 77 head B batch: 200 avg loss -2.114467 avg loss no lamb -2.114467 time 2019-02-22 12:16:55.475795
Model ind 685 epoch 77 head B batch: 300 avg loss -2.146135 avg loss no lamb -2.146135 time 2019-02-22 12:17:49.343144
Model ind 685 epoch 77 head B batch: 400 avg loss -2.151804 avg loss no lamb -2.151804 time 2019-02-22 12:18:43.349025
Model ind 685 epoch 77 head B batch: 0 avg loss -2.184798 avg loss no lamb -2.184798 time 2019-02-22 12:19:36.466718
Model ind 685 epoch 77 head B batch: 100 avg loss -2.123157 avg loss no lamb -2.123157 time 2019-02-22 12:20:29.675271
Model ind 685 epoch 77 head B batch: 200 avg loss -2.090779 avg loss no lamb -2.090779 time 2019-02-22 12:21:22.477397
Model ind 685 epoch 77 head B batch: 300 avg loss -2.160782 avg loss no lamb -2.160782 time 2019-02-22 12:22:16.362417
Model ind 685 epoch 77 head B batch: 400 avg loss -2.089328 avg loss no lamb -2.089328 time 2019-02-22 12:23:10.304118
Model ind 685 epoch 77 head A batch: 0 avg loss -2.157408 avg loss no lamb -2.157408 time 2019-02-22 12:24:03.751973
Model ind 685 epoch 77 head A batch: 100 avg loss -2.125226 avg loss no lamb -2.125226 time 2019-02-22 12:24:57.385220
Model ind 685 epoch 77 head A batch: 200 avg loss -2.126541 avg loss no lamb -2.126541 time 2019-02-22 12:25:50.644280
Model ind 685 epoch 77 head A batch: 300 avg loss -2.180227 avg loss no lamb -2.180227 time 2019-02-22 12:26:44.540222
Model ind 685 epoch 77 head A batch: 400 avg loss -2.145148 avg loss no lamb -2.145148 time 2019-02-22 12:27:38.536133
Pre: time 2019-02-22 12:28:46.509323: 
 	std: 0.032370042
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97907144, 0.9025286, 0.97908574, 0.97907144]
	train_accs: [0.9924286, 0.97907144, 0.9025286, 0.97908574, 0.97907144]
	best_train_sub_head: 0
	worst: 0.9025286
	avg: 0.96643716
	best: 0.9924286

Starting e_i: 78
Model ind 685 epoch 78 head B batch: 0 avg loss -2.194872 avg loss no lamb -2.194872 time 2019-02-22 12:28:49.466331
Model ind 685 epoch 78 head B batch: 100 avg loss -2.126716 avg loss no lamb -2.126716 time 2019-02-22 12:29:42.937707
Model ind 685 epoch 78 head B batch: 200 avg loss -2.110625 avg loss no lamb -2.110625 time 2019-02-22 12:30:37.449561
Model ind 685 epoch 78 head B batch: 300 avg loss -2.183981 avg loss no lamb -2.183981 time 2019-02-22 12:31:31.947006
Model ind 685 epoch 78 head B batch: 400 avg loss -2.151567 avg loss no lamb -2.151567 time 2019-02-22 12:32:25.873594
Model ind 685 epoch 78 head B batch: 0 avg loss -2.169614 avg loss no lamb -2.169614 time 2019-02-22 12:33:19.463882
Model ind 685 epoch 78 head B batch: 100 avg loss -2.108155 avg loss no lamb -2.108155 time 2019-02-22 12:34:14.819975
Model ind 685 epoch 78 head B batch: 200 avg loss -2.128757 avg loss no lamb -2.128757 time 2019-02-22 12:35:08.680539
Model ind 685 epoch 78 head B batch: 300 avg loss -2.190250 avg loss no lamb -2.190250 time 2019-02-22 12:36:02.182221
Model ind 685 epoch 78 head B batch: 400 avg loss -2.084088 avg loss no lamb -2.084088 time 2019-02-22 12:36:54.539085
Model ind 685 epoch 78 head A batch: 0 avg loss -2.182978 avg loss no lamb -2.182978 time 2019-02-22 12:37:49.970546
Model ind 685 epoch 78 head A batch: 100 avg loss -2.132947 avg loss no lamb -2.132947 time 2019-02-22 12:38:44.438464
Model ind 685 epoch 78 head A batch: 200 avg loss -2.121997 avg loss no lamb -2.121997 time 2019-02-22 12:39:38.845509
Model ind 685 epoch 78 head A batch: 300 avg loss -2.201676 avg loss no lamb -2.201676 time 2019-02-22 12:40:35.338425
Model ind 685 epoch 78 head A batch: 400 avg loss -2.164473 avg loss no lamb -2.164473 time 2019-02-22 12:41:32.092600
Pre: time 2019-02-22 12:42:40.009300: 
 	std: 0.03241863
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921857, 0.97877145, 0.9021429, 0.97882855, 0.9788]
	train_accs: [0.9921857, 0.97877145, 0.9021429, 0.97882855, 0.9788]
	best_train_sub_head: 0
	worst: 0.9021429
	avg: 0.9661457
	best: 0.9921857

Starting e_i: 79
Model ind 685 epoch 79 head B batch: 0 avg loss -2.147610 avg loss no lamb -2.147610 time 2019-02-22 12:42:41.676396
Model ind 685 epoch 79 head B batch: 100 avg loss -2.134367 avg loss no lamb -2.134367 time 2019-02-22 12:43:37.425590
Model ind 685 epoch 79 head B batch: 200 avg loss -2.176687 avg loss no lamb -2.176687 time 2019-02-22 12:44:31.165468
Model ind 685 epoch 79 head B batch: 300 avg loss -2.142345 avg loss no lamb -2.142345 time 2019-02-22 12:45:25.958132
Model ind 685 epoch 79 head B batch: 400 avg loss -2.162651 avg loss no lamb -2.162651 time 2019-02-22 12:46:19.052411
Model ind 685 epoch 79 head B batch: 0 avg loss -2.108342 avg loss no lamb -2.108342 time 2019-02-22 12:47:12.456969
Model ind 685 epoch 79 head B batch: 100 avg loss -2.156129 avg loss no lamb -2.156129 time 2019-02-22 12:48:05.891325
Model ind 685 epoch 79 head B batch: 200 avg loss -2.156890 avg loss no lamb -2.156890 time 2019-02-22 12:49:00.541396
Model ind 685 epoch 79 head B batch: 300 avg loss -2.181358 avg loss no lamb -2.181358 time 2019-02-22 12:49:54.910247
Model ind 685 epoch 79 head B batch: 400 avg loss -2.176156 avg loss no lamb -2.176156 time 2019-02-22 12:50:49.103299
Model ind 685 epoch 79 head A batch: 0 avg loss -2.201770 avg loss no lamb -2.201770 time 2019-02-22 12:51:44.067811
Model ind 685 epoch 79 head A batch: 100 avg loss -2.156340 avg loss no lamb -2.156340 time 2019-02-22 12:52:38.122128
Model ind 685 epoch 79 head A batch: 200 avg loss -2.129686 avg loss no lamb -2.129686 time 2019-02-22 12:53:32.562560
Model ind 685 epoch 79 head A batch: 300 avg loss -2.135604 avg loss no lamb -2.135604 time 2019-02-22 12:54:27.116300
Model ind 685 epoch 79 head A batch: 400 avg loss -2.177355 avg loss no lamb -2.177355 time 2019-02-22 12:55:21.862827
Pre: time 2019-02-22 12:56:30.648717: 
 	std: 0.031871438
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790286, 0.90374285, 0.9790428, 0.9790286]
	train_accs: [0.99237144, 0.9790286, 0.90374285, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.90374285
	avg: 0.96664286
	best: 0.99237144

Starting e_i: 80
Model ind 685 epoch 80 head B batch: 0 avg loss -2.200316 avg loss no lamb -2.200316 time 2019-02-22 12:56:32.341912
Model ind 685 epoch 80 head B batch: 100 avg loss -2.112131 avg loss no lamb -2.112131 time 2019-02-22 12:57:27.717664
Model ind 685 epoch 80 head B batch: 200 avg loss -2.151840 avg loss no lamb -2.151840 time 2019-02-22 12:58:22.127083
Model ind 685 epoch 80 head B batch: 300 avg loss -2.102986 avg loss no lamb -2.102986 time 2019-02-22 12:59:17.609002
Model ind 685 epoch 80 head B batch: 400 avg loss -2.126719 avg loss no lamb -2.126719 time 2019-02-22 13:00:13.909135
Model ind 685 epoch 80 head B batch: 0 avg loss -2.183042 avg loss no lamb -2.183042 time 2019-02-22 13:01:09.726903
Model ind 685 epoch 80 head B batch: 100 avg loss -2.153600 avg loss no lamb -2.153600 time 2019-02-22 13:02:05.516084
Model ind 685 epoch 80 head B batch: 200 avg loss -2.117694 avg loss no lamb -2.117694 time 2019-02-22 13:02:58.947967
Model ind 685 epoch 80 head B batch: 300 avg loss -2.133961 avg loss no lamb -2.133961 time 2019-02-22 13:03:54.448748
Model ind 685 epoch 80 head B batch: 400 avg loss -2.137646 avg loss no lamb -2.137646 time 2019-02-22 13:04:50.244329
Model ind 685 epoch 80 head A batch: 0 avg loss -2.171050 avg loss no lamb -2.171050 time 2019-02-22 13:05:43.797253
Model ind 685 epoch 80 head A batch: 100 avg loss -2.155005 avg loss no lamb -2.155005 time 2019-02-22 13:06:37.756890
Model ind 685 epoch 80 head A batch: 200 avg loss -2.149894 avg loss no lamb -2.149894 time 2019-02-22 13:07:31.788483
Model ind 685 epoch 80 head A batch: 300 avg loss -2.135862 avg loss no lamb -2.135862 time 2019-02-22 13:08:25.679384
Model ind 685 epoch 80 head A batch: 400 avg loss -2.188491 avg loss no lamb -2.188491 time 2019-02-22 13:09:20.163845
Pre: time 2019-02-22 13:10:29.022962: 
 	std: 0.032285623
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789571, 0.9026286, 0.97891426, 0.97892857]
	train_accs: [0.99235713, 0.9789571, 0.9026286, 0.97891426, 0.97892857]
	best_train_sub_head: 0
	worst: 0.9026286
	avg: 0.9663571
	best: 0.99235713

Starting e_i: 81
Model ind 685 epoch 81 head B batch: 0 avg loss -2.161065 avg loss no lamb -2.161065 time 2019-02-22 13:10:31.696795
Model ind 685 epoch 81 head B batch: 100 avg loss -2.139210 avg loss no lamb -2.139210 time 2019-02-22 13:11:26.279506
Model ind 685 epoch 81 head B batch: 200 avg loss -2.142127 avg loss no lamb -2.142127 time 2019-02-22 13:12:21.613235
Model ind 685 epoch 81 head B batch: 300 avg loss -2.140805 avg loss no lamb -2.140805 time 2019-02-22 13:13:15.743742
Model ind 685 epoch 81 head B batch: 400 avg loss -2.115689 avg loss no lamb -2.115689 time 2019-02-22 13:14:10.057265
Model ind 685 epoch 81 head B batch: 0 avg loss -2.169093 avg loss no lamb -2.169093 time 2019-02-22 13:15:03.466774
Model ind 685 epoch 81 head B batch: 100 avg loss -2.169846 avg loss no lamb -2.169846 time 2019-02-22 13:15:57.142170
Model ind 685 epoch 81 head B batch: 200 avg loss -2.143836 avg loss no lamb -2.143836 time 2019-02-22 13:16:52.077263
Model ind 685 epoch 81 head B batch: 300 avg loss -2.190546 avg loss no lamb -2.190546 time 2019-02-22 13:17:45.254879
Model ind 685 epoch 81 head B batch: 400 avg loss -2.146312 avg loss no lamb -2.146312 time 2019-02-22 13:18:38.526158
Model ind 685 epoch 81 head A batch: 0 avg loss -2.140811 avg loss no lamb -2.140811 time 2019-02-22 13:19:32.813712
Model ind 685 epoch 81 head A batch: 100 avg loss -2.168810 avg loss no lamb -2.168810 time 2019-02-22 13:20:27.992921
Model ind 685 epoch 81 head A batch: 200 avg loss -2.165797 avg loss no lamb -2.165797 time 2019-02-22 13:21:22.369965
Model ind 685 epoch 81 head A batch: 300 avg loss -2.196947 avg loss no lamb -2.196947 time 2019-02-22 13:22:18.515760
Model ind 685 epoch 81 head A batch: 400 avg loss -2.151786 avg loss no lamb -2.151786 time 2019-02-22 13:23:14.721920
Pre: time 2019-02-22 13:24:21.620290: 
 	std: 0.032526936
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789571, 0.90202856, 0.9789857, 0.9789429]
	train_accs: [0.9923428, 0.9789571, 0.90202856, 0.9789857, 0.9789429]
	best_train_sub_head: 0
	worst: 0.90202856
	avg: 0.9662514
	best: 0.9923428

Starting e_i: 82
Model ind 685 epoch 82 head B batch: 0 avg loss -2.170241 avg loss no lamb -2.170241 time 2019-02-22 13:24:23.605895
Model ind 685 epoch 82 head B batch: 100 avg loss -2.172604 avg loss no lamb -2.172604 time 2019-02-22 13:25:18.514464
Model ind 685 epoch 82 head B batch: 200 avg loss -2.152710 avg loss no lamb -2.152710 time 2019-02-22 13:26:15.922839
Model ind 685 epoch 82 head B batch: 300 avg loss -2.162893 avg loss no lamb -2.162893 time 2019-02-22 13:27:10.040185
Model ind 685 epoch 82 head B batch: 400 avg loss -2.133664 avg loss no lamb -2.133664 time 2019-02-22 13:28:03.869664
Model ind 685 epoch 82 head B batch: 0 avg loss -2.132758 avg loss no lamb -2.132758 time 2019-02-22 13:28:57.729004
Model ind 685 epoch 82 head B batch: 100 avg loss -2.166756 avg loss no lamb -2.166756 time 2019-02-22 13:29:51.538640
Model ind 685 epoch 82 head B batch: 200 avg loss -2.183785 avg loss no lamb -2.183785 time 2019-02-22 13:30:45.137641
Model ind 685 epoch 82 head B batch: 300 avg loss -2.184766 avg loss no lamb -2.184766 time 2019-02-22 13:31:38.688018
Model ind 685 epoch 82 head B batch: 400 avg loss -2.129046 avg loss no lamb -2.129046 time 2019-02-22 13:32:33.049865
Model ind 685 epoch 82 head A batch: 0 avg loss -2.157477 avg loss no lamb -2.157477 time 2019-02-22 13:33:25.881735
Model ind 685 epoch 82 head A batch: 100 avg loss -2.157730 avg loss no lamb -2.157730 time 2019-02-22 13:34:19.073050
Model ind 685 epoch 82 head A batch: 200 avg loss -2.094423 avg loss no lamb -2.094423 time 2019-02-22 13:35:13.383162
Model ind 685 epoch 82 head A batch: 300 avg loss -2.153356 avg loss no lamb -2.153356 time 2019-02-22 13:36:07.432432
Model ind 685 epoch 82 head A batch: 400 avg loss -2.171335 avg loss no lamb -2.171335 time 2019-02-22 13:37:00.728485
Pre: time 2019-02-22 13:38:08.315116: 
 	std: 0.03233507
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.9789, 0.9024429, 0.9789, 0.9789]
	train_accs: [0.9922571, 0.9789, 0.9024429, 0.9789, 0.9789]
	best_train_sub_head: 0
	worst: 0.9024429
	avg: 0.96628
	best: 0.9922571

Starting e_i: 83
Model ind 685 epoch 83 head B batch: 0 avg loss -2.162720 avg loss no lamb -2.162720 time 2019-02-22 13:38:09.908942
Model ind 685 epoch 83 head B batch: 100 avg loss -2.154738 avg loss no lamb -2.154738 time 2019-02-22 13:39:04.484153
Model ind 685 epoch 83 head B batch: 200 avg loss -2.135642 avg loss no lamb -2.135642 time 2019-02-22 13:39:58.619889
Model ind 685 epoch 83 head B batch: 300 avg loss -2.158515 avg loss no lamb -2.158515 time 2019-02-22 13:40:52.981567
Model ind 685 epoch 83 head B batch: 400 avg loss -2.130089 avg loss no lamb -2.130089 time 2019-02-22 13:41:46.470823
Model ind 685 epoch 83 head B batch: 0 avg loss -2.180044 avg loss no lamb -2.180044 time 2019-02-22 13:42:41.282133
Model ind 685 epoch 83 head B batch: 100 avg loss -2.176117 avg loss no lamb -2.176117 time 2019-02-22 13:43:38.940658
Model ind 685 epoch 83 head B batch: 200 avg loss -2.142174 avg loss no lamb -2.142174 time 2019-02-22 13:44:35.440885
Model ind 685 epoch 83 head B batch: 300 avg loss -2.165912 avg loss no lamb -2.165912 time 2019-02-22 13:45:29.522510
Model ind 685 epoch 83 head B batch: 400 avg loss -2.166634 avg loss no lamb -2.166634 time 2019-02-22 13:46:23.963572
Model ind 685 epoch 83 head A batch: 0 avg loss -2.193248 avg loss no lamb -2.193248 time 2019-02-22 13:47:17.518897
Model ind 685 epoch 83 head A batch: 100 avg loss -2.128586 avg loss no lamb -2.128586 time 2019-02-22 13:48:11.852840
Model ind 685 epoch 83 head A batch: 200 avg loss -2.113098 avg loss no lamb -2.113098 time 2019-02-22 13:49:06.934079
Model ind 685 epoch 83 head A batch: 300 avg loss -2.154314 avg loss no lamb -2.154314 time 2019-02-22 13:50:01.449809
Model ind 685 epoch 83 head A batch: 400 avg loss -2.189519 avg loss no lamb -2.189519 time 2019-02-22 13:50:55.348249
Pre: time 2019-02-22 13:52:03.344804: 
 	std: 0.032084692
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9032, 0.9790286, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9032, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9032
	avg: 0.9665286
	best: 0.9923857

Starting e_i: 84
Model ind 685 epoch 84 head B batch: 0 avg loss -2.161567 avg loss no lamb -2.161567 time 2019-02-22 13:52:04.894702
Model ind 685 epoch 84 head B batch: 100 avg loss -2.163112 avg loss no lamb -2.163112 time 2019-02-22 13:52:59.420500
Model ind 685 epoch 84 head B batch: 200 avg loss -2.126980 avg loss no lamb -2.126980 time 2019-02-22 13:53:53.385458
Model ind 685 epoch 84 head B batch: 300 avg loss -2.178321 avg loss no lamb -2.178321 time 2019-02-22 13:54:48.278857
Model ind 685 epoch 84 head B batch: 400 avg loss -2.181645 avg loss no lamb -2.181645 time 2019-02-22 13:55:41.579633
Model ind 685 epoch 84 head B batch: 0 avg loss -2.188488 avg loss no lamb -2.188488 time 2019-02-22 13:56:34.685789
Model ind 685 epoch 84 head B batch: 100 avg loss -2.119672 avg loss no lamb -2.119672 time 2019-02-22 13:57:28.266414
Model ind 685 epoch 84 head B batch: 200 avg loss -2.144091 avg loss no lamb -2.144091 time 2019-02-22 13:58:21.722132
Model ind 685 epoch 84 head B batch: 300 avg loss -2.185897 avg loss no lamb -2.185897 time 2019-02-22 13:59:14.507792
Model ind 685 epoch 84 head B batch: 400 avg loss -2.122879 avg loss no lamb -2.122879 time 2019-02-22 14:00:08.296456
Model ind 685 epoch 84 head A batch: 0 avg loss -2.200033 avg loss no lamb -2.200033 time 2019-02-22 14:01:01.946615
Model ind 685 epoch 84 head A batch: 100 avg loss -2.200065 avg loss no lamb -2.200065 time 2019-02-22 14:01:56.297596
Model ind 685 epoch 84 head A batch: 200 avg loss -2.136944 avg loss no lamb -2.136944 time 2019-02-22 14:02:50.515205
Model ind 685 epoch 84 head A batch: 300 avg loss -2.185497 avg loss no lamb -2.185497 time 2019-02-22 14:03:43.643321
Model ind 685 epoch 84 head A batch: 400 avg loss -2.174539 avg loss no lamb -2.174539 time 2019-02-22 14:04:39.235543
Pre: time 2019-02-22 14:05:46.888245: 
 	std: 0.032355614
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.9788143, 0.90234286, 0.9788143, 0.97882855]
	train_accs: [0.9922571, 0.9788143, 0.90234286, 0.9788143, 0.97882855]
	best_train_sub_head: 0
	worst: 0.90234286
	avg: 0.96621144
	best: 0.9922571

Starting e_i: 85
Model ind 685 epoch 85 head B batch: 0 avg loss -2.161169 avg loss no lamb -2.161169 time 2019-02-22 14:05:48.838954
Model ind 685 epoch 85 head B batch: 100 avg loss -2.180801 avg loss no lamb -2.180801 time 2019-02-22 14:06:45.203490
Model ind 685 epoch 85 head B batch: 200 avg loss -2.133669 avg loss no lamb -2.133669 time 2019-02-22 14:07:39.846518
Model ind 685 epoch 85 head B batch: 300 avg loss -2.195495 avg loss no lamb -2.195495 time 2019-02-22 14:08:34.635514
Model ind 685 epoch 85 head B batch: 400 avg loss -2.159964 avg loss no lamb -2.159964 time 2019-02-22 14:09:27.788522
Model ind 685 epoch 85 head B batch: 0 avg loss -2.188230 avg loss no lamb -2.188230 time 2019-02-22 14:10:22.232492
Model ind 685 epoch 85 head B batch: 100 avg loss -2.124102 avg loss no lamb -2.124102 time 2019-02-22 14:11:15.173663
Model ind 685 epoch 85 head B batch: 200 avg loss -2.123775 avg loss no lamb -2.123775 time 2019-02-22 14:12:08.165849
Model ind 685 epoch 85 head B batch: 300 avg loss -2.202083 avg loss no lamb -2.202083 time 2019-02-22 14:13:01.219000
Model ind 685 epoch 85 head B batch: 400 avg loss -2.157096 avg loss no lamb -2.157096 time 2019-02-22 14:13:55.139953
Model ind 685 epoch 85 head A batch: 0 avg loss -2.153608 avg loss no lamb -2.153608 time 2019-02-22 14:14:48.483247
Model ind 685 epoch 85 head A batch: 100 avg loss -2.187051 avg loss no lamb -2.187051 time 2019-02-22 14:15:42.175366
Model ind 685 epoch 85 head A batch: 200 avg loss -2.151006 avg loss no lamb -2.151006 time 2019-02-22 14:16:35.564662
Model ind 685 epoch 85 head A batch: 300 avg loss -2.207463 avg loss no lamb -2.207463 time 2019-02-22 14:17:31.790488
Model ind 685 epoch 85 head A batch: 400 avg loss -2.156438 avg loss no lamb -2.156438 time 2019-02-22 14:18:26.788566
Pre: time 2019-02-22 14:19:35.622744: 
 	std: 0.032216918
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.97892857, 0.9028, 0.97891426, 0.97891426]
	train_accs: [0.99237144, 0.97892857, 0.9028, 0.97891426, 0.97891426]
	best_train_sub_head: 0
	worst: 0.9028
	avg: 0.96638566
	best: 0.99237144

Starting e_i: 86
Model ind 685 epoch 86 head B batch: 0 avg loss -2.146855 avg loss no lamb -2.146855 time 2019-02-22 14:19:37.285529
Model ind 685 epoch 86 head B batch: 100 avg loss -2.110441 avg loss no lamb -2.110441 time 2019-02-22 14:20:31.058768
Model ind 685 epoch 86 head B batch: 200 avg loss -2.185547 avg loss no lamb -2.185547 time 2019-02-22 14:21:24.408342
Model ind 685 epoch 86 head B batch: 300 avg loss -2.160207 avg loss no lamb -2.160207 time 2019-02-22 14:22:18.889007
Model ind 685 epoch 86 head B batch: 400 avg loss -2.173140 avg loss no lamb -2.173140 time 2019-02-22 14:23:11.838557
Model ind 685 epoch 86 head B batch: 0 avg loss -2.134136 avg loss no lamb -2.134136 time 2019-02-22 14:24:05.941799
Model ind 685 epoch 86 head B batch: 100 avg loss -2.145958 avg loss no lamb -2.145958 time 2019-02-22 14:25:01.643438
Model ind 685 epoch 86 head B batch: 200 avg loss -2.141566 avg loss no lamb -2.141566 time 2019-02-22 14:25:55.973913
Model ind 685 epoch 86 head B batch: 300 avg loss -2.162452 avg loss no lamb -2.162452 time 2019-02-22 14:26:50.313413
Model ind 685 epoch 86 head B batch: 400 avg loss -2.176045 avg loss no lamb -2.176045 time 2019-02-22 14:27:45.491569
Model ind 685 epoch 86 head A batch: 0 avg loss -2.206782 avg loss no lamb -2.206782 time 2019-02-22 14:28:38.753876
Model ind 685 epoch 86 head A batch: 100 avg loss -2.159001 avg loss no lamb -2.159001 time 2019-02-22 14:29:32.173645
Model ind 685 epoch 86 head A batch: 200 avg loss -2.174117 avg loss no lamb -2.174117 time 2019-02-22 14:30:25.139038
Model ind 685 epoch 86 head A batch: 300 avg loss -2.198615 avg loss no lamb -2.198615 time 2019-02-22 14:31:18.373506
Model ind 685 epoch 86 head A batch: 400 avg loss -2.148839 avg loss no lamb -2.148839 time 2019-02-22 14:32:11.615920
Pre: time 2019-02-22 14:33:19.054287: 
 	std: 0.032219067
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.979, 0.90288574, 0.9790286, 0.9790428]
	train_accs: [0.99244285, 0.979, 0.90288574, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.90288574
	avg: 0.9664801
	best: 0.99244285

Starting e_i: 87
Model ind 685 epoch 87 head B batch: 0 avg loss -2.176556 avg loss no lamb -2.176556 time 2019-02-22 14:33:21.839939
Model ind 685 epoch 87 head B batch: 100 avg loss -2.141463 avg loss no lamb -2.141463 time 2019-02-22 14:34:16.384652
Model ind 685 epoch 87 head B batch: 200 avg loss -2.151356 avg loss no lamb -2.151356 time 2019-02-22 14:35:10.345347
Model ind 685 epoch 87 head B batch: 300 avg loss -2.167571 avg loss no lamb -2.167571 time 2019-02-22 14:36:05.076541
Model ind 685 epoch 87 head B batch: 400 avg loss -2.171917 avg loss no lamb -2.171917 time 2019-02-22 14:36:58.586541
Model ind 685 epoch 87 head B batch: 0 avg loss -2.151497 avg loss no lamb -2.151497 time 2019-02-22 14:37:52.918789
Model ind 685 epoch 87 head B batch: 100 avg loss -2.135254 avg loss no lamb -2.135254 time 2019-02-22 14:38:47.267177
Model ind 685 epoch 87 head B batch: 200 avg loss -2.127084 avg loss no lamb -2.127084 time 2019-02-22 14:39:41.049352
Model ind 685 epoch 87 head B batch: 300 avg loss -2.230025 avg loss no lamb -2.230025 time 2019-02-22 14:40:35.592834
Model ind 685 epoch 87 head B batch: 400 avg loss -2.147645 avg loss no lamb -2.147645 time 2019-02-22 14:41:29.289034
Model ind 685 epoch 87 head A batch: 0 avg loss -2.179513 avg loss no lamb -2.179513 time 2019-02-22 14:42:23.108433
Model ind 685 epoch 87 head A batch: 100 avg loss -2.142919 avg loss no lamb -2.142919 time 2019-02-22 14:43:15.810076
Model ind 685 epoch 87 head A batch: 200 avg loss -2.105510 avg loss no lamb -2.105510 time 2019-02-22 14:44:09.028863
Model ind 685 epoch 87 head A batch: 300 avg loss -2.212398 avg loss no lamb -2.212398 time 2019-02-22 14:45:02.877641
Model ind 685 epoch 87 head A batch: 400 avg loss -2.169824 avg loss no lamb -2.169824 time 2019-02-22 14:45:56.083741
Pre: time 2019-02-22 14:47:05.279920: 
 	std: 0.0321322
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.97907144, 0.9031571, 0.97908574, 0.97908574]
	train_accs: [0.9924857, 0.97907144, 0.9031571, 0.97908574, 0.97908574]
	best_train_sub_head: 0
	worst: 0.9031571
	avg: 0.9665772
	best: 0.9924857

Starting e_i: 88
Model ind 685 epoch 88 head B batch: 0 avg loss -2.193091 avg loss no lamb -2.193091 time 2019-02-22 14:47:08.067394
Model ind 685 epoch 88 head B batch: 100 avg loss -2.136302 avg loss no lamb -2.136302 time 2019-02-22 14:48:02.745777
Model ind 685 epoch 88 head B batch: 200 avg loss -2.153427 avg loss no lamb -2.153427 time 2019-02-22 14:48:57.148703
Model ind 685 epoch 88 head B batch: 300 avg loss -2.136443 avg loss no lamb -2.136443 time 2019-02-22 14:49:53.631016
Model ind 685 epoch 88 head B batch: 400 avg loss -2.106260 avg loss no lamb -2.106260 time 2019-02-22 14:50:46.684545
Model ind 685 epoch 88 head B batch: 0 avg loss -2.165198 avg loss no lamb -2.165198 time 2019-02-22 14:51:40.338285
Model ind 685 epoch 88 head B batch: 100 avg loss -2.179922 avg loss no lamb -2.179922 time 2019-02-22 14:52:34.049313
Model ind 685 epoch 88 head B batch: 200 avg loss -2.140332 avg loss no lamb -2.140332 time 2019-02-22 14:53:27.215693
Model ind 685 epoch 88 head B batch: 300 avg loss -2.216644 avg loss no lamb -2.216644 time 2019-02-22 14:54:21.942353
Model ind 685 epoch 88 head B batch: 400 avg loss -2.152233 avg loss no lamb -2.152233 time 2019-02-22 14:55:17.255946
Model ind 685 epoch 88 head A batch: 0 avg loss -2.151371 avg loss no lamb -2.151371 time 2019-02-22 14:56:11.321659
Model ind 685 epoch 88 head A batch: 100 avg loss -2.157007 avg loss no lamb -2.157007 time 2019-02-22 14:57:06.690626
Model ind 685 epoch 88 head A batch: 200 avg loss -2.072344 avg loss no lamb -2.072344 time 2019-02-22 14:58:01.252952
Model ind 685 epoch 88 head A batch: 300 avg loss -2.189814 avg loss no lamb -2.189814 time 2019-02-22 14:58:54.863921
Model ind 685 epoch 88 head A batch: 400 avg loss -2.143077 avg loss no lamb -2.143077 time 2019-02-22 14:59:47.900351
Pre: time 2019-02-22 15:00:57.213307: 
 	std: 0.030473614
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.979, 0.9073, 0.9789857, 0.979]
	train_accs: [0.99245715, 0.979, 0.9073, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9073
	avg: 0.9673486
	best: 0.99245715

Starting e_i: 89
Model ind 685 epoch 89 head B batch: 0 avg loss -2.173275 avg loss no lamb -2.173275 time 2019-02-22 15:00:58.838539
Model ind 685 epoch 89 head B batch: 100 avg loss -2.140687 avg loss no lamb -2.140687 time 2019-02-22 15:01:52.668411
Model ind 685 epoch 89 head B batch: 200 avg loss -2.108791 avg loss no lamb -2.108791 time 2019-02-22 15:02:46.538796
Model ind 685 epoch 89 head B batch: 300 avg loss -2.160040 avg loss no lamb -2.160040 time 2019-02-22 15:03:41.218344
Model ind 685 epoch 89 head B batch: 400 avg loss -2.177521 avg loss no lamb -2.177521 time 2019-02-22 15:04:38.472417
Model ind 685 epoch 89 head B batch: 0 avg loss -2.162257 avg loss no lamb -2.162257 time 2019-02-22 15:05:33.502308
Model ind 685 epoch 89 head B batch: 100 avg loss -2.114412 avg loss no lamb -2.114412 time 2019-02-22 15:06:28.315524
Model ind 685 epoch 89 head B batch: 200 avg loss -2.156255 avg loss no lamb -2.156255 time 2019-02-22 15:07:21.986006
Model ind 685 epoch 89 head B batch: 300 avg loss -2.178432 avg loss no lamb -2.178432 time 2019-02-22 15:08:16.360613
Model ind 685 epoch 89 head B batch: 400 avg loss -2.143526 avg loss no lamb -2.143526 time 2019-02-22 15:09:09.927910
Model ind 685 epoch 89 head A batch: 0 avg loss -2.153812 avg loss no lamb -2.153812 time 2019-02-22 15:10:04.641105
Model ind 685 epoch 89 head A batch: 100 avg loss -2.158900 avg loss no lamb -2.158900 time 2019-02-22 15:10:59.427218
Model ind 685 epoch 89 head A batch: 200 avg loss -2.131511 avg loss no lamb -2.131511 time 2019-02-22 15:11:53.016445
Model ind 685 epoch 89 head A batch: 300 avg loss -2.158226 avg loss no lamb -2.158226 time 2019-02-22 15:12:47.369238
Model ind 685 epoch 89 head A batch: 400 avg loss -2.134986 avg loss no lamb -2.134986 time 2019-02-22 15:13:40.384761
Pre: time 2019-02-22 15:14:48.352008: 
 	std: 0.021931378
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.92908573, 0.979, 0.979]
	train_accs: [0.9924143, 0.979, 0.92908573, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.92908573
	avg: 0.9717
	best: 0.9924143

Starting e_i: 90
Model ind 685 epoch 90 head B batch: 0 avg loss -2.178503 avg loss no lamb -2.178503 time 2019-02-22 15:14:50.029314
Model ind 685 epoch 90 head B batch: 100 avg loss -2.144099 avg loss no lamb -2.144099 time 2019-02-22 15:15:45.542446
Model ind 685 epoch 90 head B batch: 200 avg loss -2.151865 avg loss no lamb -2.151865 time 2019-02-22 15:16:40.599208
Model ind 685 epoch 90 head B batch: 300 avg loss -2.205409 avg loss no lamb -2.205409 time 2019-02-22 15:17:36.033368
Model ind 685 epoch 90 head B batch: 400 avg loss -2.121284 avg loss no lamb -2.121284 time 2019-02-22 15:18:30.359257
Model ind 685 epoch 90 head B batch: 0 avg loss -2.201898 avg loss no lamb -2.201898 time 2019-02-22 15:19:25.735478
Model ind 685 epoch 90 head B batch: 100 avg loss -2.193884 avg loss no lamb -2.193884 time 2019-02-22 15:20:19.870600
Model ind 685 epoch 90 head B batch: 200 avg loss -2.162245 avg loss no lamb -2.162245 time 2019-02-22 15:21:13.512858
Model ind 685 epoch 90 head B batch: 300 avg loss -2.156810 avg loss no lamb -2.156810 time 2019-02-22 15:22:08.089282
Model ind 685 epoch 90 head B batch: 400 avg loss -2.204391 avg loss no lamb -2.204391 time 2019-02-22 15:23:01.476694
Model ind 685 epoch 90 head A batch: 0 avg loss -2.227989 avg loss no lamb -2.227989 time 2019-02-22 15:23:57.216507
Model ind 685 epoch 90 head A batch: 100 avg loss -2.183567 avg loss no lamb -2.183567 time 2019-02-22 15:24:52.997649
Model ind 685 epoch 90 head A batch: 200 avg loss -2.214995 avg loss no lamb -2.214995 time 2019-02-22 15:25:48.614804
Model ind 685 epoch 90 head A batch: 300 avg loss -2.213715 avg loss no lamb -2.213715 time 2019-02-22 15:26:43.292916
Model ind 685 epoch 90 head A batch: 400 avg loss -2.166571 avg loss no lamb -2.166571 time 2019-02-22 15:27:37.123643
Pre: time 2019-02-22 15:28:46.724304: 
 	std: 0.0063234502
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.9914143, 0.979, 0.9790143]
	train_accs: [0.99237144, 0.9789857, 0.9914143, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9841572
	best: 0.99237144

Starting e_i: 91
Model ind 685 epoch 91 head B batch: 0 avg loss -2.207163 avg loss no lamb -2.207163 time 2019-02-22 15:28:48.432320
Model ind 685 epoch 91 head B batch: 100 avg loss -2.192256 avg loss no lamb -2.192256 time 2019-02-22 15:29:42.819019
Model ind 685 epoch 91 head B batch: 200 avg loss -2.146127 avg loss no lamb -2.146127 time 2019-02-22 15:30:37.479047
Model ind 685 epoch 91 head B batch: 300 avg loss -2.210387 avg loss no lamb -2.210387 time 2019-02-22 15:31:32.218505
Model ind 685 epoch 91 head B batch: 400 avg loss -2.161765 avg loss no lamb -2.161765 time 2019-02-22 15:32:27.554967
Model ind 685 epoch 91 head B batch: 0 avg loss -2.180746 avg loss no lamb -2.180746 time 2019-02-22 15:33:22.488916
Model ind 685 epoch 91 head B batch: 100 avg loss -2.204749 avg loss no lamb -2.204749 time 2019-02-22 15:34:17.943406
Model ind 685 epoch 91 head B batch: 200 avg loss -2.192030 avg loss no lamb -2.192030 time 2019-02-22 15:35:11.372665
Model ind 685 epoch 91 head B batch: 300 avg loss -2.208101 avg loss no lamb -2.208101 time 2019-02-22 15:36:03.779684
Model ind 685 epoch 91 head B batch: 400 avg loss -2.155769 avg loss no lamb -2.155769 time 2019-02-22 15:36:57.930271
Model ind 685 epoch 91 head A batch: 0 avg loss -2.159136 avg loss no lamb -2.159136 time 2019-02-22 15:37:51.002767
Model ind 685 epoch 91 head A batch: 100 avg loss -2.214162 avg loss no lamb -2.214162 time 2019-02-22 15:38:44.941964
Model ind 685 epoch 91 head A batch: 200 avg loss -2.189226 avg loss no lamb -2.189226 time 2019-02-22 15:39:39.590335
Model ind 685 epoch 91 head A batch: 300 avg loss -2.210596 avg loss no lamb -2.210596 time 2019-02-22 15:40:34.000278
Model ind 685 epoch 91 head A batch: 400 avg loss -2.202681 avg loss no lamb -2.202681 time 2019-02-22 15:41:28.219980
Pre: time 2019-02-22 15:42:38.535078: 
 	std: 0.0062786792
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.97905713, 0.9913143, 0.97905713, 0.9790428]
	train_accs: [0.9923857, 0.97905713, 0.9913143, 0.97905713, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9841714
	best: 0.9923857

Starting e_i: 92
Model ind 685 epoch 92 head B batch: 0 avg loss -2.209249 avg loss no lamb -2.209249 time 2019-02-22 15:42:40.202015
Model ind 685 epoch 92 head B batch: 100 avg loss -2.179360 avg loss no lamb -2.179360 time 2019-02-22 15:43:35.484336
Model ind 685 epoch 92 head B batch: 200 avg loss -2.148932 avg loss no lamb -2.148932 time 2019-02-22 15:44:30.803610
Model ind 685 epoch 92 head B batch: 300 avg loss -2.186442 avg loss no lamb -2.186442 time 2019-02-22 15:45:25.832358
Model ind 685 epoch 92 head B batch: 400 avg loss -2.196004 avg loss no lamb -2.196004 time 2019-02-22 15:46:20.307579
Model ind 685 epoch 92 head B batch: 0 avg loss -2.196894 avg loss no lamb -2.196894 time 2019-02-22 15:47:14.322254
Model ind 685 epoch 92 head B batch: 100 avg loss -2.137587 avg loss no lamb -2.137587 time 2019-02-22 15:48:09.962815
Model ind 685 epoch 92 head B batch: 200 avg loss -2.198469 avg loss no lamb -2.198469 time 2019-02-22 15:49:03.909441
Model ind 685 epoch 92 head B batch: 300 avg loss -2.202957 avg loss no lamb -2.202957 time 2019-02-22 15:49:57.454619
Model ind 685 epoch 92 head B batch: 400 avg loss -2.203504 avg loss no lamb -2.203504 time 2019-02-22 15:50:52.159499
Model ind 685 epoch 92 head A batch: 0 avg loss -2.210193 avg loss no lamb -2.210193 time 2019-02-22 15:51:46.189728
Model ind 685 epoch 92 head A batch: 100 avg loss -2.161161 avg loss no lamb -2.161161 time 2019-02-22 15:52:41.818972
Model ind 685 epoch 92 head A batch: 200 avg loss -2.184914 avg loss no lamb -2.184914 time 2019-02-22 15:53:36.418795
Model ind 685 epoch 92 head A batch: 300 avg loss -2.214018 avg loss no lamb -2.214018 time 2019-02-22 15:54:31.532202
Model ind 685 epoch 92 head A batch: 400 avg loss -2.161283 avg loss no lamb -2.161283 time 2019-02-22 15:55:25.619331
Pre: time 2019-02-22 15:56:35.544987: 
 	std: 0.006278046
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.97908574, 0.99125713, 0.97908574, 0.97907144]
	train_accs: [0.9924857, 0.97908574, 0.99125713, 0.97908574, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.98419714
	best: 0.9924857

Starting e_i: 93
Model ind 685 epoch 93 head B batch: 0 avg loss -2.178488 avg loss no lamb -2.178488 time 2019-02-22 15:56:37.463472
Model ind 685 epoch 93 head B batch: 100 avg loss -2.134241 avg loss no lamb -2.134241 time 2019-02-22 15:57:33.505258
Model ind 685 epoch 93 head B batch: 200 avg loss -2.172988 avg loss no lamb -2.172988 time 2019-02-22 15:58:27.822964
Model ind 685 epoch 93 head B batch: 300 avg loss -2.194456 avg loss no lamb -2.194456 time 2019-02-22 15:59:22.185866
Model ind 685 epoch 93 head B batch: 400 avg loss -2.119785 avg loss no lamb -2.119785 time 2019-02-22 16:00:15.791160
Model ind 685 epoch 93 head B batch: 0 avg loss -2.143409 avg loss no lamb -2.143409 time 2019-02-22 16:01:11.478899
Model ind 685 epoch 93 head B batch: 100 avg loss -2.168925 avg loss no lamb -2.168925 time 2019-02-22 16:02:06.037537
Model ind 685 epoch 93 head B batch: 200 avg loss -2.210166 avg loss no lamb -2.210166 time 2019-02-22 16:03:00.271740
Model ind 685 epoch 93 head B batch: 300 avg loss -2.167936 avg loss no lamb -2.167936 time 2019-02-22 16:03:55.352420
Model ind 685 epoch 93 head B batch: 400 avg loss -2.173720 avg loss no lamb -2.173720 time 2019-02-22 16:04:51.561667
Model ind 685 epoch 93 head A batch: 0 avg loss -2.202649 avg loss no lamb -2.202649 time 2019-02-22 16:05:46.937818
Model ind 685 epoch 93 head A batch: 100 avg loss -2.209148 avg loss no lamb -2.209148 time 2019-02-22 16:06:41.131494
Model ind 685 epoch 93 head A batch: 200 avg loss -2.212079 avg loss no lamb -2.212079 time 2019-02-22 16:07:35.055515
Model ind 685 epoch 93 head A batch: 300 avg loss -2.192468 avg loss no lamb -2.192468 time 2019-02-22 16:08:28.698685
Model ind 685 epoch 93 head A batch: 400 avg loss -2.151217 avg loss no lamb -2.151217 time 2019-02-22 16:09:23.183372
Pre: time 2019-02-22 16:10:33.010472: 
 	std: 0.006357974
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.97892857, 0.99144286, 0.9789429, 0.9789571]
	train_accs: [0.99237144, 0.97892857, 0.99144286, 0.9789429, 0.9789571]
	best_train_sub_head: 0
	worst: 0.97892857
	avg: 0.9841286
	best: 0.99237144

Starting e_i: 94
Model ind 685 epoch 94 head B batch: 0 avg loss -2.205218 avg loss no lamb -2.205218 time 2019-02-22 16:10:34.601430
Model ind 685 epoch 94 head B batch: 100 avg loss -2.181227 avg loss no lamb -2.181227 time 2019-02-22 16:11:28.921134
Model ind 685 epoch 94 head B batch: 200 avg loss -2.174068 avg loss no lamb -2.174068 time 2019-02-22 16:12:25.100694
Model ind 685 epoch 94 head B batch: 300 avg loss -2.141837 avg loss no lamb -2.141837 time 2019-02-22 16:13:20.054440
Model ind 685 epoch 94 head B batch: 400 avg loss -2.160453 avg loss no lamb -2.160453 time 2019-02-22 16:14:14.009270
Model ind 685 epoch 94 head B batch: 0 avg loss -2.184901 avg loss no lamb -2.184901 time 2019-02-22 16:15:07.772876
Model ind 685 epoch 94 head B batch: 100 avg loss -2.185571 avg loss no lamb -2.185571 time 2019-02-22 16:16:03.415028
Model ind 685 epoch 94 head B batch: 200 avg loss -2.140688 avg loss no lamb -2.140688 time 2019-02-22 16:16:57.250390
Model ind 685 epoch 94 head B batch: 300 avg loss -2.167668 avg loss no lamb -2.167668 time 2019-02-22 16:17:51.148234
Model ind 685 epoch 94 head B batch: 400 avg loss -2.116271 avg loss no lamb -2.116271 time 2019-02-22 16:18:45.327460
Model ind 685 epoch 94 head A batch: 0 avg loss -2.192638 avg loss no lamb -2.192638 time 2019-02-22 16:19:38.525551
Model ind 685 epoch 94 head A batch: 100 avg loss -2.164423 avg loss no lamb -2.164423 time 2019-02-22 16:20:32.894588
Model ind 685 epoch 94 head A batch: 200 avg loss -2.162965 avg loss no lamb -2.162965 time 2019-02-22 16:21:27.138317
Model ind 685 epoch 94 head A batch: 300 avg loss -2.192208 avg loss no lamb -2.192208 time 2019-02-22 16:22:20.875163
Model ind 685 epoch 94 head A batch: 400 avg loss -2.156634 avg loss no lamb -2.156634 time 2019-02-22 16:23:16.416918
Pre: time 2019-02-22 16:24:25.883903: 
 	std: 0.006374102
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9791286, 0.9918143, 0.9791143, 0.9791286]
	train_accs: [0.99244285, 0.9791286, 0.9918143, 0.9791143, 0.9791286]
	best_train_sub_head: 0
	worst: 0.9791143
	avg: 0.98432577
	best: 0.99244285

Starting e_i: 95
Model ind 685 epoch 95 head B batch: 0 avg loss -2.175836 avg loss no lamb -2.175836 time 2019-02-22 16:24:27.677174
Model ind 685 epoch 95 head B batch: 100 avg loss -2.197224 avg loss no lamb -2.197224 time 2019-02-22 16:25:21.994937
Model ind 685 epoch 95 head B batch: 200 avg loss -2.192890 avg loss no lamb -2.192890 time 2019-02-22 16:26:17.918159
Model ind 685 epoch 95 head B batch: 300 avg loss -2.157996 avg loss no lamb -2.157996 time 2019-02-22 16:27:13.149691
Model ind 685 epoch 95 head B batch: 400 avg loss -2.149538 avg loss no lamb -2.149538 time 2019-02-22 16:28:07.782075
Model ind 685 epoch 95 head B batch: 0 avg loss -2.163088 avg loss no lamb -2.163088 time 2019-02-22 16:29:02.094001
Model ind 685 epoch 95 head B batch: 100 avg loss -2.164501 avg loss no lamb -2.164501 time 2019-02-22 16:29:56.821888
Model ind 685 epoch 95 head B batch: 200 avg loss -2.158547 avg loss no lamb -2.158547 time 2019-02-22 16:30:51.334610
Model ind 685 epoch 95 head B batch: 300 avg loss -2.184169 avg loss no lamb -2.184169 time 2019-02-22 16:31:45.396017
Model ind 685 epoch 95 head B batch: 400 avg loss -2.153345 avg loss no lamb -2.153345 time 2019-02-22 16:32:40.931148
Model ind 685 epoch 95 head A batch: 0 avg loss -2.155992 avg loss no lamb -2.155992 time 2019-02-22 16:33:35.362448
Model ind 685 epoch 95 head A batch: 100 avg loss -2.176240 avg loss no lamb -2.176240 time 2019-02-22 16:34:30.389934
Model ind 685 epoch 95 head A batch: 200 avg loss -2.183025 avg loss no lamb -2.183025 time 2019-02-22 16:35:24.907408
Model ind 685 epoch 95 head A batch: 300 avg loss -2.169287 avg loss no lamb -2.169287 time 2019-02-22 16:36:19.143109
Model ind 685 epoch 95 head A batch: 400 avg loss -2.192593 avg loss no lamb -2.192593 time 2019-02-22 16:37:14.455603
Pre: time 2019-02-22 16:38:23.239876: 
 	std: 0.0063217203
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99247146, 0.9791857, 0.99172854, 0.9792143, 0.9792143]
	train_accs: [0.99247146, 0.9791857, 0.99172854, 0.9792143, 0.9792143]
	best_train_sub_head: 0
	worst: 0.9791857
	avg: 0.9843628
	best: 0.99247146

Starting e_i: 96
Model ind 685 epoch 96 head B batch: 0 avg loss -2.146256 avg loss no lamb -2.146256 time 2019-02-22 16:38:25.168267
Model ind 685 epoch 96 head B batch: 100 avg loss -2.165841 avg loss no lamb -2.165841 time 2019-02-22 16:39:18.386033
Model ind 685 epoch 96 head B batch: 200 avg loss -2.212069 avg loss no lamb -2.212069 time 2019-02-22 16:40:13.955674
Model ind 685 epoch 96 head B batch: 300 avg loss -2.207518 avg loss no lamb -2.207518 time 2019-02-22 16:41:08.528004
Model ind 685 epoch 96 head B batch: 400 avg loss -2.132781 avg loss no lamb -2.132781 time 2019-02-22 16:42:03.161105
Model ind 685 epoch 96 head B batch: 0 avg loss -2.178397 avg loss no lamb -2.178397 time 2019-02-22 16:42:56.558899
Model ind 685 epoch 96 head B batch: 100 avg loss -2.172501 avg loss no lamb -2.172501 time 2019-02-22 16:43:51.252861
Model ind 685 epoch 96 head B batch: 200 avg loss -2.170835 avg loss no lamb -2.170835 time 2019-02-22 16:44:45.374851
Model ind 685 epoch 96 head B batch: 300 avg loss -2.225851 avg loss no lamb -2.225851 time 2019-02-22 16:45:40.620782
Model ind 685 epoch 96 head B batch: 400 avg loss -2.168423 avg loss no lamb -2.168423 time 2019-02-22 16:46:34.636946
Model ind 685 epoch 96 head A batch: 0 avg loss -2.166988 avg loss no lamb -2.166988 time 2019-02-22 16:47:29.552142
Model ind 685 epoch 96 head A batch: 100 avg loss -2.178921 avg loss no lamb -2.178921 time 2019-02-22 16:48:24.473429
Model ind 685 epoch 96 head A batch: 200 avg loss -2.182499 avg loss no lamb -2.182499 time 2019-02-22 16:49:18.573038
Model ind 685 epoch 96 head A batch: 300 avg loss -2.188445 avg loss no lamb -2.188445 time 2019-02-22 16:50:12.758612
Model ind 685 epoch 96 head A batch: 400 avg loss -2.195025 avg loss no lamb -2.195025 time 2019-02-22 16:51:07.154305
Pre: time 2019-02-22 16:52:15.802902: 
 	std: 0.006388867
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790286, 0.9917857, 0.9790428, 0.97905713]
	train_accs: [0.99237144, 0.9790286, 0.9917857, 0.9790428, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9842571
	best: 0.99237144

Starting e_i: 97
Model ind 685 epoch 97 head B batch: 0 avg loss -2.202394 avg loss no lamb -2.202394 time 2019-02-22 16:52:17.405219
Model ind 685 epoch 97 head B batch: 100 avg loss -2.197131 avg loss no lamb -2.197131 time 2019-02-22 16:53:11.015684
Model ind 685 epoch 97 head B batch: 200 avg loss -2.125929 avg loss no lamb -2.125929 time 2019-02-22 16:54:06.778847
Model ind 685 epoch 97 head B batch: 300 avg loss -2.196115 avg loss no lamb -2.196115 time 2019-02-22 16:55:02.945497
Model ind 685 epoch 97 head B batch: 400 avg loss -2.184472 avg loss no lamb -2.184472 time 2019-02-22 16:55:57.919332
Model ind 685 epoch 97 head B batch: 0 avg loss -2.212618 avg loss no lamb -2.212618 time 2019-02-22 16:56:53.126642
Model ind 685 epoch 97 head B batch: 100 avg loss -2.166703 avg loss no lamb -2.166703 time 2019-02-22 16:57:48.461846
Model ind 685 epoch 97 head B batch: 200 avg loss -2.162057 avg loss no lamb -2.162057 time 2019-02-22 16:58:43.291559
Model ind 685 epoch 97 head B batch: 300 avg loss -2.189111 avg loss no lamb -2.189111 time 2019-02-22 16:59:37.684312
Model ind 685 epoch 97 head B batch: 400 avg loss -2.165395 avg loss no lamb -2.165395 time 2019-02-22 17:00:32.203837
Model ind 685 epoch 97 head A batch: 0 avg loss -2.203737 avg loss no lamb -2.203737 time 2019-02-22 17:01:27.441416
Model ind 685 epoch 97 head A batch: 100 avg loss -2.183841 avg loss no lamb -2.183841 time 2019-02-22 17:02:21.701822
Model ind 685 epoch 97 head A batch: 200 avg loss -2.188262 avg loss no lamb -2.188262 time 2019-02-22 17:03:16.433686
Model ind 685 epoch 97 head A batch: 300 avg loss -2.148494 avg loss no lamb -2.148494 time 2019-02-22 17:04:11.514362
Model ind 685 epoch 97 head A batch: 400 avg loss -2.172458 avg loss no lamb -2.172458 time 2019-02-22 17:05:06.283969
Pre: time 2019-02-22 17:06:14.965063: 
 	std: 0.006362361
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790428, 0.99167144, 0.97907144, 0.97905713]
	train_accs: [0.9924, 0.9790428, 0.99167144, 0.97907144, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9842485
	best: 0.9924

Starting e_i: 98
Model ind 685 epoch 98 head B batch: 0 avg loss -2.217655 avg loss no lamb -2.217655 time 2019-02-22 17:06:16.551395
Model ind 685 epoch 98 head B batch: 100 avg loss -2.149169 avg loss no lamb -2.149169 time 2019-02-22 17:07:11.285642
Model ind 685 epoch 98 head B batch: 200 avg loss -2.164273 avg loss no lamb -2.164273 time 2019-02-22 17:08:08.168636
Model ind 685 epoch 98 head B batch: 300 avg loss -2.239245 avg loss no lamb -2.239245 time 2019-02-22 17:09:03.113963
Model ind 685 epoch 98 head B batch: 400 avg loss -2.173280 avg loss no lamb -2.173280 time 2019-02-22 17:09:57.289303
Model ind 685 epoch 98 head B batch: 0 avg loss -2.214345 avg loss no lamb -2.214345 time 2019-02-22 17:10:50.769714
Model ind 685 epoch 98 head B batch: 100 avg loss -2.180734 avg loss no lamb -2.180734 time 2019-02-22 17:11:44.538811
Model ind 685 epoch 98 head B batch: 200 avg loss -2.204625 avg loss no lamb -2.204625 time 2019-02-22 17:12:40.090894
Model ind 685 epoch 98 head B batch: 300 avg loss -2.179619 avg loss no lamb -2.179619 time 2019-02-22 17:13:34.797306
Model ind 685 epoch 98 head B batch: 400 avg loss -2.142716 avg loss no lamb -2.142716 time 2019-02-22 17:14:29.615960
Model ind 685 epoch 98 head A batch: 0 avg loss -2.157697 avg loss no lamb -2.157697 time 2019-02-22 17:15:23.875905
Model ind 685 epoch 98 head A batch: 100 avg loss -2.214687 avg loss no lamb -2.214687 time 2019-02-22 17:16:17.194051
Model ind 685 epoch 98 head A batch: 200 avg loss -2.200112 avg loss no lamb -2.200112 time 2019-02-22 17:17:11.073564
Model ind 685 epoch 98 head A batch: 300 avg loss -2.162550 avg loss no lamb -2.162550 time 2019-02-22 17:18:05.091155
Model ind 685 epoch 98 head A batch: 400 avg loss -2.131532 avg loss no lamb -2.131532 time 2019-02-22 17:19:00.604223
Pre: time 2019-02-22 17:20:10.196988: 
 	std: 0.0063713496
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.9789429, 0.9916, 0.9789571, 0.9789571]
	train_accs: [0.9923, 0.9789429, 0.9916, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.9841515
	best: 0.9923

Starting e_i: 99
Model ind 685 epoch 99 head B batch: 0 avg loss -2.191625 avg loss no lamb -2.191625 time 2019-02-22 17:20:11.830437
Model ind 685 epoch 99 head B batch: 100 avg loss -2.175563 avg loss no lamb -2.175563 time 2019-02-22 17:21:05.285166
Model ind 685 epoch 99 head B batch: 200 avg loss -2.194653 avg loss no lamb -2.194653 time 2019-02-22 17:22:00.675770
Model ind 685 epoch 99 head B batch: 300 avg loss -2.223770 avg loss no lamb -2.223770 time 2019-02-22 17:22:56.460477
Model ind 685 epoch 99 head B batch: 400 avg loss -2.181527 avg loss no lamb -2.181527 time 2019-02-22 17:23:49.868326
Model ind 685 epoch 99 head B batch: 0 avg loss -2.179866 avg loss no lamb -2.179866 time 2019-02-22 17:24:43.660633
Model ind 685 epoch 99 head B batch: 100 avg loss -2.190973 avg loss no lamb -2.190973 time 2019-02-22 17:25:37.456778
Model ind 685 epoch 99 head B batch: 200 avg loss -2.172396 avg loss no lamb -2.172396 time 2019-02-22 17:26:32.943903
Model ind 685 epoch 99 head B batch: 300 avg loss -2.195187 avg loss no lamb -2.195187 time 2019-02-22 17:27:26.803252
Model ind 685 epoch 99 head B batch: 400 avg loss -2.164418 avg loss no lamb -2.164418 time 2019-02-22 17:28:20.289512
Model ind 685 epoch 99 head A batch: 0 avg loss -2.208467 avg loss no lamb -2.208467 time 2019-02-22 17:29:12.610762
Model ind 685 epoch 99 head A batch: 100 avg loss -2.195157 avg loss no lamb -2.195157 time 2019-02-22 17:30:05.017870
Model ind 685 epoch 99 head A batch: 200 avg loss -2.214962 avg loss no lamb -2.214962 time 2019-02-22 17:30:58.560531
Model ind 685 epoch 99 head A batch: 300 avg loss -2.183161 avg loss no lamb -2.183161 time 2019-02-22 17:31:51.544241
Model ind 685 epoch 99 head A batch: 400 avg loss -2.121518 avg loss no lamb -2.121518 time 2019-02-22 17:32:44.949966
Pre: time 2019-02-22 17:33:54.090402: 
 	std: 0.0064503094
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.992, 0.9790428, 0.9790286]
	train_accs: [0.9923857, 0.9790143, 0.992, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98429424
	best: 0.9923857

Starting e_i: 100
Model ind 685 epoch 100 head B batch: 0 avg loss -2.215241 avg loss no lamb -2.215241 time 2019-02-22 17:33:56.319440
Model ind 685 epoch 100 head B batch: 100 avg loss -2.223985 avg loss no lamb -2.223985 time 2019-02-22 17:34:50.218207
Model ind 685 epoch 100 head B batch: 200 avg loss -2.225699 avg loss no lamb -2.225699 time 2019-02-22 17:35:44.325547
Model ind 685 epoch 100 head B batch: 300 avg loss -2.171365 avg loss no lamb -2.171365 time 2019-02-22 17:36:38.448914
Model ind 685 epoch 100 head B batch: 400 avg loss -2.178331 avg loss no lamb -2.178331 time 2019-02-22 17:37:31.262864
Model ind 685 epoch 100 head B batch: 0 avg loss -2.145839 avg loss no lamb -2.145839 time 2019-02-22 17:38:24.367989
Model ind 685 epoch 100 head B batch: 100 avg loss -2.187099 avg loss no lamb -2.187099 time 2019-02-22 17:39:17.840941
Model ind 685 epoch 100 head B batch: 200 avg loss -2.173530 avg loss no lamb -2.173530 time 2019-02-22 17:40:11.440773
Model ind 685 epoch 100 head B batch: 300 avg loss -2.201352 avg loss no lamb -2.201352 time 2019-02-22 17:41:04.891917
Model ind 685 epoch 100 head B batch: 400 avg loss -2.153559 avg loss no lamb -2.153559 time 2019-02-22 17:41:59.197758
Model ind 685 epoch 100 head A batch: 0 avg loss -2.219234 avg loss no lamb -2.219234 time 2019-02-22 17:42:52.149113
Model ind 685 epoch 100 head A batch: 100 avg loss -2.178490 avg loss no lamb -2.178490 time 2019-02-22 17:43:46.119677
Model ind 685 epoch 100 head A batch: 200 avg loss -2.167270 avg loss no lamb -2.167270 time 2019-02-22 17:44:39.584050
Model ind 685 epoch 100 head A batch: 300 avg loss -2.196747 avg loss no lamb -2.196747 time 2019-02-22 17:45:33.647410
Model ind 685 epoch 100 head A batch: 400 avg loss -2.158233 avg loss no lamb -2.158233 time 2019-02-22 17:46:27.067966
Pre: time 2019-02-22 17:47:36.065084: 
 	std: 0.006530272
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9788857, 0.9920857, 0.9789, 0.97891426]
	train_accs: [0.99237144, 0.9788857, 0.9920857, 0.9789, 0.97891426]
	best_train_sub_head: 0
	worst: 0.9788857
	avg: 0.98423135
	best: 0.99237144

Starting e_i: 101
Model ind 685 epoch 101 head B batch: 0 avg loss -2.195957 avg loss no lamb -2.195957 time 2019-02-22 17:47:38.896647
Model ind 685 epoch 101 head B batch: 100 avg loss -2.190043 avg loss no lamb -2.190043 time 2019-02-22 17:48:32.261876
Model ind 685 epoch 101 head B batch: 200 avg loss -2.191361 avg loss no lamb -2.191361 time 2019-02-22 17:49:26.049630
Model ind 685 epoch 101 head B batch: 300 avg loss -2.185328 avg loss no lamb -2.185328 time 2019-02-22 17:50:19.424527
Model ind 685 epoch 101 head B batch: 400 avg loss -2.156665 avg loss no lamb -2.156665 time 2019-02-22 17:51:13.147958
Model ind 685 epoch 101 head B batch: 0 avg loss -2.195029 avg loss no lamb -2.195029 time 2019-02-22 17:52:06.711190
Model ind 685 epoch 101 head B batch: 100 avg loss -2.181195 avg loss no lamb -2.181195 time 2019-02-22 17:53:00.059822
Model ind 685 epoch 101 head B batch: 200 avg loss -2.198846 avg loss no lamb -2.198846 time 2019-02-22 17:53:52.928959
Model ind 685 epoch 101 head B batch: 300 avg loss -2.194393 avg loss no lamb -2.194393 time 2019-02-22 17:54:45.872137
Model ind 685 epoch 101 head B batch: 400 avg loss -2.137932 avg loss no lamb -2.137932 time 2019-02-22 17:55:39.218867
Model ind 685 epoch 101 head A batch: 0 avg loss -2.225453 avg loss no lamb -2.225453 time 2019-02-22 17:56:32.616714
Model ind 685 epoch 101 head A batch: 100 avg loss -2.126739 avg loss no lamb -2.126739 time 2019-02-22 17:57:25.943167
Model ind 685 epoch 101 head A batch: 200 avg loss -2.192345 avg loss no lamb -2.192345 time 2019-02-22 17:58:20.148626
Model ind 685 epoch 101 head A batch: 300 avg loss -2.153462 avg loss no lamb -2.153462 time 2019-02-22 17:59:14.032004
Model ind 685 epoch 101 head A batch: 400 avg loss -2.179494 avg loss no lamb -2.179494 time 2019-02-22 18:00:08.551286
Pre: time 2019-02-22 18:01:16.228593: 
 	std: 0.0065111155
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97907144, 0.9923, 0.97908574, 0.97908574]
	train_accs: [0.99244285, 0.97907144, 0.9923, 0.97908574, 0.97908574]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.9843971
	best: 0.99244285

Starting e_i: 102
Model ind 685 epoch 102 head B batch: 0 avg loss -2.191970 avg loss no lamb -2.191970 time 2019-02-22 18:01:18.068206
Model ind 685 epoch 102 head B batch: 100 avg loss -2.184627 avg loss no lamb -2.184627 time 2019-02-22 18:02:11.406337
Model ind 685 epoch 102 head B batch: 200 avg loss -2.181418 avg loss no lamb -2.181418 time 2019-02-22 18:03:05.783237
Model ind 685 epoch 102 head B batch: 300 avg loss -2.159354 avg loss no lamb -2.159354 time 2019-02-22 18:03:58.882096
Model ind 685 epoch 102 head B batch: 400 avg loss -2.165357 avg loss no lamb -2.165357 time 2019-02-22 18:04:51.901496
Model ind 685 epoch 102 head B batch: 0 avg loss -2.178409 avg loss no lamb -2.178409 time 2019-02-22 18:05:45.423954
Model ind 685 epoch 102 head B batch: 100 avg loss -2.201658 avg loss no lamb -2.201658 time 2019-02-22 18:06:38.334481
Model ind 685 epoch 102 head B batch: 200 avg loss -2.156419 avg loss no lamb -2.156419 time 2019-02-22 18:07:32.300056
Model ind 685 epoch 102 head B batch: 300 avg loss -2.237348 avg loss no lamb -2.237348 time 2019-02-22 18:08:24.996664
Model ind 685 epoch 102 head B batch: 400 avg loss -2.145672 avg loss no lamb -2.145672 time 2019-02-22 18:09:19.087043
Model ind 685 epoch 102 head A batch: 0 avg loss -2.191052 avg loss no lamb -2.191052 time 2019-02-22 18:10:14.246970
Model ind 685 epoch 102 head A batch: 100 avg loss -2.161868 avg loss no lamb -2.161868 time 2019-02-22 18:11:08.272350
Model ind 685 epoch 102 head A batch: 200 avg loss -2.182115 avg loss no lamb -2.182115 time 2019-02-22 18:12:02.439880
Model ind 685 epoch 102 head A batch: 300 avg loss -2.204468 avg loss no lamb -2.204468 time 2019-02-22 18:12:56.284059
Model ind 685 epoch 102 head A batch: 400 avg loss -2.124004 avg loss no lamb -2.124004 time 2019-02-22 18:13:49.458935
Pre: time 2019-02-22 18:14:57.323369: 
 	std: 0.006445076
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9789857, 0.9920143, 0.979, 0.979]
	train_accs: [0.9922857, 0.9789857, 0.9920143, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842572
	best: 0.9922857

Starting e_i: 103
Model ind 685 epoch 103 head B batch: 0 avg loss -2.175321 avg loss no lamb -2.175321 time 2019-02-22 18:14:58.926780
Model ind 685 epoch 103 head B batch: 100 avg loss -2.132595 avg loss no lamb -2.132595 time 2019-02-22 18:15:51.381738
Model ind 685 epoch 103 head B batch: 200 avg loss -2.180720 avg loss no lamb -2.180720 time 2019-02-22 18:16:45.995206
Model ind 685 epoch 103 head B batch: 300 avg loss -2.191218 avg loss no lamb -2.191218 time 2019-02-22 18:17:40.139352
Model ind 685 epoch 103 head B batch: 400 avg loss -2.200593 avg loss no lamb -2.200593 time 2019-02-22 18:18:33.519302
Model ind 685 epoch 103 head B batch: 0 avg loss -2.237209 avg loss no lamb -2.237209 time 2019-02-22 18:19:28.228589
Model ind 685 epoch 103 head B batch: 100 avg loss -2.206246 avg loss no lamb -2.206246 time 2019-02-22 18:20:20.570695
Model ind 685 epoch 103 head B batch: 200 avg loss -2.171639 avg loss no lamb -2.171639 time 2019-02-22 18:21:13.432868
Model ind 685 epoch 103 head B batch: 300 avg loss -2.168459 avg loss no lamb -2.168459 time 2019-02-22 18:22:07.389706
Model ind 685 epoch 103 head B batch: 400 avg loss -2.142654 avg loss no lamb -2.142654 time 2019-02-22 18:23:00.889654
Model ind 685 epoch 103 head A batch: 0 avg loss -2.209714 avg loss no lamb -2.209714 time 2019-02-22 18:23:54.551392
Model ind 685 epoch 103 head A batch: 100 avg loss -2.211154 avg loss no lamb -2.211154 time 2019-02-22 18:24:47.819871
Model ind 685 epoch 103 head A batch: 200 avg loss -2.121756 avg loss no lamb -2.121756 time 2019-02-22 18:25:41.777988
Model ind 685 epoch 103 head A batch: 300 avg loss -2.176398 avg loss no lamb -2.176398 time 2019-02-22 18:26:34.122803
Model ind 685 epoch 103 head A batch: 400 avg loss -2.181209 avg loss no lamb -2.181209 time 2019-02-22 18:27:28.288952
Pre: time 2019-02-22 18:28:35.750833: 
 	std: 0.0064588417
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922714, 0.9789714, 0.99205714, 0.9789857, 0.9789857]
	train_accs: [0.9922714, 0.9789714, 0.99205714, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98425424
	best: 0.9922714

Starting e_i: 104
Model ind 685 epoch 104 head B batch: 0 avg loss -2.173753 avg loss no lamb -2.173753 time 2019-02-22 18:28:37.801515
Model ind 685 epoch 104 head B batch: 100 avg loss -2.215584 avg loss no lamb -2.215584 time 2019-02-22 18:29:30.955113
Model ind 685 epoch 104 head B batch: 200 avg loss -2.216417 avg loss no lamb -2.216417 time 2019-02-22 18:30:24.875491
Model ind 685 epoch 104 head B batch: 300 avg loss -2.222129 avg loss no lamb -2.222129 time 2019-02-22 18:31:20.383141
Model ind 685 epoch 104 head B batch: 400 avg loss -2.184752 avg loss no lamb -2.184752 time 2019-02-22 18:32:13.239572
Model ind 685 epoch 104 head B batch: 0 avg loss -2.183031 avg loss no lamb -2.183031 time 2019-02-22 18:33:07.751395
Model ind 685 epoch 104 head B batch: 100 avg loss -2.188504 avg loss no lamb -2.188504 time 2019-02-22 18:34:02.949885
Model ind 685 epoch 104 head B batch: 200 avg loss -2.139839 avg loss no lamb -2.139839 time 2019-02-22 18:34:56.175267
Model ind 685 epoch 104 head B batch: 300 avg loss -2.187927 avg loss no lamb -2.187927 time 2019-02-22 18:35:48.309391
Model ind 685 epoch 104 head B batch: 400 avg loss -2.147772 avg loss no lamb -2.147772 time 2019-02-22 18:36:40.897049
Model ind 685 epoch 104 head A batch: 0 avg loss -2.191241 avg loss no lamb -2.191241 time 2019-02-22 18:37:33.231301
Model ind 685 epoch 104 head A batch: 100 avg loss -2.163188 avg loss no lamb -2.163188 time 2019-02-22 18:38:26.840229
Model ind 685 epoch 104 head A batch: 200 avg loss -2.214198 avg loss no lamb -2.214198 time 2019-02-22 18:39:20.372474
Model ind 685 epoch 104 head A batch: 300 avg loss -2.178173 avg loss no lamb -2.178173 time 2019-02-22 18:40:14.946901
Model ind 685 epoch 104 head A batch: 400 avg loss -2.138068 avg loss no lamb -2.138068 time 2019-02-22 18:41:09.142794
Pre: time 2019-02-22 18:42:16.464148: 
 	std: 0.006457825
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9790286, 0.9920857, 0.9790428, 0.9790286]
	train_accs: [0.9923428, 0.9790286, 0.9920857, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843057
	best: 0.9923428

Starting e_i: 105
Model ind 685 epoch 105 head B batch: 0 avg loss -2.173447 avg loss no lamb -2.173447 time 2019-02-22 18:42:18.086578
Model ind 685 epoch 105 head B batch: 100 avg loss -2.177014 avg loss no lamb -2.177014 time 2019-02-22 18:43:11.806717
Model ind 685 epoch 105 head B batch: 200 avg loss -2.192421 avg loss no lamb -2.192421 time 2019-02-22 18:44:06.985440
Model ind 685 epoch 105 head B batch: 300 avg loss -2.185109 avg loss no lamb -2.185109 time 2019-02-22 18:45:00.470047
Model ind 685 epoch 105 head B batch: 400 avg loss -2.137351 avg loss no lamb -2.137351 time 2019-02-22 18:45:54.519151
Model ind 685 epoch 105 head B batch: 0 avg loss -2.186805 avg loss no lamb -2.186805 time 2019-02-22 18:46:47.878194
Model ind 685 epoch 105 head B batch: 100 avg loss -2.206953 avg loss no lamb -2.206953 time 2019-02-22 18:47:42.343285
Model ind 685 epoch 105 head B batch: 200 avg loss -2.148101 avg loss no lamb -2.148101 time 2019-02-22 18:48:34.763015
Model ind 685 epoch 105 head B batch: 300 avg loss -2.189792 avg loss no lamb -2.189792 time 2019-02-22 18:49:29.213047
Model ind 685 epoch 105 head B batch: 400 avg loss -2.177104 avg loss no lamb -2.177104 time 2019-02-22 18:50:23.824444
Model ind 685 epoch 105 head A batch: 0 avg loss -2.209410 avg loss no lamb -2.209410 time 2019-02-22 18:51:17.118905
Model ind 685 epoch 105 head A batch: 100 avg loss -2.211798 avg loss no lamb -2.211798 time 2019-02-22 18:52:10.269948
Model ind 685 epoch 105 head A batch: 200 avg loss -2.183047 avg loss no lamb -2.183047 time 2019-02-22 18:53:03.368857
Model ind 685 epoch 105 head A batch: 300 avg loss -2.207355 avg loss no lamb -2.207355 time 2019-02-22 18:53:57.734662
Model ind 685 epoch 105 head A batch: 400 avg loss -2.185093 avg loss no lamb -2.185093 time 2019-02-22 18:54:51.118098
Pre: time 2019-02-22 18:55:58.330722: 
 	std: 0.006467291
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9790143, 0.99207145, 0.9790143, 0.9790143]
	train_accs: [0.99235713, 0.9790143, 0.99207145, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9842943
	best: 0.99235713

Starting e_i: 106
Model ind 685 epoch 106 head B batch: 0 avg loss -2.183094 avg loss no lamb -2.183094 time 2019-02-22 18:55:59.965281
Model ind 685 epoch 106 head B batch: 100 avg loss -2.194225 avg loss no lamb -2.194225 time 2019-02-22 18:56:52.379816
Model ind 685 epoch 106 head B batch: 200 avg loss -2.165057 avg loss no lamb -2.165057 time 2019-02-22 18:57:47.505834
Model ind 685 epoch 106 head B batch: 300 avg loss -2.191331 avg loss no lamb -2.191331 time 2019-02-22 18:58:39.875543
Model ind 685 epoch 106 head B batch: 400 avg loss -2.163971 avg loss no lamb -2.163971 time 2019-02-22 18:59:32.559654
Model ind 685 epoch 106 head B batch: 0 avg loss -2.218984 avg loss no lamb -2.218984 time 2019-02-22 19:00:26.685942
Model ind 685 epoch 106 head B batch: 100 avg loss -2.188047 avg loss no lamb -2.188047 time 2019-02-22 19:01:19.601291
Model ind 685 epoch 106 head B batch: 200 avg loss -2.148121 avg loss no lamb -2.148121 time 2019-02-22 19:02:13.984892
Model ind 685 epoch 106 head B batch: 300 avg loss -2.179994 avg loss no lamb -2.179994 time 2019-02-22 19:03:07.409437
Model ind 685 epoch 106 head B batch: 400 avg loss -2.172818 avg loss no lamb -2.172818 time 2019-02-22 19:04:00.610121
Model ind 685 epoch 106 head A batch: 0 avg loss -2.177850 avg loss no lamb -2.177850 time 2019-02-22 19:04:54.374386
Model ind 685 epoch 106 head A batch: 100 avg loss -2.167603 avg loss no lamb -2.167603 time 2019-02-22 19:05:48.873839
Model ind 685 epoch 106 head A batch: 200 avg loss -2.171522 avg loss no lamb -2.171522 time 2019-02-22 19:06:41.918764
Model ind 685 epoch 106 head A batch: 300 avg loss -2.216360 avg loss no lamb -2.216360 time 2019-02-22 19:07:35.418611
Model ind 685 epoch 106 head A batch: 400 avg loss -2.172379 avg loss no lamb -2.172379 time 2019-02-22 19:08:29.791411
Pre: time 2019-02-22 19:09:40.603747: 
 	std: 0.0064999615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790143, 0.99215716, 0.9790428, 0.9790428]
	train_accs: [0.99244285, 0.9790143, 0.99215716, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98434
	best: 0.99244285

Starting e_i: 107
Model ind 685 epoch 107 head B batch: 0 avg loss -2.190053 avg loss no lamb -2.190053 time 2019-02-22 19:09:42.504465
Model ind 685 epoch 107 head B batch: 100 avg loss -2.162126 avg loss no lamb -2.162126 time 2019-02-22 19:10:35.726620
Model ind 685 epoch 107 head B batch: 200 avg loss -2.156328 avg loss no lamb -2.156328 time 2019-02-22 19:11:31.646578
Model ind 685 epoch 107 head B batch: 300 avg loss -2.184960 avg loss no lamb -2.184960 time 2019-02-22 19:12:24.567423
Model ind 685 epoch 107 head B batch: 400 avg loss -2.148561 avg loss no lamb -2.148561 time 2019-02-22 19:13:19.063994
Model ind 685 epoch 107 head B batch: 0 avg loss -2.187222 avg loss no lamb -2.187222 time 2019-02-22 19:14:13.613045
Model ind 685 epoch 107 head B batch: 100 avg loss -2.215420 avg loss no lamb -2.215420 time 2019-02-22 19:15:07.491511
Model ind 685 epoch 107 head B batch: 200 avg loss -2.165946 avg loss no lamb -2.165946 time 2019-02-22 19:16:01.500858
Model ind 685 epoch 107 head B batch: 300 avg loss -2.155687 avg loss no lamb -2.155687 time 2019-02-22 19:16:55.218946
Model ind 685 epoch 107 head B batch: 400 avg loss -2.144491 avg loss no lamb -2.144491 time 2019-02-22 19:17:48.968409
Model ind 685 epoch 107 head A batch: 0 avg loss -2.177796 avg loss no lamb -2.177796 time 2019-02-22 19:18:42.395976
Model ind 685 epoch 107 head A batch: 100 avg loss -2.126590 avg loss no lamb -2.126590 time 2019-02-22 19:19:36.183081
Model ind 685 epoch 107 head A batch: 200 avg loss -2.132966 avg loss no lamb -2.132966 time 2019-02-22 19:20:28.929519
Model ind 685 epoch 107 head A batch: 300 avg loss -2.202104 avg loss no lamb -2.202104 time 2019-02-22 19:21:22.867094
Model ind 685 epoch 107 head A batch: 400 avg loss -2.199806 avg loss no lamb -2.199806 time 2019-02-22 19:22:15.890016
Pre: time 2019-02-22 19:23:24.002850: 
 	std: 0.0064529795
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99222857, 0.9789429, 0.9920286, 0.9789571, 0.9789714]
	train_accs: [0.99222857, 0.9789429, 0.9920286, 0.9789571, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98422575
	best: 0.99222857

Starting e_i: 108
Model ind 685 epoch 108 head B batch: 0 avg loss -2.160026 avg loss no lamb -2.160026 time 2019-02-22 19:23:25.759825
Model ind 685 epoch 108 head B batch: 100 avg loss -2.201563 avg loss no lamb -2.201563 time 2019-02-22 19:24:20.058615
Model ind 685 epoch 108 head B batch: 200 avg loss -2.157859 avg loss no lamb -2.157859 time 2019-02-22 19:25:14.923866
Model ind 685 epoch 108 head B batch: 300 avg loss -2.233630 avg loss no lamb -2.233630 time 2019-02-22 19:26:08.546313
Model ind 685 epoch 108 head B batch: 400 avg loss -2.166466 avg loss no lamb -2.166466 time 2019-02-22 19:27:02.227155
Model ind 685 epoch 108 head B batch: 0 avg loss -2.215117 avg loss no lamb -2.215117 time 2019-02-22 19:27:54.658446
Model ind 685 epoch 108 head B batch: 100 avg loss -2.195338 avg loss no lamb -2.195338 time 2019-02-22 19:28:47.910138
Model ind 685 epoch 108 head B batch: 200 avg loss -2.215185 avg loss no lamb -2.215185 time 2019-02-22 19:29:40.892225
Model ind 685 epoch 108 head B batch: 300 avg loss -2.230151 avg loss no lamb -2.230151 time 2019-02-22 19:30:33.257163
Model ind 685 epoch 108 head B batch: 400 avg loss -2.198483 avg loss no lamb -2.198483 time 2019-02-22 19:31:27.074655
Model ind 685 epoch 108 head A batch: 0 avg loss -2.206702 avg loss no lamb -2.206702 time 2019-02-22 19:32:20.342051
Model ind 685 epoch 108 head A batch: 100 avg loss -2.194051 avg loss no lamb -2.194051 time 2019-02-22 19:33:13.137148
Model ind 685 epoch 108 head A batch: 200 avg loss -2.159133 avg loss no lamb -2.159133 time 2019-02-22 19:34:08.326906
Model ind 685 epoch 108 head A batch: 300 avg loss -2.220045 avg loss no lamb -2.220045 time 2019-02-22 19:35:01.711234
Model ind 685 epoch 108 head A batch: 400 avg loss -2.169942 avg loss no lamb -2.169942 time 2019-02-22 19:35:55.153718
Pre: time 2019-02-22 19:37:02.276569: 
 	std: 0.006449726
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922429, 0.9789429, 0.99197143, 0.9789429, 0.9789429]
	train_accs: [0.9922429, 0.9789429, 0.99197143, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.9842086
	best: 0.9922429

Starting e_i: 109
Model ind 685 epoch 109 head B batch: 0 avg loss -2.175000 avg loss no lamb -2.175000 time 2019-02-22 19:37:04.249486
Model ind 685 epoch 109 head B batch: 100 avg loss -2.196425 avg loss no lamb -2.196425 time 2019-02-22 19:37:57.614242
Model ind 685 epoch 109 head B batch: 200 avg loss -2.180150 avg loss no lamb -2.180150 time 2019-02-22 19:38:51.916387
Model ind 685 epoch 109 head B batch: 300 avg loss -2.167964 avg loss no lamb -2.167964 time 2019-02-22 19:39:45.620707
Model ind 685 epoch 109 head B batch: 400 avg loss -2.185960 avg loss no lamb -2.185960 time 2019-02-22 19:40:39.986296
Model ind 685 epoch 109 head B batch: 0 avg loss -2.190628 avg loss no lamb -2.190628 time 2019-02-22 19:41:34.398268
Model ind 685 epoch 109 head B batch: 100 avg loss -2.202887 avg loss no lamb -2.202887 time 2019-02-22 19:42:27.708440
Model ind 685 epoch 109 head B batch: 200 avg loss -2.179649 avg loss no lamb -2.179649 time 2019-02-22 19:43:20.355025
Model ind 685 epoch 109 head B batch: 300 avg loss -2.157642 avg loss no lamb -2.157642 time 2019-02-22 19:44:13.816128
Model ind 685 epoch 109 head B batch: 400 avg loss -2.182544 avg loss no lamb -2.182544 time 2019-02-22 19:45:06.826784
Model ind 685 epoch 109 head A batch: 0 avg loss -2.154150 avg loss no lamb -2.154150 time 2019-02-22 19:46:00.907698
Model ind 685 epoch 109 head A batch: 100 avg loss -2.206305 avg loss no lamb -2.206305 time 2019-02-22 19:46:55.052063
Model ind 685 epoch 109 head A batch: 200 avg loss -2.203043 avg loss no lamb -2.203043 time 2019-02-22 19:47:49.487835
Model ind 685 epoch 109 head A batch: 300 avg loss -2.180803 avg loss no lamb -2.180803 time 2019-02-22 19:48:42.618617
Model ind 685 epoch 109 head A batch: 400 avg loss -2.147387 avg loss no lamb -2.147387 time 2019-02-22 19:49:35.772793
Pre: time 2019-02-22 19:50:43.832406: 
 	std: 0.00645195
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.979, 0.99205714, 0.9790143, 0.9790143]
	train_accs: [0.9923, 0.979, 0.99205714, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842771
	best: 0.9923

Starting e_i: 110
Model ind 685 epoch 110 head B batch: 0 avg loss -2.245146 avg loss no lamb -2.245146 time 2019-02-22 19:50:45.877570
Model ind 685 epoch 110 head B batch: 100 avg loss -2.133080 avg loss no lamb -2.133080 time 2019-02-22 19:51:39.305257
Model ind 685 epoch 110 head B batch: 200 avg loss -2.191660 avg loss no lamb -2.191660 time 2019-02-22 19:52:33.251267
Model ind 685 epoch 110 head B batch: 300 avg loss -2.228599 avg loss no lamb -2.228599 time 2019-02-22 19:53:26.157588
Model ind 685 epoch 110 head B batch: 400 avg loss -2.186854 avg loss no lamb -2.186854 time 2019-02-22 19:54:19.494102
Model ind 685 epoch 110 head B batch: 0 avg loss -2.220624 avg loss no lamb -2.220624 time 2019-02-22 19:55:13.376185
Model ind 685 epoch 110 head B batch: 100 avg loss -2.156356 avg loss no lamb -2.156356 time 2019-02-22 19:56:06.665350
Model ind 685 epoch 110 head B batch: 200 avg loss -2.180241 avg loss no lamb -2.180241 time 2019-02-22 19:56:59.236455
Model ind 685 epoch 110 head B batch: 300 avg loss -2.209172 avg loss no lamb -2.209172 time 2019-02-22 19:57:51.675151
Model ind 685 epoch 110 head B batch: 400 avg loss -2.150318 avg loss no lamb -2.150318 time 2019-02-22 19:58:44.878496
Model ind 685 epoch 110 head A batch: 0 avg loss -2.181298 avg loss no lamb -2.181298 time 2019-02-22 19:59:37.308842
Model ind 685 epoch 110 head A batch: 100 avg loss -2.179488 avg loss no lamb -2.179488 time 2019-02-22 20:00:31.420382
Model ind 685 epoch 110 head A batch: 200 avg loss -2.190030 avg loss no lamb -2.190030 time 2019-02-22 20:01:25.226953
Model ind 685 epoch 110 head A batch: 300 avg loss -2.176223 avg loss no lamb -2.176223 time 2019-02-22 20:02:19.412293
Model ind 685 epoch 110 head A batch: 400 avg loss -2.173335 avg loss no lamb -2.173335 time 2019-02-22 20:03:12.720465
Pre: time 2019-02-22 20:04:21.445639: 
 	std: 0.0064286245
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.97907144, 0.99207145, 0.97905713, 0.97908574]
	train_accs: [0.9923143, 0.97907144, 0.99207145, 0.97905713, 0.97908574]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98432
	best: 0.9923143

Starting e_i: 111
Model ind 685 epoch 111 head B batch: 0 avg loss -2.210714 avg loss no lamb -2.210714 time 2019-02-22 20:04:23.005130
Model ind 685 epoch 111 head B batch: 100 avg loss -2.138969 avg loss no lamb -2.138969 time 2019-02-22 20:05:15.228083
Model ind 685 epoch 111 head B batch: 200 avg loss -2.164670 avg loss no lamb -2.164670 time 2019-02-22 20:06:09.179365
Model ind 685 epoch 111 head B batch: 300 avg loss -2.171766 avg loss no lamb -2.171766 time 2019-02-22 20:07:02.078934
Model ind 685 epoch 111 head B batch: 400 avg loss -2.176821 avg loss no lamb -2.176821 time 2019-02-22 20:07:55.698150
Model ind 685 epoch 111 head B batch: 0 avg loss -2.182037 avg loss no lamb -2.182037 time 2019-02-22 20:08:48.780026
Model ind 685 epoch 111 head B batch: 100 avg loss -2.174681 avg loss no lamb -2.174681 time 2019-02-22 20:09:42.258895
Model ind 685 epoch 111 head B batch: 200 avg loss -2.190447 avg loss no lamb -2.190447 time 2019-02-22 20:10:35.735713
Model ind 685 epoch 111 head B batch: 300 avg loss -2.224873 avg loss no lamb -2.224873 time 2019-02-22 20:11:28.694754
Model ind 685 epoch 111 head B batch: 400 avg loss -2.143310 avg loss no lamb -2.143310 time 2019-02-22 20:12:21.971465
Model ind 685 epoch 111 head A batch: 0 avg loss -2.226331 avg loss no lamb -2.226331 time 2019-02-22 20:13:14.388737
Model ind 685 epoch 111 head A batch: 100 avg loss -2.206515 avg loss no lamb -2.206515 time 2019-02-22 20:14:07.696822
Model ind 685 epoch 111 head A batch: 200 avg loss -2.168627 avg loss no lamb -2.168627 time 2019-02-22 20:15:01.485522
Model ind 685 epoch 111 head A batch: 300 avg loss -2.154628 avg loss no lamb -2.154628 time 2019-02-22 20:15:53.700318
Model ind 685 epoch 111 head A batch: 400 avg loss -2.186193 avg loss no lamb -2.186193 time 2019-02-22 20:16:49.390936
Pre: time 2019-02-22 20:17:57.950989: 
 	std: 0.0064555085
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9790428, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.99235713, 0.9790428, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432285
	best: 0.99235713

Starting e_i: 112
Model ind 685 epoch 112 head B batch: 0 avg loss -2.210597 avg loss no lamb -2.210597 time 2019-02-22 20:17:59.826645
Model ind 685 epoch 112 head B batch: 100 avg loss -2.146380 avg loss no lamb -2.146380 time 2019-02-22 20:18:54.153902
Model ind 685 epoch 112 head B batch: 200 avg loss -2.184027 avg loss no lamb -2.184027 time 2019-02-22 20:19:49.984427
Model ind 685 epoch 112 head B batch: 300 avg loss -2.192626 avg loss no lamb -2.192626 time 2019-02-22 20:20:43.197081
Model ind 685 epoch 112 head B batch: 400 avg loss -2.171638 avg loss no lamb -2.171638 time 2019-02-22 20:21:38.545752
Model ind 685 epoch 112 head B batch: 0 avg loss -2.236334 avg loss no lamb -2.236334 time 2019-02-22 20:22:31.671219
Model ind 685 epoch 112 head B batch: 100 avg loss -2.181236 avg loss no lamb -2.181236 time 2019-02-22 20:23:24.777104
Model ind 685 epoch 112 head B batch: 200 avg loss -2.192690 avg loss no lamb -2.192690 time 2019-02-22 20:24:18.882606
Model ind 685 epoch 112 head B batch: 300 avg loss -2.152283 avg loss no lamb -2.152283 time 2019-02-22 20:25:12.489596
Model ind 685 epoch 112 head B batch: 400 avg loss -2.137101 avg loss no lamb -2.137101 time 2019-02-22 20:26:06.184473
Model ind 685 epoch 112 head A batch: 0 avg loss -2.196552 avg loss no lamb -2.196552 time 2019-02-22 20:26:59.363601
Model ind 685 epoch 112 head A batch: 100 avg loss -2.141892 avg loss no lamb -2.141892 time 2019-02-22 20:27:53.309492
Model ind 685 epoch 112 head A batch: 200 avg loss -2.211233 avg loss no lamb -2.211233 time 2019-02-22 20:28:47.108367
Model ind 685 epoch 112 head A batch: 300 avg loss -2.197014 avg loss no lamb -2.197014 time 2019-02-22 20:29:40.051922
Model ind 685 epoch 112 head A batch: 400 avg loss -2.169315 avg loss no lamb -2.169315 time 2019-02-22 20:30:34.226956
Pre: time 2019-02-22 20:31:43.498952: 
 	std: 0.006495291
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9921, 0.9789714, 0.9789714]
	train_accs: [0.9923857, 0.9790143, 0.9921, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842886
	best: 0.9923857

Starting e_i: 113
Model ind 685 epoch 113 head B batch: 0 avg loss -2.212627 avg loss no lamb -2.212627 time 2019-02-22 20:31:45.403003
Model ind 685 epoch 113 head B batch: 100 avg loss -2.139499 avg loss no lamb -2.139499 time 2019-02-22 20:32:39.633931
Model ind 685 epoch 113 head B batch: 200 avg loss -2.140230 avg loss no lamb -2.140230 time 2019-02-22 20:33:33.397075
Model ind 685 epoch 113 head B batch: 300 avg loss -2.218939 avg loss no lamb -2.218939 time 2019-02-22 20:34:27.625799
Model ind 685 epoch 113 head B batch: 400 avg loss -2.169442 avg loss no lamb -2.169442 time 2019-02-22 20:35:20.956509
Model ind 685 epoch 113 head B batch: 0 avg loss -2.202179 avg loss no lamb -2.202179 time 2019-02-22 20:36:14.271906
Model ind 685 epoch 113 head B batch: 100 avg loss -2.241316 avg loss no lamb -2.241316 time 2019-02-22 20:37:06.805676
Model ind 685 epoch 113 head B batch: 200 avg loss -2.193562 avg loss no lamb -2.193562 time 2019-02-22 20:38:00.251587
Model ind 685 epoch 113 head B batch: 300 avg loss -2.200426 avg loss no lamb -2.200426 time 2019-02-22 20:38:53.868399
Model ind 685 epoch 113 head B batch: 400 avg loss -2.186721 avg loss no lamb -2.186721 time 2019-02-22 20:39:47.317682
Model ind 685 epoch 113 head A batch: 0 avg loss -2.209908 avg loss no lamb -2.209908 time 2019-02-22 20:40:41.161913
Model ind 685 epoch 113 head A batch: 100 avg loss -2.184497 avg loss no lamb -2.184497 time 2019-02-22 20:41:34.502035
Model ind 685 epoch 113 head A batch: 200 avg loss -2.206477 avg loss no lamb -2.206477 time 2019-02-22 20:42:27.861605
Model ind 685 epoch 113 head A batch: 300 avg loss -2.199934 avg loss no lamb -2.199934 time 2019-02-22 20:43:22.088620
Model ind 685 epoch 113 head A batch: 400 avg loss -2.191921 avg loss no lamb -2.191921 time 2019-02-22 20:44:15.800789
Pre: time 2019-02-22 20:45:24.278294: 
 	std: 0.006477726
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789571, 0.99204284, 0.9789571, 0.9789571]
	train_accs: [0.9923143, 0.9789571, 0.99204284, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.98424566
	best: 0.9923143

Starting e_i: 114
Model ind 685 epoch 114 head B batch: 0 avg loss -2.231764 avg loss no lamb -2.231764 time 2019-02-22 20:45:25.891177
Model ind 685 epoch 114 head B batch: 100 avg loss -2.175936 avg loss no lamb -2.175936 time 2019-02-22 20:46:19.872585
Model ind 685 epoch 114 head B batch: 200 avg loss -2.151406 avg loss no lamb -2.151406 time 2019-02-22 20:47:14.106613
Model ind 685 epoch 114 head B batch: 300 avg loss -2.202167 avg loss no lamb -2.202167 time 2019-02-22 20:48:07.244171
Model ind 685 epoch 114 head B batch: 400 avg loss -2.170816 avg loss no lamb -2.170816 time 2019-02-22 20:49:00.848759
Model ind 685 epoch 114 head B batch: 0 avg loss -2.178786 avg loss no lamb -2.178786 time 2019-02-22 20:49:53.587994
Model ind 685 epoch 114 head B batch: 100 avg loss -2.201298 avg loss no lamb -2.201298 time 2019-02-22 20:50:48.147308
Model ind 685 epoch 114 head B batch: 200 avg loss -2.159184 avg loss no lamb -2.159184 time 2019-02-22 20:51:42.175433
Model ind 685 epoch 114 head B batch: 300 avg loss -2.201520 avg loss no lamb -2.201520 time 2019-02-22 20:52:34.956684
Model ind 685 epoch 114 head B batch: 400 avg loss -2.206602 avg loss no lamb -2.206602 time 2019-02-22 20:53:27.835943
Model ind 685 epoch 114 head A batch: 0 avg loss -2.248640 avg loss no lamb -2.248640 time 2019-02-22 20:54:22.348315
Model ind 685 epoch 114 head A batch: 100 avg loss -2.186566 avg loss no lamb -2.186566 time 2019-02-22 20:55:16.553718
Model ind 685 epoch 114 head A batch: 200 avg loss -2.169920 avg loss no lamb -2.169920 time 2019-02-22 20:56:10.717214
Model ind 685 epoch 114 head A batch: 300 avg loss -2.175048 avg loss no lamb -2.175048 time 2019-02-22 20:57:03.725466
Model ind 685 epoch 114 head A batch: 400 avg loss -2.203698 avg loss no lamb -2.203698 time 2019-02-22 20:57:57.775561
Pre: time 2019-02-22 20:59:06.308032: 
 	std: 0.0065022856
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789429, 0.99207145, 0.97892857, 0.9789571]
	train_accs: [0.99235713, 0.9789429, 0.99207145, 0.97892857, 0.9789571]
	best_train_sub_head: 0
	worst: 0.97892857
	avg: 0.9842514
	best: 0.99235713

Starting e_i: 115
Model ind 685 epoch 115 head B batch: 0 avg loss -2.172932 avg loss no lamb -2.172932 time 2019-02-22 20:59:07.910799
Model ind 685 epoch 115 head B batch: 100 avg loss -2.153352 avg loss no lamb -2.153352 time 2019-02-22 21:00:01.715370
Model ind 685 epoch 115 head B batch: 200 avg loss -2.153226 avg loss no lamb -2.153226 time 2019-02-22 21:00:55.093479
Model ind 685 epoch 115 head B batch: 300 avg loss -2.142269 avg loss no lamb -2.142269 time 2019-02-22 21:01:48.474206
Model ind 685 epoch 115 head B batch: 400 avg loss -2.182844 avg loss no lamb -2.182844 time 2019-02-22 21:02:43.864169
Model ind 685 epoch 115 head B batch: 0 avg loss -2.175925 avg loss no lamb -2.175925 time 2019-02-22 21:03:37.216193
Model ind 685 epoch 115 head B batch: 100 avg loss -2.146551 avg loss no lamb -2.146551 time 2019-02-22 21:04:30.594936
Model ind 685 epoch 115 head B batch: 200 avg loss -2.188137 avg loss no lamb -2.188137 time 2019-02-22 21:05:24.866458
Model ind 685 epoch 115 head B batch: 300 avg loss -2.220885 avg loss no lamb -2.220885 time 2019-02-22 21:06:18.364026
Model ind 685 epoch 115 head B batch: 400 avg loss -2.142401 avg loss no lamb -2.142401 time 2019-02-22 21:07:11.167714
Model ind 685 epoch 115 head A batch: 0 avg loss -2.229135 avg loss no lamb -2.229135 time 2019-02-22 21:08:04.531179
Model ind 685 epoch 115 head A batch: 100 avg loss -2.151051 avg loss no lamb -2.151051 time 2019-02-22 21:08:57.617286
Model ind 685 epoch 115 head A batch: 200 avg loss -2.214687 avg loss no lamb -2.214687 time 2019-02-22 21:09:51.138032
Model ind 685 epoch 115 head A batch: 300 avg loss -2.195593 avg loss no lamb -2.195593 time 2019-02-22 21:10:46.150528
Model ind 685 epoch 115 head A batch: 400 avg loss -2.156914 avg loss no lamb -2.156914 time 2019-02-22 21:11:40.122946
Pre: time 2019-02-22 21:12:48.266792: 
 	std: 0.006513545
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.979, 0.9922, 0.9789857, 0.9789857]
	train_accs: [0.99237144, 0.979, 0.9922, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843086
	best: 0.99237144

Starting e_i: 116
Model ind 685 epoch 116 head B batch: 0 avg loss -2.185064 avg loss no lamb -2.185064 time 2019-02-22 21:12:49.873604
Model ind 685 epoch 116 head B batch: 100 avg loss -2.164118 avg loss no lamb -2.164118 time 2019-02-22 21:13:43.726960
Model ind 685 epoch 116 head B batch: 200 avg loss -2.201378 avg loss no lamb -2.201378 time 2019-02-22 21:14:39.322093
Model ind 685 epoch 116 head B batch: 300 avg loss -2.222367 avg loss no lamb -2.222367 time 2019-02-22 21:15:32.869902
Model ind 685 epoch 116 head B batch: 400 avg loss -2.178384 avg loss no lamb -2.178384 time 2019-02-22 21:16:28.187512
Model ind 685 epoch 116 head B batch: 0 avg loss -2.206895 avg loss no lamb -2.206895 time 2019-02-22 21:17:21.807130
Model ind 685 epoch 116 head B batch: 100 avg loss -2.197762 avg loss no lamb -2.197762 time 2019-02-22 21:18:17.108522
Model ind 685 epoch 116 head B batch: 200 avg loss -2.214652 avg loss no lamb -2.214652 time 2019-02-22 21:19:11.930512
Model ind 685 epoch 116 head B batch: 300 avg loss -2.153815 avg loss no lamb -2.153815 time 2019-02-22 21:20:05.506837
Model ind 685 epoch 116 head B batch: 400 avg loss -2.198816 avg loss no lamb -2.198816 time 2019-02-22 21:20:59.612445
Model ind 685 epoch 116 head A batch: 0 avg loss -2.203204 avg loss no lamb -2.203204 time 2019-02-22 21:21:53.768584
Model ind 685 epoch 116 head A batch: 100 avg loss -2.177568 avg loss no lamb -2.177568 time 2019-02-22 21:22:46.767442
Model ind 685 epoch 116 head A batch: 200 avg loss -2.172934 avg loss no lamb -2.172934 time 2019-02-22 21:23:40.581409
Model ind 685 epoch 116 head A batch: 300 avg loss -2.178843 avg loss no lamb -2.178843 time 2019-02-22 21:24:34.599707
Model ind 685 epoch 116 head A batch: 400 avg loss -2.130514 avg loss no lamb -2.130514 time 2019-02-22 21:25:28.990820
Pre: time 2019-02-22 21:26:35.695360: 
 	std: 0.0064717536
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9789714, 0.99205714, 0.9789857, 0.97892857]
	train_accs: [0.9922857, 0.9789714, 0.99205714, 0.9789857, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97892857
	avg: 0.98424566
	best: 0.9922857

Starting e_i: 117
Model ind 685 epoch 117 head B batch: 0 avg loss -2.186492 avg loss no lamb -2.186492 time 2019-02-22 21:26:37.244595
Model ind 685 epoch 117 head B batch: 100 avg loss -2.200897 avg loss no lamb -2.200897 time 2019-02-22 21:27:30.743364
Model ind 685 epoch 117 head B batch: 200 avg loss -2.187484 avg loss no lamb -2.187484 time 2019-02-22 21:28:25.433583
Model ind 685 epoch 117 head B batch: 300 avg loss -2.180342 avg loss no lamb -2.180342 time 2019-02-22 21:29:20.133830
Model ind 685 epoch 117 head B batch: 400 avg loss -2.193742 avg loss no lamb -2.193742 time 2019-02-22 21:30:14.648347
Model ind 685 epoch 117 head B batch: 0 avg loss -2.180357 avg loss no lamb -2.180357 time 2019-02-22 21:31:08.713513
Model ind 685 epoch 117 head B batch: 100 avg loss -2.195012 avg loss no lamb -2.195012 time 2019-02-22 21:32:02.672265
Model ind 685 epoch 117 head B batch: 200 avg loss -2.220328 avg loss no lamb -2.220328 time 2019-02-22 21:32:56.243425
Model ind 685 epoch 117 head B batch: 300 avg loss -2.197880 avg loss no lamb -2.197880 time 2019-02-22 21:33:49.801220
Model ind 685 epoch 117 head B batch: 400 avg loss -2.176418 avg loss no lamb -2.176418 time 2019-02-22 21:34:43.300754
Model ind 685 epoch 117 head A batch: 0 avg loss -2.170247 avg loss no lamb -2.170247 time 2019-02-22 21:35:36.216171
Model ind 685 epoch 117 head A batch: 100 avg loss -2.193129 avg loss no lamb -2.193129 time 2019-02-22 21:36:30.208704
Model ind 685 epoch 117 head A batch: 200 avg loss -2.133705 avg loss no lamb -2.133705 time 2019-02-22 21:37:22.322219
Model ind 685 epoch 117 head A batch: 300 avg loss -2.216095 avg loss no lamb -2.216095 time 2019-02-22 21:38:16.265849
Model ind 685 epoch 117 head A batch: 400 avg loss -2.184495 avg loss no lamb -2.184495 time 2019-02-22 21:39:09.921938
Pre: time 2019-02-22 21:40:18.977247: 
 	std: 0.0064745615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.99204284, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.99204284, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842857
	best: 0.9923857

Starting e_i: 118
Model ind 685 epoch 118 head B batch: 0 avg loss -2.226943 avg loss no lamb -2.226943 time 2019-02-22 21:40:20.553987
Model ind 685 epoch 118 head B batch: 100 avg loss -2.175047 avg loss no lamb -2.175047 time 2019-02-22 21:41:14.939936
Model ind 685 epoch 118 head B batch: 200 avg loss -2.177559 avg loss no lamb -2.177559 time 2019-02-22 21:42:09.107162
Model ind 685 epoch 118 head B batch: 300 avg loss -2.187516 avg loss no lamb -2.187516 time 2019-02-22 21:43:02.413576
Model ind 685 epoch 118 head B batch: 400 avg loss -2.172824 avg loss no lamb -2.172824 time 2019-02-22 21:43:55.996366
Model ind 685 epoch 118 head B batch: 0 avg loss -2.205133 avg loss no lamb -2.205133 time 2019-02-22 21:44:49.447858
Model ind 685 epoch 118 head B batch: 100 avg loss -2.173119 avg loss no lamb -2.173119 time 2019-02-22 21:45:42.492831
Model ind 685 epoch 118 head B batch: 200 avg loss -2.224816 avg loss no lamb -2.224816 time 2019-02-22 21:46:36.205888
Model ind 685 epoch 118 head B batch: 300 avg loss -2.225371 avg loss no lamb -2.225371 time 2019-02-22 21:47:28.788486
Model ind 685 epoch 118 head B batch: 400 avg loss -2.180115 avg loss no lamb -2.180115 time 2019-02-22 21:48:23.197302
Model ind 685 epoch 118 head A batch: 0 avg loss -2.202281 avg loss no lamb -2.202281 time 2019-02-22 21:49:16.442026
Model ind 685 epoch 118 head A batch: 100 avg loss -2.172059 avg loss no lamb -2.172059 time 2019-02-22 21:50:09.626215
Model ind 685 epoch 118 head A batch: 200 avg loss -2.195119 avg loss no lamb -2.195119 time 2019-02-22 21:51:02.288786
Model ind 685 epoch 118 head A batch: 300 avg loss -2.174420 avg loss no lamb -2.174420 time 2019-02-22 21:51:55.744467
Model ind 685 epoch 118 head A batch: 400 avg loss -2.164565 avg loss no lamb -2.164565 time 2019-02-22 21:52:49.811275
Pre: time 2019-02-22 21:53:58.672075: 
 	std: 0.006456241
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99207145, 0.9787, 0.99167144, 0.9787, 0.97868574]
	train_accs: [0.99207145, 0.9787, 0.99167144, 0.9787, 0.97868574]
	best_train_sub_head: 0
	worst: 0.97868574
	avg: 0.9839657
	best: 0.99207145

Starting e_i: 119
Model ind 685 epoch 119 head B batch: 0 avg loss -2.216986 avg loss no lamb -2.216986 time 2019-02-22 21:54:00.873566
Model ind 685 epoch 119 head B batch: 100 avg loss -2.204452 avg loss no lamb -2.204452 time 2019-02-22 21:54:54.547790
Model ind 685 epoch 119 head B batch: 200 avg loss -2.177868 avg loss no lamb -2.177868 time 2019-02-22 21:55:49.605188
Model ind 685 epoch 119 head B batch: 300 avg loss -2.180050 avg loss no lamb -2.180050 time 2019-02-22 21:56:44.109574
Model ind 685 epoch 119 head B batch: 400 avg loss -2.142677 avg loss no lamb -2.142677 time 2019-02-22 21:57:37.606805
Model ind 685 epoch 119 head B batch: 0 avg loss -2.180485 avg loss no lamb -2.180485 time 2019-02-22 21:58:31.549862
Model ind 685 epoch 119 head B batch: 100 avg loss -2.171052 avg loss no lamb -2.171052 time 2019-02-22 21:59:24.184370
Model ind 685 epoch 119 head B batch: 200 avg loss -2.177033 avg loss no lamb -2.177033 time 2019-02-22 22:00:18.272120
Model ind 685 epoch 119 head B batch: 300 avg loss -2.240143 avg loss no lamb -2.240143 time 2019-02-22 22:01:11.725155
Model ind 685 epoch 119 head B batch: 400 avg loss -2.164305 avg loss no lamb -2.164305 time 2019-02-22 22:02:05.660347
Model ind 685 epoch 119 head A batch: 0 avg loss -2.159352 avg loss no lamb -2.159352 time 2019-02-22 22:02:59.277735
Model ind 685 epoch 119 head A batch: 100 avg loss -2.191353 avg loss no lamb -2.191353 time 2019-02-22 22:03:52.459673
Model ind 685 epoch 119 head A batch: 200 avg loss -2.177929 avg loss no lamb -2.177929 time 2019-02-22 22:04:46.298239
Model ind 685 epoch 119 head A batch: 300 avg loss -2.191945 avg loss no lamb -2.191945 time 2019-02-22 22:05:38.891600
Model ind 685 epoch 119 head A batch: 400 avg loss -2.208876 avg loss no lamb -2.208876 time 2019-02-22 22:06:31.860118
Pre: time 2019-02-22 22:07:39.251678: 
 	std: 0.0064849835
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.97892857, 0.992, 0.97892857, 0.97892857]
	train_accs: [0.9923286, 0.97892857, 0.992, 0.97892857, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97892857
	avg: 0.9842229
	best: 0.9923286

Starting e_i: 120
Model ind 685 epoch 120 head B batch: 0 avg loss -2.198623 avg loss no lamb -2.198623 time 2019-02-22 22:07:40.884588
Model ind 685 epoch 120 head B batch: 100 avg loss -2.191128 avg loss no lamb -2.191128 time 2019-02-22 22:08:33.967294
Model ind 685 epoch 120 head B batch: 200 avg loss -2.174793 avg loss no lamb -2.174793 time 2019-02-22 22:09:28.956300
Model ind 685 epoch 120 head B batch: 300 avg loss -2.187950 avg loss no lamb -2.187950 time 2019-02-22 22:10:21.619879
Model ind 685 epoch 120 head B batch: 400 avg loss -2.151506 avg loss no lamb -2.151506 time 2019-02-22 22:11:15.746320
Model ind 685 epoch 120 head B batch: 0 avg loss -2.227270 avg loss no lamb -2.227270 time 2019-02-22 22:12:09.185468
Model ind 685 epoch 120 head B batch: 100 avg loss -2.155002 avg loss no lamb -2.155002 time 2019-02-22 22:13:02.941725
Model ind 685 epoch 120 head B batch: 200 avg loss -2.174135 avg loss no lamb -2.174135 time 2019-02-22 22:13:55.577578
Model ind 685 epoch 120 head B batch: 300 avg loss -2.192304 avg loss no lamb -2.192304 time 2019-02-22 22:14:49.097989
Model ind 685 epoch 120 head B batch: 400 avg loss -2.136855 avg loss no lamb -2.136855 time 2019-02-22 22:15:43.645831
Model ind 685 epoch 120 head A batch: 0 avg loss -2.198472 avg loss no lamb -2.198472 time 2019-02-22 22:16:37.070717
Model ind 685 epoch 120 head A batch: 100 avg loss -2.188909 avg loss no lamb -2.188909 time 2019-02-22 22:17:30.766010
Model ind 685 epoch 120 head A batch: 200 avg loss -2.185462 avg loss no lamb -2.185462 time 2019-02-22 22:18:25.032035
Model ind 685 epoch 120 head A batch: 300 avg loss -2.232542 avg loss no lamb -2.232542 time 2019-02-22 22:19:20.062980
Model ind 685 epoch 120 head A batch: 400 avg loss -2.175293 avg loss no lamb -2.175293 time 2019-02-22 22:20:13.382912
Pre: time 2019-02-22 22:21:20.075665: 
 	std: 0.0064582233
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789857, 0.992, 0.9789857, 0.979]
	train_accs: [0.9923428, 0.9789857, 0.992, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842628
	best: 0.9923428

Starting e_i: 121
Model ind 685 epoch 121 head B batch: 0 avg loss -2.168516 avg loss no lamb -2.168516 time 2019-02-22 22:21:22.878044
Model ind 685 epoch 121 head B batch: 100 avg loss -2.160039 avg loss no lamb -2.160039 time 2019-02-22 22:22:17.343197
Model ind 685 epoch 121 head B batch: 200 avg loss -2.197100 avg loss no lamb -2.197100 time 2019-02-22 22:23:11.151175
Model ind 685 epoch 121 head B batch: 300 avg loss -2.202152 avg loss no lamb -2.202152 time 2019-02-22 22:24:04.595500
Model ind 685 epoch 121 head B batch: 400 avg loss -2.134374 avg loss no lamb -2.134374 time 2019-02-22 22:24:57.885561
Model ind 685 epoch 121 head B batch: 0 avg loss -2.210356 avg loss no lamb -2.210356 time 2019-02-22 22:25:51.746121
Model ind 685 epoch 121 head B batch: 100 avg loss -2.208444 avg loss no lamb -2.208444 time 2019-02-22 22:26:45.918141
Model ind 685 epoch 121 head B batch: 200 avg loss -2.178613 avg loss no lamb -2.178613 time 2019-02-22 22:27:40.990844
Model ind 685 epoch 121 head B batch: 300 avg loss -2.177583 avg loss no lamb -2.177583 time 2019-02-22 22:28:36.173607
Model ind 685 epoch 121 head B batch: 400 avg loss -2.187897 avg loss no lamb -2.187897 time 2019-02-22 22:29:29.735900
Model ind 685 epoch 121 head A batch: 0 avg loss -2.165050 avg loss no lamb -2.165050 time 2019-02-22 22:30:22.717706
Model ind 685 epoch 121 head A batch: 100 avg loss -2.187767 avg loss no lamb -2.187767 time 2019-02-22 22:31:15.817766
Model ind 685 epoch 121 head A batch: 200 avg loss -2.212908 avg loss no lamb -2.212908 time 2019-02-22 22:32:09.006182
Model ind 685 epoch 121 head A batch: 300 avg loss -2.189342 avg loss no lamb -2.189342 time 2019-02-22 22:33:02.705406
Model ind 685 epoch 121 head A batch: 400 avg loss -2.178410 avg loss no lamb -2.178410 time 2019-02-22 22:33:57.119596
Pre: time 2019-02-22 22:35:04.896068: 
 	std: 0.0064858166
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99212855, 0.9790286, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.99212855, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9923857

Starting e_i: 122
Model ind 685 epoch 122 head B batch: 0 avg loss -2.190099 avg loss no lamb -2.190099 time 2019-02-22 22:35:06.525022
Model ind 685 epoch 122 head B batch: 100 avg loss -2.173412 avg loss no lamb -2.173412 time 2019-02-22 22:36:01.343434
Model ind 685 epoch 122 head B batch: 200 avg loss -2.166911 avg loss no lamb -2.166911 time 2019-02-22 22:36:56.181945
Model ind 685 epoch 122 head B batch: 300 avg loss -2.179837 avg loss no lamb -2.179837 time 2019-02-22 22:37:49.465357
Model ind 685 epoch 122 head B batch: 400 avg loss -2.164098 avg loss no lamb -2.164098 time 2019-02-22 22:38:44.033356
Model ind 685 epoch 122 head B batch: 0 avg loss -2.184512 avg loss no lamb -2.184512 time 2019-02-22 22:39:38.055909
Model ind 685 epoch 122 head B batch: 100 avg loss -2.196071 avg loss no lamb -2.196071 time 2019-02-22 22:40:32.660132
Model ind 685 epoch 122 head B batch: 200 avg loss -2.208643 avg loss no lamb -2.208643 time 2019-02-22 22:41:26.364970
Model ind 685 epoch 122 head B batch: 300 avg loss -2.211108 avg loss no lamb -2.211108 time 2019-02-22 22:42:20.174906
Model ind 685 epoch 122 head B batch: 400 avg loss -2.215559 avg loss no lamb -2.215559 time 2019-02-22 22:43:13.539303
Model ind 685 epoch 122 head A batch: 0 avg loss -2.202504 avg loss no lamb -2.202504 time 2019-02-22 22:44:07.474193
Model ind 685 epoch 122 head A batch: 100 avg loss -2.216983 avg loss no lamb -2.216983 time 2019-02-22 22:45:00.104230
Model ind 685 epoch 122 head A batch: 200 avg loss -2.169601 avg loss no lamb -2.169601 time 2019-02-22 22:45:53.638158
Model ind 685 epoch 122 head A batch: 300 avg loss -2.213128 avg loss no lamb -2.213128 time 2019-02-22 22:46:47.072800
Model ind 685 epoch 122 head A batch: 400 avg loss -2.186747 avg loss no lamb -2.186747 time 2019-02-22 22:47:40.716152
Pre: time 2019-02-22 22:48:48.467250: 
 	std: 0.006467186
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.99237144, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432285
	best: 0.99237144

Starting e_i: 123
Model ind 685 epoch 123 head B batch: 0 avg loss -2.175388 avg loss no lamb -2.175388 time 2019-02-22 22:48:50.090875
Model ind 685 epoch 123 head B batch: 100 avg loss -2.222579 avg loss no lamb -2.222579 time 2019-02-22 22:49:44.291725
Model ind 685 epoch 123 head B batch: 200 avg loss -2.215074 avg loss no lamb -2.215074 time 2019-02-22 22:50:38.289876
Model ind 685 epoch 123 head B batch: 300 avg loss -2.218661 avg loss no lamb -2.218661 time 2019-02-22 22:51:30.933762
Model ind 685 epoch 123 head B batch: 400 avg loss -2.127635 avg loss no lamb -2.127635 time 2019-02-22 22:52:23.663234
Model ind 685 epoch 123 head B batch: 0 avg loss -2.165255 avg loss no lamb -2.165255 time 2019-02-22 22:53:18.192675
Model ind 685 epoch 123 head B batch: 100 avg loss -2.213174 avg loss no lamb -2.213174 time 2019-02-22 22:54:10.959325
Model ind 685 epoch 123 head B batch: 200 avg loss -2.207298 avg loss no lamb -2.207298 time 2019-02-22 22:55:03.763408
Model ind 685 epoch 123 head B batch: 300 avg loss -2.178890 avg loss no lamb -2.178890 time 2019-02-22 22:55:56.908996
Model ind 685 epoch 123 head B batch: 400 avg loss -2.161624 avg loss no lamb -2.161624 time 2019-02-22 22:56:51.506618
Model ind 685 epoch 123 head A batch: 0 avg loss -2.222078 avg loss no lamb -2.222078 time 2019-02-22 22:57:45.133051
Model ind 685 epoch 123 head A batch: 100 avg loss -2.181157 avg loss no lamb -2.181157 time 2019-02-22 22:58:37.808125
Model ind 685 epoch 123 head A batch: 200 avg loss -2.207619 avg loss no lamb -2.207619 time 2019-02-22 22:59:31.745690
Model ind 685 epoch 123 head A batch: 300 avg loss -2.215313 avg loss no lamb -2.215313 time 2019-02-22 23:00:24.144488
Model ind 685 epoch 123 head A batch: 400 avg loss -2.157328 avg loss no lamb -2.157328 time 2019-02-22 23:01:19.244046
Pre: time 2019-02-22 23:02:29.128927: 
 	std: 0.0064708306
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98431146
	best: 0.9923857

Starting e_i: 124
Model ind 685 epoch 124 head B batch: 0 avg loss -2.181581 avg loss no lamb -2.181581 time 2019-02-22 23:02:30.826658
Model ind 685 epoch 124 head B batch: 100 avg loss -2.170406 avg loss no lamb -2.170406 time 2019-02-22 23:03:24.545225
Model ind 685 epoch 124 head B batch: 200 avg loss -2.226993 avg loss no lamb -2.226993 time 2019-02-22 23:04:18.614364
Model ind 685 epoch 124 head B batch: 300 avg loss -2.180195 avg loss no lamb -2.180195 time 2019-02-22 23:05:11.782582
Model ind 685 epoch 124 head B batch: 400 avg loss -2.163455 avg loss no lamb -2.163455 time 2019-02-22 23:06:06.011459
Model ind 685 epoch 124 head B batch: 0 avg loss -2.193731 avg loss no lamb -2.193731 time 2019-02-22 23:06:59.391917
Model ind 685 epoch 124 head B batch: 100 avg loss -2.202461 avg loss no lamb -2.202461 time 2019-02-22 23:07:53.499028
Model ind 685 epoch 124 head B batch: 200 avg loss -2.164114 avg loss no lamb -2.164114 time 2019-02-22 23:08:47.306243
Model ind 685 epoch 124 head B batch: 300 avg loss -2.218619 avg loss no lamb -2.218619 time 2019-02-22 23:09:39.911425
Model ind 685 epoch 124 head B batch: 400 avg loss -2.172116 avg loss no lamb -2.172116 time 2019-02-22 23:10:33.666760
Model ind 685 epoch 124 head A batch: 0 avg loss -2.202959 avg loss no lamb -2.202959 time 2019-02-22 23:11:26.291113
Model ind 685 epoch 124 head A batch: 100 avg loss -2.107453 avg loss no lamb -2.107453 time 2019-02-22 23:12:19.379803
Model ind 685 epoch 124 head A batch: 200 avg loss -2.128421 avg loss no lamb -2.128421 time 2019-02-22 23:13:13.044969
Model ind 685 epoch 124 head A batch: 300 avg loss -2.206434 avg loss no lamb -2.206434 time 2019-02-22 23:14:06.853299
Model ind 685 epoch 124 head A batch: 400 avg loss -2.123743 avg loss no lamb -2.123743 time 2019-02-22 23:15:00.459578
Pre: time 2019-02-22 23:16:07.917334: 
 	std: 0.0064890073
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.97905713, 0.9922429, 0.97908574, 0.97908574]
	train_accs: [0.9924, 0.97905713, 0.9922429, 0.97908574, 0.97908574]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98437434
	best: 0.9924

Starting e_i: 125
Model ind 685 epoch 125 head B batch: 0 avg loss -2.223693 avg loss no lamb -2.223693 time 2019-02-22 23:16:09.497152
Model ind 685 epoch 125 head B batch: 100 avg loss -2.187018 avg loss no lamb -2.187018 time 2019-02-22 23:17:02.660606
Model ind 685 epoch 125 head B batch: 200 avg loss -2.206892 avg loss no lamb -2.206892 time 2019-02-22 23:17:57.517151
Model ind 685 epoch 125 head B batch: 300 avg loss -2.202618 avg loss no lamb -2.202618 time 2019-02-22 23:18:51.831130
Model ind 685 epoch 125 head B batch: 400 avg loss -2.191712 avg loss no lamb -2.191712 time 2019-02-22 23:19:46.035757
Model ind 685 epoch 125 head B batch: 0 avg loss -2.242887 avg loss no lamb -2.242887 time 2019-02-22 23:20:39.890036
Model ind 685 epoch 125 head B batch: 100 avg loss -2.161066 avg loss no lamb -2.161066 time 2019-02-22 23:21:33.086827
Model ind 685 epoch 125 head B batch: 200 avg loss -2.209441 avg loss no lamb -2.209441 time 2019-02-22 23:22:26.608830
Model ind 685 epoch 125 head B batch: 300 avg loss -2.219371 avg loss no lamb -2.219371 time 2019-02-22 23:23:20.840055
Model ind 685 epoch 125 head B batch: 400 avg loss -2.152583 avg loss no lamb -2.152583 time 2019-02-22 23:24:13.981915
Model ind 685 epoch 125 head A batch: 0 avg loss -2.214909 avg loss no lamb -2.214909 time 2019-02-22 23:25:07.900020
Model ind 685 epoch 125 head A batch: 100 avg loss -2.173570 avg loss no lamb -2.173570 time 2019-02-22 23:26:00.455411
Model ind 685 epoch 125 head A batch: 200 avg loss -2.227597 avg loss no lamb -2.227597 time 2019-02-22 23:26:54.579729
Model ind 685 epoch 125 head A batch: 300 avg loss -2.192826 avg loss no lamb -2.192826 time 2019-02-22 23:27:47.570222
Model ind 685 epoch 125 head A batch: 400 avg loss -2.192226 avg loss no lamb -2.192226 time 2019-02-22 23:28:41.021703
Pre: time 2019-02-22 23:29:48.368009: 
 	std: 0.0065171574
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99252856, 0.9791286, 0.9923143, 0.9791, 0.9791286]
	train_accs: [0.99252856, 0.9791286, 0.9923143, 0.9791, 0.9791286]
	best_train_sub_head: 0
	worst: 0.9791
	avg: 0.98444
	best: 0.99252856

Starting e_i: 126
Model ind 685 epoch 126 head B batch: 0 avg loss -2.191189 avg loss no lamb -2.191189 time 2019-02-22 23:29:51.530364
Model ind 685 epoch 126 head B batch: 100 avg loss -2.166665 avg loss no lamb -2.166665 time 2019-02-22 23:30:43.843770
Model ind 685 epoch 126 head B batch: 200 avg loss -2.189313 avg loss no lamb -2.189313 time 2019-02-22 23:31:38.008355
Model ind 685 epoch 126 head B batch: 300 avg loss -2.212131 avg loss no lamb -2.212131 time 2019-02-22 23:32:31.491745
Model ind 685 epoch 126 head B batch: 400 avg loss -2.176525 avg loss no lamb -2.176525 time 2019-02-22 23:33:25.262221
Model ind 685 epoch 126 head B batch: 0 avg loss -2.174080 avg loss no lamb -2.174080 time 2019-02-22 23:34:18.730985
Model ind 685 epoch 126 head B batch: 100 avg loss -2.166635 avg loss no lamb -2.166635 time 2019-02-22 23:35:11.685284
Model ind 685 epoch 126 head B batch: 200 avg loss -2.198451 avg loss no lamb -2.198451 time 2019-02-22 23:36:04.964985
Model ind 685 epoch 126 head B batch: 300 avg loss -2.228826 avg loss no lamb -2.228826 time 2019-02-22 23:36:57.754675
Model ind 685 epoch 126 head B batch: 400 avg loss -2.175715 avg loss no lamb -2.175715 time 2019-02-22 23:37:51.191480
Model ind 685 epoch 126 head A batch: 0 avg loss -2.177555 avg loss no lamb -2.177555 time 2019-02-22 23:38:44.878017
Model ind 685 epoch 126 head A batch: 100 avg loss -2.173887 avg loss no lamb -2.173887 time 2019-02-22 23:39:38.628600
Model ind 685 epoch 126 head A batch: 200 avg loss -2.158746 avg loss no lamb -2.158746 time 2019-02-22 23:40:31.665491
Model ind 685 epoch 126 head A batch: 300 avg loss -2.195296 avg loss no lamb -2.195296 time 2019-02-22 23:41:25.725913
Model ind 685 epoch 126 head A batch: 400 avg loss -2.222406 avg loss no lamb -2.222406 time 2019-02-22 23:42:20.485131
Pre: time 2019-02-22 23:43:27.763012: 
 	std: 0.0065425644
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99221426, 0.9788143, 0.9921143, 0.97882855, 0.9787857]
	train_accs: [0.99221426, 0.9788143, 0.9921143, 0.97882855, 0.9787857]
	best_train_sub_head: 0
	worst: 0.9787857
	avg: 0.98415136
	best: 0.99221426

Starting e_i: 127
Model ind 685 epoch 127 head B batch: 0 avg loss -2.192228 avg loss no lamb -2.192228 time 2019-02-22 23:43:29.814628
Model ind 685 epoch 127 head B batch: 100 avg loss -2.226240 avg loss no lamb -2.226240 time 2019-02-22 23:44:24.875337
Model ind 685 epoch 127 head B batch: 200 avg loss -2.231838 avg loss no lamb -2.231838 time 2019-02-22 23:45:20.425433
Model ind 685 epoch 127 head B batch: 300 avg loss -2.218693 avg loss no lamb -2.218693 time 2019-02-22 23:46:12.552575
Model ind 685 epoch 127 head B batch: 400 avg loss -2.147312 avg loss no lamb -2.147312 time 2019-02-22 23:47:03.930777
Model ind 685 epoch 127 head B batch: 0 avg loss -2.215655 avg loss no lamb -2.215655 time 2019-02-22 23:47:55.388725
Model ind 685 epoch 127 head B batch: 100 avg loss -2.192537 avg loss no lamb -2.192537 time 2019-02-22 23:48:46.920978
Model ind 685 epoch 127 head B batch: 200 avg loss -2.225130 avg loss no lamb -2.225130 time 2019-02-22 23:49:39.029167
Model ind 685 epoch 127 head B batch: 300 avg loss -2.207341 avg loss no lamb -2.207341 time 2019-02-22 23:50:30.620020
Model ind 685 epoch 127 head B batch: 400 avg loss -2.190415 avg loss no lamb -2.190415 time 2019-02-22 23:51:22.053197
Model ind 685 epoch 127 head A batch: 0 avg loss -2.234364 avg loss no lamb -2.234364 time 2019-02-22 23:52:13.398799
Model ind 685 epoch 127 head A batch: 100 avg loss -2.214166 avg loss no lamb -2.214166 time 2019-02-22 23:53:05.424486
Model ind 685 epoch 127 head A batch: 200 avg loss -2.220428 avg loss no lamb -2.220428 time 2019-02-22 23:53:57.818013
Model ind 685 epoch 127 head A batch: 300 avg loss -2.214226 avg loss no lamb -2.214226 time 2019-02-22 23:54:47.473301
Model ind 685 epoch 127 head A batch: 400 avg loss -2.189686 avg loss no lamb -2.189686 time 2019-02-22 23:55:37.355519
Pre: time 2019-02-22 23:56:41.207433: 
 	std: 0.0064884257
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9925143, 0.9791286, 0.9922, 0.97908574, 0.9791286]
	train_accs: [0.9925143, 0.9791286, 0.9922, 0.97908574, 0.9791286]
	best_train_sub_head: 0
	worst: 0.97908574
	avg: 0.9844114
	best: 0.9925143

Starting e_i: 128
Model ind 685 epoch 128 head B batch: 0 avg loss -2.218802 avg loss no lamb -2.218802 time 2019-02-22 23:56:42.694157
Model ind 685 epoch 128 head B batch: 100 avg loss -2.199591 avg loss no lamb -2.199591 time 2019-02-22 23:57:33.901611
Model ind 685 epoch 128 head B batch: 200 avg loss -2.161924 avg loss no lamb -2.161924 time 2019-02-22 23:58:25.525040
Model ind 685 epoch 128 head B batch: 300 avg loss -2.192267 avg loss no lamb -2.192267 time 2019-02-22 23:59:17.935818
Model ind 685 epoch 128 head B batch: 400 avg loss -2.210882 avg loss no lamb -2.210882 time 2019-02-23 00:00:09.790690
Model ind 685 epoch 128 head B batch: 0 avg loss -2.157107 avg loss no lamb -2.157107 time 2019-02-23 00:01:00.474890
Model ind 685 epoch 128 head B batch: 100 avg loss -2.199599 avg loss no lamb -2.199599 time 2019-02-23 00:01:52.307295
Model ind 685 epoch 128 head B batch: 200 avg loss -2.234229 avg loss no lamb -2.234229 time 2019-02-23 00:02:43.918224
Model ind 685 epoch 128 head B batch: 300 avg loss -2.212047 avg loss no lamb -2.212047 time 2019-02-23 00:03:36.049146
Model ind 685 epoch 128 head B batch: 400 avg loss -2.175166 avg loss no lamb -2.175166 time 2019-02-23 00:04:26.985293
Model ind 685 epoch 128 head A batch: 0 avg loss -2.205333 avg loss no lamb -2.205333 time 2019-02-23 00:05:18.236805
Model ind 685 epoch 128 head A batch: 100 avg loss -2.173893 avg loss no lamb -2.173893 time 2019-02-23 00:06:09.210728
Model ind 685 epoch 128 head A batch: 200 avg loss -2.204090 avg loss no lamb -2.204090 time 2019-02-23 00:07:00.311527
Model ind 685 epoch 128 head A batch: 300 avg loss -2.195394 avg loss no lamb -2.195394 time 2019-02-23 00:07:50.881520
Model ind 685 epoch 128 head A batch: 400 avg loss -2.180217 avg loss no lamb -2.180217 time 2019-02-23 00:08:41.942092
Pre: time 2019-02-23 00:09:46.021422: 
 	std: 0.0064787166
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.97915715, 0.9922571, 0.97914284, 0.97914284]
	train_accs: [0.9924857, 0.97915715, 0.9922571, 0.97914284, 0.97914284]
	best_train_sub_head: 0
	worst: 0.97914284
	avg: 0.98443717
	best: 0.9924857

Starting e_i: 129
Model ind 685 epoch 129 head B batch: 0 avg loss -2.185546 avg loss no lamb -2.185546 time 2019-02-23 00:09:47.604422
Model ind 685 epoch 129 head B batch: 100 avg loss -2.234443 avg loss no lamb -2.234443 time 2019-02-23 00:10:37.558112
Model ind 685 epoch 129 head B batch: 200 avg loss -2.161206 avg loss no lamb -2.161206 time 2019-02-23 00:11:28.420964
Model ind 685 epoch 129 head B batch: 300 avg loss -2.176365 avg loss no lamb -2.176365 time 2019-02-23 00:12:18.299735
Model ind 685 epoch 129 head B batch: 400 avg loss -2.195068 avg loss no lamb -2.195068 time 2019-02-23 00:13:32.616552
Model ind 685 epoch 129 head B batch: 0 avg loss -2.178537 avg loss no lamb -2.178537 time 2019-02-23 00:14:51.794259
Model ind 685 epoch 129 head B batch: 100 avg loss -2.166260 avg loss no lamb -2.166260 time 2019-02-23 00:16:17.967799
Model ind 685 epoch 129 head B batch: 200 avg loss -2.198546 avg loss no lamb -2.198546 time 2019-02-23 00:17:42.750680
Model ind 685 epoch 129 head B batch: 300 avg loss -2.154807 avg loss no lamb -2.154807 time 2019-02-23 00:19:06.841033
Model ind 685 epoch 129 head B batch: 400 avg loss -2.166726 avg loss no lamb -2.166726 time 2019-02-23 00:20:32.388308
Model ind 685 epoch 129 head A batch: 0 avg loss -2.179969 avg loss no lamb -2.179969 time 2019-02-23 00:21:58.204484
Model ind 685 epoch 129 head A batch: 100 avg loss -2.151133 avg loss no lamb -2.151133 time 2019-02-23 00:23:22.147677
Model ind 685 epoch 129 head A batch: 200 avg loss -2.196022 avg loss no lamb -2.196022 time 2019-02-23 00:24:48.900359
Model ind 685 epoch 129 head A batch: 300 avg loss -2.209042 avg loss no lamb -2.209042 time 2019-02-23 00:26:11.354178
Model ind 685 epoch 129 head A batch: 400 avg loss -2.209778 avg loss no lamb -2.209778 time 2019-02-23 00:27:25.478281
Pre: time 2019-02-23 00:29:01.819025: 
 	std: 0.0065036565
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.9790428, 0.99215716, 0.97905713, 0.9790428]
	train_accs: [0.9924857, 0.9790428, 0.99215716, 0.97905713, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843572
	best: 0.9924857

Starting e_i: 130
Model ind 685 epoch 130 head B batch: 0 avg loss -2.191372 avg loss no lamb -2.191372 time 2019-02-23 00:29:03.568498
Model ind 685 epoch 130 head B batch: 100 avg loss -2.175177 avg loss no lamb -2.175177 time 2019-02-23 00:30:24.804190
Model ind 685 epoch 130 head B batch: 200 avg loss -2.152422 avg loss no lamb -2.152422 time 2019-02-23 00:31:44.406961
Model ind 685 epoch 130 head B batch: 300 avg loss -2.199247 avg loss no lamb -2.199247 time 2019-02-23 00:32:58.483226
Model ind 685 epoch 130 head B batch: 400 avg loss -2.171598 avg loss no lamb -2.171598 time 2019-02-23 00:34:14.274950
Model ind 685 epoch 130 head B batch: 0 avg loss -2.205318 avg loss no lamb -2.205318 time 2019-02-23 00:35:30.123322
Model ind 685 epoch 130 head B batch: 100 avg loss -2.208426 avg loss no lamb -2.208426 time 2019-02-23 00:36:42.500528
Model ind 685 epoch 130 head B batch: 200 avg loss -2.171748 avg loss no lamb -2.171748 time 2019-02-23 00:37:53.912985
Model ind 685 epoch 130 head B batch: 300 avg loss -2.213471 avg loss no lamb -2.213471 time 2019-02-23 00:39:10.550904
Model ind 685 epoch 130 head B batch: 400 avg loss -2.181491 avg loss no lamb -2.181491 time 2019-02-23 00:40:27.837972
Model ind 685 epoch 130 head A batch: 0 avg loss -2.219282 avg loss no lamb -2.219282 time 2019-02-23 00:41:46.578616
Model ind 685 epoch 130 head A batch: 100 avg loss -2.120002 avg loss no lamb -2.120002 time 2019-02-23 00:43:04.587409
Model ind 685 epoch 130 head A batch: 200 avg loss -2.225610 avg loss no lamb -2.225610 time 2019-02-23 00:44:22.177326
Model ind 685 epoch 130 head A batch: 300 avg loss -2.153953 avg loss no lamb -2.153953 time 2019-02-23 00:45:38.467053
Model ind 685 epoch 130 head A batch: 400 avg loss -2.149109 avg loss no lamb -2.149109 time 2019-02-23 00:46:56.090830
Pre: time 2019-02-23 00:48:29.726006: 
 	std: 0.006492824
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789714, 0.9921, 0.9789714, 0.9789857]
	train_accs: [0.99235713, 0.9789714, 0.9921, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842771
	best: 0.99235713

Starting e_i: 131
Model ind 685 epoch 131 head B batch: 0 avg loss -2.194473 avg loss no lamb -2.194473 time 2019-02-23 00:48:31.667425
Model ind 685 epoch 131 head B batch: 100 avg loss -2.210225 avg loss no lamb -2.210225 time 2019-02-23 00:49:51.381096
Model ind 685 epoch 131 head B batch: 200 avg loss -2.181601 avg loss no lamb -2.181601 time 2019-02-23 00:51:08.688146
Model ind 685 epoch 131 head B batch: 300 avg loss -2.230424 avg loss no lamb -2.230424 time 2019-02-23 00:52:24.212917
Model ind 685 epoch 131 head B batch: 400 avg loss -2.201077 avg loss no lamb -2.201077 time 2019-02-23 00:53:29.489379
Model ind 685 epoch 131 head B batch: 0 avg loss -2.157146 avg loss no lamb -2.157146 time 2019-02-23 00:54:38.046015
Model ind 685 epoch 131 head B batch: 100 avg loss -2.174132 avg loss no lamb -2.174132 time 2019-02-23 00:55:54.580402
Model ind 685 epoch 131 head B batch: 200 avg loss -2.215005 avg loss no lamb -2.215005 time 2019-02-23 00:57:10.698006
Model ind 685 epoch 131 head B batch: 300 avg loss -2.170463 avg loss no lamb -2.170463 time 2019-02-23 00:58:30.521629
Model ind 685 epoch 131 head B batch: 400 avg loss -2.184569 avg loss no lamb -2.184569 time 2019-02-23 00:59:49.968740
Model ind 685 epoch 131 head A batch: 0 avg loss -2.233714 avg loss no lamb -2.233714 time 2019-02-23 01:01:11.399437
Model ind 685 epoch 131 head A batch: 100 avg loss -2.185877 avg loss no lamb -2.185877 time 2019-02-23 01:02:31.548210
Model ind 685 epoch 131 head A batch: 200 avg loss -2.175030 avg loss no lamb -2.175030 time 2019-02-23 01:03:52.269104
Model ind 685 epoch 131 head A batch: 300 avg loss -2.204799 avg loss no lamb -2.204799 time 2019-02-23 01:05:10.273488
Model ind 685 epoch 131 head A batch: 400 avg loss -2.194963 avg loss no lamb -2.194963 time 2019-02-23 01:06:22.691884
Pre: time 2019-02-23 01:07:52.247882: 
 	std: 0.006470984
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97907144, 0.9921143, 0.97907144, 0.97907144]
	train_accs: [0.99244285, 0.97907144, 0.9921143, 0.97907144, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.9843543
	best: 0.99244285

Starting e_i: 132
Model ind 685 epoch 132 head B batch: 0 avg loss -2.189817 avg loss no lamb -2.189817 time 2019-02-23 01:07:53.990328
Model ind 685 epoch 132 head B batch: 100 avg loss -2.181375 avg loss no lamb -2.181375 time 2019-02-23 01:09:17.000770
Model ind 685 epoch 132 head B batch: 200 avg loss -2.185521 avg loss no lamb -2.185521 time 2019-02-23 01:10:33.155238
Model ind 685 epoch 132 head B batch: 300 avg loss -2.158678 avg loss no lamb -2.158678 time 2019-02-23 01:11:50.376033
Model ind 685 epoch 132 head B batch: 400 avg loss -2.181021 avg loss no lamb -2.181021 time 2019-02-23 01:13:07.229351
Model ind 685 epoch 132 head B batch: 0 avg loss -2.226691 avg loss no lamb -2.226691 time 2019-02-23 01:14:26.452888
Model ind 685 epoch 132 head B batch: 100 avg loss -2.178135 avg loss no lamb -2.178135 time 2019-02-23 01:15:43.960416
Model ind 685 epoch 132 head B batch: 200 avg loss -2.131104 avg loss no lamb -2.131104 time 2019-02-23 01:17:01.711998
Model ind 685 epoch 132 head B batch: 300 avg loss -2.217846 avg loss no lamb -2.217846 time 2019-02-23 01:18:17.567881
Model ind 685 epoch 132 head B batch: 400 avg loss -2.153779 avg loss no lamb -2.153779 time 2019-02-23 01:19:38.180955
Model ind 685 epoch 132 head A batch: 0 avg loss -2.196367 avg loss no lamb -2.196367 time 2019-02-23 01:20:57.838920
Model ind 685 epoch 132 head A batch: 100 avg loss -2.212470 avg loss no lamb -2.212470 time 2019-02-23 01:22:17.069411
Model ind 685 epoch 132 head A batch: 200 avg loss -2.206891 avg loss no lamb -2.206891 time 2019-02-23 01:23:31.336720
Model ind 685 epoch 132 head A batch: 300 avg loss -2.190365 avg loss no lamb -2.190365 time 2019-02-23 01:24:43.224001
Model ind 685 epoch 132 head A batch: 400 avg loss -2.178838 avg loss no lamb -2.178838 time 2019-02-23 01:25:56.638605
Pre: time 2019-02-23 01:27:27.702642: 
 	std: 0.006490757
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.9788857, 0.99197143, 0.97885716, 0.9789]
	train_accs: [0.9922857, 0.9788857, 0.99197143, 0.97885716, 0.9789]
	best_train_sub_head: 0
	worst: 0.97885716
	avg: 0.98418
	best: 0.9922857

Starting e_i: 133
Model ind 685 epoch 133 head B batch: 0 avg loss -2.111394 avg loss no lamb -2.111394 time 2019-02-23 01:27:29.393609
Model ind 685 epoch 133 head B batch: 100 avg loss -2.181052 avg loss no lamb -2.181052 time 2019-02-23 01:28:43.469274
Model ind 685 epoch 133 head B batch: 200 avg loss -2.204778 avg loss no lamb -2.204778 time 2019-02-23 01:30:01.464936
Model ind 685 epoch 133 head B batch: 300 avg loss -2.182862 avg loss no lamb -2.182862 time 2019-02-23 01:31:18.354742
Model ind 685 epoch 133 head B batch: 400 avg loss -2.217680 avg loss no lamb -2.217680 time 2019-02-23 01:32:34.488363
Model ind 685 epoch 133 head B batch: 0 avg loss -2.180725 avg loss no lamb -2.180725 time 2019-02-23 01:33:53.263060
Model ind 685 epoch 133 head B batch: 100 avg loss -2.206644 avg loss no lamb -2.206644 time 2019-02-23 01:35:09.442776
Model ind 685 epoch 133 head B batch: 200 avg loss -2.144010 avg loss no lamb -2.144010 time 2019-02-23 01:36:21.765986
Model ind 685 epoch 133 head B batch: 300 avg loss -2.210283 avg loss no lamb -2.210283 time 2019-02-23 01:37:39.782288
Model ind 685 epoch 133 head B batch: 400 avg loss -2.207712 avg loss no lamb -2.207712 time 2019-02-23 01:38:56.691353
Model ind 685 epoch 133 head A batch: 0 avg loss -2.231371 avg loss no lamb -2.231371 time 2019-02-23 01:40:10.456570
Model ind 685 epoch 133 head A batch: 100 avg loss -2.200322 avg loss no lamb -2.200322 time 2019-02-23 01:41:24.230149
Model ind 685 epoch 133 head A batch: 200 avg loss -2.147518 avg loss no lamb -2.147518 time 2019-02-23 01:42:37.043392
Model ind 685 epoch 133 head A batch: 300 avg loss -2.165468 avg loss no lamb -2.165468 time 2019-02-23 01:43:50.038098
Model ind 685 epoch 133 head A batch: 400 avg loss -2.193813 avg loss no lamb -2.193813 time 2019-02-23 01:45:06.741233
Pre: time 2019-02-23 01:46:41.162569: 
 	std: 0.0064733266
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9920857, 0.9790286, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9920857, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 134
Model ind 685 epoch 134 head B batch: 0 avg loss -2.197619 avg loss no lamb -2.197619 time 2019-02-23 01:46:43.011119
Model ind 685 epoch 134 head B batch: 100 avg loss -2.143281 avg loss no lamb -2.143281 time 2019-02-23 01:48:02.954583
Model ind 685 epoch 134 head B batch: 200 avg loss -2.164845 avg loss no lamb -2.164845 time 2019-02-23 01:49:21.757139
Model ind 685 epoch 134 head B batch: 300 avg loss -2.227735 avg loss no lamb -2.227735 time 2019-02-23 01:50:36.909910
Model ind 685 epoch 134 head B batch: 400 avg loss -2.150838 avg loss no lamb -2.150838 time 2019-02-23 01:51:49.415699
Model ind 685 epoch 134 head B batch: 0 avg loss -2.198468 avg loss no lamb -2.198468 time 2019-02-23 01:53:03.230953
Model ind 685 epoch 134 head B batch: 100 avg loss -2.192891 avg loss no lamb -2.192891 time 2019-02-23 01:54:25.630312
Model ind 685 epoch 134 head B batch: 200 avg loss -2.205410 avg loss no lamb -2.205410 time 2019-02-23 01:55:45.750509
Model ind 685 epoch 134 head B batch: 300 avg loss -2.183353 avg loss no lamb -2.183353 time 2019-02-23 01:57:01.802450
Model ind 685 epoch 134 head B batch: 400 avg loss -2.154832 avg loss no lamb -2.154832 time 2019-02-23 01:58:15.366881
Model ind 685 epoch 134 head A batch: 0 avg loss -2.142733 avg loss no lamb -2.142733 time 2019-02-23 01:59:27.812720
Model ind 685 epoch 134 head A batch: 100 avg loss -2.187860 avg loss no lamb -2.187860 time 2019-02-23 02:00:46.528639
Model ind 685 epoch 134 head A batch: 200 avg loss -2.192493 avg loss no lamb -2.192493 time 2019-02-23 02:02:05.452154
Model ind 685 epoch 134 head A batch: 300 avg loss -2.199753 avg loss no lamb -2.199753 time 2019-02-23 02:03:24.779179
Model ind 685 epoch 134 head A batch: 400 avg loss -2.164799 avg loss no lamb -2.164799 time 2019-02-23 02:04:43.856926
Pre: time 2019-02-23 02:06:16.319583: 
 	std: 0.0065348195
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9789571, 0.99215716, 0.9789429, 0.9789429]
	train_accs: [0.9924143, 0.9789571, 0.99215716, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98428285
	best: 0.9924143

Starting e_i: 135
Model ind 685 epoch 135 head B batch: 0 avg loss -2.228621 avg loss no lamb -2.228621 time 2019-02-23 02:06:18.028837
Model ind 685 epoch 135 head B batch: 100 avg loss -2.194342 avg loss no lamb -2.194342 time 2019-02-23 02:07:31.173732
Model ind 685 epoch 135 head B batch: 200 avg loss -2.199677 avg loss no lamb -2.199677 time 2019-02-23 02:08:50.929407
Model ind 685 epoch 135 head B batch: 300 avg loss -2.186802 avg loss no lamb -2.186802 time 2019-02-23 02:10:09.747810
Model ind 685 epoch 135 head B batch: 400 avg loss -2.177345 avg loss no lamb -2.177345 time 2019-02-23 02:11:26.568563
Model ind 685 epoch 135 head B batch: 0 avg loss -2.190970 avg loss no lamb -2.190970 time 2019-02-23 02:12:43.060563
Model ind 685 epoch 135 head B batch: 100 avg loss -2.174067 avg loss no lamb -2.174067 time 2019-02-23 02:13:59.968469
Model ind 685 epoch 135 head B batch: 200 avg loss -2.149608 avg loss no lamb -2.149608 time 2019-02-23 02:15:16.773236
Model ind 685 epoch 135 head B batch: 300 avg loss -2.143771 avg loss no lamb -2.143771 time 2019-02-23 02:16:30.342906
Model ind 685 epoch 135 head B batch: 400 avg loss -2.142102 avg loss no lamb -2.142102 time 2019-02-23 02:17:32.918316
Model ind 685 epoch 135 head A batch: 0 avg loss -2.210737 avg loss no lamb -2.210737 time 2019-02-23 02:18:50.661732
Model ind 685 epoch 135 head A batch: 100 avg loss -2.179863 avg loss no lamb -2.179863 time 2019-02-23 02:20:07.154568
Model ind 685 epoch 135 head A batch: 200 avg loss -2.176711 avg loss no lamb -2.176711 time 2019-02-23 02:21:26.855064
Model ind 685 epoch 135 head A batch: 300 avg loss -2.211441 avg loss no lamb -2.211441 time 2019-02-23 02:22:45.127440
Model ind 685 epoch 135 head A batch: 400 avg loss -2.179225 avg loss no lamb -2.179225 time 2019-02-23 02:24:03.508841
Pre: time 2019-02-23 02:25:36.508989: 
 	std: 0.0064770635
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99247146, 0.97905713, 0.9921, 0.97908574, 0.97905713]
	train_accs: [0.99247146, 0.97905713, 0.9921, 0.97908574, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.9843543
	best: 0.99247146

Starting e_i: 136
Model ind 685 epoch 136 head B batch: 0 avg loss -2.196431 avg loss no lamb -2.196431 time 2019-02-23 02:25:38.261667
Model ind 685 epoch 136 head B batch: 100 avg loss -2.209938 avg loss no lamb -2.209938 time 2019-02-23 02:26:58.030842
Model ind 685 epoch 136 head B batch: 200 avg loss -2.201508 avg loss no lamb -2.201508 time 2019-02-23 02:28:17.514570
Model ind 685 epoch 136 head B batch: 300 avg loss -2.249445 avg loss no lamb -2.249445 time 2019-02-23 02:29:31.979874
Model ind 685 epoch 136 head B batch: 400 avg loss -2.183530 avg loss no lamb -2.183530 time 2019-02-23 02:30:43.385300
Model ind 685 epoch 136 head B batch: 0 avg loss -2.214699 avg loss no lamb -2.214699 time 2019-02-23 02:32:01.052148
Model ind 685 epoch 136 head B batch: 100 avg loss -2.179678 avg loss no lamb -2.179678 time 2019-02-23 02:33:20.731020
Model ind 685 epoch 136 head B batch: 200 avg loss -2.180799 avg loss no lamb -2.180799 time 2019-02-23 02:34:39.624519
Model ind 685 epoch 136 head B batch: 300 avg loss -2.212466 avg loss no lamb -2.212466 time 2019-02-23 02:35:57.978710
Model ind 685 epoch 136 head B batch: 400 avg loss -2.161535 avg loss no lamb -2.161535 time 2019-02-23 02:37:16.100983
Model ind 685 epoch 136 head A batch: 0 avg loss -2.190950 avg loss no lamb -2.190950 time 2019-02-23 02:38:37.723920
Model ind 685 epoch 136 head A batch: 100 avg loss -2.216075 avg loss no lamb -2.216075 time 2019-02-23 02:39:49.604892
Model ind 685 epoch 136 head A batch: 200 avg loss -2.159801 avg loss no lamb -2.159801 time 2019-02-23 02:41:02.500728
Model ind 685 epoch 136 head A batch: 300 avg loss -2.204162 avg loss no lamb -2.204162 time 2019-02-23 02:42:18.083806
Model ind 685 epoch 136 head A batch: 400 avg loss -2.144545 avg loss no lamb -2.144545 time 2019-02-23 02:43:33.073198
Pre: time 2019-02-23 02:45:00.541043: 
 	std: 0.0064792573
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.97891426, 0.99198574, 0.9789571, 0.97892857]
	train_accs: [0.9923286, 0.97891426, 0.99198574, 0.9789571, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.9842229
	best: 0.9923286

Starting e_i: 137
Model ind 685 epoch 137 head B batch: 0 avg loss -2.199875 avg loss no lamb -2.199875 time 2019-02-23 02:45:02.211487
Model ind 685 epoch 137 head B batch: 100 avg loss -2.166995 avg loss no lamb -2.166995 time 2019-02-23 02:46:20.658447
Model ind 685 epoch 137 head B batch: 200 avg loss -2.191428 avg loss no lamb -2.191428 time 2019-02-23 02:47:33.344683
Model ind 685 epoch 137 head B batch: 300 avg loss -2.241617 avg loss no lamb -2.241617 time 2019-02-23 02:48:44.555034
Model ind 685 epoch 137 head B batch: 400 avg loss -2.160872 avg loss no lamb -2.160872 time 2019-02-23 02:50:00.663630
Model ind 685 epoch 137 head B batch: 0 avg loss -2.198267 avg loss no lamb -2.198267 time 2019-02-23 02:51:11.432001
Model ind 685 epoch 137 head B batch: 100 avg loss -2.159202 avg loss no lamb -2.159202 time 2019-02-23 02:52:24.135379
Model ind 685 epoch 137 head B batch: 200 avg loss -2.170339 avg loss no lamb -2.170339 time 2019-02-23 02:53:37.820398
Model ind 685 epoch 137 head B batch: 300 avg loss -2.202110 avg loss no lamb -2.202110 time 2019-02-23 02:54:49.224350
Model ind 685 epoch 137 head B batch: 400 avg loss -2.179280 avg loss no lamb -2.179280 time 2019-02-23 02:56:04.620706
Model ind 685 epoch 137 head A batch: 0 avg loss -2.225737 avg loss no lamb -2.225737 time 2019-02-23 02:57:20.287589
Model ind 685 epoch 137 head A batch: 100 avg loss -2.226433 avg loss no lamb -2.226433 time 2019-02-23 02:58:35.283530
Model ind 685 epoch 137 head A batch: 200 avg loss -2.220319 avg loss no lamb -2.220319 time 2019-02-23 02:59:39.239452
Model ind 685 epoch 137 head A batch: 300 avg loss -2.182563 avg loss no lamb -2.182563 time 2019-02-23 03:00:57.269801
Model ind 685 epoch 137 head A batch: 400 avg loss -2.216361 avg loss no lamb -2.216361 time 2019-02-23 03:02:20.986856
Pre: time 2019-02-23 03:03:54.609784: 
 	std: 0.0064918525
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789571, 0.99205714, 0.9789571, 0.9789571]
	train_accs: [0.99235713, 0.9789571, 0.99205714, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.9842571
	best: 0.99235713

Starting e_i: 138
Model ind 685 epoch 138 head B batch: 0 avg loss -2.187871 avg loss no lamb -2.187871 time 2019-02-23 03:03:56.411991
Model ind 685 epoch 138 head B batch: 100 avg loss -2.218762 avg loss no lamb -2.218762 time 2019-02-23 03:05:17.197401
Model ind 685 epoch 138 head B batch: 200 avg loss -2.197157 avg loss no lamb -2.197157 time 2019-02-23 03:06:33.954829
Model ind 685 epoch 138 head B batch: 300 avg loss -2.187820 avg loss no lamb -2.187820 time 2019-02-23 03:07:53.800501
Model ind 685 epoch 138 head B batch: 400 avg loss -2.141402 avg loss no lamb -2.141402 time 2019-02-23 03:09:13.684321
Model ind 685 epoch 138 head B batch: 0 avg loss -2.154191 avg loss no lamb -2.154191 time 2019-02-23 03:10:34.906157
Model ind 685 epoch 138 head B batch: 100 avg loss -2.186670 avg loss no lamb -2.186670 time 2019-02-23 03:11:54.143682
Model ind 685 epoch 138 head B batch: 200 avg loss -2.142772 avg loss no lamb -2.142772 time 2019-02-23 03:13:14.545549
Model ind 685 epoch 138 head B batch: 300 avg loss -2.196329 avg loss no lamb -2.196329 time 2019-02-23 03:14:31.157399
Model ind 685 epoch 138 head B batch: 400 avg loss -2.197594 avg loss no lamb -2.197594 time 2019-02-23 03:15:53.410686
Model ind 685 epoch 138 head A batch: 0 avg loss -2.214529 avg loss no lamb -2.214529 time 2019-02-23 03:17:11.622900
Model ind 685 epoch 138 head A batch: 100 avg loss -2.154813 avg loss no lamb -2.154813 time 2019-02-23 03:18:26.558157
Model ind 685 epoch 138 head A batch: 200 avg loss -2.186223 avg loss no lamb -2.186223 time 2019-02-23 03:19:44.987057
Model ind 685 epoch 138 head A batch: 300 avg loss -2.186816 avg loss no lamb -2.186816 time 2019-02-23 03:21:03.890227
Model ind 685 epoch 138 head A batch: 400 avg loss -2.222664 avg loss no lamb -2.222664 time 2019-02-23 03:22:21.427659
Pre: time 2019-02-23 03:23:51.030335: 
 	std: 0.0065057273
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790428, 0.9921857, 0.9790428, 0.9790428]
	train_accs: [0.99245715, 0.9790428, 0.9921857, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843542
	best: 0.99245715

Starting e_i: 139
Model ind 685 epoch 139 head B batch: 0 avg loss -2.187691 avg loss no lamb -2.187691 time 2019-02-23 03:23:52.967807
Model ind 685 epoch 139 head B batch: 100 avg loss -2.179771 avg loss no lamb -2.179771 time 2019-02-23 03:25:15.115586
Model ind 685 epoch 139 head B batch: 200 avg loss -2.164318 avg loss no lamb -2.164318 time 2019-02-23 03:26:31.655322
Model ind 685 epoch 139 head B batch: 300 avg loss -2.205658 avg loss no lamb -2.205658 time 2019-02-23 03:27:47.978683
Model ind 685 epoch 139 head B batch: 400 avg loss -2.172325 avg loss no lamb -2.172325 time 2019-02-23 03:29:05.066199
Model ind 685 epoch 139 head B batch: 0 avg loss -2.185772 avg loss no lamb -2.185772 time 2019-02-23 03:30:22.032298
Model ind 685 epoch 139 head B batch: 100 avg loss -2.209093 avg loss no lamb -2.209093 time 2019-02-23 03:31:34.209293
Model ind 685 epoch 139 head B batch: 200 avg loss -2.166603 avg loss no lamb -2.166603 time 2019-02-23 03:32:50.270298
Model ind 685 epoch 139 head B batch: 300 avg loss -2.135301 avg loss no lamb -2.135301 time 2019-02-23 03:34:03.090700
Model ind 685 epoch 139 head B batch: 400 avg loss -2.189189 avg loss no lamb -2.189189 time 2019-02-23 03:35:18.235355
Model ind 685 epoch 139 head A batch: 0 avg loss -2.194527 avg loss no lamb -2.194527 time 2019-02-23 03:36:38.700201
Model ind 685 epoch 139 head A batch: 100 avg loss -2.216014 avg loss no lamb -2.216014 time 2019-02-23 03:37:55.627284
Model ind 685 epoch 139 head A batch: 200 avg loss -2.184678 avg loss no lamb -2.184678 time 2019-02-23 03:39:14.227469
Model ind 685 epoch 139 head A batch: 300 avg loss -2.167910 avg loss no lamb -2.167910 time 2019-02-23 03:40:31.920778
Model ind 685 epoch 139 head A batch: 400 avg loss -2.192658 avg loss no lamb -2.192658 time 2019-02-23 03:41:42.060476
Pre: time 2019-02-23 03:43:18.763104: 
 	std: 0.0064909034
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790286, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.99245715, 0.9790286, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98433715
	best: 0.99245715

Starting e_i: 140
Model ind 685 epoch 140 head B batch: 0 avg loss -2.224619 avg loss no lamb -2.224619 time 2019-02-23 03:43:20.475823
Model ind 685 epoch 140 head B batch: 100 avg loss -2.182578 avg loss no lamb -2.182578 time 2019-02-23 03:44:40.200761
Model ind 685 epoch 140 head B batch: 200 avg loss -2.178323 avg loss no lamb -2.178323 time 2019-02-23 03:46:00.827707
Model ind 685 epoch 140 head B batch: 300 avg loss -2.190839 avg loss no lamb -2.190839 time 2019-02-23 03:47:18.418850
Model ind 685 epoch 140 head B batch: 400 avg loss -2.203632 avg loss no lamb -2.203632 time 2019-02-23 03:48:36.507710
Model ind 685 epoch 140 head B batch: 0 avg loss -2.161735 avg loss no lamb -2.161735 time 2019-02-23 03:49:53.326624
Model ind 685 epoch 140 head B batch: 100 avg loss -2.202091 avg loss no lamb -2.202091 time 2019-02-23 03:51:12.601553
Model ind 685 epoch 140 head B batch: 200 avg loss -2.143697 avg loss no lamb -2.143697 time 2019-02-23 03:52:33.632675
Model ind 685 epoch 140 head B batch: 300 avg loss -2.169517 avg loss no lamb -2.169517 time 2019-02-23 03:53:51.825551
Model ind 685 epoch 140 head B batch: 400 avg loss -2.162382 avg loss no lamb -2.162382 time 2019-02-23 03:55:09.316615
Model ind 685 epoch 140 head A batch: 0 avg loss -2.195534 avg loss no lamb -2.195534 time 2019-02-23 03:56:22.173455
Model ind 685 epoch 140 head A batch: 100 avg loss -2.227849 avg loss no lamb -2.227849 time 2019-02-23 03:57:32.905606
Model ind 685 epoch 140 head A batch: 200 avg loss -2.179050 avg loss no lamb -2.179050 time 2019-02-23 03:58:47.828643
Model ind 685 epoch 140 head A batch: 300 avg loss -2.204120 avg loss no lamb -2.204120 time 2019-02-23 04:00:04.811069
Model ind 685 epoch 140 head A batch: 400 avg loss -2.154768 avg loss no lamb -2.154768 time 2019-02-23 04:01:21.098359
Pre: time 2019-02-23 04:02:53.274473: 
 	std: 0.0064693876
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.97907144, 0.99215716, 0.97905713, 0.97907144]
	train_accs: [0.9923857, 0.97907144, 0.99215716, 0.97905713, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.9843486
	best: 0.9923857

Starting e_i: 141
Model ind 685 epoch 141 head B batch: 0 avg loss -2.187616 avg loss no lamb -2.187616 time 2019-02-23 04:02:55.895448
Model ind 685 epoch 141 head B batch: 100 avg loss -2.195351 avg loss no lamb -2.195351 time 2019-02-23 04:04:10.679345
Model ind 685 epoch 141 head B batch: 200 avg loss -2.153643 avg loss no lamb -2.153643 time 2019-02-23 04:05:26.653920
Model ind 685 epoch 141 head B batch: 300 avg loss -2.212570 avg loss no lamb -2.212570 time 2019-02-23 04:06:43.811751
Model ind 685 epoch 141 head B batch: 400 avg loss -2.183829 avg loss no lamb -2.183829 time 2019-02-23 04:08:03.360044
Model ind 685 epoch 141 head B batch: 0 avg loss -2.215674 avg loss no lamb -2.215674 time 2019-02-23 04:09:21.574680
Model ind 685 epoch 141 head B batch: 100 avg loss -2.148922 avg loss no lamb -2.148922 time 2019-02-23 04:10:41.144116
Model ind 685 epoch 141 head B batch: 200 avg loss -2.228820 avg loss no lamb -2.228820 time 2019-02-23 04:11:58.314959
Model ind 685 epoch 141 head B batch: 300 avg loss -2.176552 avg loss no lamb -2.176552 time 2019-02-23 04:13:13.064684
Model ind 685 epoch 141 head B batch: 400 avg loss -2.193758 avg loss no lamb -2.193758 time 2019-02-23 04:14:32.555821
Model ind 685 epoch 141 head A batch: 0 avg loss -2.190371 avg loss no lamb -2.190371 time 2019-02-23 04:15:54.774017
Model ind 685 epoch 141 head A batch: 100 avg loss -2.189645 avg loss no lamb -2.189645 time 2019-02-23 04:17:13.719572
Model ind 685 epoch 141 head A batch: 200 avg loss -2.173677 avg loss no lamb -2.173677 time 2019-02-23 04:18:33.798426
Model ind 685 epoch 141 head A batch: 300 avg loss -2.221201 avg loss no lamb -2.221201 time 2019-02-23 04:19:52.079669
Model ind 685 epoch 141 head A batch: 400 avg loss -2.179384 avg loss no lamb -2.179384 time 2019-02-23 04:21:04.670420
Pre: time 2019-02-23 04:22:33.837423: 
 	std: 0.006460284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.97907144, 0.9921143, 0.97907144, 0.97907144]
	train_accs: [0.9924, 0.97907144, 0.9921143, 0.97907144, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.98434573
	best: 0.9924

Starting e_i: 142
Model ind 685 epoch 142 head B batch: 0 avg loss -2.200166 avg loss no lamb -2.200166 time 2019-02-23 04:22:35.578410
Model ind 685 epoch 142 head B batch: 100 avg loss -2.125596 avg loss no lamb -2.125596 time 2019-02-23 04:23:46.236995
Model ind 685 epoch 142 head B batch: 200 avg loss -2.178386 avg loss no lamb -2.178386 time 2019-02-23 04:24:58.906878
Model ind 685 epoch 142 head B batch: 300 avg loss -2.135744 avg loss no lamb -2.135744 time 2019-02-23 04:26:18.397020
Model ind 685 epoch 142 head B batch: 400 avg loss -2.158059 avg loss no lamb -2.158059 time 2019-02-23 04:27:38.589082
Model ind 685 epoch 142 head B batch: 0 avg loss -2.197470 avg loss no lamb -2.197470 time 2019-02-23 04:28:55.069399
Model ind 685 epoch 142 head B batch: 100 avg loss -2.163677 avg loss no lamb -2.163677 time 2019-02-23 04:30:09.276797
Model ind 685 epoch 142 head B batch: 200 avg loss -2.200010 avg loss no lamb -2.200010 time 2019-02-23 04:31:25.748000
Model ind 685 epoch 142 head B batch: 300 avg loss -2.224924 avg loss no lamb -2.224924 time 2019-02-23 04:32:42.605005
Model ind 685 epoch 142 head B batch: 400 avg loss -2.157238 avg loss no lamb -2.157238 time 2019-02-23 04:34:00.164913
Model ind 685 epoch 142 head A batch: 0 avg loss -2.231677 avg loss no lamb -2.231677 time 2019-02-23 04:35:18.822105
Model ind 685 epoch 142 head A batch: 100 avg loss -2.181861 avg loss no lamb -2.181861 time 2019-02-23 04:36:36.147768
Model ind 685 epoch 142 head A batch: 200 avg loss -2.135495 avg loss no lamb -2.135495 time 2019-02-23 04:37:47.575400
Model ind 685 epoch 142 head A batch: 300 avg loss -2.219859 avg loss no lamb -2.219859 time 2019-02-23 04:39:05.519493
Model ind 685 epoch 142 head A batch: 400 avg loss -2.182179 avg loss no lamb -2.182179 time 2019-02-23 04:40:24.265648
Pre: time 2019-02-23 04:41:56.439549: 
 	std: 0.0064800545
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922714, 0.97891426, 0.992, 0.9789, 0.97891426]
	train_accs: [0.9922714, 0.97891426, 0.992, 0.9789, 0.97891426]
	best_train_sub_head: 0
	worst: 0.9789
	avg: 0.9842
	best: 0.9922714

Starting e_i: 143
Model ind 685 epoch 143 head B batch: 0 avg loss -2.177138 avg loss no lamb -2.177138 time 2019-02-23 04:41:58.306598
Model ind 685 epoch 143 head B batch: 100 avg loss -2.202001 avg loss no lamb -2.202001 time 2019-02-23 04:43:15.051998
Model ind 685 epoch 143 head B batch: 200 avg loss -2.167490 avg loss no lamb -2.167490 time 2019-02-23 04:44:31.874112
Model ind 685 epoch 143 head B batch: 300 avg loss -2.213627 avg loss no lamb -2.213627 time 2019-02-23 04:45:51.676614
Model ind 685 epoch 143 head B batch: 400 avg loss -2.221178 avg loss no lamb -2.221178 time 2019-02-23 04:47:11.117980
Model ind 685 epoch 143 head B batch: 0 avg loss -2.196328 avg loss no lamb -2.196328 time 2019-02-23 04:48:32.760881
Model ind 685 epoch 143 head B batch: 100 avg loss -2.204532 avg loss no lamb -2.204532 time 2019-02-23 04:49:56.680528
Model ind 685 epoch 143 head B batch: 200 avg loss -2.187772 avg loss no lamb -2.187772 time 2019-02-23 04:51:13.417342
Model ind 685 epoch 143 head B batch: 300 avg loss -2.238800 avg loss no lamb -2.238800 time 2019-02-23 04:52:31.636098
Model ind 685 epoch 143 head B batch: 400 avg loss -2.179823 avg loss no lamb -2.179823 time 2019-02-23 04:53:51.742618
Model ind 685 epoch 143 head A batch: 0 avg loss -2.173340 avg loss no lamb -2.173340 time 2019-02-23 04:55:08.482000
Model ind 685 epoch 143 head A batch: 100 avg loss -2.176246 avg loss no lamb -2.176246 time 2019-02-23 04:56:26.086959
Model ind 685 epoch 143 head A batch: 200 avg loss -2.140519 avg loss no lamb -2.140519 time 2019-02-23 04:57:40.992632
Model ind 685 epoch 143 head A batch: 300 avg loss -2.213623 avg loss no lamb -2.213623 time 2019-02-23 04:59:01.349203
Model ind 685 epoch 143 head A batch: 400 avg loss -2.192641 avg loss no lamb -2.192641 time 2019-02-23 05:00:22.063965
Pre: time 2019-02-23 05:01:55.073568: 
 	std: 0.0064708595
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789571, 0.9920143, 0.9789571, 0.9789571]
	train_accs: [0.9923143, 0.9789571, 0.9920143, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.98423994
	best: 0.9923143

Starting e_i: 144
Model ind 685 epoch 144 head B batch: 0 avg loss -2.182124 avg loss no lamb -2.182124 time 2019-02-23 05:01:56.999437
Model ind 685 epoch 144 head B batch: 100 avg loss -2.184868 avg loss no lamb -2.184868 time 2019-02-23 05:03:15.724931
Model ind 685 epoch 144 head B batch: 200 avg loss -2.172169 avg loss no lamb -2.172169 time 2019-02-23 05:04:28.000281
Model ind 685 epoch 144 head B batch: 300 avg loss -2.180864 avg loss no lamb -2.180864 time 2019-02-23 05:05:42.059307
Model ind 685 epoch 144 head B batch: 400 avg loss -2.159638 avg loss no lamb -2.159638 time 2019-02-23 05:06:59.240883
Model ind 685 epoch 144 head B batch: 0 avg loss -2.200927 avg loss no lamb -2.200927 time 2019-02-23 05:08:16.506650
Model ind 685 epoch 144 head B batch: 100 avg loss -2.190018 avg loss no lamb -2.190018 time 2019-02-23 05:09:32.414165
Model ind 685 epoch 144 head B batch: 200 avg loss -2.208986 avg loss no lamb -2.208986 time 2019-02-23 05:10:49.783707
Model ind 685 epoch 144 head B batch: 300 avg loss -2.185784 avg loss no lamb -2.185784 time 2019-02-23 05:12:08.246241
Model ind 685 epoch 144 head B batch: 400 avg loss -2.203885 avg loss no lamb -2.203885 time 2019-02-23 05:13:26.771724
Model ind 685 epoch 144 head A batch: 0 avg loss -2.185695 avg loss no lamb -2.185695 time 2019-02-23 05:14:44.264209
Model ind 685 epoch 144 head A batch: 100 avg loss -2.193583 avg loss no lamb -2.193583 time 2019-02-23 05:16:03.184804
Model ind 685 epoch 144 head A batch: 200 avg loss -2.208044 avg loss no lamb -2.208044 time 2019-02-23 05:17:21.650133
Model ind 685 epoch 144 head A batch: 300 avg loss -2.225970 avg loss no lamb -2.225970 time 2019-02-23 05:18:33.526267
Model ind 685 epoch 144 head A batch: 400 avg loss -2.154170 avg loss no lamb -2.154170 time 2019-02-23 05:19:45.718088
Pre: time 2019-02-23 05:21:19.022804: 
 	std: 0.006527711
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.9790428, 0.9922571, 0.97905713, 0.9790428]
	train_accs: [0.9924857, 0.9790428, 0.9922571, 0.97905713, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843771
	best: 0.9924857

Starting e_i: 145
Model ind 685 epoch 145 head B batch: 0 avg loss -2.207333 avg loss no lamb -2.207333 time 2019-02-23 05:21:20.814335
Model ind 685 epoch 145 head B batch: 100 avg loss -2.216045 avg loss no lamb -2.216045 time 2019-02-23 05:22:37.979690
Model ind 685 epoch 145 head B batch: 200 avg loss -2.214670 avg loss no lamb -2.214670 time 2019-02-23 05:23:55.743558
Model ind 685 epoch 145 head B batch: 300 avg loss -2.178428 avg loss no lamb -2.178428 time 2019-02-23 05:25:13.597924
Model ind 685 epoch 145 head B batch: 400 avg loss -2.149616 avg loss no lamb -2.149616 time 2019-02-23 05:26:27.103596
Model ind 685 epoch 145 head B batch: 0 avg loss -2.220332 avg loss no lamb -2.220332 time 2019-02-23 05:27:43.450080
Model ind 685 epoch 145 head B batch: 100 avg loss -2.210056 avg loss no lamb -2.210056 time 2019-02-23 05:28:58.098492
Model ind 685 epoch 145 head B batch: 200 avg loss -2.184390 avg loss no lamb -2.184390 time 2019-02-23 05:30:12.646412
Model ind 685 epoch 145 head B batch: 300 avg loss -2.216386 avg loss no lamb -2.216386 time 2019-02-23 05:31:30.251407
Model ind 685 epoch 145 head B batch: 400 avg loss -2.192430 avg loss no lamb -2.192430 time 2019-02-23 05:32:49.445065
Model ind 685 epoch 145 head A batch: 0 avg loss -2.168174 avg loss no lamb -2.168174 time 2019-02-23 05:34:06.566766
Model ind 685 epoch 145 head A batch: 100 avg loss -2.186831 avg loss no lamb -2.186831 time 2019-02-23 05:35:24.548227
Model ind 685 epoch 145 head A batch: 200 avg loss -2.179803 avg loss no lamb -2.179803 time 2019-02-23 05:36:44.469547
Model ind 685 epoch 145 head A batch: 300 avg loss -2.194085 avg loss no lamb -2.194085 time 2019-02-23 05:38:02.018917
Model ind 685 epoch 145 head A batch: 400 avg loss -2.190649 avg loss no lamb -2.190649 time 2019-02-23 05:39:14.900561
Pre: time 2019-02-23 05:40:42.922203: 
 	std: 0.0064849835
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789714, 0.99204284, 0.9789714, 0.9789714]
	train_accs: [0.99237144, 0.9789714, 0.99204284, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842657
	best: 0.99237144

Starting e_i: 146
Model ind 685 epoch 146 head B batch: 0 avg loss -2.205505 avg loss no lamb -2.205505 time 2019-02-23 05:40:44.702435
Model ind 685 epoch 146 head B batch: 100 avg loss -2.183426 avg loss no lamb -2.183426 time 2019-02-23 05:42:01.118332
Model ind 685 epoch 146 head B batch: 200 avg loss -2.215229 avg loss no lamb -2.215229 time 2019-02-23 05:43:19.416202
Model ind 685 epoch 146 head B batch: 300 avg loss -2.218107 avg loss no lamb -2.218107 time 2019-02-23 05:44:38.936324
Model ind 685 epoch 146 head B batch: 400 avg loss -2.176399 avg loss no lamb -2.176399 time 2019-02-23 05:45:54.288601
Model ind 685 epoch 146 head B batch: 0 avg loss -2.221207 avg loss no lamb -2.221207 time 2019-02-23 05:47:01.821936
Model ind 685 epoch 146 head B batch: 100 avg loss -2.206471 avg loss no lamb -2.206471 time 2019-02-23 05:48:18.816690
Model ind 685 epoch 146 head B batch: 200 avg loss -2.163006 avg loss no lamb -2.163006 time 2019-02-23 05:49:36.785643
Model ind 685 epoch 146 head B batch: 300 avg loss -2.196782 avg loss no lamb -2.196782 time 2019-02-23 05:50:58.325921
Model ind 685 epoch 146 head B batch: 400 avg loss -2.201270 avg loss no lamb -2.201270 time 2019-02-23 05:52:14.637565
Model ind 685 epoch 146 head A batch: 0 avg loss -2.143584 avg loss no lamb -2.143584 time 2019-02-23 05:53:30.187469
Model ind 685 epoch 146 head A batch: 100 avg loss -2.188450 avg loss no lamb -2.188450 time 2019-02-23 05:54:46.793428
Model ind 685 epoch 146 head A batch: 200 avg loss -2.202739 avg loss no lamb -2.202739 time 2019-02-23 05:56:02.584441
Model ind 685 epoch 146 head A batch: 300 avg loss -2.203411 avg loss no lamb -2.203411 time 2019-02-23 05:57:23.953203
Model ind 685 epoch 146 head A batch: 400 avg loss -2.193107 avg loss no lamb -2.193107 time 2019-02-23 05:58:43.688153
Pre: time 2019-02-23 06:00:14.643905: 
 	std: 0.0064870426
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.99212855, 0.9790286, 0.9790143]
	train_accs: [0.9924, 0.9790286, 0.99212855, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.9924

Starting e_i: 147
Model ind 685 epoch 147 head B batch: 0 avg loss -2.219513 avg loss no lamb -2.219513 time 2019-02-23 06:00:16.496242
Model ind 685 epoch 147 head B batch: 100 avg loss -2.184573 avg loss no lamb -2.184573 time 2019-02-23 06:01:36.394829
Model ind 685 epoch 147 head B batch: 200 avg loss -2.135371 avg loss no lamb -2.135371 time 2019-02-23 06:02:56.656620
Model ind 685 epoch 147 head B batch: 300 avg loss -2.224957 avg loss no lamb -2.224957 time 2019-02-23 06:04:15.437831
Model ind 685 epoch 147 head B batch: 400 avg loss -2.180407 avg loss no lamb -2.180407 time 2019-02-23 06:05:31.208593
Model ind 685 epoch 147 head B batch: 0 avg loss -2.189262 avg loss no lamb -2.189262 time 2019-02-23 06:06:48.376337
Model ind 685 epoch 147 head B batch: 100 avg loss -2.176118 avg loss no lamb -2.176118 time 2019-02-23 06:08:04.302307
Model ind 685 epoch 147 head B batch: 200 avg loss -2.169978 avg loss no lamb -2.169978 time 2019-02-23 06:09:24.366332
Model ind 685 epoch 147 head B batch: 300 avg loss -2.188396 avg loss no lamb -2.188396 time 2019-02-23 06:10:41.517325
Model ind 685 epoch 147 head B batch: 400 avg loss -2.168518 avg loss no lamb -2.168518 time 2019-02-23 06:11:59.334055
Model ind 685 epoch 147 head A batch: 0 avg loss -2.209590 avg loss no lamb -2.209590 time 2019-02-23 06:13:17.477760
Model ind 685 epoch 147 head A batch: 100 avg loss -2.194582 avg loss no lamb -2.194582 time 2019-02-23 06:14:36.147967
Model ind 685 epoch 147 head A batch: 200 avg loss -2.164116 avg loss no lamb -2.164116 time 2019-02-23 06:15:56.268301
Model ind 685 epoch 147 head A batch: 300 avg loss -2.161132 avg loss no lamb -2.161132 time 2019-02-23 06:17:12.636510
Model ind 685 epoch 147 head A batch: 400 avg loss -2.209558 avg loss no lamb -2.209558 time 2019-02-23 06:18:29.845749
Pre: time 2019-02-23 06:19:55.902489: 
 	std: 0.00647674
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9920857, 0.9790143, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9920857, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924

Starting e_i: 148
Model ind 685 epoch 148 head B batch: 0 avg loss -2.230311 avg loss no lamb -2.230311 time 2019-02-23 06:19:57.689480
Model ind 685 epoch 148 head B batch: 100 avg loss -2.197016 avg loss no lamb -2.197016 time 2019-02-23 06:21:17.077556
Model ind 685 epoch 148 head B batch: 200 avg loss -2.188995 avg loss no lamb -2.188995 time 2019-02-23 06:22:33.196115
Model ind 685 epoch 148 head B batch: 300 avg loss -2.260162 avg loss no lamb -2.260162 time 2019-02-23 06:23:53.855228
Model ind 685 epoch 148 head B batch: 400 avg loss -2.175570 avg loss no lamb -2.175570 time 2019-02-23 06:25:10.372627
Model ind 685 epoch 148 head B batch: 0 avg loss -2.194248 avg loss no lamb -2.194248 time 2019-02-23 06:26:28.002837
Model ind 685 epoch 148 head B batch: 100 avg loss -2.157568 avg loss no lamb -2.157568 time 2019-02-23 06:27:47.365410
Model ind 685 epoch 148 head B batch: 200 avg loss -2.188946 avg loss no lamb -2.188946 time 2019-02-23 06:28:55.290096
Model ind 685 epoch 148 head B batch: 300 avg loss -2.229009 avg loss no lamb -2.229009 time 2019-02-23 06:30:14.009037
Model ind 685 epoch 148 head B batch: 400 avg loss -2.168603 avg loss no lamb -2.168603 time 2019-02-23 06:31:35.386158
Model ind 685 epoch 148 head A batch: 0 avg loss -2.198278 avg loss no lamb -2.198278 time 2019-02-23 06:32:53.506106
Model ind 685 epoch 148 head A batch: 100 avg loss -2.184895 avg loss no lamb -2.184895 time 2019-02-23 06:34:12.639993
Model ind 685 epoch 148 head A batch: 200 avg loss -2.180080 avg loss no lamb -2.180080 time 2019-02-23 06:35:30.981264
Model ind 685 epoch 148 head A batch: 300 avg loss -2.244537 avg loss no lamb -2.244537 time 2019-02-23 06:36:45.821699
Model ind 685 epoch 148 head A batch: 400 avg loss -2.160761 avg loss no lamb -2.160761 time 2019-02-23 06:38:05.316716
Pre: time 2019-02-23 06:39:38.776799: 
 	std: 0.0064962725
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789714, 0.9921143, 0.9789857, 0.9789714]
	train_accs: [0.99235713, 0.9789714, 0.9921143, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98428
	best: 0.99235713

Starting e_i: 149
Model ind 685 epoch 149 head B batch: 0 avg loss -2.193144 avg loss no lamb -2.193144 time 2019-02-23 06:39:40.490267
Model ind 685 epoch 149 head B batch: 100 avg loss -2.163590 avg loss no lamb -2.163590 time 2019-02-23 06:41:00.018885
Model ind 685 epoch 149 head B batch: 200 avg loss -2.193723 avg loss no lamb -2.193723 time 2019-02-23 06:42:16.670046
Model ind 685 epoch 149 head B batch: 300 avg loss -2.187963 avg loss no lamb -2.187963 time 2019-02-23 06:43:29.424005
Model ind 685 epoch 149 head B batch: 400 avg loss -2.149039 avg loss no lamb -2.149039 time 2019-02-23 06:44:45.321419
Model ind 685 epoch 149 head B batch: 0 avg loss -2.228422 avg loss no lamb -2.228422 time 2019-02-23 06:46:00.659734
Model ind 685 epoch 149 head B batch: 100 avg loss -2.214447 avg loss no lamb -2.214447 time 2019-02-23 06:47:19.746315
Model ind 685 epoch 149 head B batch: 200 avg loss -2.175186 avg loss no lamb -2.175186 time 2019-02-23 06:48:37.308003
Model ind 685 epoch 149 head B batch: 300 avg loss -2.239489 avg loss no lamb -2.239489 time 2019-02-23 06:49:52.312998
Model ind 685 epoch 149 head B batch: 400 avg loss -2.184035 avg loss no lamb -2.184035 time 2019-02-23 06:51:11.572823
Model ind 685 epoch 149 head A batch: 0 avg loss -2.168363 avg loss no lamb -2.168363 time 2019-02-23 06:52:31.969438
Model ind 685 epoch 149 head A batch: 100 avg loss -2.201901 avg loss no lamb -2.201901 time 2019-02-23 06:53:50.517303
Model ind 685 epoch 149 head A batch: 200 avg loss -2.218812 avg loss no lamb -2.218812 time 2019-02-23 06:55:04.741635
Model ind 685 epoch 149 head A batch: 300 avg loss -2.221562 avg loss no lamb -2.221562 time 2019-02-23 06:56:19.631873
Model ind 685 epoch 149 head A batch: 400 avg loss -2.165635 avg loss no lamb -2.165635 time 2019-02-23 06:57:33.807072
Pre: time 2019-02-23 06:59:02.398791: 
 	std: 0.0064963857
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.99212855, 0.9790143, 0.979]
	train_accs: [0.9924, 0.979, 0.99212855, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924

Starting e_i: 150
Model ind 685 epoch 150 head B batch: 0 avg loss -2.215670 avg loss no lamb -2.215670 time 2019-02-23 06:59:04.190030
Model ind 685 epoch 150 head B batch: 100 avg loss -2.211991 avg loss no lamb -2.211991 time 2019-02-23 07:00:23.112274
Model ind 685 epoch 150 head B batch: 200 avg loss -2.171524 avg loss no lamb -2.171524 time 2019-02-23 07:01:41.732267
Model ind 685 epoch 150 head B batch: 300 avg loss -2.232826 avg loss no lamb -2.232826 time 2019-02-23 07:03:02.466913
Model ind 685 epoch 150 head B batch: 400 avg loss -2.142551 avg loss no lamb -2.142551 time 2019-02-23 07:04:22.567837
Model ind 685 epoch 150 head B batch: 0 avg loss -2.189481 avg loss no lamb -2.189481 time 2019-02-23 07:05:34.930246
Model ind 685 epoch 150 head B batch: 100 avg loss -2.203009 avg loss no lamb -2.203009 time 2019-02-23 07:06:49.321472
Model ind 685 epoch 150 head B batch: 200 avg loss -2.159630 avg loss no lamb -2.159630 time 2019-02-23 07:08:03.885851
Model ind 685 epoch 150 head B batch: 300 avg loss -2.195992 avg loss no lamb -2.195992 time 2019-02-23 07:09:19.118420
Model ind 685 epoch 150 head B batch: 400 avg loss -2.168401 avg loss no lamb -2.168401 time 2019-02-23 07:10:24.361733
Model ind 685 epoch 150 head A batch: 0 avg loss -2.173881 avg loss no lamb -2.173881 time 2019-02-23 07:11:41.334732
Model ind 685 epoch 150 head A batch: 100 avg loss -2.148498 avg loss no lamb -2.148498 time 2019-02-23 07:12:59.909712
Model ind 685 epoch 150 head A batch: 200 avg loss -2.237475 avg loss no lamb -2.237475 time 2019-02-23 07:14:12.805620
Model ind 685 epoch 150 head A batch: 300 avg loss -2.175293 avg loss no lamb -2.175293 time 2019-02-23 07:15:30.239367
Model ind 685 epoch 150 head A batch: 400 avg loss -2.181376 avg loss no lamb -2.181376 time 2019-02-23 07:16:45.954892
Pre: time 2019-02-23 07:18:15.890470: 
 	std: 0.0064517646
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9921714, 0.97891426, 0.99198574, 0.9789, 0.97891426]
	train_accs: [0.9921714, 0.97891426, 0.99198574, 0.9789, 0.97891426]
	best_train_sub_head: 0
	worst: 0.9789
	avg: 0.9841771
	best: 0.9921714

Starting e_i: 151
Model ind 685 epoch 151 head B batch: 0 avg loss -2.190173 avg loss no lamb -2.190173 time 2019-02-23 07:18:17.630369
Model ind 685 epoch 151 head B batch: 100 avg loss -2.162997 avg loss no lamb -2.162997 time 2019-02-23 07:19:35.558538
Model ind 685 epoch 151 head B batch: 200 avg loss -2.189147 avg loss no lamb -2.189147 time 2019-02-23 07:20:55.172828
Model ind 685 epoch 151 head B batch: 300 avg loss -2.197150 avg loss no lamb -2.197150 time 2019-02-23 07:22:16.797418
Model ind 685 epoch 151 head B batch: 400 avg loss -2.216502 avg loss no lamb -2.216502 time 2019-02-23 07:23:34.256039
Model ind 685 epoch 151 head B batch: 0 avg loss -2.186702 avg loss no lamb -2.186702 time 2019-02-23 07:24:50.288186
Model ind 685 epoch 151 head B batch: 100 avg loss -2.188045 avg loss no lamb -2.188045 time 2019-02-23 07:26:09.945912
Model ind 685 epoch 151 head B batch: 200 avg loss -2.190203 avg loss no lamb -2.190203 time 2019-02-23 07:27:28.668227
Model ind 685 epoch 151 head B batch: 300 avg loss -2.211301 avg loss no lamb -2.211301 time 2019-02-23 07:28:47.909095
Model ind 685 epoch 151 head B batch: 400 avg loss -2.141249 avg loss no lamb -2.141249 time 2019-02-23 07:30:06.159333
Model ind 685 epoch 151 head A batch: 0 avg loss -2.214520 avg loss no lamb -2.214520 time 2019-02-23 07:31:22.951219
Model ind 685 epoch 151 head A batch: 100 avg loss -2.188139 avg loss no lamb -2.188139 time 2019-02-23 07:32:42.794458
Model ind 685 epoch 151 head A batch: 200 avg loss -2.208580 avg loss no lamb -2.208580 time 2019-02-23 07:34:03.003073
Model ind 685 epoch 151 head A batch: 300 avg loss -2.249636 avg loss no lamb -2.249636 time 2019-02-23 07:35:23.385888
Model ind 685 epoch 151 head A batch: 400 avg loss -2.181109 avg loss no lamb -2.181109 time 2019-02-23 07:36:43.363904
Pre: time 2019-02-23 07:38:16.904582: 
 	std: 0.006477727
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9921143, 0.9790143, 0.9790428]
	train_accs: [0.9923857, 0.9790286, 0.9921143, 0.9790143, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9923857

Starting e_i: 152
Model ind 685 epoch 152 head B batch: 0 avg loss -2.170704 avg loss no lamb -2.170704 time 2019-02-23 07:38:18.668632
Model ind 685 epoch 152 head B batch: 100 avg loss -2.202269 avg loss no lamb -2.202269 time 2019-02-23 07:39:33.999351
Model ind 685 epoch 152 head B batch: 200 avg loss -2.203200 avg loss no lamb -2.203200 time 2019-02-23 07:40:45.912997
Model ind 685 epoch 152 head B batch: 300 avg loss -2.208413 avg loss no lamb -2.208413 time 2019-02-23 07:41:58.611433
Model ind 685 epoch 152 head B batch: 400 avg loss -2.163958 avg loss no lamb -2.163958 time 2019-02-23 07:43:19.053099
Model ind 685 epoch 152 head B batch: 0 avg loss -2.183016 avg loss no lamb -2.183016 time 2019-02-23 07:44:37.013149
Model ind 685 epoch 152 head B batch: 100 avg loss -2.196912 avg loss no lamb -2.196912 time 2019-02-23 07:45:56.680065
Model ind 685 epoch 152 head B batch: 200 avg loss -2.175236 avg loss no lamb -2.175236 time 2019-02-23 07:47:15.600288
Model ind 685 epoch 152 head B batch: 300 avg loss -2.199582 avg loss no lamb -2.199582 time 2019-02-23 07:48:32.348788
Model ind 685 epoch 152 head B batch: 400 avg loss -2.160984 avg loss no lamb -2.160984 time 2019-02-23 07:49:50.251744
Model ind 685 epoch 152 head A batch: 0 avg loss -2.187687 avg loss no lamb -2.187687 time 2019-02-23 07:51:08.790472
Model ind 685 epoch 152 head A batch: 100 avg loss -2.161299 avg loss no lamb -2.161299 time 2019-02-23 07:52:13.388120
Model ind 685 epoch 152 head A batch: 200 avg loss -2.186881 avg loss no lamb -2.186881 time 2019-02-23 07:53:27.469263
Model ind 685 epoch 152 head A batch: 300 avg loss -2.203923 avg loss no lamb -2.203923 time 2019-02-23 07:54:45.665234
Model ind 685 epoch 152 head A batch: 400 avg loss -2.160494 avg loss no lamb -2.160494 time 2019-02-23 07:56:05.351421
Pre: time 2019-02-23 07:57:40.320104: 
 	std: 0.0064984243
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789857, 0.99214286, 0.9789571, 0.9789714]
	train_accs: [0.9923286, 0.9789857, 0.99214286, 0.9789571, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.9842771
	best: 0.9923286

Starting e_i: 153
Model ind 685 epoch 153 head B batch: 0 avg loss -2.231044 avg loss no lamb -2.231044 time 2019-02-23 07:57:42.074405
Model ind 685 epoch 153 head B batch: 100 avg loss -2.208083 avg loss no lamb -2.208083 time 2019-02-23 07:59:00.674622
Model ind 685 epoch 153 head B batch: 200 avg loss -2.191420 avg loss no lamb -2.191420 time 2019-02-23 08:00:13.057082
Model ind 685 epoch 153 head B batch: 300 avg loss -2.218720 avg loss no lamb -2.218720 time 2019-02-23 08:01:24.688048
Model ind 685 epoch 153 head B batch: 400 avg loss -2.142871 avg loss no lamb -2.142871 time 2019-02-23 08:02:41.797906
Model ind 685 epoch 153 head B batch: 0 avg loss -2.191965 avg loss no lamb -2.191965 time 2019-02-23 08:03:56.304789
Model ind 685 epoch 153 head B batch: 100 avg loss -2.203223 avg loss no lamb -2.203223 time 2019-02-23 08:05:12.269742
Model ind 685 epoch 153 head B batch: 200 avg loss -2.219712 avg loss no lamb -2.219712 time 2019-02-23 08:06:32.504143
Model ind 685 epoch 153 head B batch: 300 avg loss -2.216709 avg loss no lamb -2.216709 time 2019-02-23 08:07:52.279277
Model ind 685 epoch 153 head B batch: 400 avg loss -2.208925 avg loss no lamb -2.208925 time 2019-02-23 08:09:11.348142
Model ind 685 epoch 153 head A batch: 0 avg loss -2.203483 avg loss no lamb -2.203483 time 2019-02-23 08:10:29.975521
Model ind 685 epoch 153 head A batch: 100 avg loss -2.205256 avg loss no lamb -2.205256 time 2019-02-23 08:11:50.378870
Model ind 685 epoch 153 head A batch: 200 avg loss -2.191122 avg loss no lamb -2.191122 time 2019-02-23 08:13:10.344373
Model ind 685 epoch 153 head A batch: 300 avg loss -2.187952 avg loss no lamb -2.187952 time 2019-02-23 08:14:23.485118
Model ind 685 epoch 153 head A batch: 400 avg loss -2.180268 avg loss no lamb -2.180268 time 2019-02-23 08:15:34.492104
Pre: time 2019-02-23 08:17:02.694011: 
 	std: 0.006463852
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	train_accs: [0.9923143, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842485
	best: 0.9923143

Starting e_i: 154
Model ind 685 epoch 154 head B batch: 0 avg loss -2.210948 avg loss no lamb -2.210948 time 2019-02-23 08:17:04.614577
Model ind 685 epoch 154 head B batch: 100 avg loss -2.125069 avg loss no lamb -2.125069 time 2019-02-23 08:18:22.117611
Model ind 685 epoch 154 head B batch: 200 avg loss -2.192824 avg loss no lamb -2.192824 time 2019-02-23 08:19:38.309610
Model ind 685 epoch 154 head B batch: 300 avg loss -2.213818 avg loss no lamb -2.213818 time 2019-02-23 08:20:56.479742
Model ind 685 epoch 154 head B batch: 400 avg loss -2.178582 avg loss no lamb -2.178582 time 2019-02-23 08:22:14.702912
Model ind 685 epoch 154 head B batch: 0 avg loss -2.201699 avg loss no lamb -2.201699 time 2019-02-23 08:23:32.272373
Model ind 685 epoch 154 head B batch: 100 avg loss -2.183688 avg loss no lamb -2.183688 time 2019-02-23 08:24:52.508186
Model ind 685 epoch 154 head B batch: 200 avg loss -2.210732 avg loss no lamb -2.210732 time 2019-02-23 08:26:11.549679
Model ind 685 epoch 154 head B batch: 300 avg loss -2.179615 avg loss no lamb -2.179615 time 2019-02-23 08:27:28.632479
Model ind 685 epoch 154 head B batch: 400 avg loss -2.177580 avg loss no lamb -2.177580 time 2019-02-23 08:28:45.643758
Model ind 685 epoch 154 head A batch: 0 avg loss -2.227821 avg loss no lamb -2.227821 time 2019-02-23 08:30:01.871811
Model ind 685 epoch 154 head A batch: 100 avg loss -2.148351 avg loss no lamb -2.148351 time 2019-02-23 08:31:19.375868
Model ind 685 epoch 154 head A batch: 200 avg loss -2.185563 avg loss no lamb -2.185563 time 2019-02-23 08:32:34.851829
Model ind 685 epoch 154 head A batch: 300 avg loss -2.161569 avg loss no lamb -2.161569 time 2019-02-23 08:33:48.098673
Model ind 685 epoch 154 head A batch: 400 avg loss -2.135836 avg loss no lamb -2.135836 time 2019-02-23 08:35:02.981078
Pre: time 2019-02-23 08:36:37.214541: 
 	std: 0.0064951926
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.97892857, 0.9920286, 0.9789, 0.9788714]
	train_accs: [0.9922857, 0.97892857, 0.9920286, 0.9789, 0.9788714]
	best_train_sub_head: 0
	worst: 0.9788714
	avg: 0.98420286
	best: 0.9922857

Starting e_i: 155
Model ind 685 epoch 155 head B batch: 0 avg loss -2.222817 avg loss no lamb -2.222817 time 2019-02-23 08:36:38.930028
Model ind 685 epoch 155 head B batch: 100 avg loss -2.214192 avg loss no lamb -2.214192 time 2019-02-23 08:37:58.813108
Model ind 685 epoch 155 head B batch: 200 avg loss -2.191718 avg loss no lamb -2.191718 time 2019-02-23 08:39:15.255027
Model ind 685 epoch 155 head B batch: 300 avg loss -2.203461 avg loss no lamb -2.203461 time 2019-02-23 08:40:30.855986
Model ind 685 epoch 155 head B batch: 400 avg loss -2.205302 avg loss no lamb -2.205302 time 2019-02-23 08:41:44.920822
Model ind 685 epoch 155 head B batch: 0 avg loss -2.224764 avg loss no lamb -2.224764 time 2019-02-23 08:43:03.570809
Model ind 685 epoch 155 head B batch: 100 avg loss -2.177433 avg loss no lamb -2.177433 time 2019-02-23 08:44:22.453638
Model ind 685 epoch 155 head B batch: 200 avg loss -2.189747 avg loss no lamb -2.189747 time 2019-02-23 08:45:40.920026
Model ind 685 epoch 155 head B batch: 300 avg loss -2.216861 avg loss no lamb -2.216861 time 2019-02-23 08:46:59.954859
Model ind 685 epoch 155 head B batch: 400 avg loss -2.208833 avg loss no lamb -2.208833 time 2019-02-23 08:48:16.241222
Model ind 685 epoch 155 head A batch: 0 avg loss -2.189337 avg loss no lamb -2.189337 time 2019-02-23 08:49:30.909804
Model ind 685 epoch 155 head A batch: 100 avg loss -2.195083 avg loss no lamb -2.195083 time 2019-02-23 08:50:47.967845
Model ind 685 epoch 155 head A batch: 200 avg loss -2.182941 avg loss no lamb -2.182941 time 2019-02-23 08:52:00.354888
Model ind 685 epoch 155 head A batch: 300 avg loss -2.218984 avg loss no lamb -2.218984 time 2019-02-23 08:53:11.470972
Model ind 685 epoch 155 head A batch: 400 avg loss -2.182290 avg loss no lamb -2.182290 time 2019-02-23 08:54:26.237262
Pre: time 2019-02-23 08:55:58.108637: 
 	std: 0.006481277
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 156
Model ind 685 epoch 156 head B batch: 0 avg loss -2.166660 avg loss no lamb -2.166660 time 2019-02-23 08:55:59.887647
Model ind 685 epoch 156 head B batch: 100 avg loss -2.192209 avg loss no lamb -2.192209 time 2019-02-23 08:57:17.066089
Model ind 685 epoch 156 head B batch: 200 avg loss -2.158236 avg loss no lamb -2.158236 time 2019-02-23 08:58:34.350783
Model ind 685 epoch 156 head B batch: 300 avg loss -2.206334 avg loss no lamb -2.206334 time 2019-02-23 08:59:54.195702
Model ind 685 epoch 156 head B batch: 400 avg loss -2.204118 avg loss no lamb -2.204118 time 2019-02-23 09:01:12.309445
Model ind 685 epoch 156 head B batch: 0 avg loss -2.156875 avg loss no lamb -2.156875 time 2019-02-23 09:02:28.699313
Model ind 685 epoch 156 head B batch: 100 avg loss -2.150273 avg loss no lamb -2.150273 time 2019-02-23 09:03:47.944791
Model ind 685 epoch 156 head B batch: 200 avg loss -2.167039 avg loss no lamb -2.167039 time 2019-02-23 09:05:05.617608
Model ind 685 epoch 156 head B batch: 300 avg loss -2.159406 avg loss no lamb -2.159406 time 2019-02-23 09:06:20.286909
Model ind 685 epoch 156 head B batch: 400 avg loss -2.167209 avg loss no lamb -2.167209 time 2019-02-23 09:07:32.898783
Model ind 685 epoch 156 head A batch: 0 avg loss -2.220391 avg loss no lamb -2.220391 time 2019-02-23 09:08:49.836036
Model ind 685 epoch 156 head A batch: 100 avg loss -2.204958 avg loss no lamb -2.204958 time 2019-02-23 09:10:06.979355
Model ind 685 epoch 156 head A batch: 200 avg loss -2.180309 avg loss no lamb -2.180309 time 2019-02-23 09:11:27.577291
Model ind 685 epoch 156 head A batch: 300 avg loss -2.181623 avg loss no lamb -2.181623 time 2019-02-23 09:12:49.411548
Model ind 685 epoch 156 head A batch: 400 avg loss -2.146330 avg loss no lamb -2.146330 time 2019-02-23 09:14:11.193059
Pre: time 2019-02-23 09:15:38.240673: 
 	std: 0.0064951577
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97905713, 0.9921857, 0.97905713, 0.97905713]
	train_accs: [0.99244285, 0.97905713, 0.9921857, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.9843599
	best: 0.99244285

Starting e_i: 157
Model ind 685 epoch 157 head B batch: 0 avg loss -2.191790 avg loss no lamb -2.191790 time 2019-02-23 09:15:40.023421
Model ind 685 epoch 157 head B batch: 100 avg loss -2.162463 avg loss no lamb -2.162463 time 2019-02-23 09:16:50.403115
Model ind 685 epoch 157 head B batch: 200 avg loss -2.237159 avg loss no lamb -2.237159 time 2019-02-23 09:18:08.465587
Model ind 685 epoch 157 head B batch: 300 avg loss -2.209987 avg loss no lamb -2.209987 time 2019-02-23 09:19:27.640782
Model ind 685 epoch 157 head B batch: 400 avg loss -2.228386 avg loss no lamb -2.228386 time 2019-02-23 09:20:47.239859
Model ind 685 epoch 157 head B batch: 0 avg loss -2.192801 avg loss no lamb -2.192801 time 2019-02-23 09:22:04.307777
Model ind 685 epoch 157 head B batch: 100 avg loss -2.215688 avg loss no lamb -2.215688 time 2019-02-23 09:23:25.232973
Model ind 685 epoch 157 head B batch: 200 avg loss -2.230633 avg loss no lamb -2.230633 time 2019-02-23 09:24:45.453968
Model ind 685 epoch 157 head B batch: 300 avg loss -2.238299 avg loss no lamb -2.238299 time 2019-02-23 09:26:05.848579
Model ind 685 epoch 157 head B batch: 400 avg loss -2.189721 avg loss no lamb -2.189721 time 2019-02-23 09:27:24.143314
Model ind 685 epoch 157 head A batch: 0 avg loss -2.210858 avg loss no lamb -2.210858 time 2019-02-23 09:28:38.648326
Model ind 685 epoch 157 head A batch: 100 avg loss -2.170329 avg loss no lamb -2.170329 time 2019-02-23 09:29:54.845617
Model ind 685 epoch 157 head A batch: 200 avg loss -2.209790 avg loss no lamb -2.209790 time 2019-02-23 09:31:12.475820
Model ind 685 epoch 157 head A batch: 300 avg loss -2.213722 avg loss no lamb -2.213722 time 2019-02-23 09:32:30.827163
Model ind 685 epoch 157 head A batch: 400 avg loss -2.177663 avg loss no lamb -2.177663 time 2019-02-23 09:33:46.299050
Pre: time 2019-02-23 09:35:14.727711: 
 	std: 0.006484734
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789571, 0.99205714, 0.9789571, 0.9789571]
	train_accs: [0.9923286, 0.9789571, 0.99205714, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.9842514
	best: 0.9923286

Starting e_i: 158
Model ind 685 epoch 158 head B batch: 0 avg loss -2.214715 avg loss no lamb -2.214715 time 2019-02-23 09:35:16.550833
Model ind 685 epoch 158 head B batch: 100 avg loss -2.253645 avg loss no lamb -2.253645 time 2019-02-23 09:36:34.497113
Model ind 685 epoch 158 head B batch: 200 avg loss -2.219917 avg loss no lamb -2.219917 time 2019-02-23 09:37:54.024884
Model ind 685 epoch 158 head B batch: 300 avg loss -2.188843 avg loss no lamb -2.188843 time 2019-02-23 09:39:10.538493
Model ind 685 epoch 158 head B batch: 400 avg loss -2.172008 avg loss no lamb -2.172008 time 2019-02-23 09:40:28.122350
Model ind 685 epoch 158 head B batch: 0 avg loss -2.223739 avg loss no lamb -2.223739 time 2019-02-23 09:41:48.957059
Model ind 685 epoch 158 head B batch: 100 avg loss -2.176203 avg loss no lamb -2.176203 time 2019-02-23 09:43:06.586235
Model ind 685 epoch 158 head B batch: 200 avg loss -2.228002 avg loss no lamb -2.228002 time 2019-02-23 09:44:23.081873
Model ind 685 epoch 158 head B batch: 300 avg loss -2.157732 avg loss no lamb -2.157732 time 2019-02-23 09:45:42.905183
Model ind 685 epoch 158 head B batch: 400 avg loss -2.176371 avg loss no lamb -2.176371 time 2019-02-23 09:47:01.398774
Model ind 685 epoch 158 head A batch: 0 avg loss -2.198520 avg loss no lamb -2.198520 time 2019-02-23 09:48:18.240890
Model ind 685 epoch 158 head A batch: 100 avg loss -2.155134 avg loss no lamb -2.155134 time 2019-02-23 09:49:36.881003
Model ind 685 epoch 158 head A batch: 200 avg loss -2.186834 avg loss no lamb -2.186834 time 2019-02-23 09:50:56.156752
Model ind 685 epoch 158 head A batch: 300 avg loss -2.200390 avg loss no lamb -2.200390 time 2019-02-23 09:52:16.912435
Model ind 685 epoch 158 head A batch: 400 avg loss -2.180451 avg loss no lamb -2.180451 time 2019-02-23 09:53:33.927456
Pre: time 2019-02-23 09:55:04.142105: 
 	std: 0.0064917123
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.9921, 0.9789857, 0.9789857]
	train_accs: [0.99237144, 0.9789857, 0.9921, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.99237144

Starting e_i: 159
Model ind 685 epoch 159 head B batch: 0 avg loss -2.198020 avg loss no lamb -2.198020 time 2019-02-23 09:55:05.911595
Model ind 685 epoch 159 head B batch: 100 avg loss -2.203416 avg loss no lamb -2.203416 time 2019-02-23 09:56:24.141059
Model ind 685 epoch 159 head B batch: 200 avg loss -2.204062 avg loss no lamb -2.204062 time 2019-02-23 09:57:34.528835
Model ind 685 epoch 159 head B batch: 300 avg loss -2.204437 avg loss no lamb -2.204437 time 2019-02-23 09:58:49.181619
Model ind 685 epoch 159 head B batch: 400 avg loss -2.132173 avg loss no lamb -2.132173 time 2019-02-23 10:00:05.677123
Model ind 685 epoch 159 head B batch: 0 avg loss -2.172380 avg loss no lamb -2.172380 time 2019-02-23 10:01:22.856369
Model ind 685 epoch 159 head B batch: 100 avg loss -2.175807 avg loss no lamb -2.175807 time 2019-02-23 10:02:43.116424
Model ind 685 epoch 159 head B batch: 200 avg loss -2.186825 avg loss no lamb -2.186825 time 2019-02-23 10:04:04.391254
Model ind 685 epoch 159 head B batch: 300 avg loss -2.219502 avg loss no lamb -2.219502 time 2019-02-23 10:05:21.245947
Model ind 685 epoch 159 head B batch: 400 avg loss -2.188742 avg loss no lamb -2.188742 time 2019-02-23 10:06:41.906807
Model ind 685 epoch 159 head A batch: 0 avg loss -2.184723 avg loss no lamb -2.184723 time 2019-02-23 10:08:00.521212
Model ind 685 epoch 159 head A batch: 100 avg loss -2.164621 avg loss no lamb -2.164621 time 2019-02-23 10:09:17.081262
Model ind 685 epoch 159 head A batch: 200 avg loss -2.165381 avg loss no lamb -2.165381 time 2019-02-23 10:10:34.421843
Model ind 685 epoch 159 head A batch: 300 avg loss -2.203201 avg loss no lamb -2.203201 time 2019-02-23 10:11:54.366520
Model ind 685 epoch 159 head A batch: 400 avg loss -2.179388 avg loss no lamb -2.179388 time 2019-02-23 10:13:12.411693
Pre: time 2019-02-23 10:14:47.457124: 
 	std: 0.006495154
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789857, 0.99212855, 0.979, 0.9790143]
	train_accs: [0.9923857, 0.9789857, 0.99212855, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 160
Model ind 685 epoch 160 head B batch: 0 avg loss -2.217626 avg loss no lamb -2.217626 time 2019-02-23 10:14:49.181776
Model ind 685 epoch 160 head B batch: 100 avg loss -2.205464 avg loss no lamb -2.205464 time 2019-02-23 10:16:06.698946
Model ind 685 epoch 160 head B batch: 200 avg loss -2.208664 avg loss no lamb -2.208664 time 2019-02-23 10:17:23.801466
Model ind 685 epoch 160 head B batch: 300 avg loss -2.179091 avg loss no lamb -2.179091 time 2019-02-23 10:18:42.128233
Model ind 685 epoch 160 head B batch: 400 avg loss -2.180043 avg loss no lamb -2.180043 time 2019-02-23 10:20:01.625032
Model ind 685 epoch 160 head B batch: 0 avg loss -2.200824 avg loss no lamb -2.200824 time 2019-02-23 10:21:19.007989
Model ind 685 epoch 160 head B batch: 100 avg loss -2.207144 avg loss no lamb -2.207144 time 2019-02-23 10:22:36.545619
Model ind 685 epoch 160 head B batch: 200 avg loss -2.185371 avg loss no lamb -2.185371 time 2019-02-23 10:23:51.044743
Model ind 685 epoch 160 head B batch: 300 avg loss -2.189410 avg loss no lamb -2.189410 time 2019-02-23 10:25:04.338363
Model ind 685 epoch 160 head B batch: 400 avg loss -2.136981 avg loss no lamb -2.136981 time 2019-02-23 10:26:23.669780
Model ind 685 epoch 160 head A batch: 0 avg loss -2.248457 avg loss no lamb -2.248457 time 2019-02-23 10:27:44.889965
Model ind 685 epoch 160 head A batch: 100 avg loss -2.232551 avg loss no lamb -2.232551 time 2019-02-23 10:29:02.924289
Model ind 685 epoch 160 head A batch: 200 avg loss -2.168247 avg loss no lamb -2.168247 time 2019-02-23 10:30:21.258181
Model ind 685 epoch 160 head A batch: 300 avg loss -2.202632 avg loss no lamb -2.202632 time 2019-02-23 10:31:41.129692
Model ind 685 epoch 160 head A batch: 400 avg loss -2.147415 avg loss no lamb -2.147415 time 2019-02-23 10:33:00.119295
Pre: time 2019-02-23 10:34:28.877143: 
 	std: 0.0065162843
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.979, 0.99215716, 0.979, 0.979]
	train_accs: [0.99244285, 0.979, 0.99215716, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98432
	best: 0.99244285

Starting e_i: 161
Model ind 685 epoch 161 head B batch: 0 avg loss -2.234171 avg loss no lamb -2.234171 time 2019-02-23 10:34:31.729912
Model ind 685 epoch 161 head B batch: 100 avg loss -2.195485 avg loss no lamb -2.195485 time 2019-02-23 10:35:49.942270
Model ind 685 epoch 161 head B batch: 200 avg loss -2.198196 avg loss no lamb -2.198196 time 2019-02-23 10:37:08.115950
Model ind 685 epoch 161 head B batch: 300 avg loss -2.248559 avg loss no lamb -2.248559 time 2019-02-23 10:38:23.320762
Model ind 685 epoch 161 head B batch: 400 avg loss -2.191588 avg loss no lamb -2.191588 time 2019-02-23 10:39:31.317869
Model ind 685 epoch 161 head B batch: 0 avg loss -2.183523 avg loss no lamb -2.183523 time 2019-02-23 10:40:44.564803
Model ind 685 epoch 161 head B batch: 100 avg loss -2.159269 avg loss no lamb -2.159269 time 2019-02-23 10:42:02.441188
Model ind 685 epoch 161 head B batch: 200 avg loss -2.170768 avg loss no lamb -2.170768 time 2019-02-23 10:43:22.200039
Model ind 685 epoch 161 head B batch: 300 avg loss -2.199568 avg loss no lamb -2.199568 time 2019-02-23 10:44:40.513221
Model ind 685 epoch 161 head B batch: 400 avg loss -2.188234 avg loss no lamb -2.188234 time 2019-02-23 10:45:58.034754
Model ind 685 epoch 161 head A batch: 0 avg loss -2.202852 avg loss no lamb -2.202852 time 2019-02-23 10:47:16.770350
Model ind 685 epoch 161 head A batch: 100 avg loss -2.245691 avg loss no lamb -2.245691 time 2019-02-23 10:48:36.050902
Model ind 685 epoch 161 head A batch: 200 avg loss -2.160714 avg loss no lamb -2.160714 time 2019-02-23 10:49:55.265422
Model ind 685 epoch 161 head A batch: 300 avg loss -2.200099 avg loss no lamb -2.200099 time 2019-02-23 10:51:11.572392
Model ind 685 epoch 161 head A batch: 400 avg loss -2.177650 avg loss no lamb -2.177650 time 2019-02-23 10:52:29.698907
Pre: time 2019-02-23 10:53:57.523252: 
 	std: 0.0064988453
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.9924286

Starting e_i: 162
Model ind 685 epoch 162 head B batch: 0 avg loss -2.229747 avg loss no lamb -2.229747 time 2019-02-23 10:53:59.311745
Model ind 685 epoch 162 head B batch: 100 avg loss -2.182462 avg loss no lamb -2.182462 time 2019-02-23 10:55:15.083430
Model ind 685 epoch 162 head B batch: 200 avg loss -2.160011 avg loss no lamb -2.160011 time 2019-02-23 10:56:32.506462
Model ind 685 epoch 162 head B batch: 300 avg loss -2.234425 avg loss no lamb -2.234425 time 2019-02-23 10:57:48.371658
Model ind 685 epoch 162 head B batch: 400 avg loss -2.177800 avg loss no lamb -2.177800 time 2019-02-23 10:59:07.410159
Model ind 685 epoch 162 head B batch: 0 avg loss -2.197118 avg loss no lamb -2.197118 time 2019-02-23 11:00:25.458461
Model ind 685 epoch 162 head B batch: 100 avg loss -2.191032 avg loss no lamb -2.191032 time 2019-02-23 11:01:43.100682
Model ind 685 epoch 162 head B batch: 200 avg loss -2.236343 avg loss no lamb -2.236343 time 2019-02-23 11:02:57.144314
Model ind 685 epoch 162 head B batch: 300 avg loss -2.239790 avg loss no lamb -2.239790 time 2019-02-23 11:04:15.045023
Model ind 685 epoch 162 head B batch: 400 avg loss -2.134567 avg loss no lamb -2.134567 time 2019-02-23 11:05:31.346797
Model ind 685 epoch 162 head A batch: 0 avg loss -2.228121 avg loss no lamb -2.228121 time 2019-02-23 11:06:46.880012
Model ind 685 epoch 162 head A batch: 100 avg loss -2.220118 avg loss no lamb -2.220118 time 2019-02-23 11:08:05.785082
Model ind 685 epoch 162 head A batch: 200 avg loss -2.197979 avg loss no lamb -2.197979 time 2019-02-23 11:09:21.404203
Model ind 685 epoch 162 head A batch: 300 avg loss -2.227253 avg loss no lamb -2.227253 time 2019-02-23 11:10:36.979876
Model ind 685 epoch 162 head A batch: 400 avg loss -2.191223 avg loss no lamb -2.191223 time 2019-02-23 11:11:53.014578
Pre: time 2019-02-23 11:13:25.382747: 
 	std: 0.0064906427
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790143, 0.9920857, 0.9789571, 0.9789714]
	train_accs: [0.99237144, 0.9790143, 0.9920857, 0.9789571, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.98428
	best: 0.99237144

Starting e_i: 163
Model ind 685 epoch 163 head B batch: 0 avg loss -2.229333 avg loss no lamb -2.229333 time 2019-02-23 11:13:27.414332
Model ind 685 epoch 163 head B batch: 100 avg loss -2.187624 avg loss no lamb -2.187624 time 2019-02-23 11:14:44.725659
Model ind 685 epoch 163 head B batch: 200 avg loss -2.222981 avg loss no lamb -2.222981 time 2019-02-23 11:16:02.972142
Model ind 685 epoch 163 head B batch: 300 avg loss -2.218392 avg loss no lamb -2.218392 time 2019-02-23 11:17:20.220007
Model ind 685 epoch 163 head B batch: 400 avg loss -2.135445 avg loss no lamb -2.135445 time 2019-02-23 11:18:37.706763
Model ind 685 epoch 163 head B batch: 0 avg loss -2.188670 avg loss no lamb -2.188670 time 2019-02-23 11:19:54.493953
Model ind 685 epoch 163 head B batch: 100 avg loss -2.192368 avg loss no lamb -2.192368 time 2019-02-23 11:21:04.850443
Model ind 685 epoch 163 head B batch: 200 avg loss -2.176096 avg loss no lamb -2.176096 time 2019-02-23 11:22:18.679010
Model ind 685 epoch 163 head B batch: 300 avg loss -2.158092 avg loss no lamb -2.158092 time 2019-02-23 11:23:35.048810
Model ind 685 epoch 163 head B batch: 400 avg loss -2.181652 avg loss no lamb -2.181652 time 2019-02-23 11:24:52.558637
Model ind 685 epoch 163 head A batch: 0 avg loss -2.192142 avg loss no lamb -2.192142 time 2019-02-23 11:26:11.458188
Model ind 685 epoch 163 head A batch: 100 avg loss -2.134477 avg loss no lamb -2.134477 time 2019-02-23 11:27:30.416224
Model ind 685 epoch 163 head A batch: 200 avg loss -2.193237 avg loss no lamb -2.193237 time 2019-02-23 11:28:49.288288
Model ind 685 epoch 163 head A batch: 300 avg loss -2.203236 avg loss no lamb -2.203236 time 2019-02-23 11:30:06.509492
Model ind 685 epoch 163 head A batch: 400 avg loss -2.186294 avg loss no lamb -2.186294 time 2019-02-23 11:31:24.470006
Pre: time 2019-02-23 11:32:54.311256: 
 	std: 0.006462628
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9790286, 0.99205714, 0.979, 0.979]
	train_accs: [0.9923428, 0.9790286, 0.99205714, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842857
	best: 0.9923428

Starting e_i: 164
Model ind 685 epoch 164 head B batch: 0 avg loss -2.192415 avg loss no lamb -2.192415 time 2019-02-23 11:32:56.117953
Model ind 685 epoch 164 head B batch: 100 avg loss -2.200652 avg loss no lamb -2.200652 time 2019-02-23 11:34:08.102754
Model ind 685 epoch 164 head B batch: 200 avg loss -2.200352 avg loss no lamb -2.200352 time 2019-02-23 11:35:27.755731
Model ind 685 epoch 164 head B batch: 300 avg loss -2.172357 avg loss no lamb -2.172357 time 2019-02-23 11:36:48.469154
Model ind 685 epoch 164 head B batch: 400 avg loss -2.180779 avg loss no lamb -2.180779 time 2019-02-23 11:38:06.642883
Model ind 685 epoch 164 head B batch: 0 avg loss -2.156757 avg loss no lamb -2.156757 time 2019-02-23 11:39:23.936601
Model ind 685 epoch 164 head B batch: 100 avg loss -2.140315 avg loss no lamb -2.140315 time 2019-02-23 11:40:39.567723
Model ind 685 epoch 164 head B batch: 200 avg loss -2.226054 avg loss no lamb -2.226054 time 2019-02-23 11:41:50.355016
Model ind 685 epoch 164 head B batch: 300 avg loss -2.219233 avg loss no lamb -2.219233 time 2019-02-23 11:43:08.116907
Model ind 685 epoch 164 head B batch: 400 avg loss -2.204545 avg loss no lamb -2.204545 time 2019-02-23 11:44:29.647290
Model ind 685 epoch 164 head A batch: 0 avg loss -2.193027 avg loss no lamb -2.193027 time 2019-02-23 11:45:42.897514
Model ind 685 epoch 164 head A batch: 100 avg loss -2.195616 avg loss no lamb -2.195616 time 2019-02-23 11:47:00.754272
Model ind 685 epoch 164 head A batch: 200 avg loss -2.218334 avg loss no lamb -2.218334 time 2019-02-23 11:48:21.456706
Model ind 685 epoch 164 head A batch: 300 avg loss -2.188931 avg loss no lamb -2.188931 time 2019-02-23 11:49:40.457954
Model ind 685 epoch 164 head A batch: 400 avg loss -2.188205 avg loss no lamb -2.188205 time 2019-02-23 11:51:00.625232
Pre: time 2019-02-23 11:52:39.194606: 
 	std: 0.006488284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.97891426, 0.9920143, 0.97891426, 0.97891426]
	train_accs: [0.9923, 0.97891426, 0.9920143, 0.97891426, 0.97891426]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.98421144
	best: 0.9923

Starting e_i: 165
Model ind 685 epoch 165 head B batch: 0 avg loss -2.186057 avg loss no lamb -2.186057 time 2019-02-23 11:52:40.961228
Model ind 685 epoch 165 head B batch: 100 avg loss -2.214001 avg loss no lamb -2.214001 time 2019-02-23 11:54:00.413041
Model ind 685 epoch 165 head B batch: 200 avg loss -2.214369 avg loss no lamb -2.214369 time 2019-02-23 11:55:14.403171
Model ind 685 epoch 165 head B batch: 300 avg loss -2.192173 avg loss no lamb -2.192173 time 2019-02-23 11:56:31.879338
Model ind 685 epoch 165 head B batch: 400 avg loss -2.187618 avg loss no lamb -2.187618 time 2019-02-23 11:57:50.687750
Model ind 685 epoch 165 head B batch: 0 avg loss -2.165604 avg loss no lamb -2.165604 time 2019-02-23 11:59:03.402030
Model ind 685 epoch 165 head B batch: 100 avg loss -2.213902 avg loss no lamb -2.213902 time 2019-02-23 12:00:17.974456
Model ind 685 epoch 165 head B batch: 200 avg loss -2.207517 avg loss no lamb -2.207517 time 2019-02-23 12:01:38.000706
Model ind 685 epoch 165 head B batch: 300 avg loss -2.184547 avg loss no lamb -2.184547 time 2019-02-23 12:02:53.462511
Model ind 685 epoch 165 head B batch: 400 avg loss -2.165999 avg loss no lamb -2.165999 time 2019-02-23 12:04:06.134681
Model ind 685 epoch 165 head A batch: 0 avg loss -2.201076 avg loss no lamb -2.201076 time 2019-02-23 12:05:25.577350
Model ind 685 epoch 165 head A batch: 100 avg loss -2.190371 avg loss no lamb -2.190371 time 2019-02-23 12:06:46.243203
Model ind 685 epoch 165 head A batch: 200 avg loss -2.210134 avg loss no lamb -2.210134 time 2019-02-23 12:08:05.056622
Model ind 685 epoch 165 head A batch: 300 avg loss -2.211198 avg loss no lamb -2.211198 time 2019-02-23 12:09:24.281424
Model ind 685 epoch 165 head A batch: 400 avg loss -2.213562 avg loss no lamb -2.213562 time 2019-02-23 12:10:45.858503
Pre: time 2019-02-23 12:12:23.665635: 
 	std: 0.006471965
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.979, 0.99207145, 0.9790143, 0.979]
	train_accs: [0.99235713, 0.979, 0.99207145, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842886
	best: 0.99235713

Starting e_i: 166
Model ind 685 epoch 166 head B batch: 0 avg loss -2.203811 avg loss no lamb -2.203811 time 2019-02-23 12:12:25.487162
Model ind 685 epoch 166 head B batch: 100 avg loss -2.192934 avg loss no lamb -2.192934 time 2019-02-23 12:13:44.767659
Model ind 685 epoch 166 head B batch: 200 avg loss -2.152618 avg loss no lamb -2.152618 time 2019-02-23 12:15:01.453305
Model ind 685 epoch 166 head B batch: 300 avg loss -2.212681 avg loss no lamb -2.212681 time 2019-02-23 12:16:14.512152
Model ind 685 epoch 166 head B batch: 400 avg loss -2.183412 avg loss no lamb -2.183412 time 2019-02-23 12:17:31.890419
Model ind 685 epoch 166 head B batch: 0 avg loss -2.215559 avg loss no lamb -2.215559 time 2019-02-23 12:18:51.288824
Model ind 685 epoch 166 head B batch: 100 avg loss -2.222698 avg loss no lamb -2.222698 time 2019-02-23 12:20:10.327930
Model ind 685 epoch 166 head B batch: 200 avg loss -2.193066 avg loss no lamb -2.193066 time 2019-02-23 12:21:28.548107
Model ind 685 epoch 166 head B batch: 300 avg loss -2.222710 avg loss no lamb -2.222710 time 2019-02-23 12:22:49.437898
Model ind 685 epoch 166 head B batch: 400 avg loss -2.197417 avg loss no lamb -2.197417 time 2019-02-23 12:24:04.849087
Model ind 685 epoch 166 head A batch: 0 avg loss -2.152982 avg loss no lamb -2.152982 time 2019-02-23 12:25:20.255923
Model ind 685 epoch 166 head A batch: 100 avg loss -2.232183 avg loss no lamb -2.232183 time 2019-02-23 12:26:36.449514
Model ind 685 epoch 166 head A batch: 200 avg loss -2.173836 avg loss no lamb -2.173836 time 2019-02-23 12:27:48.840893
Model ind 685 epoch 166 head A batch: 300 avg loss -2.242954 avg loss no lamb -2.242954 time 2019-02-23 12:29:09.219448
Model ind 685 epoch 166 head A batch: 400 avg loss -2.170890 avg loss no lamb -2.170890 time 2019-02-23 12:30:29.284932
Pre: time 2019-02-23 12:32:06.484719: 
 	std: 0.006497621
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.97891426, 0.99204284, 0.9789429, 0.97891426]
	train_accs: [0.9923286, 0.97891426, 0.99204284, 0.9789429, 0.97891426]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.9842285
	best: 0.9923286

Starting e_i: 167
Model ind 685 epoch 167 head B batch: 0 avg loss -2.207339 avg loss no lamb -2.207339 time 2019-02-23 12:32:08.518097
Model ind 685 epoch 167 head B batch: 100 avg loss -2.198991 avg loss no lamb -2.198991 time 2019-02-23 12:33:28.571833
Model ind 685 epoch 167 head B batch: 200 avg loss -2.233234 avg loss no lamb -2.233234 time 2019-02-23 12:34:45.999522
Model ind 685 epoch 167 head B batch: 300 avg loss -2.238230 avg loss no lamb -2.238230 time 2019-02-23 12:36:04.630073
Model ind 685 epoch 167 head B batch: 400 avg loss -2.196119 avg loss no lamb -2.196119 time 2019-02-23 12:37:24.338278
Model ind 685 epoch 167 head B batch: 0 avg loss -2.202172 avg loss no lamb -2.202172 time 2019-02-23 12:38:45.232417
Model ind 685 epoch 167 head B batch: 100 avg loss -2.183799 avg loss no lamb -2.183799 time 2019-02-23 12:40:05.383955
Model ind 685 epoch 167 head B batch: 200 avg loss -2.193312 avg loss no lamb -2.193312 time 2019-02-23 12:41:25.294071
Model ind 685 epoch 167 head B batch: 300 avg loss -2.183079 avg loss no lamb -2.183079 time 2019-02-23 12:42:42.835875
Model ind 685 epoch 167 head B batch: 400 avg loss -2.202968 avg loss no lamb -2.202968 time 2019-02-23 12:44:01.393027
Model ind 685 epoch 167 head A batch: 0 avg loss -2.186632 avg loss no lamb -2.186632 time 2019-02-23 12:45:11.672786
Model ind 685 epoch 167 head A batch: 100 avg loss -2.190321 avg loss no lamb -2.190321 time 2019-02-23 12:46:32.945701
Model ind 685 epoch 167 head A batch: 200 avg loss -2.209627 avg loss no lamb -2.209627 time 2019-02-23 12:47:52.653411
Model ind 685 epoch 167 head A batch: 300 avg loss -2.215194 avg loss no lamb -2.215194 time 2019-02-23 12:49:12.174070
Model ind 685 epoch 167 head A batch: 400 avg loss -2.166139 avg loss no lamb -2.166139 time 2019-02-23 12:50:30.367577
Pre: time 2019-02-23 12:52:05.968685: 
 	std: 0.006475387
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789857, 0.99207145, 0.9789857, 0.979]
	train_accs: [0.9923428, 0.9789857, 0.99207145, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842771
	best: 0.9923428

Starting e_i: 168
Model ind 685 epoch 168 head B batch: 0 avg loss -2.185079 avg loss no lamb -2.185079 time 2019-02-23 12:52:07.793699
Model ind 685 epoch 168 head B batch: 100 avg loss -2.210707 avg loss no lamb -2.210707 time 2019-02-23 12:53:24.705680
Model ind 685 epoch 168 head B batch: 200 avg loss -2.217651 avg loss no lamb -2.217651 time 2019-02-23 12:54:42.314647
Model ind 685 epoch 168 head B batch: 300 avg loss -2.149342 avg loss no lamb -2.149342 time 2019-02-23 12:55:59.923327
Model ind 685 epoch 168 head B batch: 400 avg loss -2.177430 avg loss no lamb -2.177430 time 2019-02-23 12:57:18.345785
Model ind 685 epoch 168 head B batch: 0 avg loss -2.190131 avg loss no lamb -2.190131 time 2019-02-23 12:58:36.784323
Model ind 685 epoch 168 head B batch: 100 avg loss -2.203064 avg loss no lamb -2.203064 time 2019-02-23 12:59:54.396192
Model ind 685 epoch 168 head B batch: 200 avg loss -2.179759 avg loss no lamb -2.179759 time 2019-02-23 13:01:10.769485
Model ind 685 epoch 168 head B batch: 300 avg loss -2.194558 avg loss no lamb -2.194558 time 2019-02-23 13:02:27.736760
Model ind 685 epoch 168 head B batch: 400 avg loss -2.166909 avg loss no lamb -2.166909 time 2019-02-23 13:03:43.858907
Model ind 685 epoch 168 head A batch: 0 avg loss -2.207339 avg loss no lamb -2.207339 time 2019-02-23 13:05:00.866179
Model ind 685 epoch 168 head A batch: 100 avg loss -2.199140 avg loss no lamb -2.199140 time 2019-02-23 13:06:20.128449
Model ind 685 epoch 168 head A batch: 200 avg loss -2.204176 avg loss no lamb -2.204176 time 2019-02-23 13:07:38.479080
Model ind 685 epoch 168 head A batch: 300 avg loss -2.184385 avg loss no lamb -2.184385 time 2019-02-23 13:08:56.222013
Model ind 685 epoch 168 head A batch: 400 avg loss -2.156720 avg loss no lamb -2.156720 time 2019-02-23 13:10:17.447857
Pre: time 2019-02-23 13:11:50.922540: 
 	std: 0.0064708595
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789714, 0.9920286, 0.9789714, 0.9789714]
	train_accs: [0.9923286, 0.9789714, 0.9920286, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98425424
	best: 0.9923286

Starting e_i: 169
Model ind 685 epoch 169 head B batch: 0 avg loss -2.179504 avg loss no lamb -2.179504 time 2019-02-23 13:11:52.719926
Model ind 685 epoch 169 head B batch: 100 avg loss -2.192438 avg loss no lamb -2.192438 time 2019-02-23 13:13:10.987638
Model ind 685 epoch 169 head B batch: 200 avg loss -2.181345 avg loss no lamb -2.181345 time 2019-02-23 13:14:27.091470
Model ind 685 epoch 169 head B batch: 300 avg loss -2.176158 avg loss no lamb -2.176158 time 2019-02-23 13:15:47.322667
Model ind 685 epoch 169 head B batch: 400 avg loss -2.170246 avg loss no lamb -2.170246 time 2019-02-23 13:17:06.327586
Model ind 685 epoch 169 head B batch: 0 avg loss -2.181524 avg loss no lamb -2.181524 time 2019-02-23 13:18:22.459718
Model ind 685 epoch 169 head B batch: 100 avg loss -2.184596 avg loss no lamb -2.184596 time 2019-02-23 13:19:40.412878
Model ind 685 epoch 169 head B batch: 200 avg loss -2.143814 avg loss no lamb -2.143814 time 2019-02-23 13:20:58.180245
Model ind 685 epoch 169 head B batch: 300 avg loss -2.201896 avg loss no lamb -2.201896 time 2019-02-23 13:22:16.198774
Model ind 685 epoch 169 head B batch: 400 avg loss -2.189636 avg loss no lamb -2.189636 time 2019-02-23 13:23:36.150988
Model ind 685 epoch 169 head A batch: 0 avg loss -2.212539 avg loss no lamb -2.212539 time 2019-02-23 13:24:54.939704
Model ind 685 epoch 169 head A batch: 100 avg loss -2.224025 avg loss no lamb -2.224025 time 2019-02-23 13:26:14.235746
Model ind 685 epoch 169 head A batch: 200 avg loss -2.214127 avg loss no lamb -2.214127 time 2019-02-23 13:27:27.598732
Model ind 685 epoch 169 head A batch: 300 avg loss -2.197584 avg loss no lamb -2.197584 time 2019-02-23 13:28:48.464835
Model ind 685 epoch 169 head A batch: 400 avg loss -2.163830 avg loss no lamb -2.163830 time 2019-02-23 13:30:09.623457
Pre: time 2019-02-23 13:31:40.356048: 
 	std: 0.0064623957
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.97907144, 0.99215716, 0.97907144, 0.9791]
	train_accs: [0.9923857, 0.97907144, 0.99215716, 0.97907144, 0.9791]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.98435706
	best: 0.9923857

Starting e_i: 170
Model ind 685 epoch 170 head B batch: 0 avg loss -2.223796 avg loss no lamb -2.223796 time 2019-02-23 13:31:42.120730
Model ind 685 epoch 170 head B batch: 100 avg loss -2.176811 avg loss no lamb -2.176811 time 2019-02-23 13:33:01.287560
Model ind 685 epoch 170 head B batch: 200 avg loss -2.186624 avg loss no lamb -2.186624 time 2019-02-23 13:34:21.706032
Model ind 685 epoch 170 head B batch: 300 avg loss -2.249367 avg loss no lamb -2.249367 time 2019-02-23 13:35:37.942766
Model ind 685 epoch 170 head B batch: 400 avg loss -2.223267 avg loss no lamb -2.223267 time 2019-02-23 13:36:55.462747
Model ind 685 epoch 170 head B batch: 0 avg loss -2.192719 avg loss no lamb -2.192719 time 2019-02-23 13:38:15.476126
Model ind 685 epoch 170 head B batch: 100 avg loss -2.165364 avg loss no lamb -2.165364 time 2019-02-23 13:39:32.907804
Model ind 685 epoch 170 head B batch: 200 avg loss -2.156460 avg loss no lamb -2.156460 time 2019-02-23 13:40:49.133761
Model ind 685 epoch 170 head B batch: 300 avg loss -2.208648 avg loss no lamb -2.208648 time 2019-02-23 13:42:05.828292
Model ind 685 epoch 170 head B batch: 400 avg loss -2.142868 avg loss no lamb -2.142868 time 2019-02-23 13:43:21.816444
Model ind 685 epoch 170 head A batch: 0 avg loss -2.218535 avg loss no lamb -2.218535 time 2019-02-23 13:44:39.597176
Model ind 685 epoch 170 head A batch: 100 avg loss -2.190188 avg loss no lamb -2.190188 time 2019-02-23 13:45:55.377976
Model ind 685 epoch 170 head A batch: 200 avg loss -2.223999 avg loss no lamb -2.223999 time 2019-02-23 13:47:13.525884
Model ind 685 epoch 170 head A batch: 300 avg loss -2.194716 avg loss no lamb -2.194716 time 2019-02-23 13:48:26.448208
Model ind 685 epoch 170 head A batch: 400 avg loss -2.184828 avg loss no lamb -2.184828 time 2019-02-23 13:49:45.142318
Pre: time 2019-02-23 13:51:18.079133: 
 	std: 0.0065044737
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.97885716, 0.992, 0.97884285, 0.97885716]
	train_accs: [0.9922571, 0.97885716, 0.992, 0.97884285, 0.97885716]
	best_train_sub_head: 0
	worst: 0.97884285
	avg: 0.9841628
	best: 0.9922571

Starting e_i: 171
Model ind 685 epoch 171 head B batch: 0 avg loss -2.239375 avg loss no lamb -2.239375 time 2019-02-23 13:51:19.844811
Model ind 685 epoch 171 head B batch: 100 avg loss -2.211541 avg loss no lamb -2.211541 time 2019-02-23 13:52:39.467506
Model ind 685 epoch 171 head B batch: 200 avg loss -2.154066 avg loss no lamb -2.154066 time 2019-02-23 13:53:57.193399
Model ind 685 epoch 171 head B batch: 300 avg loss -2.231725 avg loss no lamb -2.231725 time 2019-02-23 13:55:15.093426
Model ind 685 epoch 171 head B batch: 400 avg loss -2.158962 avg loss no lamb -2.158962 time 2019-02-23 13:56:26.508682
Model ind 685 epoch 171 head B batch: 0 avg loss -2.204743 avg loss no lamb -2.204743 time 2019-02-23 13:57:41.364514
Model ind 685 epoch 171 head B batch: 100 avg loss -2.220469 avg loss no lamb -2.220469 time 2019-02-23 13:58:59.110514
Model ind 685 epoch 171 head B batch: 200 avg loss -2.159948 avg loss no lamb -2.159948 time 2019-02-23 14:00:17.444339
Model ind 685 epoch 171 head B batch: 300 avg loss -2.232104 avg loss no lamb -2.232104 time 2019-02-23 14:01:35.367775
Model ind 685 epoch 171 head B batch: 400 avg loss -2.146037 avg loss no lamb -2.146037 time 2019-02-23 14:02:54.321005
Model ind 685 epoch 171 head A batch: 0 avg loss -2.213983 avg loss no lamb -2.213983 time 2019-02-23 14:04:12.357355
Model ind 685 epoch 171 head A batch: 100 avg loss -2.183674 avg loss no lamb -2.183674 time 2019-02-23 14:05:29.134096
Model ind 685 epoch 171 head A batch: 200 avg loss -2.208102 avg loss no lamb -2.208102 time 2019-02-23 14:06:46.995734
Model ind 685 epoch 171 head A batch: 300 avg loss -2.211086 avg loss no lamb -2.211086 time 2019-02-23 14:08:05.531439
Model ind 685 epoch 171 head A batch: 400 avg loss -2.176844 avg loss no lamb -2.176844 time 2019-02-23 14:09:10.436893
Pre: time 2019-02-23 14:10:41.520318: 
 	std: 0.00650227
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789714, 0.9921, 0.9789714, 0.9789714]
	train_accs: [0.9923857, 0.9789714, 0.9921, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98428
	best: 0.9923857

Starting e_i: 172
Model ind 685 epoch 172 head B batch: 0 avg loss -2.218478 avg loss no lamb -2.218478 time 2019-02-23 14:10:43.637471
Model ind 685 epoch 172 head B batch: 100 avg loss -2.179755 avg loss no lamb -2.179755 time 2019-02-23 14:12:02.297563
Model ind 685 epoch 172 head B batch: 200 avg loss -2.191538 avg loss no lamb -2.191538 time 2019-02-23 14:13:23.423993
Model ind 685 epoch 172 head B batch: 300 avg loss -2.216270 avg loss no lamb -2.216270 time 2019-02-23 14:14:42.713245
Model ind 685 epoch 172 head B batch: 400 avg loss -2.163086 avg loss no lamb -2.163086 time 2019-02-23 14:16:03.078395
Model ind 685 epoch 172 head B batch: 0 avg loss -2.152941 avg loss no lamb -2.152941 time 2019-02-23 14:17:23.864254
Model ind 685 epoch 172 head B batch: 100 avg loss -2.190555 avg loss no lamb -2.190555 time 2019-02-23 14:18:44.013903
Model ind 685 epoch 172 head B batch: 200 avg loss -2.169163 avg loss no lamb -2.169163 time 2019-02-23 14:20:04.913496
Model ind 685 epoch 172 head B batch: 300 avg loss -2.226886 avg loss no lamb -2.226886 time 2019-02-23 14:21:25.493461
Model ind 685 epoch 172 head B batch: 400 avg loss -2.184947 avg loss no lamb -2.184947 time 2019-02-23 14:22:44.280382
Model ind 685 epoch 172 head A batch: 0 avg loss -2.217689 avg loss no lamb -2.217689 time 2019-02-23 14:24:05.227254
Model ind 685 epoch 172 head A batch: 100 avg loss -2.215029 avg loss no lamb -2.215029 time 2019-02-23 14:25:24.118999
Model ind 685 epoch 172 head A batch: 200 avg loss -2.178941 avg loss no lamb -2.178941 time 2019-02-23 14:26:44.208293
Model ind 685 epoch 172 head A batch: 300 avg loss -2.231480 avg loss no lamb -2.231480 time 2019-02-23 14:28:04.433920
Model ind 685 epoch 172 head A batch: 400 avg loss -2.230453 avg loss no lamb -2.230453 time 2019-02-23 14:29:25.241010
Pre: time 2019-02-23 14:31:02.442984: 
 	std: 0.006515183
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.97884285, 0.99198574, 0.97884285, 0.97882855]
	train_accs: [0.9922857, 0.97884285, 0.99198574, 0.97884285, 0.97882855]
	best_train_sub_head: 0
	worst: 0.97882855
	avg: 0.9841571
	best: 0.9922857

Starting e_i: 173
Model ind 685 epoch 173 head B batch: 0 avg loss -2.170492 avg loss no lamb -2.170492 time 2019-02-23 14:31:04.672952
Model ind 685 epoch 173 head B batch: 100 avg loss -2.223236 avg loss no lamb -2.223236 time 2019-02-23 14:32:26.573767
Model ind 685 epoch 173 head B batch: 200 avg loss -2.157074 avg loss no lamb -2.157074 time 2019-02-23 14:33:46.920812
Model ind 685 epoch 173 head B batch: 300 avg loss -2.202054 avg loss no lamb -2.202054 time 2019-02-23 14:35:07.449595
Model ind 685 epoch 173 head B batch: 400 avg loss -2.172328 avg loss no lamb -2.172328 time 2019-02-23 14:36:28.366283
Model ind 685 epoch 173 head B batch: 0 avg loss -2.158372 avg loss no lamb -2.158372 time 2019-02-23 14:37:49.792348
Model ind 685 epoch 173 head B batch: 100 avg loss -2.208708 avg loss no lamb -2.208708 time 2019-02-23 14:39:09.391759
Model ind 685 epoch 173 head B batch: 200 avg loss -2.214422 avg loss no lamb -2.214422 time 2019-02-23 14:40:31.097582
Model ind 685 epoch 173 head B batch: 300 avg loss -2.190796 avg loss no lamb -2.190796 time 2019-02-23 14:41:50.374334
Model ind 685 epoch 173 head B batch: 400 avg loss -2.119366 avg loss no lamb -2.119366 time 2019-02-23 14:43:11.764178
Model ind 685 epoch 173 head A batch: 0 avg loss -2.206096 avg loss no lamb -2.206096 time 2019-02-23 14:44:33.251422
Model ind 685 epoch 173 head A batch: 100 avg loss -2.180285 avg loss no lamb -2.180285 time 2019-02-23 14:45:52.046118
Model ind 685 epoch 173 head A batch: 200 avg loss -2.215201 avg loss no lamb -2.215201 time 2019-02-23 14:47:13.030723
Model ind 685 epoch 173 head A batch: 300 avg loss -2.195397 avg loss no lamb -2.195397 time 2019-02-23 14:48:32.578452
Model ind 685 epoch 173 head A batch: 400 avg loss -2.175868 avg loss no lamb -2.175868 time 2019-02-23 14:49:51.626793
Pre: time 2019-02-23 14:51:10.412501: 
 	std: 0.006466329
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.97905713, 0.9920857, 0.9790428, 0.97905713]
	train_accs: [0.9924143, 0.97905713, 0.9920857, 0.9790428, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843314
	best: 0.9924143

Starting e_i: 174
Model ind 685 epoch 174 head B batch: 0 avg loss -2.205227 avg loss no lamb -2.205227 time 2019-02-23 14:51:12.148231
Model ind 685 epoch 174 head B batch: 100 avg loss -2.203594 avg loss no lamb -2.203594 time 2019-02-23 14:52:28.121206
Model ind 685 epoch 174 head B batch: 200 avg loss -2.236034 avg loss no lamb -2.236034 time 2019-02-23 14:53:40.719044
Model ind 685 epoch 174 head B batch: 300 avg loss -2.258278 avg loss no lamb -2.258278 time 2019-02-23 14:54:52.198210
Model ind 685 epoch 174 head B batch: 400 avg loss -2.194504 avg loss no lamb -2.194504 time 2019-02-23 14:56:03.435842
Model ind 685 epoch 174 head B batch: 0 avg loss -2.189488 avg loss no lamb -2.189488 time 2019-02-23 14:57:14.957589
Model ind 685 epoch 174 head B batch: 100 avg loss -2.144849 avg loss no lamb -2.144849 time 2019-02-23 14:58:28.802586
Model ind 685 epoch 174 head B batch: 200 avg loss -2.201994 avg loss no lamb -2.201994 time 2019-02-23 14:59:45.467628
Model ind 685 epoch 174 head B batch: 300 avg loss -2.198961 avg loss no lamb -2.198961 time 2019-02-23 15:01:06.045543
Model ind 685 epoch 174 head B batch: 400 avg loss -2.181699 avg loss no lamb -2.181699 time 2019-02-23 15:02:23.717035
Model ind 685 epoch 174 head A batch: 0 avg loss -2.236406 avg loss no lamb -2.236406 time 2019-02-23 15:03:41.014091
Model ind 685 epoch 174 head A batch: 100 avg loss -2.208842 avg loss no lamb -2.208842 time 2019-02-23 15:05:00.023897
Model ind 685 epoch 174 head A batch: 200 avg loss -2.151087 avg loss no lamb -2.151087 time 2019-02-23 15:06:17.171073
Model ind 685 epoch 174 head A batch: 300 avg loss -2.133264 avg loss no lamb -2.133264 time 2019-02-23 15:07:33.070941
Model ind 685 epoch 174 head A batch: 400 avg loss -2.152893 avg loss no lamb -2.152893 time 2019-02-23 15:08:50.361004
Pre: time 2019-02-23 15:10:23.896338: 
 	std: 0.0064921267
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789714, 0.99204284, 0.9789714, 0.9789714]
	train_accs: [0.9924, 0.9789714, 0.99204284, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842714
	best: 0.9924

Starting e_i: 175
Model ind 685 epoch 175 head B batch: 0 avg loss -2.240324 avg loss no lamb -2.240324 time 2019-02-23 15:10:25.954821
Model ind 685 epoch 175 head B batch: 100 avg loss -2.191108 avg loss no lamb -2.191108 time 2019-02-23 15:11:42.273419
Model ind 685 epoch 175 head B batch: 200 avg loss -2.200810 avg loss no lamb -2.200810 time 2019-02-23 15:12:57.844597
Model ind 685 epoch 175 head B batch: 300 avg loss -2.185002 avg loss no lamb -2.185002 time 2019-02-23 15:14:12.691486
Model ind 685 epoch 175 head B batch: 400 avg loss -2.195872 avg loss no lamb -2.195872 time 2019-02-23 15:15:27.466194
Model ind 685 epoch 175 head B batch: 0 avg loss -2.216471 avg loss no lamb -2.216471 time 2019-02-23 15:16:44.756391
Model ind 685 epoch 175 head B batch: 100 avg loss -2.201445 avg loss no lamb -2.201445 time 2019-02-23 15:18:01.348228
Model ind 685 epoch 175 head B batch: 200 avg loss -2.179800 avg loss no lamb -2.179800 time 2019-02-23 15:19:22.220154
Model ind 685 epoch 175 head B batch: 300 avg loss -2.178288 avg loss no lamb -2.178288 time 2019-02-23 15:20:43.074437
Model ind 685 epoch 175 head B batch: 400 avg loss -2.176908 avg loss no lamb -2.176908 time 2019-02-23 15:22:02.276429
Model ind 685 epoch 175 head A batch: 0 avg loss -2.211937 avg loss no lamb -2.211937 time 2019-02-23 15:23:22.683217
Model ind 685 epoch 175 head A batch: 100 avg loss -2.193413 avg loss no lamb -2.193413 time 2019-02-23 15:24:41.571973
Model ind 685 epoch 175 head A batch: 200 avg loss -2.173773 avg loss no lamb -2.173773 time 2019-02-23 15:26:00.718445
Model ind 685 epoch 175 head A batch: 300 avg loss -2.257429 avg loss no lamb -2.257429 time 2019-02-23 15:27:20.418579
Model ind 685 epoch 175 head A batch: 400 avg loss -2.191614 avg loss no lamb -2.191614 time 2019-02-23 15:28:39.140579
Pre: time 2019-02-23 15:30:15.839607: 
 	std: 0.0065126084
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.99215716, 0.9789714, 0.9789857]
	train_accs: [0.9924, 0.979, 0.99215716, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9843029
	best: 0.9924

Starting e_i: 176
Model ind 685 epoch 176 head B batch: 0 avg loss -2.202962 avg loss no lamb -2.202962 time 2019-02-23 15:30:17.753134
Model ind 685 epoch 176 head B batch: 100 avg loss -2.183088 avg loss no lamb -2.183088 time 2019-02-23 15:31:37.180248
Model ind 685 epoch 176 head B batch: 200 avg loss -2.190640 avg loss no lamb -2.190640 time 2019-02-23 15:32:54.440631
Model ind 685 epoch 176 head B batch: 300 avg loss -2.246920 avg loss no lamb -2.246920 time 2019-02-23 15:34:09.494859
Model ind 685 epoch 176 head B batch: 400 avg loss -2.176367 avg loss no lamb -2.176367 time 2019-02-23 15:35:27.427981
Model ind 685 epoch 176 head B batch: 0 avg loss -2.205771 avg loss no lamb -2.205771 time 2019-02-23 15:36:46.447947
Model ind 685 epoch 176 head B batch: 100 avg loss -2.170318 avg loss no lamb -2.170318 time 2019-02-23 15:38:04.738449
Model ind 685 epoch 176 head B batch: 200 avg loss -2.190737 avg loss no lamb -2.190737 time 2019-02-23 15:39:22.717218
Model ind 685 epoch 176 head B batch: 300 avg loss -2.170439 avg loss no lamb -2.170439 time 2019-02-23 15:40:42.395378
Model ind 685 epoch 176 head B batch: 400 avg loss -2.202674 avg loss no lamb -2.202674 time 2019-02-23 15:42:01.784870
Model ind 685 epoch 176 head A batch: 0 avg loss -2.199976 avg loss no lamb -2.199976 time 2019-02-23 15:43:19.482613
Model ind 685 epoch 176 head A batch: 100 avg loss -2.206106 avg loss no lamb -2.206106 time 2019-02-23 15:44:37.325619
Model ind 685 epoch 176 head A batch: 200 avg loss -2.205975 avg loss no lamb -2.205975 time 2019-02-23 15:45:57.210048
Model ind 685 epoch 176 head A batch: 300 avg loss -2.218595 avg loss no lamb -2.218595 time 2019-02-23 15:47:14.729418
Model ind 685 epoch 176 head A batch: 400 avg loss -2.164388 avg loss no lamb -2.164388 time 2019-02-23 15:48:31.484080
Pre: time 2019-02-23 15:50:05.120597: 
 	std: 0.006539749
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9788714, 0.99205714, 0.9788857, 0.97884285]
	train_accs: [0.99237144, 0.9788714, 0.99205714, 0.9788857, 0.97884285]
	best_train_sub_head: 0
	worst: 0.97884285
	avg: 0.9842057
	best: 0.99237144

Starting e_i: 177
Model ind 685 epoch 177 head B batch: 0 avg loss -2.187731 avg loss no lamb -2.187731 time 2019-02-23 15:50:07.205500
Model ind 685 epoch 177 head B batch: 100 avg loss -2.219701 avg loss no lamb -2.219701 time 2019-02-23 15:51:28.142355
Model ind 685 epoch 177 head B batch: 200 avg loss -2.203028 avg loss no lamb -2.203028 time 2019-02-23 15:52:50.547623
Model ind 685 epoch 177 head B batch: 300 avg loss -2.220387 avg loss no lamb -2.220387 time 2019-02-23 15:54:12.821432
Model ind 685 epoch 177 head B batch: 400 avg loss -2.165304 avg loss no lamb -2.165304 time 2019-02-23 15:55:33.857893
Model ind 685 epoch 177 head B batch: 0 avg loss -2.241396 avg loss no lamb -2.241396 time 2019-02-23 15:56:52.871178
Model ind 685 epoch 177 head B batch: 100 avg loss -2.165704 avg loss no lamb -2.165704 time 2019-02-23 15:58:12.370341
Model ind 685 epoch 177 head B batch: 200 avg loss -2.194165 avg loss no lamb -2.194165 time 2019-02-23 15:59:31.797510
Model ind 685 epoch 177 head B batch: 300 avg loss -2.213876 avg loss no lamb -2.213876 time 2019-02-23 16:00:51.784177
Model ind 685 epoch 177 head B batch: 400 avg loss -2.196472 avg loss no lamb -2.196472 time 2019-02-23 16:02:09.550124
Model ind 685 epoch 177 head A batch: 0 avg loss -2.241754 avg loss no lamb -2.241754 time 2019-02-23 16:03:28.849535
Model ind 685 epoch 177 head A batch: 100 avg loss -2.203124 avg loss no lamb -2.203124 time 2019-02-23 16:04:48.175640
Model ind 685 epoch 177 head A batch: 200 avg loss -2.184038 avg loss no lamb -2.184038 time 2019-02-23 16:06:05.318236
Model ind 685 epoch 177 head A batch: 300 avg loss -2.194039 avg loss no lamb -2.194039 time 2019-02-23 16:07:20.647212
Model ind 685 epoch 177 head A batch: 400 avg loss -2.155285 avg loss no lamb -2.155285 time 2019-02-23 16:08:38.129648
Pre: time 2019-02-23 16:10:11.585862: 
 	std: 0.0065046237
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789714, 0.9921143, 0.979, 0.9789714]
	train_accs: [0.9924, 0.9789714, 0.9921143, 0.979, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98429143
	best: 0.9924

Starting e_i: 178
Model ind 685 epoch 178 head B batch: 0 avg loss -2.220611 avg loss no lamb -2.220611 time 2019-02-23 16:10:14.390863
Model ind 685 epoch 178 head B batch: 100 avg loss -2.160077 avg loss no lamb -2.160077 time 2019-02-23 16:11:31.736434
Model ind 685 epoch 178 head B batch: 200 avg loss -2.178365 avg loss no lamb -2.178365 time 2019-02-23 16:12:50.369871
Model ind 685 epoch 178 head B batch: 300 avg loss -2.233165 avg loss no lamb -2.233165 time 2019-02-23 16:14:09.747844
Model ind 685 epoch 178 head B batch: 400 avg loss -2.203451 avg loss no lamb -2.203451 time 2019-02-23 16:15:19.760848
Model ind 685 epoch 178 head B batch: 0 avg loss -2.172783 avg loss no lamb -2.172783 time 2019-02-23 16:16:37.012945
Model ind 685 epoch 178 head B batch: 100 avg loss -2.170824 avg loss no lamb -2.170824 time 2019-02-23 16:17:53.956143
Model ind 685 epoch 178 head B batch: 200 avg loss -2.169676 avg loss no lamb -2.169676 time 2019-02-23 16:19:12.011870
Model ind 685 epoch 178 head B batch: 300 avg loss -2.190295 avg loss no lamb -2.190295 time 2019-02-23 16:20:29.436712
Model ind 685 epoch 178 head B batch: 400 avg loss -2.184282 avg loss no lamb -2.184282 time 2019-02-23 16:21:47.559903
Model ind 685 epoch 178 head A batch: 0 avg loss -2.178327 avg loss no lamb -2.178327 time 2019-02-23 16:23:03.409930
Model ind 685 epoch 178 head A batch: 100 avg loss -2.198880 avg loss no lamb -2.198880 time 2019-02-23 16:24:19.324474
Model ind 685 epoch 178 head A batch: 200 avg loss -2.236134 avg loss no lamb -2.236134 time 2019-02-23 16:25:38.497555
Model ind 685 epoch 178 head A batch: 300 avg loss -2.176503 avg loss no lamb -2.176503 time 2019-02-23 16:26:56.809522
Model ind 685 epoch 178 head A batch: 400 avg loss -2.178002 avg loss no lamb -2.178002 time 2019-02-23 16:28:15.547833
Pre: time 2019-02-23 16:29:49.146648: 
 	std: 0.006510492
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789429, 0.9920857, 0.9789571, 0.9789429]
	train_accs: [0.9923857, 0.9789429, 0.9920857, 0.9789571, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.9842628
	best: 0.9923857

Starting e_i: 179
Model ind 685 epoch 179 head B batch: 0 avg loss -2.178437 avg loss no lamb -2.178437 time 2019-02-23 16:29:51.005259
Model ind 685 epoch 179 head B batch: 100 avg loss -2.199842 avg loss no lamb -2.199842 time 2019-02-23 16:31:08.002314
Model ind 685 epoch 179 head B batch: 200 avg loss -2.184500 avg loss no lamb -2.184500 time 2019-02-23 16:32:26.377633
Model ind 685 epoch 179 head B batch: 300 avg loss -2.168346 avg loss no lamb -2.168346 time 2019-02-23 16:33:45.143659
Model ind 685 epoch 179 head B batch: 400 avg loss -2.189397 avg loss no lamb -2.189397 time 2019-02-23 16:35:04.667284
Model ind 685 epoch 179 head B batch: 0 avg loss -2.194613 avg loss no lamb -2.194613 time 2019-02-23 16:36:23.780921
Model ind 685 epoch 179 head B batch: 100 avg loss -2.179283 avg loss no lamb -2.179283 time 2019-02-23 16:37:42.554080
Model ind 685 epoch 179 head B batch: 200 avg loss -2.236357 avg loss no lamb -2.236357 time 2019-02-23 16:39:01.352915
Model ind 685 epoch 179 head B batch: 300 avg loss -2.146375 avg loss no lamb -2.146375 time 2019-02-23 16:40:18.675942
Model ind 685 epoch 179 head B batch: 400 avg loss -2.149689 avg loss no lamb -2.149689 time 2019-02-23 16:41:38.356887
Model ind 685 epoch 179 head A batch: 0 avg loss -2.199101 avg loss no lamb -2.199101 time 2019-02-23 16:42:57.397307
Model ind 685 epoch 179 head A batch: 100 avg loss -2.198667 avg loss no lamb -2.198667 time 2019-02-23 16:44:16.568671
Model ind 685 epoch 179 head A batch: 200 avg loss -2.165622 avg loss no lamb -2.165622 time 2019-02-23 16:45:36.435760
Model ind 685 epoch 179 head A batch: 300 avg loss -2.209212 avg loss no lamb -2.209212 time 2019-02-23 16:46:54.699114
Model ind 685 epoch 179 head A batch: 400 avg loss -2.164428 avg loss no lamb -2.164428 time 2019-02-23 16:48:15.806676
Pre: time 2019-02-23 16:49:53.648487: 
 	std: 0.006511615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.97891426, 0.99207145, 0.97892857, 0.97892857]
	train_accs: [0.99235713, 0.97891426, 0.99207145, 0.97892857, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.98423994
	best: 0.99235713

Starting e_i: 180
Model ind 685 epoch 180 head B batch: 0 avg loss -2.178661 avg loss no lamb -2.178661 time 2019-02-23 16:49:55.622270
Model ind 685 epoch 180 head B batch: 100 avg loss -2.181045 avg loss no lamb -2.181045 time 2019-02-23 16:51:13.973140
Model ind 685 epoch 180 head B batch: 200 avg loss -2.172667 avg loss no lamb -2.172667 time 2019-02-23 16:52:32.465119
Model ind 685 epoch 180 head B batch: 300 avg loss -2.241627 avg loss no lamb -2.241627 time 2019-02-23 16:53:52.406998
Model ind 685 epoch 180 head B batch: 400 avg loss -2.193390 avg loss no lamb -2.193390 time 2019-02-23 16:55:12.490208
Model ind 685 epoch 180 head B batch: 0 avg loss -2.209990 avg loss no lamb -2.209990 time 2019-02-23 16:56:32.717057
Model ind 685 epoch 180 head B batch: 100 avg loss -2.156255 avg loss no lamb -2.156255 time 2019-02-23 16:57:43.660862
Model ind 685 epoch 180 head B batch: 200 avg loss -2.204482 avg loss no lamb -2.204482 time 2019-02-23 16:58:59.828857
Model ind 685 epoch 180 head B batch: 300 avg loss -2.217386 avg loss no lamb -2.217386 time 2019-02-23 17:00:16.412168
Model ind 685 epoch 180 head B batch: 400 avg loss -2.167309 avg loss no lamb -2.167309 time 2019-02-23 17:01:32.813858
Model ind 685 epoch 180 head A batch: 0 avg loss -2.171413 avg loss no lamb -2.171413 time 2019-02-23 17:02:52.714936
Model ind 685 epoch 180 head A batch: 100 avg loss -2.195566 avg loss no lamb -2.195566 time 2019-02-23 17:04:11.235008
Model ind 685 epoch 180 head A batch: 200 avg loss -2.176616 avg loss no lamb -2.176616 time 2019-02-23 17:05:28.693588
Model ind 685 epoch 180 head A batch: 300 avg loss -2.225310 avg loss no lamb -2.225310 time 2019-02-23 17:06:44.828156
Model ind 685 epoch 180 head A batch: 400 avg loss -2.216724 avg loss no lamb -2.216724 time 2019-02-23 17:07:59.724564
Pre: time 2019-02-23 17:09:32.364049: 
 	std: 0.0065002074
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789429, 0.99204284, 0.9789571, 0.9789429]
	train_accs: [0.9923857, 0.9789429, 0.99204284, 0.9789571, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98425424
	best: 0.9923857

Starting e_i: 181
Model ind 685 epoch 181 head B batch: 0 avg loss -2.192432 avg loss no lamb -2.192432 time 2019-02-23 17:09:35.675092
Model ind 685 epoch 181 head B batch: 100 avg loss -2.230471 avg loss no lamb -2.230471 time 2019-02-23 17:10:51.263931
Model ind 685 epoch 181 head B batch: 200 avg loss -2.210621 avg loss no lamb -2.210621 time 2019-02-23 17:12:04.801823
Model ind 685 epoch 181 head B batch: 300 avg loss -2.251103 avg loss no lamb -2.251103 time 2019-02-23 17:13:20.860927
Model ind 685 epoch 181 head B batch: 400 avg loss -2.173243 avg loss no lamb -2.173243 time 2019-02-23 17:14:37.951120
Model ind 685 epoch 181 head B batch: 0 avg loss -2.200498 avg loss no lamb -2.200498 time 2019-02-23 17:15:55.934536
Model ind 685 epoch 181 head B batch: 100 avg loss -2.200344 avg loss no lamb -2.200344 time 2019-02-23 17:17:13.305306
Model ind 685 epoch 181 head B batch: 200 avg loss -2.242172 avg loss no lamb -2.242172 time 2019-02-23 17:18:31.642129
Model ind 685 epoch 181 head B batch: 300 avg loss -2.142212 avg loss no lamb -2.142212 time 2019-02-23 17:19:48.690087
Model ind 685 epoch 181 head B batch: 400 avg loss -2.201554 avg loss no lamb -2.201554 time 2019-02-23 17:21:04.462434
Model ind 685 epoch 181 head A batch: 0 avg loss -2.160972 avg loss no lamb -2.160972 time 2019-02-23 17:22:19.669737
Model ind 685 epoch 181 head A batch: 100 avg loss -2.161582 avg loss no lamb -2.161582 time 2019-02-23 17:23:36.208622
Model ind 685 epoch 181 head A batch: 200 avg loss -2.196328 avg loss no lamb -2.196328 time 2019-02-23 17:24:53.244903
Model ind 685 epoch 181 head A batch: 300 avg loss -2.188560 avg loss no lamb -2.188560 time 2019-02-23 17:26:11.080546
Model ind 685 epoch 181 head A batch: 400 avg loss -2.132431 avg loss no lamb -2.132431 time 2019-02-23 17:27:27.770997
Pre: time 2019-02-23 17:28:58.875235: 
 	std: 0.006511605
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789571, 0.9921, 0.9789429, 0.9789571]
	train_accs: [0.9923857, 0.9789571, 0.9921, 0.9789429, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98426855
	best: 0.9923857

Starting e_i: 182
Model ind 685 epoch 182 head B batch: 0 avg loss -2.235182 avg loss no lamb -2.235182 time 2019-02-23 17:29:00.606348
Model ind 685 epoch 182 head B batch: 100 avg loss -2.183782 avg loss no lamb -2.183782 time 2019-02-23 17:30:13.987987
Model ind 685 epoch 182 head B batch: 200 avg loss -2.184246 avg loss no lamb -2.184246 time 2019-02-23 17:31:31.480832
Model ind 685 epoch 182 head B batch: 300 avg loss -2.208836 avg loss no lamb -2.208836 time 2019-02-23 17:32:47.691229
Model ind 685 epoch 182 head B batch: 400 avg loss -2.185024 avg loss no lamb -2.185024 time 2019-02-23 17:34:07.784738
Model ind 685 epoch 182 head B batch: 0 avg loss -2.217436 avg loss no lamb -2.217436 time 2019-02-23 17:35:23.397511
Model ind 685 epoch 182 head B batch: 100 avg loss -2.149156 avg loss no lamb -2.149156 time 2019-02-23 17:36:40.818396
Model ind 685 epoch 182 head B batch: 200 avg loss -2.204225 avg loss no lamb -2.204225 time 2019-02-23 17:37:59.093005
Model ind 685 epoch 182 head B batch: 300 avg loss -2.255977 avg loss no lamb -2.255977 time 2019-02-23 17:39:16.628083
Model ind 685 epoch 182 head B batch: 400 avg loss -2.154498 avg loss no lamb -2.154498 time 2019-02-23 17:40:24.155691
Model ind 685 epoch 182 head A batch: 0 avg loss -2.205872 avg loss no lamb -2.205872 time 2019-02-23 17:41:41.248907
Model ind 685 epoch 182 head A batch: 100 avg loss -2.186558 avg loss no lamb -2.186558 time 2019-02-23 17:42:57.749617
Model ind 685 epoch 182 head A batch: 200 avg loss -2.212237 avg loss no lamb -2.212237 time 2019-02-23 17:44:12.826295
Model ind 685 epoch 182 head A batch: 300 avg loss -2.245076 avg loss no lamb -2.245076 time 2019-02-23 17:45:28.839165
Model ind 685 epoch 182 head A batch: 400 avg loss -2.214675 avg loss no lamb -2.214675 time 2019-02-23 17:46:47.820884
Pre: time 2019-02-23 17:48:18.598928: 
 	std: 0.00652538
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789429, 0.99215716, 0.9789571, 0.9789571]
	train_accs: [0.9923857, 0.9789429, 0.99215716, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98428
	best: 0.9923857

Starting e_i: 183
Model ind 685 epoch 183 head B batch: 0 avg loss -2.208450 avg loss no lamb -2.208450 time 2019-02-23 17:48:20.369145
Model ind 685 epoch 183 head B batch: 100 avg loss -2.211135 avg loss no lamb -2.211135 time 2019-02-23 17:49:35.982953
Model ind 685 epoch 183 head B batch: 200 avg loss -2.198913 avg loss no lamb -2.198913 time 2019-02-23 17:50:51.922813
Model ind 685 epoch 183 head B batch: 300 avg loss -2.185802 avg loss no lamb -2.185802 time 2019-02-23 17:52:08.886306
Model ind 685 epoch 183 head B batch: 400 avg loss -2.175583 avg loss no lamb -2.175583 time 2019-02-23 17:53:26.115206
Model ind 685 epoch 183 head B batch: 0 avg loss -2.209884 avg loss no lamb -2.209884 time 2019-02-23 17:54:44.143012
Model ind 685 epoch 183 head B batch: 100 avg loss -2.212218 avg loss no lamb -2.212218 time 2019-02-23 17:56:01.479938
Model ind 685 epoch 183 head B batch: 200 avg loss -2.204974 avg loss no lamb -2.204974 time 2019-02-23 17:57:19.739662
Model ind 685 epoch 183 head B batch: 300 avg loss -2.237826 avg loss no lamb -2.237826 time 2019-02-23 17:58:36.874630
Model ind 685 epoch 183 head B batch: 400 avg loss -2.196394 avg loss no lamb -2.196394 time 2019-02-23 17:59:52.063683
Model ind 685 epoch 183 head A batch: 0 avg loss -2.202548 avg loss no lamb -2.202548 time 2019-02-23 18:01:10.129244
Model ind 685 epoch 183 head A batch: 100 avg loss -2.232394 avg loss no lamb -2.232394 time 2019-02-23 18:02:27.228598
Model ind 685 epoch 183 head A batch: 200 avg loss -2.138297 avg loss no lamb -2.138297 time 2019-02-23 18:03:44.463821
Model ind 685 epoch 183 head A batch: 300 avg loss -2.204628 avg loss no lamb -2.204628 time 2019-02-23 18:05:02.683689
Model ind 685 epoch 183 head A batch: 400 avg loss -2.150273 avg loss no lamb -2.150273 time 2019-02-23 18:06:20.119591
Pre: time 2019-02-23 18:07:55.318927: 
 	std: 0.0065188883
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9789571, 0.9920857, 0.9789571, 0.9789429]
	train_accs: [0.9924286, 0.9789571, 0.9920857, 0.9789571, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98427427
	best: 0.9924286

Starting e_i: 184
Model ind 685 epoch 184 head B batch: 0 avg loss -2.196389 avg loss no lamb -2.196389 time 2019-02-23 18:07:57.174695
Model ind 685 epoch 184 head B batch: 100 avg loss -2.176934 avg loss no lamb -2.176934 time 2019-02-23 18:09:13.584726
Model ind 685 epoch 184 head B batch: 200 avg loss -2.153013 avg loss no lamb -2.153013 time 2019-02-23 18:10:31.964825
Model ind 685 epoch 184 head B batch: 300 avg loss -2.193085 avg loss no lamb -2.193085 time 2019-02-23 18:11:47.447585
Model ind 685 epoch 184 head B batch: 400 avg loss -2.189614 avg loss no lamb -2.189614 time 2019-02-23 18:13:06.089462
Model ind 685 epoch 184 head B batch: 0 avg loss -2.217122 avg loss no lamb -2.217122 time 2019-02-23 18:14:24.970646
Model ind 685 epoch 184 head B batch: 100 avg loss -2.183275 avg loss no lamb -2.183275 time 2019-02-23 18:15:42.093921
Model ind 685 epoch 184 head B batch: 200 avg loss -2.182259 avg loss no lamb -2.182259 time 2019-02-23 18:16:59.889648
Model ind 685 epoch 184 head B batch: 300 avg loss -2.234429 avg loss no lamb -2.234429 time 2019-02-23 18:18:16.138266
Model ind 685 epoch 184 head B batch: 400 avg loss -2.202762 avg loss no lamb -2.202762 time 2019-02-23 18:19:33.689802
Model ind 685 epoch 184 head A batch: 0 avg loss -2.206429 avg loss no lamb -2.206429 time 2019-02-23 18:20:51.677116
Model ind 685 epoch 184 head A batch: 100 avg loss -2.192945 avg loss no lamb -2.192945 time 2019-02-23 18:22:09.819597
Model ind 685 epoch 184 head A batch: 200 avg loss -2.212427 avg loss no lamb -2.212427 time 2019-02-23 18:23:19.674407
Model ind 685 epoch 184 head A batch: 300 avg loss -2.183311 avg loss no lamb -2.183311 time 2019-02-23 18:24:36.900613
Model ind 685 epoch 184 head A batch: 400 avg loss -2.192200 avg loss no lamb -2.192200 time 2019-02-23 18:25:53.393507
Pre: time 2019-02-23 18:27:26.564969: 
 	std: 0.006501168
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9921, 0.9789714, 0.9789857]
	train_accs: [0.9924, 0.9789857, 0.9921, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842886
	best: 0.9924

Starting e_i: 185
Model ind 685 epoch 185 head B batch: 0 avg loss -2.235309 avg loss no lamb -2.235309 time 2019-02-23 18:27:28.386161
Model ind 685 epoch 185 head B batch: 100 avg loss -2.203338 avg loss no lamb -2.203338 time 2019-02-23 18:28:45.093984
Model ind 685 epoch 185 head B batch: 200 avg loss -2.196000 avg loss no lamb -2.196000 time 2019-02-23 18:30:01.906995
Model ind 685 epoch 185 head B batch: 300 avg loss -2.194964 avg loss no lamb -2.194964 time 2019-02-23 18:31:21.164829
Model ind 685 epoch 185 head B batch: 400 avg loss -2.155604 avg loss no lamb -2.155604 time 2019-02-23 18:32:40.285261
Model ind 685 epoch 185 head B batch: 0 avg loss -2.218552 avg loss no lamb -2.218552 time 2019-02-23 18:33:57.507080
Model ind 685 epoch 185 head B batch: 100 avg loss -2.260788 avg loss no lamb -2.260788 time 2019-02-23 18:35:10.280253
Model ind 685 epoch 185 head B batch: 200 avg loss -2.190414 avg loss no lamb -2.190414 time 2019-02-23 18:36:29.438699
Model ind 685 epoch 185 head B batch: 300 avg loss -2.196446 avg loss no lamb -2.196446 time 2019-02-23 18:37:48.136444
Model ind 685 epoch 185 head B batch: 400 avg loss -2.171560 avg loss no lamb -2.171560 time 2019-02-23 18:39:04.205581
Model ind 685 epoch 185 head A batch: 0 avg loss -2.198530 avg loss no lamb -2.198530 time 2019-02-23 18:40:22.443005
Model ind 685 epoch 185 head A batch: 100 avg loss -2.183182 avg loss no lamb -2.183182 time 2019-02-23 18:41:40.830787
Model ind 685 epoch 185 head A batch: 200 avg loss -2.259561 avg loss no lamb -2.259561 time 2019-02-23 18:43:02.112285
Model ind 685 epoch 185 head A batch: 300 avg loss -2.194941 avg loss no lamb -2.194941 time 2019-02-23 18:44:24.941608
Model ind 685 epoch 185 head A batch: 400 avg loss -2.189196 avg loss no lamb -2.189196 time 2019-02-23 18:45:45.561956
Pre: time 2019-02-23 18:47:23.908508: 
 	std: 0.006507923
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789857, 0.99214286, 0.9789714, 0.9789857]
	train_accs: [0.9923857, 0.9789857, 0.99214286, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 186
Model ind 685 epoch 186 head B batch: 0 avg loss -2.210777 avg loss no lamb -2.210777 time 2019-02-23 18:47:25.937956
Model ind 685 epoch 186 head B batch: 100 avg loss -2.184041 avg loss no lamb -2.184041 time 2019-02-23 18:48:47.919416
Model ind 685 epoch 186 head B batch: 200 avg loss -2.223447 avg loss no lamb -2.223447 time 2019-02-23 18:50:10.151054
Model ind 685 epoch 186 head B batch: 300 avg loss -2.188108 avg loss no lamb -2.188108 time 2019-02-23 18:51:31.838020
Model ind 685 epoch 186 head B batch: 400 avg loss -2.179334 avg loss no lamb -2.179334 time 2019-02-23 18:52:52.680930
Model ind 685 epoch 186 head B batch: 0 avg loss -2.201871 avg loss no lamb -2.201871 time 2019-02-23 18:54:12.989554
Model ind 685 epoch 186 head B batch: 100 avg loss -2.168550 avg loss no lamb -2.168550 time 2019-02-23 18:55:33.029046
Model ind 685 epoch 186 head B batch: 200 avg loss -2.208172 avg loss no lamb -2.208172 time 2019-02-23 18:56:52.121208
Model ind 685 epoch 186 head B batch: 300 avg loss -2.192688 avg loss no lamb -2.192688 time 2019-02-23 18:58:10.279340
Model ind 685 epoch 186 head B batch: 400 avg loss -2.190805 avg loss no lamb -2.190805 time 2019-02-23 18:59:26.371961
Model ind 685 epoch 186 head A batch: 0 avg loss -2.173911 avg loss no lamb -2.173911 time 2019-02-23 19:00:45.470472
Model ind 685 epoch 186 head A batch: 100 avg loss -2.202576 avg loss no lamb -2.202576 time 2019-02-23 19:02:03.081878
Model ind 685 epoch 186 head A batch: 200 avg loss -2.181378 avg loss no lamb -2.181378 time 2019-02-23 19:03:21.060055
Model ind 685 epoch 186 head A batch: 300 avg loss -2.243688 avg loss no lamb -2.243688 time 2019-02-23 19:04:33.764028
Model ind 685 epoch 186 head A batch: 400 avg loss -2.207935 avg loss no lamb -2.207935 time 2019-02-23 19:05:48.838058
Pre: time 2019-02-23 19:07:24.592397: 
 	std: 0.0064998316
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789571, 0.9921, 0.9789571, 0.9789714]
	train_accs: [0.99235713, 0.9789571, 0.9921, 0.9789571, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.98426855
	best: 0.99235713

Starting e_i: 187
Model ind 685 epoch 187 head B batch: 0 avg loss -2.215262 avg loss no lamb -2.215262 time 2019-02-23 19:07:26.443969
Model ind 685 epoch 187 head B batch: 100 avg loss -2.218094 avg loss no lamb -2.218094 time 2019-02-23 19:08:43.010587
Model ind 685 epoch 187 head B batch: 200 avg loss -2.209586 avg loss no lamb -2.209586 time 2019-02-23 19:10:01.386684
Model ind 685 epoch 187 head B batch: 300 avg loss -2.215240 avg loss no lamb -2.215240 time 2019-02-23 19:11:20.507697
Model ind 685 epoch 187 head B batch: 400 avg loss -2.209720 avg loss no lamb -2.209720 time 2019-02-23 19:12:38.140826
Model ind 685 epoch 187 head B batch: 0 avg loss -2.216767 avg loss no lamb -2.216767 time 2019-02-23 19:13:58.799003
Model ind 685 epoch 187 head B batch: 100 avg loss -2.223899 avg loss no lamb -2.223899 time 2019-02-23 19:15:19.743242
Model ind 685 epoch 187 head B batch: 200 avg loss -2.172400 avg loss no lamb -2.172400 time 2019-02-23 19:16:34.799139
Model ind 685 epoch 187 head B batch: 300 avg loss -2.181567 avg loss no lamb -2.181567 time 2019-02-23 19:17:51.906252
Model ind 685 epoch 187 head B batch: 400 avg loss -2.174916 avg loss no lamb -2.174916 time 2019-02-23 19:19:09.602433
Model ind 685 epoch 187 head A batch: 0 avg loss -2.223413 avg loss no lamb -2.223413 time 2019-02-23 19:20:26.492247
Model ind 685 epoch 187 head A batch: 100 avg loss -2.177327 avg loss no lamb -2.177327 time 2019-02-23 19:21:40.299760
Model ind 685 epoch 187 head A batch: 200 avg loss -2.189332 avg loss no lamb -2.189332 time 2019-02-23 19:22:58.642447
Model ind 685 epoch 187 head A batch: 300 avg loss -2.214498 avg loss no lamb -2.214498 time 2019-02-23 19:24:15.250288
Model ind 685 epoch 187 head A batch: 400 avg loss -2.171523 avg loss no lamb -2.171523 time 2019-02-23 19:25:32.560259
Pre: time 2019-02-23 19:27:07.468360: 
 	std: 0.0065139364
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9789714, 0.99212855, 0.9789714, 0.9789857]
	train_accs: [0.9924143, 0.9789714, 0.99212855, 0.9789714, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842943
	best: 0.9924143

Starting e_i: 188
Model ind 685 epoch 188 head B batch: 0 avg loss -2.177924 avg loss no lamb -2.177924 time 2019-02-23 19:27:09.508443
Model ind 685 epoch 188 head B batch: 100 avg loss -2.182418 avg loss no lamb -2.182418 time 2019-02-23 19:28:27.901153
Model ind 685 epoch 188 head B batch: 200 avg loss -2.191635 avg loss no lamb -2.191635 time 2019-02-23 19:29:48.489310
Model ind 685 epoch 188 head B batch: 300 avg loss -2.191795 avg loss no lamb -2.191795 time 2019-02-23 19:31:08.238166
Model ind 685 epoch 188 head B batch: 400 avg loss -2.183111 avg loss no lamb -2.183111 time 2019-02-23 19:32:26.806304
Model ind 685 epoch 188 head B batch: 0 avg loss -2.221984 avg loss no lamb -2.221984 time 2019-02-23 19:33:45.823055
Model ind 685 epoch 188 head B batch: 100 avg loss -2.117635 avg loss no lamb -2.117635 time 2019-02-23 19:35:04.307235
Model ind 685 epoch 188 head B batch: 200 avg loss -2.223907 avg loss no lamb -2.223907 time 2019-02-23 19:36:23.829826
Model ind 685 epoch 188 head B batch: 300 avg loss -2.218822 avg loss no lamb -2.218822 time 2019-02-23 19:37:45.472322
Model ind 685 epoch 188 head B batch: 400 avg loss -2.163902 avg loss no lamb -2.163902 time 2019-02-23 19:39:05.348762
Model ind 685 epoch 188 head A batch: 0 avg loss -2.177740 avg loss no lamb -2.177740 time 2019-02-23 19:40:26.184199
Model ind 685 epoch 188 head A batch: 100 avg loss -2.221069 avg loss no lamb -2.221069 time 2019-02-23 19:41:43.720103
Model ind 685 epoch 188 head A batch: 200 avg loss -2.210455 avg loss no lamb -2.210455 time 2019-02-23 19:43:03.796541
Model ind 685 epoch 188 head A batch: 300 avg loss -2.191069 avg loss no lamb -2.191069 time 2019-02-23 19:44:19.961590
Model ind 685 epoch 188 head A batch: 400 avg loss -2.213227 avg loss no lamb -2.213227 time 2019-02-23 19:45:37.377125
Pre: time 2019-02-23 19:47:06.355696: 
 	std: 0.0065044896
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789714, 0.99212855, 0.979, 0.9789714]
	train_accs: [0.9923857, 0.9789714, 0.99212855, 0.979, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98429143
	best: 0.9923857

Starting e_i: 189
Model ind 685 epoch 189 head B batch: 0 avg loss -2.201455 avg loss no lamb -2.201455 time 2019-02-23 19:47:08.246704
Model ind 685 epoch 189 head B batch: 100 avg loss -2.181019 avg loss no lamb -2.181019 time 2019-02-23 19:48:26.608649
Model ind 685 epoch 189 head B batch: 200 avg loss -2.200743 avg loss no lamb -2.200743 time 2019-02-23 19:49:45.878858
Model ind 685 epoch 189 head B batch: 300 avg loss -2.226803 avg loss no lamb -2.226803 time 2019-02-23 19:51:04.531014
Model ind 685 epoch 189 head B batch: 400 avg loss -2.191652 avg loss no lamb -2.191652 time 2019-02-23 19:52:23.788310
Model ind 685 epoch 189 head B batch: 0 avg loss -2.209419 avg loss no lamb -2.209419 time 2019-02-23 19:53:41.516674
Model ind 685 epoch 189 head B batch: 100 avg loss -2.167796 avg loss no lamb -2.167796 time 2019-02-23 19:55:01.141579
Model ind 685 epoch 189 head B batch: 200 avg loss -2.185937 avg loss no lamb -2.185937 time 2019-02-23 19:56:19.943495
Model ind 685 epoch 189 head B batch: 300 avg loss -2.203496 avg loss no lamb -2.203496 time 2019-02-23 19:57:39.329615
Model ind 685 epoch 189 head B batch: 400 avg loss -2.164101 avg loss no lamb -2.164101 time 2019-02-23 19:58:59.171627
Model ind 685 epoch 189 head A batch: 0 avg loss -2.188090 avg loss no lamb -2.188090 time 2019-02-23 20:00:19.068882
Model ind 685 epoch 189 head A batch: 100 avg loss -2.188087 avg loss no lamb -2.188087 time 2019-02-23 20:01:38.803607
Model ind 685 epoch 189 head A batch: 200 avg loss -2.237701 avg loss no lamb -2.237701 time 2019-02-23 20:02:57.201804
Model ind 685 epoch 189 head A batch: 300 avg loss -2.214632 avg loss no lamb -2.214632 time 2019-02-23 20:04:16.203875
Model ind 685 epoch 189 head A batch: 400 avg loss -2.207864 avg loss no lamb -2.207864 time 2019-02-23 20:05:32.932651
Pre: time 2019-02-23 20:07:08.013449: 
 	std: 0.0065254965
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790143, 0.9922, 0.9790143, 0.979]
	train_accs: [0.99245715, 0.9790143, 0.9922, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98433715
	best: 0.99245715

Starting e_i: 190
Model ind 685 epoch 190 head B batch: 0 avg loss -2.200883 avg loss no lamb -2.200883 time 2019-02-23 20:07:09.865369
Model ind 685 epoch 190 head B batch: 100 avg loss -2.182319 avg loss no lamb -2.182319 time 2019-02-23 20:08:28.464992
Model ind 685 epoch 190 head B batch: 200 avg loss -2.199556 avg loss no lamb -2.199556 time 2019-02-23 20:09:49.768252
Model ind 685 epoch 190 head B batch: 300 avg loss -2.212041 avg loss no lamb -2.212041 time 2019-02-23 20:11:08.188367
Model ind 685 epoch 190 head B batch: 400 avg loss -2.201757 avg loss no lamb -2.201757 time 2019-02-23 20:12:27.323794
Model ind 685 epoch 190 head B batch: 0 avg loss -2.193038 avg loss no lamb -2.193038 time 2019-02-23 20:13:46.429685
Model ind 685 epoch 190 head B batch: 100 avg loss -2.215458 avg loss no lamb -2.215458 time 2019-02-23 20:15:03.026462
Model ind 685 epoch 190 head B batch: 200 avg loss -2.250961 avg loss no lamb -2.250961 time 2019-02-23 20:16:20.970726
Model ind 685 epoch 190 head B batch: 300 avg loss -2.216312 avg loss no lamb -2.216312 time 2019-02-23 20:17:40.514603
Model ind 685 epoch 190 head B batch: 400 avg loss -2.199757 avg loss no lamb -2.199757 time 2019-02-23 20:18:58.393170
Model ind 685 epoch 190 head A batch: 0 avg loss -2.206542 avg loss no lamb -2.206542 time 2019-02-23 20:20:17.028768
Model ind 685 epoch 190 head A batch: 100 avg loss -2.173906 avg loss no lamb -2.173906 time 2019-02-23 20:21:35.742913
Model ind 685 epoch 190 head A batch: 200 avg loss -2.185783 avg loss no lamb -2.185783 time 2019-02-23 20:22:53.361401
Model ind 685 epoch 190 head A batch: 300 avg loss -2.165803 avg loss no lamb -2.165803 time 2019-02-23 20:24:11.263849
Model ind 685 epoch 190 head A batch: 400 avg loss -2.184376 avg loss no lamb -2.184376 time 2019-02-23 20:25:28.626855
Pre: time 2019-02-23 20:27:03.371600: 
 	std: 0.0064812917
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.99214286, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.99214286, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.9843486
	best: 0.9924286

Starting e_i: 191
Model ind 685 epoch 191 head B batch: 0 avg loss -2.192625 avg loss no lamb -2.192625 time 2019-02-23 20:27:05.195140
Model ind 685 epoch 191 head B batch: 100 avg loss -2.180857 avg loss no lamb -2.180857 time 2019-02-23 20:28:23.253957
Model ind 685 epoch 191 head B batch: 200 avg loss -2.214429 avg loss no lamb -2.214429 time 2019-02-23 20:29:31.406165
Model ind 685 epoch 191 head B batch: 300 avg loss -2.187114 avg loss no lamb -2.187114 time 2019-02-23 20:30:47.181461
Model ind 685 epoch 191 head B batch: 400 avg loss -2.181613 avg loss no lamb -2.181613 time 2019-02-23 20:32:04.917993
Model ind 685 epoch 191 head B batch: 0 avg loss -2.189336 avg loss no lamb -2.189336 time 2019-02-23 20:33:23.223476
Model ind 685 epoch 191 head B batch: 100 avg loss -2.191862 avg loss no lamb -2.191862 time 2019-02-23 20:34:43.734957
Model ind 685 epoch 191 head B batch: 200 avg loss -2.190121 avg loss no lamb -2.190121 time 2019-02-23 20:36:01.851681
Model ind 685 epoch 191 head B batch: 300 avg loss -2.204540 avg loss no lamb -2.204540 time 2019-02-23 20:37:19.304627
Model ind 685 epoch 191 head B batch: 400 avg loss -2.219055 avg loss no lamb -2.219055 time 2019-02-23 20:38:37.373991
Model ind 685 epoch 191 head A batch: 0 avg loss -2.201477 avg loss no lamb -2.201477 time 2019-02-23 20:39:55.525073
Model ind 685 epoch 191 head A batch: 100 avg loss -2.212523 avg loss no lamb -2.212523 time 2019-02-23 20:41:13.314342
Model ind 685 epoch 191 head A batch: 200 avg loss -2.197556 avg loss no lamb -2.197556 time 2019-02-23 20:42:32.721362
Model ind 685 epoch 191 head A batch: 300 avg loss -2.200501 avg loss no lamb -2.200501 time 2019-02-23 20:43:51.323393
Model ind 685 epoch 191 head A batch: 400 avg loss -2.191554 avg loss no lamb -2.191554 time 2019-02-23 20:45:09.545432
Pre: time 2019-02-23 20:46:45.665946: 
 	std: 0.006491741
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99215716, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99215716, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843429
	best: 0.9924286

Starting e_i: 192
Model ind 685 epoch 192 head B batch: 0 avg loss -2.208973 avg loss no lamb -2.208973 time 2019-02-23 20:46:47.646320
Model ind 685 epoch 192 head B batch: 100 avg loss -2.189982 avg loss no lamb -2.189982 time 2019-02-23 20:48:06.810012
Model ind 685 epoch 192 head B batch: 200 avg loss -2.171901 avg loss no lamb -2.171901 time 2019-02-23 20:49:26.338299
Model ind 685 epoch 192 head B batch: 300 avg loss -2.200922 avg loss no lamb -2.200922 time 2019-02-23 20:50:43.955536
Model ind 685 epoch 192 head B batch: 400 avg loss -2.172580 avg loss no lamb -2.172580 time 2019-02-23 20:52:01.763068
Model ind 685 epoch 192 head B batch: 0 avg loss -2.252472 avg loss no lamb -2.252472 time 2019-02-23 20:53:18.505802
Model ind 685 epoch 192 head B batch: 100 avg loss -2.209259 avg loss no lamb -2.209259 time 2019-02-23 20:54:35.433264
Model ind 685 epoch 192 head B batch: 200 avg loss -2.217814 avg loss no lamb -2.217814 time 2019-02-23 20:55:53.163349
Model ind 685 epoch 192 head B batch: 300 avg loss -2.197363 avg loss no lamb -2.197363 time 2019-02-23 20:57:11.515947
Model ind 685 epoch 192 head B batch: 400 avg loss -2.154016 avg loss no lamb -2.154016 time 2019-02-23 20:58:30.174853
Model ind 685 epoch 192 head A batch: 0 avg loss -2.181285 avg loss no lamb -2.181285 time 2019-02-23 20:59:48.945671
Model ind 685 epoch 192 head A batch: 100 avg loss -2.186687 avg loss no lamb -2.186687 time 2019-02-23 21:01:08.486568
Model ind 685 epoch 192 head A batch: 200 avg loss -2.162061 avg loss no lamb -2.162061 time 2019-02-23 21:02:26.299026
Model ind 685 epoch 192 head A batch: 300 avg loss -2.204917 avg loss no lamb -2.204917 time 2019-02-23 21:03:43.700292
Model ind 685 epoch 192 head A batch: 400 avg loss -2.194700 avg loss no lamb -2.194700 time 2019-02-23 21:05:02.964622
Pre: time 2019-02-23 21:06:37.885840: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 193
Model ind 685 epoch 193 head B batch: 0 avg loss -2.206226 avg loss no lamb -2.206226 time 2019-02-23 21:06:40.043753
Model ind 685 epoch 193 head B batch: 100 avg loss -2.218107 avg loss no lamb -2.218107 time 2019-02-23 21:07:59.214517
Model ind 685 epoch 193 head B batch: 200 avg loss -2.211891 avg loss no lamb -2.211891 time 2019-02-23 21:09:16.062595
Model ind 685 epoch 193 head B batch: 300 avg loss -2.168490 avg loss no lamb -2.168490 time 2019-02-23 21:10:34.482258
Model ind 685 epoch 193 head B batch: 400 avg loss -2.157559 avg loss no lamb -2.157559 time 2019-02-23 21:11:40.989570
Model ind 685 epoch 193 head B batch: 0 avg loss -2.230518 avg loss no lamb -2.230518 time 2019-02-23 21:12:58.928533
Model ind 685 epoch 193 head B batch: 100 avg loss -2.215358 avg loss no lamb -2.215358 time 2019-02-23 21:14:18.515888
Model ind 685 epoch 193 head B batch: 200 avg loss -2.199699 avg loss no lamb -2.199699 time 2019-02-23 21:15:39.137959
Model ind 685 epoch 193 head B batch: 300 avg loss -2.222291 avg loss no lamb -2.222291 time 2019-02-23 21:16:57.541662
Model ind 685 epoch 193 head B batch: 400 avg loss -2.155745 avg loss no lamb -2.155745 time 2019-02-23 21:18:13.861833
Model ind 685 epoch 193 head A batch: 0 avg loss -2.188595 avg loss no lamb -2.188595 time 2019-02-23 21:19:34.289659
Model ind 685 epoch 193 head A batch: 100 avg loss -2.190315 avg loss no lamb -2.190315 time 2019-02-23 21:20:52.203169
Model ind 685 epoch 193 head A batch: 200 avg loss -2.211051 avg loss no lamb -2.211051 time 2019-02-23 21:22:11.535115
Model ind 685 epoch 193 head A batch: 300 avg loss -2.222624 avg loss no lamb -2.222624 time 2019-02-23 21:23:30.139244
Model ind 685 epoch 193 head A batch: 400 avg loss -2.179010 avg loss no lamb -2.179010 time 2019-02-23 21:24:50.177173
Pre: time 2019-02-23 21:26:23.062954: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 194
Model ind 685 epoch 194 head B batch: 0 avg loss -2.229095 avg loss no lamb -2.229095 time 2019-02-23 21:26:25.007295
Model ind 685 epoch 194 head B batch: 100 avg loss -2.181519 avg loss no lamb -2.181519 time 2019-02-23 21:27:44.953522
Model ind 685 epoch 194 head B batch: 200 avg loss -2.170846 avg loss no lamb -2.170846 time 2019-02-23 21:29:02.802819
Model ind 685 epoch 194 head B batch: 300 avg loss -2.144197 avg loss no lamb -2.144197 time 2019-02-23 21:30:21.301776
Model ind 685 epoch 194 head B batch: 400 avg loss -2.179031 avg loss no lamb -2.179031 time 2019-02-23 21:31:40.884478
Model ind 685 epoch 194 head B batch: 0 avg loss -2.223810 avg loss no lamb -2.223810 time 2019-02-23 21:32:59.711892
Model ind 685 epoch 194 head B batch: 100 avg loss -2.210184 avg loss no lamb -2.210184 time 2019-02-23 21:34:18.563823
Model ind 685 epoch 194 head B batch: 200 avg loss -2.156406 avg loss no lamb -2.156406 time 2019-02-23 21:35:39.460712
Model ind 685 epoch 194 head B batch: 300 avg loss -2.202714 avg loss no lamb -2.202714 time 2019-02-23 21:36:57.797600
Model ind 685 epoch 194 head B batch: 400 avg loss -2.196382 avg loss no lamb -2.196382 time 2019-02-23 21:38:13.398648
Model ind 685 epoch 194 head A batch: 0 avg loss -2.237218 avg loss no lamb -2.237218 time 2019-02-23 21:39:30.482157
Model ind 685 epoch 194 head A batch: 100 avg loss -2.121413 avg loss no lamb -2.121413 time 2019-02-23 21:40:48.107617
Model ind 685 epoch 194 head A batch: 200 avg loss -2.239457 avg loss no lamb -2.239457 time 2019-02-23 21:42:07.932282
Model ind 685 epoch 194 head A batch: 300 avg loss -2.242337 avg loss no lamb -2.242337 time 2019-02-23 21:43:26.892944
Model ind 685 epoch 194 head A batch: 400 avg loss -2.200427 avg loss no lamb -2.200427 time 2019-02-23 21:44:45.055734
Pre: time 2019-02-23 21:46:18.195042: 
 	std: 0.006468526
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9790143, 0.99205714, 0.979, 0.979]
	train_accs: [0.99235713, 0.9790143, 0.99205714, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842857
	best: 0.99235713

Starting e_i: 195
Model ind 685 epoch 195 head B batch: 0 avg loss -2.208684 avg loss no lamb -2.208684 time 2019-02-23 21:46:20.048648
Model ind 685 epoch 195 head B batch: 100 avg loss -2.168087 avg loss no lamb -2.168087 time 2019-02-23 21:47:38.668820
Model ind 685 epoch 195 head B batch: 200 avg loss -2.187826 avg loss no lamb -2.187826 time 2019-02-23 21:48:57.148005
Model ind 685 epoch 195 head B batch: 300 avg loss -2.221964 avg loss no lamb -2.221964 time 2019-02-23 21:50:15.487539
Model ind 685 epoch 195 head B batch: 400 avg loss -2.150415 avg loss no lamb -2.150415 time 2019-02-23 21:51:35.029261
Model ind 685 epoch 195 head B batch: 0 avg loss -2.215055 avg loss no lamb -2.215055 time 2019-02-23 21:52:52.927387
Model ind 685 epoch 195 head B batch: 100 avg loss -2.223615 avg loss no lamb -2.223615 time 2019-02-23 21:54:02.146907
Model ind 685 epoch 195 head B batch: 200 avg loss -2.236347 avg loss no lamb -2.236347 time 2019-02-23 21:55:19.846200
Model ind 685 epoch 195 head B batch: 300 avg loss -2.242839 avg loss no lamb -2.242839 time 2019-02-23 21:56:37.683361
Model ind 685 epoch 195 head B batch: 400 avg loss -2.202297 avg loss no lamb -2.202297 time 2019-02-23 21:57:52.252198
Model ind 685 epoch 195 head A batch: 0 avg loss -2.240700 avg loss no lamb -2.240700 time 2019-02-23 21:59:08.101104
Model ind 685 epoch 195 head A batch: 100 avg loss -2.259067 avg loss no lamb -2.259067 time 2019-02-23 22:00:26.219503
Model ind 685 epoch 195 head A batch: 200 avg loss -2.168508 avg loss no lamb -2.168508 time 2019-02-23 22:01:42.964975
Model ind 685 epoch 195 head A batch: 300 avg loss -2.180551 avg loss no lamb -2.180551 time 2019-02-23 22:02:56.657666
Model ind 685 epoch 195 head A batch: 400 avg loss -2.168311 avg loss no lamb -2.168311 time 2019-02-23 22:04:13.804506
Pre: time 2019-02-23 22:05:48.119771: 
 	std: 0.006487065
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.99212855, 0.979, 0.9790428]
	train_accs: [0.9924, 0.9790286, 0.99212855, 0.979, 0.9790428]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98432004
	best: 0.9924

Starting e_i: 196
Model ind 685 epoch 196 head B batch: 0 avg loss -2.221586 avg loss no lamb -2.221586 time 2019-02-23 22:05:49.944050
Model ind 685 epoch 196 head B batch: 100 avg loss -2.184796 avg loss no lamb -2.184796 time 2019-02-23 22:07:09.785746
Model ind 685 epoch 196 head B batch: 200 avg loss -2.215953 avg loss no lamb -2.215953 time 2019-02-23 22:08:28.710039
Model ind 685 epoch 196 head B batch: 300 avg loss -2.217122 avg loss no lamb -2.217122 time 2019-02-23 22:09:46.269506
Model ind 685 epoch 196 head B batch: 400 avg loss -2.154901 avg loss no lamb -2.154901 time 2019-02-23 22:11:04.108983
Model ind 685 epoch 196 head B batch: 0 avg loss -2.210730 avg loss no lamb -2.210730 time 2019-02-23 22:12:23.742162
Model ind 685 epoch 196 head B batch: 100 avg loss -2.199857 avg loss no lamb -2.199857 time 2019-02-23 22:13:41.580071
Model ind 685 epoch 196 head B batch: 200 avg loss -2.218158 avg loss no lamb -2.218158 time 2019-02-23 22:15:00.078699
Model ind 685 epoch 196 head B batch: 300 avg loss -2.201239 avg loss no lamb -2.201239 time 2019-02-23 22:16:18.749338
Model ind 685 epoch 196 head B batch: 400 avg loss -2.195226 avg loss no lamb -2.195226 time 2019-02-23 22:17:38.482973
Model ind 685 epoch 196 head A batch: 0 avg loss -2.220275 avg loss no lamb -2.220275 time 2019-02-23 22:18:57.540112
Model ind 685 epoch 196 head A batch: 100 avg loss -2.213009 avg loss no lamb -2.213009 time 2019-02-23 22:20:16.977421
Model ind 685 epoch 196 head A batch: 200 avg loss -2.214993 avg loss no lamb -2.214993 time 2019-02-23 22:21:36.865185
Model ind 685 epoch 196 head A batch: 300 avg loss -2.246513 avg loss no lamb -2.246513 time 2019-02-23 22:22:55.208797
Model ind 685 epoch 196 head A batch: 400 avg loss -2.179676 avg loss no lamb -2.179676 time 2019-02-23 22:24:14.382646
Pre: time 2019-02-23 22:25:48.343623: 
 	std: 0.0064943186
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789429, 0.99204284, 0.9789571, 0.9789571]
	train_accs: [0.99237144, 0.9789429, 0.99204284, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98425424
	best: 0.99237144

Starting e_i: 197
Model ind 685 epoch 197 head B batch: 0 avg loss -2.215967 avg loss no lamb -2.215967 time 2019-02-23 22:25:50.526419
Model ind 685 epoch 197 head B batch: 100 avg loss -2.221124 avg loss no lamb -2.221124 time 2019-02-23 22:27:07.177548
Model ind 685 epoch 197 head B batch: 200 avg loss -2.184401 avg loss no lamb -2.184401 time 2019-02-23 22:28:26.746056
Model ind 685 epoch 197 head B batch: 300 avg loss -2.219371 avg loss no lamb -2.219371 time 2019-02-23 22:29:44.522723
Model ind 685 epoch 197 head B batch: 400 avg loss -2.172629 avg loss no lamb -2.172629 time 2019-02-23 22:31:03.075161
Model ind 685 epoch 197 head B batch: 0 avg loss -2.207515 avg loss no lamb -2.207515 time 2019-02-23 22:32:21.461444
Model ind 685 epoch 197 head B batch: 100 avg loss -2.208604 avg loss no lamb -2.208604 time 2019-02-23 22:33:40.958053
Model ind 685 epoch 197 head B batch: 200 avg loss -2.184273 avg loss no lamb -2.184273 time 2019-02-23 22:35:00.118818
Model ind 685 epoch 197 head B batch: 300 avg loss -2.221658 avg loss no lamb -2.221658 time 2019-02-23 22:36:09.848777
Model ind 685 epoch 197 head B batch: 400 avg loss -2.188867 avg loss no lamb -2.188867 time 2019-02-23 22:37:25.159011
Model ind 685 epoch 197 head A batch: 0 avg loss -2.169140 avg loss no lamb -2.169140 time 2019-02-23 22:38:43.815350
Model ind 685 epoch 197 head A batch: 100 avg loss -2.221053 avg loss no lamb -2.221053 time 2019-02-23 22:40:02.356407
Model ind 685 epoch 197 head A batch: 200 avg loss -2.196017 avg loss no lamb -2.196017 time 2019-02-23 22:41:21.598185
Model ind 685 epoch 197 head A batch: 300 avg loss -2.181286 avg loss no lamb -2.181286 time 2019-02-23 22:42:41.110321
Model ind 685 epoch 197 head A batch: 400 avg loss -2.216492 avg loss no lamb -2.216492 time 2019-02-23 22:43:59.406479
Pre: time 2019-02-23 22:45:33.491270: 
 	std: 0.006491712
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789429, 0.99205714, 0.9789429, 0.9789429]
	train_accs: [0.9923286, 0.9789429, 0.99205714, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.9842428
	best: 0.9923286

Starting e_i: 198
Model ind 685 epoch 198 head B batch: 0 avg loss -2.177846 avg loss no lamb -2.177846 time 2019-02-23 22:45:35.353372
Model ind 685 epoch 198 head B batch: 100 avg loss -2.199183 avg loss no lamb -2.199183 time 2019-02-23 22:46:55.107527
Model ind 685 epoch 198 head B batch: 200 avg loss -2.204333 avg loss no lamb -2.204333 time 2019-02-23 22:48:13.109716
Model ind 685 epoch 198 head B batch: 300 avg loss -2.229860 avg loss no lamb -2.229860 time 2019-02-23 22:49:31.580426
Model ind 685 epoch 198 head B batch: 400 avg loss -2.160384 avg loss no lamb -2.160384 time 2019-02-23 22:50:48.209525
Model ind 685 epoch 198 head B batch: 0 avg loss -2.213450 avg loss no lamb -2.213450 time 2019-02-23 22:52:07.546214
Model ind 685 epoch 198 head B batch: 100 avg loss -2.156801 avg loss no lamb -2.156801 time 2019-02-23 22:53:26.975129
Model ind 685 epoch 198 head B batch: 200 avg loss -2.202688 avg loss no lamb -2.202688 time 2019-02-23 22:54:45.388379
Model ind 685 epoch 198 head B batch: 300 avg loss -2.167239 avg loss no lamb -2.167239 time 2019-02-23 22:56:04.562217
Model ind 685 epoch 198 head B batch: 400 avg loss -2.199585 avg loss no lamb -2.199585 time 2019-02-23 22:57:25.485308
Model ind 685 epoch 198 head A batch: 0 avg loss -2.221668 avg loss no lamb -2.221668 time 2019-02-23 22:58:43.958229
Model ind 685 epoch 198 head A batch: 100 avg loss -2.198079 avg loss no lamb -2.198079 time 2019-02-23 23:00:05.027995
Model ind 685 epoch 198 head A batch: 200 avg loss -2.206882 avg loss no lamb -2.206882 time 2019-02-23 23:01:24.234685
Model ind 685 epoch 198 head A batch: 300 avg loss -2.179485 avg loss no lamb -2.179485 time 2019-02-23 23:02:43.668608
Model ind 685 epoch 198 head A batch: 400 avg loss -2.148867 avg loss no lamb -2.148867 time 2019-02-23 23:04:02.433463
Pre: time 2019-02-23 23:05:37.785860: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 199
Model ind 685 epoch 199 head B batch: 0 avg loss -2.181887 avg loss no lamb -2.181887 time 2019-02-23 23:05:39.802235
Model ind 685 epoch 199 head B batch: 100 avg loss -2.210616 avg loss no lamb -2.210616 time 2019-02-23 23:06:57.149391
Model ind 685 epoch 199 head B batch: 200 avg loss -2.210199 avg loss no lamb -2.210199 time 2019-02-23 23:08:15.489733
Model ind 685 epoch 199 head B batch: 300 avg loss -2.196531 avg loss no lamb -2.196531 time 2019-02-23 23:09:34.150177
Model ind 685 epoch 199 head B batch: 400 avg loss -2.160832 avg loss no lamb -2.160832 time 2019-02-23 23:10:49.969570
Model ind 685 epoch 199 head B batch: 0 avg loss -2.236750 avg loss no lamb -2.236750 time 2019-02-23 23:12:07.704344
Model ind 685 epoch 199 head B batch: 100 avg loss -2.200216 avg loss no lamb -2.200216 time 2019-02-23 23:13:24.073894
Model ind 685 epoch 199 head B batch: 200 avg loss -2.234373 avg loss no lamb -2.234373 time 2019-02-23 23:14:37.744159
Model ind 685 epoch 199 head B batch: 300 avg loss -2.223643 avg loss no lamb -2.223643 time 2019-02-23 23:15:54.071035
Model ind 685 epoch 199 head B batch: 400 avg loss -2.164995 avg loss no lamb -2.164995 time 2019-02-23 23:17:07.074997
Model ind 685 epoch 199 head A batch: 0 avg loss -2.177119 avg loss no lamb -2.177119 time 2019-02-23 23:18:20.401562
Model ind 685 epoch 199 head A batch: 100 avg loss -2.199307 avg loss no lamb -2.199307 time 2019-02-23 23:19:30.728289
Model ind 685 epoch 199 head A batch: 200 avg loss -2.192253 avg loss no lamb -2.192253 time 2019-02-23 23:20:46.634287
Model ind 685 epoch 199 head A batch: 300 avg loss -2.219260 avg loss no lamb -2.219260 time 2019-02-23 23:22:01.382914
Model ind 685 epoch 199 head A batch: 400 avg loss -2.199604 avg loss no lamb -2.199604 time 2019-02-23 23:23:16.620003
Pre: time 2019-02-23 23:24:52.977566: 
 	std: 0.006481424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429143
	best: 0.9923857

Starting e_i: 200
Model ind 685 epoch 200 head B batch: 0 avg loss -2.203004 avg loss no lamb -2.203004 time 2019-02-23 23:24:54.916420
Model ind 685 epoch 200 head B batch: 100 avg loss -2.208840 avg loss no lamb -2.208840 time 2019-02-23 23:26:10.856276
Model ind 685 epoch 200 head B batch: 200 avg loss -2.198215 avg loss no lamb -2.198215 time 2019-02-23 23:27:28.713154
Model ind 685 epoch 200 head B batch: 300 avg loss -2.228978 avg loss no lamb -2.228978 time 2019-02-23 23:28:42.951232
Model ind 685 epoch 200 head B batch: 400 avg loss -2.166138 avg loss no lamb -2.166138 time 2019-02-23 23:29:58.473907
Model ind 685 epoch 200 head B batch: 0 avg loss -2.188216 avg loss no lamb -2.188216 time 2019-02-23 23:31:13.318604
Model ind 685 epoch 200 head B batch: 100 avg loss -2.185448 avg loss no lamb -2.185448 time 2019-02-23 23:32:29.065412
Model ind 685 epoch 200 head B batch: 200 avg loss -2.185029 avg loss no lamb -2.185029 time 2019-02-23 23:33:45.154527
Model ind 685 epoch 200 head B batch: 300 avg loss -2.218295 avg loss no lamb -2.218295 time 2019-02-23 23:35:01.674418
Model ind 685 epoch 200 head B batch: 400 avg loss -2.142200 avg loss no lamb -2.142200 time 2019-02-23 23:36:16.255321
Model ind 685 epoch 200 head A batch: 0 avg loss -2.216591 avg loss no lamb -2.216591 time 2019-02-23 23:37:33.547660
Model ind 685 epoch 200 head A batch: 100 avg loss -2.190987 avg loss no lamb -2.190987 time 2019-02-23 23:38:48.537198
Model ind 685 epoch 200 head A batch: 200 avg loss -2.224728 avg loss no lamb -2.224728 time 2019-02-23 23:40:04.696774
Model ind 685 epoch 200 head A batch: 300 avg loss -2.216024 avg loss no lamb -2.216024 time 2019-02-23 23:41:21.358127
Model ind 685 epoch 200 head A batch: 400 avg loss -2.155060 avg loss no lamb -2.155060 time 2019-02-23 23:42:35.138998
Pre: time 2019-02-23 23:44:06.805915: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 201
Model ind 685 epoch 201 head B batch: 0 avg loss -2.228098 avg loss no lamb -2.228098 time 2019-02-23 23:44:09.600872
Model ind 685 epoch 201 head B batch: 100 avg loss -2.207415 avg loss no lamb -2.207415 time 2019-02-23 23:45:24.003789
Model ind 685 epoch 201 head B batch: 200 avg loss -2.225934 avg loss no lamb -2.225934 time 2019-02-23 23:46:40.660677
Model ind 685 epoch 201 head B batch: 300 avg loss -2.196735 avg loss no lamb -2.196735 time 2019-02-23 23:47:56.666803
Model ind 685 epoch 201 head B batch: 400 avg loss -2.216843 avg loss no lamb -2.216843 time 2019-02-23 23:49:12.986411
Model ind 685 epoch 201 head B batch: 0 avg loss -2.221431 avg loss no lamb -2.221431 time 2019-02-23 23:50:31.051211
Model ind 685 epoch 201 head B batch: 100 avg loss -2.183342 avg loss no lamb -2.183342 time 2019-02-23 23:51:48.039368
Model ind 685 epoch 201 head B batch: 200 avg loss -2.212707 avg loss no lamb -2.212707 time 2019-02-23 23:53:05.747851
Model ind 685 epoch 201 head B batch: 300 avg loss -2.187524 avg loss no lamb -2.187524 time 2019-02-23 23:54:21.616453
Model ind 685 epoch 201 head B batch: 400 avg loss -2.195889 avg loss no lamb -2.195889 time 2019-02-23 23:55:38.879509
Model ind 685 epoch 201 head A batch: 0 avg loss -2.171126 avg loss no lamb -2.171126 time 2019-02-23 23:56:54.089771
Model ind 685 epoch 201 head A batch: 100 avg loss -2.170294 avg loss no lamb -2.170294 time 2019-02-23 23:58:09.678274
Model ind 685 epoch 201 head A batch: 200 avg loss -2.236000 avg loss no lamb -2.236000 time 2019-02-23 23:59:26.431503
Model ind 685 epoch 201 head A batch: 300 avg loss -2.185606 avg loss no lamb -2.185606 time 2019-02-24 00:00:44.084757
Model ind 685 epoch 201 head A batch: 400 avg loss -2.154864 avg loss no lamb -2.154864 time 2019-02-24 00:01:52.539824
Pre: time 2019-02-24 00:03:24.154908: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 202
Model ind 685 epoch 202 head B batch: 0 avg loss -2.192729 avg loss no lamb -2.192729 time 2019-02-24 00:03:26.370163
Model ind 685 epoch 202 head B batch: 100 avg loss -2.186963 avg loss no lamb -2.186963 time 2019-02-24 00:04:43.124860
Model ind 685 epoch 202 head B batch: 200 avg loss -2.196902 avg loss no lamb -2.196902 time 2019-02-24 00:05:58.912397
Model ind 685 epoch 202 head B batch: 300 avg loss -2.199775 avg loss no lamb -2.199775 time 2019-02-24 00:07:14.825639
Model ind 685 epoch 202 head B batch: 400 avg loss -2.195432 avg loss no lamb -2.195432 time 2019-02-24 00:08:31.280120
Model ind 685 epoch 202 head B batch: 0 avg loss -2.169307 avg loss no lamb -2.169307 time 2019-02-24 00:09:45.319610
Model ind 685 epoch 202 head B batch: 100 avg loss -2.169888 avg loss no lamb -2.169888 time 2019-02-24 00:11:00.369563
Model ind 685 epoch 202 head B batch: 200 avg loss -2.193459 avg loss no lamb -2.193459 time 2019-02-24 00:12:16.514695
Model ind 685 epoch 202 head B batch: 300 avg loss -2.221219 avg loss no lamb -2.221219 time 2019-02-24 00:13:31.604618
Model ind 685 epoch 202 head B batch: 400 avg loss -2.176258 avg loss no lamb -2.176258 time 2019-02-24 00:14:47.150680
Model ind 685 epoch 202 head A batch: 0 avg loss -2.212321 avg loss no lamb -2.212321 time 2019-02-24 00:16:03.117693
Model ind 685 epoch 202 head A batch: 100 avg loss -2.197709 avg loss no lamb -2.197709 time 2019-02-24 00:17:17.309953
Model ind 685 epoch 202 head A batch: 200 avg loss -2.163760 avg loss no lamb -2.163760 time 2019-02-24 00:18:31.294895
Model ind 685 epoch 202 head A batch: 300 avg loss -2.227756 avg loss no lamb -2.227756 time 2019-02-24 00:19:46.987899
Model ind 685 epoch 202 head A batch: 400 avg loss -2.191005 avg loss no lamb -2.191005 time 2019-02-24 00:21:01.025397
Pre: time 2019-02-24 00:22:32.731888: 
 	std: 0.0064801755
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 203
Model ind 685 epoch 203 head B batch: 0 avg loss -2.236084 avg loss no lamb -2.236084 time 2019-02-24 00:22:34.693888
Model ind 685 epoch 203 head B batch: 100 avg loss -2.216619 avg loss no lamb -2.216619 time 2019-02-24 00:23:48.460071
Model ind 685 epoch 203 head B batch: 200 avg loss -2.171128 avg loss no lamb -2.171128 time 2019-02-24 00:25:03.774551
Model ind 685 epoch 203 head B batch: 300 avg loss -2.209621 avg loss no lamb -2.209621 time 2019-02-24 00:26:17.210896
Model ind 685 epoch 203 head B batch: 400 avg loss -2.171745 avg loss no lamb -2.171745 time 2019-02-24 00:27:31.601192
Model ind 685 epoch 203 head B batch: 0 avg loss -2.217847 avg loss no lamb -2.217847 time 2019-02-24 00:28:46.319389
Model ind 685 epoch 203 head B batch: 100 avg loss -2.180693 avg loss no lamb -2.180693 time 2019-02-24 00:30:00.320036
Model ind 685 epoch 203 head B batch: 200 avg loss -2.193956 avg loss no lamb -2.193956 time 2019-02-24 00:31:16.781715
Model ind 685 epoch 203 head B batch: 300 avg loss -2.242984 avg loss no lamb -2.242984 time 2019-02-24 00:32:29.397378
Model ind 685 epoch 203 head B batch: 400 avg loss -2.195041 avg loss no lamb -2.195041 time 2019-02-24 00:33:41.554536
Model ind 685 epoch 203 head A batch: 0 avg loss -2.221981 avg loss no lamb -2.221981 time 2019-02-24 00:34:55.208422
Model ind 685 epoch 203 head A batch: 100 avg loss -2.232011 avg loss no lamb -2.232011 time 2019-02-24 00:36:10.000131
Model ind 685 epoch 203 head A batch: 200 avg loss -2.179115 avg loss no lamb -2.179115 time 2019-02-24 00:37:24.721344
Model ind 685 epoch 203 head A batch: 300 avg loss -2.226156 avg loss no lamb -2.226156 time 2019-02-24 00:38:40.058235
Model ind 685 epoch 203 head A batch: 400 avg loss -2.172093 avg loss no lamb -2.172093 time 2019-02-24 00:39:54.316284
Pre: time 2019-02-24 00:41:25.919625: 
 	std: 0.0064708306
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98431146
	best: 0.9923857

Starting e_i: 204
Model ind 685 epoch 204 head B batch: 0 avg loss -2.218503 avg loss no lamb -2.218503 time 2019-02-24 00:41:27.771893
Model ind 685 epoch 204 head B batch: 100 avg loss -2.201584 avg loss no lamb -2.201584 time 2019-02-24 00:42:42.768070
Model ind 685 epoch 204 head B batch: 200 avg loss -2.184007 avg loss no lamb -2.184007 time 2019-02-24 00:43:56.084786
Model ind 685 epoch 204 head B batch: 300 avg loss -2.187349 avg loss no lamb -2.187349 time 2019-02-24 00:45:02.614438
Model ind 685 epoch 204 head B batch: 400 avg loss -2.165846 avg loss no lamb -2.165846 time 2019-02-24 00:46:14.913101
Model ind 685 epoch 204 head B batch: 0 avg loss -2.208752 avg loss no lamb -2.208752 time 2019-02-24 00:47:30.229213
Model ind 685 epoch 204 head B batch: 100 avg loss -2.183686 avg loss no lamb -2.183686 time 2019-02-24 00:48:43.467241
Model ind 685 epoch 204 head B batch: 200 avg loss -2.231298 avg loss no lamb -2.231298 time 2019-02-24 00:49:56.133748
Model ind 685 epoch 204 head B batch: 300 avg loss -2.174026 avg loss no lamb -2.174026 time 2019-02-24 00:51:09.591464
Model ind 685 epoch 204 head B batch: 400 avg loss -2.193497 avg loss no lamb -2.193497 time 2019-02-24 00:52:23.105317
Model ind 685 epoch 204 head A batch: 0 avg loss -2.224562 avg loss no lamb -2.224562 time 2019-02-24 00:53:36.657590
Model ind 685 epoch 204 head A batch: 100 avg loss -2.195448 avg loss no lamb -2.195448 time 2019-02-24 00:54:52.015176
Model ind 685 epoch 204 head A batch: 200 avg loss -2.197325 avg loss no lamb -2.197325 time 2019-02-24 00:56:06.640053
Model ind 685 epoch 204 head A batch: 300 avg loss -2.199146 avg loss no lamb -2.199146 time 2019-02-24 00:57:22.579496
Model ind 685 epoch 204 head A batch: 400 avg loss -2.182405 avg loss no lamb -2.182405 time 2019-02-24 00:58:39.620442
Pre: time 2019-02-24 01:00:07.467922: 
 	std: 0.006476905
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97907144, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.99244285, 0.97907144, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.99244285

Starting e_i: 205
Model ind 685 epoch 205 head B batch: 0 avg loss -2.200658 avg loss no lamb -2.200658 time 2019-02-24 01:00:09.697555
Model ind 685 epoch 205 head B batch: 100 avg loss -2.179315 avg loss no lamb -2.179315 time 2019-02-24 01:01:22.189038
Model ind 685 epoch 205 head B batch: 200 avg loss -2.181768 avg loss no lamb -2.181768 time 2019-02-24 01:02:35.214984
Model ind 685 epoch 205 head B batch: 300 avg loss -2.196641 avg loss no lamb -2.196641 time 2019-02-24 01:03:48.181765
Model ind 685 epoch 205 head B batch: 400 avg loss -2.203660 avg loss no lamb -2.203660 time 2019-02-24 01:05:00.525703
Model ind 685 epoch 205 head B batch: 0 avg loss -2.183200 avg loss no lamb -2.183200 time 2019-02-24 01:06:12.537192
Model ind 685 epoch 205 head B batch: 100 avg loss -2.189039 avg loss no lamb -2.189039 time 2019-02-24 01:07:26.299507
Model ind 685 epoch 205 head B batch: 200 avg loss -2.190913 avg loss no lamb -2.190913 time 2019-02-24 01:08:37.950254
Model ind 685 epoch 205 head B batch: 300 avg loss -2.220318 avg loss no lamb -2.220318 time 2019-02-24 01:09:51.039372
Model ind 685 epoch 205 head B batch: 400 avg loss -2.198853 avg loss no lamb -2.198853 time 2019-02-24 01:11:03.646337
Model ind 685 epoch 205 head A batch: 0 avg loss -2.229803 avg loss no lamb -2.229803 time 2019-02-24 01:12:16.502672
Model ind 685 epoch 205 head A batch: 100 avg loss -2.214617 avg loss no lamb -2.214617 time 2019-02-24 01:13:32.538456
Model ind 685 epoch 205 head A batch: 200 avg loss -2.206481 avg loss no lamb -2.206481 time 2019-02-24 01:14:45.848613
Model ind 685 epoch 205 head A batch: 300 avg loss -2.219512 avg loss no lamb -2.219512 time 2019-02-24 01:15:59.093278
Model ind 685 epoch 205 head A batch: 400 avg loss -2.187752 avg loss no lamb -2.187752 time 2019-02-24 01:17:15.102628
Pre: time 2019-02-24 01:18:46.625775: 
 	std: 0.0064941803
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790428, 0.99214286, 0.9790286, 0.9790428]
	train_accs: [0.99244285, 0.9790428, 0.99214286, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98434
	best: 0.99244285

Starting e_i: 206
Model ind 685 epoch 206 head B batch: 0 avg loss -2.238161 avg loss no lamb -2.238161 time 2019-02-24 01:18:48.938105
Model ind 685 epoch 206 head B batch: 100 avg loss -2.242599 avg loss no lamb -2.242599 time 2019-02-24 01:20:05.982106
Model ind 685 epoch 206 head B batch: 200 avg loss -2.189014 avg loss no lamb -2.189014 time 2019-02-24 01:21:21.543833
Model ind 685 epoch 206 head B batch: 300 avg loss -2.174301 avg loss no lamb -2.174301 time 2019-02-24 01:22:34.107754
Model ind 685 epoch 206 head B batch: 400 avg loss -2.188598 avg loss no lamb -2.188598 time 2019-02-24 01:23:47.770578
Model ind 685 epoch 206 head B batch: 0 avg loss -2.165958 avg loss no lamb -2.165958 time 2019-02-24 01:25:00.102265
Model ind 685 epoch 206 head B batch: 100 avg loss -2.197817 avg loss no lamb -2.197817 time 2019-02-24 01:26:15.679592
Model ind 685 epoch 206 head B batch: 200 avg loss -2.179884 avg loss no lamb -2.179884 time 2019-02-24 01:27:29.039074
Model ind 685 epoch 206 head B batch: 300 avg loss -2.191488 avg loss no lamb -2.191488 time 2019-02-24 01:28:30.711449
Model ind 685 epoch 206 head B batch: 400 avg loss -2.206023 avg loss no lamb -2.206023 time 2019-02-24 01:29:46.133886
Model ind 685 epoch 206 head A batch: 0 avg loss -2.180299 avg loss no lamb -2.180299 time 2019-02-24 01:31:00.569815
Model ind 685 epoch 206 head A batch: 100 avg loss -2.166283 avg loss no lamb -2.166283 time 2019-02-24 01:32:13.558363
Model ind 685 epoch 206 head A batch: 200 avg loss -2.214430 avg loss no lamb -2.214430 time 2019-02-24 01:33:26.288489
Model ind 685 epoch 206 head A batch: 300 avg loss -2.178942 avg loss no lamb -2.178942 time 2019-02-24 01:34:40.896941
Model ind 685 epoch 206 head A batch: 400 avg loss -2.157211 avg loss no lamb -2.157211 time 2019-02-24 01:35:53.983820
Pre: time 2019-02-24 01:37:24.315932: 
 	std: 0.0064987196
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99214286, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.99214286, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 207
Model ind 685 epoch 207 head B batch: 0 avg loss -2.225766 avg loss no lamb -2.225766 time 2019-02-24 01:37:26.627973
Model ind 685 epoch 207 head B batch: 100 avg loss -2.168084 avg loss no lamb -2.168084 time 2019-02-24 01:38:40.282099
Model ind 685 epoch 207 head B batch: 200 avg loss -2.232056 avg loss no lamb -2.232056 time 2019-02-24 01:39:53.829319
Model ind 685 epoch 207 head B batch: 300 avg loss -2.197671 avg loss no lamb -2.197671 time 2019-02-24 01:41:07.899938
Model ind 685 epoch 207 head B batch: 400 avg loss -2.176073 avg loss no lamb -2.176073 time 2019-02-24 01:42:23.930085
Model ind 685 epoch 207 head B batch: 0 avg loss -2.223300 avg loss no lamb -2.223300 time 2019-02-24 01:43:37.534429
Model ind 685 epoch 207 head B batch: 100 avg loss -2.176701 avg loss no lamb -2.176701 time 2019-02-24 01:44:51.553064
Model ind 685 epoch 207 head B batch: 200 avg loss -2.230871 avg loss no lamb -2.230871 time 2019-02-24 01:46:05.195627
Model ind 685 epoch 207 head B batch: 300 avg loss -2.197690 avg loss no lamb -2.197690 time 2019-02-24 01:47:17.918372
Model ind 685 epoch 207 head B batch: 400 avg loss -2.179257 avg loss no lamb -2.179257 time 2019-02-24 01:48:34.379857
Model ind 685 epoch 207 head A batch: 0 avg loss -2.235986 avg loss no lamb -2.235986 time 2019-02-24 01:49:50.490853
Model ind 685 epoch 207 head A batch: 100 avg loss -2.188684 avg loss no lamb -2.188684 time 2019-02-24 01:51:03.669316
Model ind 685 epoch 207 head A batch: 200 avg loss -2.212962 avg loss no lamb -2.212962 time 2019-02-24 01:52:21.281149
Model ind 685 epoch 207 head A batch: 300 avg loss -2.233629 avg loss no lamb -2.233629 time 2019-02-24 01:53:36.139578
Model ind 685 epoch 207 head A batch: 400 avg loss -2.170837 avg loss no lamb -2.170837 time 2019-02-24 01:54:51.556319
Pre: time 2019-02-24 01:56:19.592021: 
 	std: 0.006484734
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.99214286, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.99214286, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98433715
	best: 0.9924143

Starting e_i: 208
Model ind 685 epoch 208 head B batch: 0 avg loss -2.212199 avg loss no lamb -2.212199 time 2019-02-24 01:56:21.862075
Model ind 685 epoch 208 head B batch: 100 avg loss -2.196786 avg loss no lamb -2.196786 time 2019-02-24 01:57:35.672495
Model ind 685 epoch 208 head B batch: 200 avg loss -2.195222 avg loss no lamb -2.195222 time 2019-02-24 01:58:48.489776
Model ind 685 epoch 208 head B batch: 300 avg loss -2.185917 avg loss no lamb -2.185917 time 2019-02-24 02:00:01.608808
Model ind 685 epoch 208 head B batch: 400 avg loss -2.208075 avg loss no lamb -2.208075 time 2019-02-24 02:01:14.193164
Model ind 685 epoch 208 head B batch: 0 avg loss -2.228180 avg loss no lamb -2.228180 time 2019-02-24 02:02:26.783809
Model ind 685 epoch 208 head B batch: 100 avg loss -2.196038 avg loss no lamb -2.196038 time 2019-02-24 02:03:39.007123
Model ind 685 epoch 208 head B batch: 200 avg loss -2.185086 avg loss no lamb -2.185086 time 2019-02-24 02:04:53.645679
Model ind 685 epoch 208 head B batch: 300 avg loss -2.240576 avg loss no lamb -2.240576 time 2019-02-24 02:06:05.987110
Model ind 685 epoch 208 head B batch: 400 avg loss -2.177477 avg loss no lamb -2.177477 time 2019-02-24 02:07:19.094017
Model ind 685 epoch 208 head A batch: 0 avg loss -2.212601 avg loss no lamb -2.212601 time 2019-02-24 02:08:31.205825
Model ind 685 epoch 208 head A batch: 100 avg loss -2.158168 avg loss no lamb -2.158168 time 2019-02-24 02:09:43.853769
Model ind 685 epoch 208 head A batch: 200 avg loss -2.237482 avg loss no lamb -2.237482 time 2019-02-24 02:10:56.500878
Model ind 685 epoch 208 head A batch: 300 avg loss -2.210810 avg loss no lamb -2.210810 time 2019-02-24 02:12:01.296943
Model ind 685 epoch 208 head A batch: 400 avg loss -2.183109 avg loss no lamb -2.183109 time 2019-02-24 02:13:14.001602
Pre: time 2019-02-24 02:14:42.959835: 
 	std: 0.006499854
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.99215716, 0.9790428, 0.9790143]
	train_accs: [0.9924143, 0.979, 0.99215716, 0.9790428, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843257
	best: 0.9924143

Starting e_i: 209
Model ind 685 epoch 209 head B batch: 0 avg loss -2.168688 avg loss no lamb -2.168688 time 2019-02-24 02:14:45.281152
Model ind 685 epoch 209 head B batch: 100 avg loss -2.188343 avg loss no lamb -2.188343 time 2019-02-24 02:15:58.078070
Model ind 685 epoch 209 head B batch: 200 avg loss -2.217975 avg loss no lamb -2.217975 time 2019-02-24 02:17:10.336819
Model ind 685 epoch 209 head B batch: 300 avg loss -2.250755 avg loss no lamb -2.250755 time 2019-02-24 02:18:24.907114
Model ind 685 epoch 209 head B batch: 400 avg loss -2.176846 avg loss no lamb -2.176846 time 2019-02-24 02:19:37.303133
Model ind 685 epoch 209 head B batch: 0 avg loss -2.210848 avg loss no lamb -2.210848 time 2019-02-24 02:20:49.516247
Model ind 685 epoch 209 head B batch: 100 avg loss -2.188293 avg loss no lamb -2.188293 time 2019-02-24 02:22:01.991958
Model ind 685 epoch 209 head B batch: 200 avg loss -2.225968 avg loss no lamb -2.225968 time 2019-02-24 02:23:15.306624
Model ind 685 epoch 209 head B batch: 300 avg loss -2.223546 avg loss no lamb -2.223546 time 2019-02-24 02:24:30.671820
Model ind 685 epoch 209 head B batch: 400 avg loss -2.220549 avg loss no lamb -2.220549 time 2019-02-24 02:25:44.267792
Model ind 685 epoch 209 head A batch: 0 avg loss -2.207534 avg loss no lamb -2.207534 time 2019-02-24 02:26:57.855543
Model ind 685 epoch 209 head A batch: 100 avg loss -2.232776 avg loss no lamb -2.232776 time 2019-02-24 02:28:09.914820
Model ind 685 epoch 209 head A batch: 200 avg loss -2.206378 avg loss no lamb -2.206378 time 2019-02-24 02:29:21.960701
Model ind 685 epoch 209 head A batch: 300 avg loss -2.239932 avg loss no lamb -2.239932 time 2019-02-24 02:30:35.811507
Model ind 685 epoch 209 head A batch: 400 avg loss -2.188604 avg loss no lamb -2.188604 time 2019-02-24 02:31:47.504434
Pre: time 2019-02-24 02:33:15.288639: 
 	std: 0.006492824
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99212855, 0.979, 0.979]
	train_accs: [0.9923857, 0.9790143, 0.99212855, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 210
Model ind 685 epoch 210 head B batch: 0 avg loss -2.213318 avg loss no lamb -2.213318 time 2019-02-24 02:33:17.078245
Model ind 685 epoch 210 head B batch: 100 avg loss -2.197941 avg loss no lamb -2.197941 time 2019-02-24 02:34:29.619503
Model ind 685 epoch 210 head B batch: 200 avg loss -2.223912 avg loss no lamb -2.223912 time 2019-02-24 02:35:44.348876
Model ind 685 epoch 210 head B batch: 300 avg loss -2.232086 avg loss no lamb -2.232086 time 2019-02-24 02:36:56.002239
Model ind 685 epoch 210 head B batch: 400 avg loss -2.156691 avg loss no lamb -2.156691 time 2019-02-24 02:38:09.533929
Model ind 685 epoch 210 head B batch: 0 avg loss -2.238983 avg loss no lamb -2.238983 time 2019-02-24 02:39:22.173405
Model ind 685 epoch 210 head B batch: 100 avg loss -2.137936 avg loss no lamb -2.137936 time 2019-02-24 02:40:36.694880
Model ind 685 epoch 210 head B batch: 200 avg loss -2.178828 avg loss no lamb -2.178828 time 2019-02-24 02:41:49.110055
Model ind 685 epoch 210 head B batch: 300 avg loss -2.215146 avg loss no lamb -2.215146 time 2019-02-24 02:43:00.873600
Model ind 685 epoch 210 head B batch: 400 avg loss -2.220969 avg loss no lamb -2.220969 time 2019-02-24 02:44:14.115618
Model ind 685 epoch 210 head A batch: 0 avg loss -2.189008 avg loss no lamb -2.189008 time 2019-02-24 02:45:29.297010
Model ind 685 epoch 210 head A batch: 100 avg loss -2.192296 avg loss no lamb -2.192296 time 2019-02-24 02:46:43.061754
Model ind 685 epoch 210 head A batch: 200 avg loss -2.167996 avg loss no lamb -2.167996 time 2019-02-24 02:47:55.061017
Model ind 685 epoch 210 head A batch: 300 avg loss -2.201287 avg loss no lamb -2.201287 time 2019-02-24 02:49:06.728420
Model ind 685 epoch 210 head A batch: 400 avg loss -2.194885 avg loss no lamb -2.194885 time 2019-02-24 02:50:18.358028
Pre: time 2019-02-24 02:51:45.354596: 
 	std: 0.0064720684
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97907144, 0.99212855, 0.97908574, 0.97907144]
	train_accs: [0.99244285, 0.97907144, 0.99212855, 0.97908574, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.98436004
	best: 0.99244285

Starting e_i: 211
Model ind 685 epoch 211 head B batch: 0 avg loss -2.159031 avg loss no lamb -2.159031 time 2019-02-24 02:51:47.262858
Model ind 685 epoch 211 head B batch: 100 avg loss -2.203583 avg loss no lamb -2.203583 time 2019-02-24 02:52:59.468085
Model ind 685 epoch 211 head B batch: 200 avg loss -2.181167 avg loss no lamb -2.181167 time 2019-02-24 02:54:15.855658
Model ind 685 epoch 211 head B batch: 300 avg loss -2.215780 avg loss no lamb -2.215780 time 2019-02-24 02:55:21.886534
Model ind 685 epoch 211 head B batch: 400 avg loss -2.162842 avg loss no lamb -2.162842 time 2019-02-24 02:56:37.592035
Model ind 685 epoch 211 head B batch: 0 avg loss -2.236800 avg loss no lamb -2.236800 time 2019-02-24 02:57:53.048192
Model ind 685 epoch 211 head B batch: 100 avg loss -2.189096 avg loss no lamb -2.189096 time 2019-02-24 02:59:06.549183
Model ind 685 epoch 211 head B batch: 200 avg loss -2.206640 avg loss no lamb -2.206640 time 2019-02-24 03:00:19.855999
Model ind 685 epoch 211 head B batch: 300 avg loss -2.202494 avg loss no lamb -2.202494 time 2019-02-24 03:01:36.693258
Model ind 685 epoch 211 head B batch: 400 avg loss -2.180299 avg loss no lamb -2.180299 time 2019-02-24 03:02:52.929952
Model ind 685 epoch 211 head A batch: 0 avg loss -2.225969 avg loss no lamb -2.225969 time 2019-02-24 03:04:06.654030
Model ind 685 epoch 211 head A batch: 100 avg loss -2.129215 avg loss no lamb -2.129215 time 2019-02-24 03:05:22.636657
Model ind 685 epoch 211 head A batch: 200 avg loss -2.201057 avg loss no lamb -2.201057 time 2019-02-24 03:06:34.772126
Model ind 685 epoch 211 head A batch: 300 avg loss -2.215483 avg loss no lamb -2.215483 time 2019-02-24 03:07:50.633725
Model ind 685 epoch 211 head A batch: 400 avg loss -2.177215 avg loss no lamb -2.177215 time 2019-02-24 03:09:03.364016
Pre: time 2019-02-24 03:10:34.320942: 
 	std: 0.0065022847
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.99214286, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.99214286, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843229
	best: 0.9924286

Starting e_i: 212
Model ind 685 epoch 212 head B batch: 0 avg loss -2.175138 avg loss no lamb -2.175138 time 2019-02-24 03:10:36.151570
Model ind 685 epoch 212 head B batch: 100 avg loss -2.163269 avg loss no lamb -2.163269 time 2019-02-24 03:11:51.688013
Model ind 685 epoch 212 head B batch: 200 avg loss -2.155514 avg loss no lamb -2.155514 time 2019-02-24 03:13:06.248804
Model ind 685 epoch 212 head B batch: 300 avg loss -2.236570 avg loss no lamb -2.236570 time 2019-02-24 03:14:20.337851
Model ind 685 epoch 212 head B batch: 400 avg loss -2.197430 avg loss no lamb -2.197430 time 2019-02-24 03:15:34.640254
Model ind 685 epoch 212 head B batch: 0 avg loss -2.201443 avg loss no lamb -2.201443 time 2019-02-24 03:16:51.717167
Model ind 685 epoch 212 head B batch: 100 avg loss -2.189701 avg loss no lamb -2.189701 time 2019-02-24 03:18:05.940747
Model ind 685 epoch 212 head B batch: 200 avg loss -2.224399 avg loss no lamb -2.224399 time 2019-02-24 03:19:21.287905
Model ind 685 epoch 212 head B batch: 300 avg loss -2.207571 avg loss no lamb -2.207571 time 2019-02-24 03:20:36.460654
Model ind 685 epoch 212 head B batch: 400 avg loss -2.172186 avg loss no lamb -2.172186 time 2019-02-24 03:21:49.973937
Model ind 685 epoch 212 head A batch: 0 avg loss -2.246990 avg loss no lamb -2.246990 time 2019-02-24 03:23:01.809679
Model ind 685 epoch 212 head A batch: 100 avg loss -2.201998 avg loss no lamb -2.201998 time 2019-02-24 03:24:13.217735
Model ind 685 epoch 212 head A batch: 200 avg loss -2.211516 avg loss no lamb -2.211516 time 2019-02-24 03:25:25.332532
Model ind 685 epoch 212 head A batch: 300 avg loss -2.198594 avg loss no lamb -2.198594 time 2019-02-24 03:26:37.603482
Model ind 685 epoch 212 head A batch: 400 avg loss -2.195590 avg loss no lamb -2.195590 time 2019-02-24 03:27:50.359441
Pre: time 2019-02-24 03:29:17.393991: 
 	std: 0.006514063
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9789857, 0.99212855, 0.979, 0.9789857]
	train_accs: [0.99244285, 0.9789857, 0.99212855, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843086
	best: 0.99244285

Starting e_i: 213
Model ind 685 epoch 213 head B batch: 0 avg loss -2.180204 avg loss no lamb -2.180204 time 2019-02-24 03:29:19.660611
Model ind 685 epoch 213 head B batch: 100 avg loss -2.184084 avg loss no lamb -2.184084 time 2019-02-24 03:30:31.814894
Model ind 685 epoch 213 head B batch: 200 avg loss -2.170968 avg loss no lamb -2.170968 time 2019-02-24 03:31:44.728467
Model ind 685 epoch 213 head B batch: 300 avg loss -2.185706 avg loss no lamb -2.185706 time 2019-02-24 03:33:01.109700
Model ind 685 epoch 213 head B batch: 400 avg loss -2.173712 avg loss no lamb -2.173712 time 2019-02-24 03:34:16.886523
Model ind 685 epoch 213 head B batch: 0 avg loss -2.182875 avg loss no lamb -2.182875 time 2019-02-24 03:35:30.374862
Model ind 685 epoch 213 head B batch: 100 avg loss -2.207544 avg loss no lamb -2.207544 time 2019-02-24 03:36:44.823767
Model ind 685 epoch 213 head B batch: 200 avg loss -2.200340 avg loss no lamb -2.200340 time 2019-02-24 03:37:58.437661
Model ind 685 epoch 213 head B batch: 300 avg loss -2.215503 avg loss no lamb -2.215503 time 2019-02-24 03:39:07.667184
Model ind 685 epoch 213 head B batch: 400 avg loss -2.200908 avg loss no lamb -2.200908 time 2019-02-24 03:40:22.272302
Model ind 685 epoch 213 head A batch: 0 avg loss -2.237163 avg loss no lamb -2.237163 time 2019-02-24 03:41:35.339523
Model ind 685 epoch 213 head A batch: 100 avg loss -2.192760 avg loss no lamb -2.192760 time 2019-02-24 03:42:49.315852
Model ind 685 epoch 213 head A batch: 200 avg loss -2.161132 avg loss no lamb -2.161132 time 2019-02-24 03:44:03.447100
Model ind 685 epoch 213 head A batch: 300 avg loss -2.202377 avg loss no lamb -2.202377 time 2019-02-24 03:45:18.599690
Model ind 685 epoch 213 head A batch: 400 avg loss -2.173500 avg loss no lamb -2.173500 time 2019-02-24 03:46:35.889553
Pre: time 2019-02-24 03:48:08.299349: 
 	std: 0.006488284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9921, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.9921, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9923857

Starting e_i: 214
Model ind 685 epoch 214 head B batch: 0 avg loss -2.209444 avg loss no lamb -2.209444 time 2019-02-24 03:48:10.492114
Model ind 685 epoch 214 head B batch: 100 avg loss -2.189637 avg loss no lamb -2.189637 time 2019-02-24 03:49:25.688517
Model ind 685 epoch 214 head B batch: 200 avg loss -2.229412 avg loss no lamb -2.229412 time 2019-02-24 03:50:39.307359
Model ind 685 epoch 214 head B batch: 300 avg loss -2.207207 avg loss no lamb -2.207207 time 2019-02-24 03:51:54.501479
Model ind 685 epoch 214 head B batch: 400 avg loss -2.202509 avg loss no lamb -2.202509 time 2019-02-24 03:53:07.972026
Model ind 685 epoch 214 head B batch: 0 avg loss -2.221492 avg loss no lamb -2.221492 time 2019-02-24 03:54:20.888714
Model ind 685 epoch 214 head B batch: 100 avg loss -2.210990 avg loss no lamb -2.210990 time 2019-02-24 03:55:35.210153
Model ind 685 epoch 214 head B batch: 200 avg loss -2.192979 avg loss no lamb -2.192979 time 2019-02-24 03:56:52.030013
Model ind 685 epoch 214 head B batch: 300 avg loss -2.229785 avg loss no lamb -2.229785 time 2019-02-24 03:58:07.754199
Model ind 685 epoch 214 head B batch: 400 avg loss -2.213546 avg loss no lamb -2.213546 time 2019-02-24 03:59:23.791226
Model ind 685 epoch 214 head A batch: 0 avg loss -2.209850 avg loss no lamb -2.209850 time 2019-02-24 04:00:38.770990
Model ind 685 epoch 214 head A batch: 100 avg loss -2.220144 avg loss no lamb -2.220144 time 2019-02-24 04:01:50.459557
Model ind 685 epoch 214 head A batch: 200 avg loss -2.194994 avg loss no lamb -2.194994 time 2019-02-24 04:03:04.127515
Model ind 685 epoch 214 head A batch: 300 avg loss -2.248102 avg loss no lamb -2.248102 time 2019-02-24 04:04:15.860357
Model ind 685 epoch 214 head A batch: 400 avg loss -2.135246 avg loss no lamb -2.135246 time 2019-02-24 04:05:30.551816
Pre: time 2019-02-24 04:07:02.406214: 
 	std: 0.006497897
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789571, 0.99205714, 0.9789857, 0.9789571]
	train_accs: [0.9924, 0.9789571, 0.99205714, 0.9789857, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.9842714
	best: 0.9924

Starting e_i: 215
Model ind 685 epoch 215 head B batch: 0 avg loss -2.248715 avg loss no lamb -2.248715 time 2019-02-24 04:07:04.190027
Model ind 685 epoch 215 head B batch: 100 avg loss -2.223339 avg loss no lamb -2.223339 time 2019-02-24 04:08:19.991938
Model ind 685 epoch 215 head B batch: 200 avg loss -2.200731 avg loss no lamb -2.200731 time 2019-02-24 04:09:32.549962
Model ind 685 epoch 215 head B batch: 300 avg loss -2.220610 avg loss no lamb -2.220610 time 2019-02-24 04:10:44.566105
Model ind 685 epoch 215 head B batch: 400 avg loss -2.151371 avg loss no lamb -2.151371 time 2019-02-24 04:11:58.076896
Model ind 685 epoch 215 head B batch: 0 avg loss -2.215525 avg loss no lamb -2.215525 time 2019-02-24 04:13:09.879881
Model ind 685 epoch 215 head B batch: 100 avg loss -2.207003 avg loss no lamb -2.207003 time 2019-02-24 04:14:21.587503
Model ind 685 epoch 215 head B batch: 200 avg loss -2.168756 avg loss no lamb -2.168756 time 2019-02-24 04:15:35.386343
Model ind 685 epoch 215 head B batch: 300 avg loss -2.186051 avg loss no lamb -2.186051 time 2019-02-24 04:16:47.413629
Model ind 685 epoch 215 head B batch: 400 avg loss -2.210506 avg loss no lamb -2.210506 time 2019-02-24 04:18:02.341604
Model ind 685 epoch 215 head A batch: 0 avg loss -2.226869 avg loss no lamb -2.226869 time 2019-02-24 04:19:19.703962
Model ind 685 epoch 215 head A batch: 100 avg loss -2.213719 avg loss no lamb -2.213719 time 2019-02-24 04:20:32.572412
Model ind 685 epoch 215 head A batch: 200 avg loss -2.197181 avg loss no lamb -2.197181 time 2019-02-24 04:21:36.840458
Model ind 685 epoch 215 head A batch: 300 avg loss -2.219136 avg loss no lamb -2.219136 time 2019-02-24 04:22:52.861765
Model ind 685 epoch 215 head A batch: 400 avg loss -2.183218 avg loss no lamb -2.183218 time 2019-02-24 04:24:08.926706
Pre: time 2019-02-24 04:25:42.365021: 
 	std: 0.006488423
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9921, 0.9790143, 0.9790286]
	train_accs: [0.9924143, 0.979, 0.9921, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 216
Model ind 685 epoch 216 head B batch: 0 avg loss -2.224104 avg loss no lamb -2.224104 time 2019-02-24 04:25:44.198980
Model ind 685 epoch 216 head B batch: 100 avg loss -2.180558 avg loss no lamb -2.180558 time 2019-02-24 04:26:58.683414
Model ind 685 epoch 216 head B batch: 200 avg loss -2.201095 avg loss no lamb -2.201095 time 2019-02-24 04:28:10.423424
Model ind 685 epoch 216 head B batch: 300 avg loss -2.219908 avg loss no lamb -2.219908 time 2019-02-24 04:29:25.075461
Model ind 685 epoch 216 head B batch: 400 avg loss -2.173384 avg loss no lamb -2.173384 time 2019-02-24 04:30:37.997974
Model ind 685 epoch 216 head B batch: 0 avg loss -2.219668 avg loss no lamb -2.219668 time 2019-02-24 04:31:51.146852
Model ind 685 epoch 216 head B batch: 100 avg loss -2.216331 avg loss no lamb -2.216331 time 2019-02-24 04:33:05.989830
Model ind 685 epoch 216 head B batch: 200 avg loss -2.186919 avg loss no lamb -2.186919 time 2019-02-24 04:34:19.799518
Model ind 685 epoch 216 head B batch: 300 avg loss -2.205370 avg loss no lamb -2.205370 time 2019-02-24 04:35:34.259425
Model ind 685 epoch 216 head B batch: 400 avg loss -2.209131 avg loss no lamb -2.209131 time 2019-02-24 04:36:45.940914
Model ind 685 epoch 216 head A batch: 0 avg loss -2.191839 avg loss no lamb -2.191839 time 2019-02-24 04:38:00.272319
Model ind 685 epoch 216 head A batch: 100 avg loss -2.176558 avg loss no lamb -2.176558 time 2019-02-24 04:39:12.328751
Model ind 685 epoch 216 head A batch: 200 avg loss -2.214194 avg loss no lamb -2.214194 time 2019-02-24 04:40:28.651640
Model ind 685 epoch 216 head A batch: 300 avg loss -2.225381 avg loss no lamb -2.225381 time 2019-02-24 04:41:44.095728
Model ind 685 epoch 216 head A batch: 400 avg loss -2.180075 avg loss no lamb -2.180075 time 2019-02-24 04:42:59.006631
Pre: time 2019-02-24 04:44:26.621650: 
 	std: 0.0064734733
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99205714, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.99205714, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 217
Model ind 685 epoch 217 head B batch: 0 avg loss -2.217045 avg loss no lamb -2.217045 time 2019-02-24 04:44:28.444867
Model ind 685 epoch 217 head B batch: 100 avg loss -2.202680 avg loss no lamb -2.202680 time 2019-02-24 04:45:44.337154
Model ind 685 epoch 217 head B batch: 200 avg loss -2.214754 avg loss no lamb -2.214754 time 2019-02-24 04:46:58.537703
Model ind 685 epoch 217 head B batch: 300 avg loss -2.218767 avg loss no lamb -2.218767 time 2019-02-24 04:48:13.391311
Model ind 685 epoch 217 head B batch: 400 avg loss -2.218363 avg loss no lamb -2.218363 time 2019-02-24 04:49:25.581794
Model ind 685 epoch 217 head B batch: 0 avg loss -2.201052 avg loss no lamb -2.201052 time 2019-02-24 04:50:39.291441
Model ind 685 epoch 217 head B batch: 100 avg loss -2.190476 avg loss no lamb -2.190476 time 2019-02-24 04:51:53.160635
Model ind 685 epoch 217 head B batch: 200 avg loss -2.220098 avg loss no lamb -2.220098 time 2019-02-24 04:53:10.071423
Model ind 685 epoch 217 head B batch: 300 avg loss -2.194938 avg loss no lamb -2.194938 time 2019-02-24 04:54:24.541992
Model ind 685 epoch 217 head B batch: 400 avg loss -2.175227 avg loss no lamb -2.175227 time 2019-02-24 04:55:37.727876
Model ind 685 epoch 217 head A batch: 0 avg loss -2.184171 avg loss no lamb -2.184171 time 2019-02-24 04:56:51.356075
Model ind 685 epoch 217 head A batch: 100 avg loss -2.200270 avg loss no lamb -2.200270 time 2019-02-24 04:58:03.263481
Model ind 685 epoch 217 head A batch: 200 avg loss -2.205157 avg loss no lamb -2.205157 time 2019-02-24 04:59:15.892760
Model ind 685 epoch 217 head A batch: 300 avg loss -2.240710 avg loss no lamb -2.240710 time 2019-02-24 05:00:27.765065
Model ind 685 epoch 217 head A batch: 400 avg loss -2.203268 avg loss no lamb -2.203268 time 2019-02-24 05:01:42.473571
Pre: time 2019-02-24 05:03:11.117725: 
 	std: 0.0065036523
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.979, 0.9921, 0.9789857, 0.9789857]
	train_accs: [0.9924286, 0.979, 0.9921, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843
	best: 0.9924286

Starting e_i: 218
Model ind 685 epoch 218 head B batch: 0 avg loss -2.229427 avg loss no lamb -2.229427 time 2019-02-24 05:03:12.965039
Model ind 685 epoch 218 head B batch: 100 avg loss -2.207284 avg loss no lamb -2.207284 time 2019-02-24 05:04:27.465211
Model ind 685 epoch 218 head B batch: 200 avg loss -2.196186 avg loss no lamb -2.196186 time 2019-02-24 05:05:35.254116
Model ind 685 epoch 218 head B batch: 300 avg loss -2.196544 avg loss no lamb -2.196544 time 2019-02-24 05:06:49.684961
Model ind 685 epoch 218 head B batch: 400 avg loss -2.189826 avg loss no lamb -2.189826 time 2019-02-24 05:08:03.252473
Model ind 685 epoch 218 head B batch: 0 avg loss -2.229216 avg loss no lamb -2.229216 time 2019-02-24 05:09:19.038214
Model ind 685 epoch 218 head B batch: 100 avg loss -2.222469 avg loss no lamb -2.222469 time 2019-02-24 05:10:34.106265
Model ind 685 epoch 218 head B batch: 200 avg loss -2.214469 avg loss no lamb -2.214469 time 2019-02-24 05:11:47.948379
Model ind 685 epoch 218 head B batch: 300 avg loss -2.183060 avg loss no lamb -2.183060 time 2019-02-24 05:13:00.073643
Model ind 685 epoch 218 head B batch: 400 avg loss -2.191625 avg loss no lamb -2.191625 time 2019-02-24 05:14:13.325313
Model ind 685 epoch 218 head A batch: 0 avg loss -2.159038 avg loss no lamb -2.159038 time 2019-02-24 05:15:25.943991
Model ind 685 epoch 218 head A batch: 100 avg loss -2.206816 avg loss no lamb -2.206816 time 2019-02-24 05:16:40.823835
Model ind 685 epoch 218 head A batch: 200 avg loss -2.221495 avg loss no lamb -2.221495 time 2019-02-24 05:17:55.128542
Model ind 685 epoch 218 head A batch: 300 avg loss -2.253539 avg loss no lamb -2.253539 time 2019-02-24 05:19:11.791014
Model ind 685 epoch 218 head A batch: 400 avg loss -2.210544 avg loss no lamb -2.210544 time 2019-02-24 05:20:24.740845
Pre: time 2019-02-24 05:21:58.158608: 
 	std: 0.0064907605
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9921, 0.9790286, 0.979]
	train_accs: [0.9924143, 0.979, 0.9921, 0.9790286, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 219
Model ind 685 epoch 219 head B batch: 0 avg loss -2.218135 avg loss no lamb -2.218135 time 2019-02-24 05:21:59.956067
Model ind 685 epoch 219 head B batch: 100 avg loss -2.190154 avg loss no lamb -2.190154 time 2019-02-24 05:23:15.418171
Model ind 685 epoch 219 head B batch: 200 avg loss -2.229818 avg loss no lamb -2.229818 time 2019-02-24 05:24:30.285798
Model ind 685 epoch 219 head B batch: 300 avg loss -2.209224 avg loss no lamb -2.209224 time 2019-02-24 05:25:44.622749
Model ind 685 epoch 219 head B batch: 400 avg loss -2.210153 avg loss no lamb -2.210153 time 2019-02-24 05:26:57.585546
Model ind 685 epoch 219 head B batch: 0 avg loss -2.205537 avg loss no lamb -2.205537 time 2019-02-24 05:28:09.714803
Model ind 685 epoch 219 head B batch: 100 avg loss -2.227129 avg loss no lamb -2.227129 time 2019-02-24 05:29:21.670426
Model ind 685 epoch 219 head B batch: 200 avg loss -2.226836 avg loss no lamb -2.226836 time 2019-02-24 05:30:34.536638
Model ind 685 epoch 219 head B batch: 300 avg loss -2.230389 avg loss no lamb -2.230389 time 2019-02-24 05:31:47.305448
Model ind 685 epoch 219 head B batch: 400 avg loss -2.195261 avg loss no lamb -2.195261 time 2019-02-24 05:33:00.861814
Model ind 685 epoch 219 head A batch: 0 avg loss -2.172841 avg loss no lamb -2.172841 time 2019-02-24 05:34:14.670034
Model ind 685 epoch 219 head A batch: 100 avg loss -2.180344 avg loss no lamb -2.180344 time 2019-02-24 05:35:29.047706
Model ind 685 epoch 219 head A batch: 200 avg loss -2.189183 avg loss no lamb -2.189183 time 2019-02-24 05:36:44.522900
Model ind 685 epoch 219 head A batch: 300 avg loss -2.235011 avg loss no lamb -2.235011 time 2019-02-24 05:37:56.562087
Model ind 685 epoch 219 head A batch: 400 avg loss -2.178375 avg loss no lamb -2.178375 time 2019-02-24 05:39:09.490958
Pre: time 2019-02-24 05:40:37.624272: 
 	std: 0.00650227
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	train_accs: [0.9924, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842943
	best: 0.9924

Starting e_i: 220
Model ind 685 epoch 220 head B batch: 0 avg loss -2.208534 avg loss no lamb -2.208534 time 2019-02-24 05:40:39.412948
Model ind 685 epoch 220 head B batch: 100 avg loss -2.175863 avg loss no lamb -2.175863 time 2019-02-24 05:41:51.682364
Model ind 685 epoch 220 head B batch: 200 avg loss -2.182314 avg loss no lamb -2.182314 time 2019-02-24 05:43:05.289010
Model ind 685 epoch 220 head B batch: 300 avg loss -2.189887 avg loss no lamb -2.189887 time 2019-02-24 05:44:21.010553
Model ind 685 epoch 220 head B batch: 400 avg loss -2.173699 avg loss no lamb -2.173699 time 2019-02-24 05:45:34.859517
Model ind 685 epoch 220 head B batch: 0 avg loss -2.229256 avg loss no lamb -2.229256 time 2019-02-24 05:46:52.017755
Model ind 685 epoch 220 head B batch: 100 avg loss -2.154613 avg loss no lamb -2.154613 time 2019-02-24 05:48:03.388855
Model ind 685 epoch 220 head B batch: 200 avg loss -2.208481 avg loss no lamb -2.208481 time 2019-02-24 05:49:12.621365
Model ind 685 epoch 220 head B batch: 300 avg loss -2.209995 avg loss no lamb -2.209995 time 2019-02-24 05:50:29.478015
Model ind 685 epoch 220 head B batch: 400 avg loss -2.209002 avg loss no lamb -2.209002 time 2019-02-24 05:51:42.383811
Model ind 685 epoch 220 head A batch: 0 avg loss -2.202232 avg loss no lamb -2.202232 time 2019-02-24 05:52:54.616722
Model ind 685 epoch 220 head A batch: 100 avg loss -2.199847 avg loss no lamb -2.199847 time 2019-02-24 05:54:06.637207
Model ind 685 epoch 220 head A batch: 200 avg loss -2.175218 avg loss no lamb -2.175218 time 2019-02-24 05:55:19.143610
Model ind 685 epoch 220 head A batch: 300 avg loss -2.216817 avg loss no lamb -2.216817 time 2019-02-24 05:56:33.382567
Model ind 685 epoch 220 head A batch: 400 avg loss -2.195930 avg loss no lamb -2.195930 time 2019-02-24 05:57:46.273450
Pre: time 2019-02-24 05:59:16.380247: 
 	std: 0.006495288
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921143, 0.9789857, 0.9790143]
	train_accs: [0.9924, 0.979, 0.9921143, 0.9789857, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843029
	best: 0.9924

Starting e_i: 221
Model ind 685 epoch 221 head B batch: 0 avg loss -2.230406 avg loss no lamb -2.230406 time 2019-02-24 05:59:19.656724
Model ind 685 epoch 221 head B batch: 100 avg loss -2.207835 avg loss no lamb -2.207835 time 2019-02-24 06:00:35.936844
Model ind 685 epoch 221 head B batch: 200 avg loss -2.197780 avg loss no lamb -2.197780 time 2019-02-24 06:01:50.374603
Model ind 685 epoch 221 head B batch: 300 avg loss -2.254412 avg loss no lamb -2.254412 time 2019-02-24 06:03:04.011830
Model ind 685 epoch 221 head B batch: 400 avg loss -2.224579 avg loss no lamb -2.224579 time 2019-02-24 06:04:16.098471
Model ind 685 epoch 221 head B batch: 0 avg loss -2.215602 avg loss no lamb -2.215602 time 2019-02-24 06:05:28.794834
Model ind 685 epoch 221 head B batch: 100 avg loss -2.186681 avg loss no lamb -2.186681 time 2019-02-24 06:06:40.197117
Model ind 685 epoch 221 head B batch: 200 avg loss -2.176108 avg loss no lamb -2.176108 time 2019-02-24 06:07:53.411498
Model ind 685 epoch 221 head B batch: 300 avg loss -2.208449 avg loss no lamb -2.208449 time 2019-02-24 06:09:09.168487
Model ind 685 epoch 221 head B batch: 400 avg loss -2.171656 avg loss no lamb -2.171656 time 2019-02-24 06:10:22.194531
Model ind 685 epoch 221 head A batch: 0 avg loss -2.190544 avg loss no lamb -2.190544 time 2019-02-24 06:11:37.440357
Model ind 685 epoch 221 head A batch: 100 avg loss -2.210582 avg loss no lamb -2.210582 time 2019-02-24 06:12:49.643340
Model ind 685 epoch 221 head A batch: 200 avg loss -2.191185 avg loss no lamb -2.191185 time 2019-02-24 06:14:05.061237
Model ind 685 epoch 221 head A batch: 300 avg loss -2.236393 avg loss no lamb -2.236393 time 2019-02-24 06:15:16.536502
Model ind 685 epoch 221 head A batch: 400 avg loss -2.170943 avg loss no lamb -2.170943 time 2019-02-24 06:16:30.044599
Pre: time 2019-02-24 06:18:00.110516: 
 	std: 0.0064859507
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790286, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 222
Model ind 685 epoch 222 head B batch: 0 avg loss -2.233803 avg loss no lamb -2.233803 time 2019-02-24 06:18:01.993143
Model ind 685 epoch 222 head B batch: 100 avg loss -2.199759 avg loss no lamb -2.199759 time 2019-02-24 06:19:14.856932
Model ind 685 epoch 222 head B batch: 200 avg loss -2.196070 avg loss no lamb -2.196070 time 2019-02-24 06:20:28.494007
Model ind 685 epoch 222 head B batch: 300 avg loss -2.189071 avg loss no lamb -2.189071 time 2019-02-24 06:21:42.912589
Model ind 685 epoch 222 head B batch: 400 avg loss -2.132673 avg loss no lamb -2.132673 time 2019-02-24 06:22:55.768775
Model ind 685 epoch 222 head B batch: 0 avg loss -2.203932 avg loss no lamb -2.203932 time 2019-02-24 06:24:09.385784
Model ind 685 epoch 222 head B batch: 100 avg loss -2.179718 avg loss no lamb -2.179718 time 2019-02-24 06:25:23.660541
Model ind 685 epoch 222 head B batch: 200 avg loss -2.169862 avg loss no lamb -2.169862 time 2019-02-24 06:26:38.362083
Model ind 685 epoch 222 head B batch: 300 avg loss -2.224504 avg loss no lamb -2.224504 time 2019-02-24 06:27:55.526826
Model ind 685 epoch 222 head B batch: 400 avg loss -2.184954 avg loss no lamb -2.184954 time 2019-02-24 06:29:10.263576
Model ind 685 epoch 222 head A batch: 0 avg loss -2.158452 avg loss no lamb -2.158452 time 2019-02-24 06:30:22.837395
Model ind 685 epoch 222 head A batch: 100 avg loss -2.214189 avg loss no lamb -2.214189 time 2019-02-24 06:31:27.925333
Model ind 685 epoch 222 head A batch: 200 avg loss -2.200155 avg loss no lamb -2.200155 time 2019-02-24 06:32:38.835009
Model ind 685 epoch 222 head A batch: 300 avg loss -2.237309 avg loss no lamb -2.237309 time 2019-02-24 06:33:53.349605
Model ind 685 epoch 222 head A batch: 400 avg loss -2.218376 avg loss no lamb -2.218376 time 2019-02-24 06:35:10.253176
Pre: time 2019-02-24 06:36:38.078403: 
 	std: 0.0065067285
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.97891426, 0.99207145, 0.97891426, 0.9788857]
	train_accs: [0.9923, 0.97891426, 0.99207145, 0.97891426, 0.9788857]
	best_train_sub_head: 0
	worst: 0.9788857
	avg: 0.98421717
	best: 0.9923

Starting e_i: 223
Model ind 685 epoch 223 head B batch: 0 avg loss -2.206512 avg loss no lamb -2.206512 time 2019-02-24 06:36:39.834764
Model ind 685 epoch 223 head B batch: 100 avg loss -2.231392 avg loss no lamb -2.231392 time 2019-02-24 06:37:56.278890
Model ind 685 epoch 223 head B batch: 200 avg loss -2.185531 avg loss no lamb -2.185531 time 2019-02-24 06:39:08.646941
Model ind 685 epoch 223 head B batch: 300 avg loss -2.225647 avg loss no lamb -2.225647 time 2019-02-24 06:40:22.502493
Model ind 685 epoch 223 head B batch: 400 avg loss -2.168171 avg loss no lamb -2.168171 time 2019-02-24 06:41:38.116371
Model ind 685 epoch 223 head B batch: 0 avg loss -2.221672 avg loss no lamb -2.221672 time 2019-02-24 06:42:51.822560
Model ind 685 epoch 223 head B batch: 100 avg loss -2.186020 avg loss no lamb -2.186020 time 2019-02-24 06:44:06.581444
Model ind 685 epoch 223 head B batch: 200 avg loss -2.167168 avg loss no lamb -2.167168 time 2019-02-24 06:45:21.342694
Model ind 685 epoch 223 head B batch: 300 avg loss -2.229791 avg loss no lamb -2.229791 time 2019-02-24 06:46:34.698064
Model ind 685 epoch 223 head B batch: 400 avg loss -2.186216 avg loss no lamb -2.186216 time 2019-02-24 06:47:50.527530
Model ind 685 epoch 223 head A batch: 0 avg loss -2.226656 avg loss no lamb -2.226656 time 2019-02-24 06:49:06.701579
Model ind 685 epoch 223 head A batch: 100 avg loss -2.187544 avg loss no lamb -2.187544 time 2019-02-24 06:50:21.980445
Model ind 685 epoch 223 head A batch: 200 avg loss -2.206106 avg loss no lamb -2.206106 time 2019-02-24 06:51:34.738241
Model ind 685 epoch 223 head A batch: 300 avg loss -2.204435 avg loss no lamb -2.204435 time 2019-02-24 06:52:47.233272
Model ind 685 epoch 223 head A batch: 400 avg loss -2.177475 avg loss no lamb -2.177475 time 2019-02-24 06:53:58.758091
Pre: time 2019-02-24 06:55:29.018741: 
 	std: 0.0064952625
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789429, 0.99205714, 0.9789429, 0.9789429]
	train_accs: [0.9923428, 0.9789429, 0.99205714, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98424566
	best: 0.9923428

Starting e_i: 224
Model ind 685 epoch 224 head B batch: 0 avg loss -2.195706 avg loss no lamb -2.195706 time 2019-02-24 06:55:31.104814
Model ind 685 epoch 224 head B batch: 100 avg loss -2.203045 avg loss no lamb -2.203045 time 2019-02-24 06:56:43.344464
Model ind 685 epoch 224 head B batch: 200 avg loss -2.211344 avg loss no lamb -2.211344 time 2019-02-24 06:57:58.809794
Model ind 685 epoch 224 head B batch: 300 avg loss -2.243959 avg loss no lamb -2.243959 time 2019-02-24 06:59:14.738831
Model ind 685 epoch 224 head B batch: 400 avg loss -2.178671 avg loss no lamb -2.178671 time 2019-02-24 07:00:30.180497
Model ind 685 epoch 224 head B batch: 0 avg loss -2.207067 avg loss no lamb -2.207067 time 2019-02-24 07:01:46.686477
Model ind 685 epoch 224 head B batch: 100 avg loss -2.185403 avg loss no lamb -2.185403 time 2019-02-24 07:03:00.976643
Model ind 685 epoch 224 head B batch: 200 avg loss -2.184678 avg loss no lamb -2.184678 time 2019-02-24 07:04:14.056913
Model ind 685 epoch 224 head B batch: 300 avg loss -2.213152 avg loss no lamb -2.213152 time 2019-02-24 07:05:26.218868
Model ind 685 epoch 224 head B batch: 400 avg loss -2.167475 avg loss no lamb -2.167475 time 2019-02-24 07:06:42.397454
Model ind 685 epoch 224 head A batch: 0 avg loss -2.181527 avg loss no lamb -2.181527 time 2019-02-24 07:07:56.265649
Model ind 685 epoch 224 head A batch: 100 avg loss -2.184015 avg loss no lamb -2.184015 time 2019-02-24 07:09:11.505681
Model ind 685 epoch 224 head A batch: 200 avg loss -2.193053 avg loss no lamb -2.193053 time 2019-02-24 07:10:26.023216
Model ind 685 epoch 224 head A batch: 300 avg loss -2.196880 avg loss no lamb -2.196880 time 2019-02-24 07:11:39.039886
Model ind 685 epoch 224 head A batch: 400 avg loss -2.188755 avg loss no lamb -2.188755 time 2019-02-24 07:12:52.311403
Pre: time 2019-02-24 07:14:21.636099: 
 	std: 0.0064733266
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.99204284, 0.979, 0.979]
	train_accs: [0.99237144, 0.9789857, 0.99204284, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98428
	best: 0.99237144

Starting e_i: 225
Model ind 685 epoch 225 head B batch: 0 avg loss -2.203280 avg loss no lamb -2.203280 time 2019-02-24 07:14:23.196707
Model ind 685 epoch 225 head B batch: 100 avg loss -2.184665 avg loss no lamb -2.184665 time 2019-02-24 07:15:28.945927
Model ind 685 epoch 225 head B batch: 200 avg loss -2.222663 avg loss no lamb -2.222663 time 2019-02-24 07:16:43.123256
Model ind 685 epoch 225 head B batch: 300 avg loss -2.222926 avg loss no lamb -2.222926 time 2019-02-24 07:17:58.779210
Model ind 685 epoch 225 head B batch: 400 avg loss -2.215449 avg loss no lamb -2.215449 time 2019-02-24 07:19:13.970393
Model ind 685 epoch 225 head B batch: 0 avg loss -2.204713 avg loss no lamb -2.204713 time 2019-02-24 07:20:29.262482
Model ind 685 epoch 225 head B batch: 100 avg loss -2.184672 avg loss no lamb -2.184672 time 2019-02-24 07:21:42.996395
Model ind 685 epoch 225 head B batch: 200 avg loss -2.218480 avg loss no lamb -2.218480 time 2019-02-24 07:22:57.147891
Model ind 685 epoch 225 head B batch: 300 avg loss -2.218450 avg loss no lamb -2.218450 time 2019-02-24 07:24:16.079479
Model ind 685 epoch 225 head B batch: 400 avg loss -2.165677 avg loss no lamb -2.165677 time 2019-02-24 07:25:28.499886
Model ind 685 epoch 225 head A batch: 0 avg loss -2.197114 avg loss no lamb -2.197114 time 2019-02-24 07:26:42.864715
Model ind 685 epoch 225 head A batch: 100 avg loss -2.191468 avg loss no lamb -2.191468 time 2019-02-24 07:27:56.129531
Model ind 685 epoch 225 head A batch: 200 avg loss -2.210734 avg loss no lamb -2.210734 time 2019-02-24 07:29:09.016362
Model ind 685 epoch 225 head A batch: 300 avg loss -2.224775 avg loss no lamb -2.224775 time 2019-02-24 07:30:25.401057
Model ind 685 epoch 225 head A batch: 400 avg loss -2.198251 avg loss no lamb -2.198251 time 2019-02-24 07:31:39.850496
Pre: time 2019-02-24 07:33:09.552720: 
 	std: 0.006481424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429143
	best: 0.9923857

Starting e_i: 226
Model ind 685 epoch 226 head B batch: 0 avg loss -2.213305 avg loss no lamb -2.213305 time 2019-02-24 07:33:11.515054
Model ind 685 epoch 226 head B batch: 100 avg loss -2.186904 avg loss no lamb -2.186904 time 2019-02-24 07:34:24.647048
Model ind 685 epoch 226 head B batch: 200 avg loss -2.243161 avg loss no lamb -2.243161 time 2019-02-24 07:35:37.488312
Model ind 685 epoch 226 head B batch: 300 avg loss -2.176644 avg loss no lamb -2.176644 time 2019-02-24 07:36:48.948213
Model ind 685 epoch 226 head B batch: 400 avg loss -2.218376 avg loss no lamb -2.218376 time 2019-02-24 07:38:00.787272
Model ind 685 epoch 226 head B batch: 0 avg loss -2.218415 avg loss no lamb -2.218415 time 2019-02-24 07:39:13.794215
Model ind 685 epoch 226 head B batch: 100 avg loss -2.195677 avg loss no lamb -2.195677 time 2019-02-24 07:40:27.106263
Model ind 685 epoch 226 head B batch: 200 avg loss -2.203355 avg loss no lamb -2.203355 time 2019-02-24 07:41:41.788550
Model ind 685 epoch 226 head B batch: 300 avg loss -2.174072 avg loss no lamb -2.174072 time 2019-02-24 07:42:55.588146
Model ind 685 epoch 226 head B batch: 400 avg loss -2.211253 avg loss no lamb -2.211253 time 2019-02-24 07:44:10.874886
Model ind 685 epoch 226 head A batch: 0 avg loss -2.216519 avg loss no lamb -2.216519 time 2019-02-24 07:45:23.567746
Model ind 685 epoch 226 head A batch: 100 avg loss -2.227370 avg loss no lamb -2.227370 time 2019-02-24 07:46:36.753574
Model ind 685 epoch 226 head A batch: 200 avg loss -2.199959 avg loss no lamb -2.199959 time 2019-02-24 07:47:49.931805
Model ind 685 epoch 226 head A batch: 300 avg loss -2.232797 avg loss no lamb -2.232797 time 2019-02-24 07:49:01.301264
Model ind 685 epoch 226 head A batch: 400 avg loss -2.190437 avg loss no lamb -2.190437 time 2019-02-24 07:50:16.054333
Pre: time 2019-02-24 07:51:46.653682: 
 	std: 0.006476622
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790143, 0.9920857, 0.9790143, 0.979]
	train_accs: [0.99237144, 0.9790143, 0.9920857, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.99237144

Starting e_i: 227
Model ind 685 epoch 227 head B batch: 0 avg loss -2.210193 avg loss no lamb -2.210193 time 2019-02-24 07:51:48.766428
Model ind 685 epoch 227 head B batch: 100 avg loss -2.178955 avg loss no lamb -2.178955 time 2019-02-24 07:53:02.916955
Model ind 685 epoch 227 head B batch: 200 avg loss -2.225446 avg loss no lamb -2.225446 time 2019-02-24 07:54:19.055182
Model ind 685 epoch 227 head B batch: 300 avg loss -2.212813 avg loss no lamb -2.212813 time 2019-02-24 07:55:34.500090
Model ind 685 epoch 227 head B batch: 400 avg loss -2.181054 avg loss no lamb -2.181054 time 2019-02-24 07:56:47.251635
Model ind 685 epoch 227 head B batch: 0 avg loss -2.199464 avg loss no lamb -2.199464 time 2019-02-24 07:57:56.180282
Model ind 685 epoch 227 head B batch: 100 avg loss -2.187063 avg loss no lamb -2.187063 time 2019-02-24 07:59:03.449976
Model ind 685 epoch 227 head B batch: 200 avg loss -2.171059 avg loss no lamb -2.171059 time 2019-02-24 08:00:19.404128
Model ind 685 epoch 227 head B batch: 300 avg loss -2.177173 avg loss no lamb -2.177173 time 2019-02-24 08:01:33.552484
Model ind 685 epoch 227 head B batch: 400 avg loss -2.182653 avg loss no lamb -2.182653 time 2019-02-24 08:02:46.796531
Model ind 685 epoch 227 head A batch: 0 avg loss -2.174277 avg loss no lamb -2.174277 time 2019-02-24 08:04:02.237427
Model ind 685 epoch 227 head A batch: 100 avg loss -2.181880 avg loss no lamb -2.181880 time 2019-02-24 08:05:21.386605
Model ind 685 epoch 227 head A batch: 200 avg loss -2.138345 avg loss no lamb -2.138345 time 2019-02-24 08:06:36.320449
Model ind 685 epoch 227 head A batch: 300 avg loss -2.164118 avg loss no lamb -2.164118 time 2019-02-24 08:07:49.765086
Model ind 685 epoch 227 head A batch: 400 avg loss -2.218054 avg loss no lamb -2.218054 time 2019-02-24 08:09:02.412659
Pre: time 2019-02-24 08:10:29.288766: 
 	std: 0.0064764
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790428, 0.99214286, 0.9790428, 0.9790286]
	train_accs: [0.99237144, 0.9790428, 0.99214286, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843257
	best: 0.99237144

Starting e_i: 228
Model ind 685 epoch 228 head B batch: 0 avg loss -2.244723 avg loss no lamb -2.244723 time 2019-02-24 08:10:30.989098
Model ind 685 epoch 228 head B batch: 100 avg loss -2.188467 avg loss no lamb -2.188467 time 2019-02-24 08:11:43.797369
Model ind 685 epoch 228 head B batch: 200 avg loss -2.215570 avg loss no lamb -2.215570 time 2019-02-24 08:12:55.864816
Model ind 685 epoch 228 head B batch: 300 avg loss -2.240871 avg loss no lamb -2.240871 time 2019-02-24 08:14:07.249297
Model ind 685 epoch 228 head B batch: 400 avg loss -2.227712 avg loss no lamb -2.227712 time 2019-02-24 08:15:20.474806
Model ind 685 epoch 228 head B batch: 0 avg loss -2.213235 avg loss no lamb -2.213235 time 2019-02-24 08:16:33.395149
Model ind 685 epoch 228 head B batch: 100 avg loss -2.218429 avg loss no lamb -2.218429 time 2019-02-24 08:17:46.178861
Model ind 685 epoch 228 head B batch: 200 avg loss -2.207885 avg loss no lamb -2.207885 time 2019-02-24 08:19:04.339830
Model ind 685 epoch 228 head B batch: 300 avg loss -2.236976 avg loss no lamb -2.236976 time 2019-02-24 08:20:20.554661
Model ind 685 epoch 228 head B batch: 400 avg loss -2.168945 avg loss no lamb -2.168945 time 2019-02-24 08:21:36.433620
Model ind 685 epoch 228 head A batch: 0 avg loss -2.248600 avg loss no lamb -2.248600 time 2019-02-24 08:22:48.804742
Model ind 685 epoch 228 head A batch: 100 avg loss -2.229271 avg loss no lamb -2.229271 time 2019-02-24 08:24:00.672871
Model ind 685 epoch 228 head A batch: 200 avg loss -2.167335 avg loss no lamb -2.167335 time 2019-02-24 08:25:13.474877
Model ind 685 epoch 228 head A batch: 300 avg loss -2.233582 avg loss no lamb -2.233582 time 2019-02-24 08:26:30.982531
Model ind 685 epoch 228 head A batch: 400 avg loss -2.155281 avg loss no lamb -2.155281 time 2019-02-24 08:27:47.979491
Pre: time 2019-02-24 08:29:20.342625: 
 	std: 0.006491712
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 229
Model ind 685 epoch 229 head B batch: 0 avg loss -2.197829 avg loss no lamb -2.197829 time 2019-02-24 08:29:22.275122
Model ind 685 epoch 229 head B batch: 100 avg loss -2.177576 avg loss no lamb -2.177576 time 2019-02-24 08:30:35.699804
Model ind 685 epoch 229 head B batch: 200 avg loss -2.205580 avg loss no lamb -2.205580 time 2019-02-24 08:31:50.193423
Model ind 685 epoch 229 head B batch: 300 avg loss -2.207938 avg loss no lamb -2.207938 time 2019-02-24 08:33:03.716676
Model ind 685 epoch 229 head B batch: 400 avg loss -2.211992 avg loss no lamb -2.211992 time 2019-02-24 08:34:16.977643
Model ind 685 epoch 229 head B batch: 0 avg loss -2.199755 avg loss no lamb -2.199755 time 2019-02-24 08:35:29.564039
Model ind 685 epoch 229 head B batch: 100 avg loss -2.222594 avg loss no lamb -2.222594 time 2019-02-24 08:36:44.305960
Model ind 685 epoch 229 head B batch: 200 avg loss -2.157595 avg loss no lamb -2.157595 time 2019-02-24 08:37:59.919987
Model ind 685 epoch 229 head B batch: 300 avg loss -2.208162 avg loss no lamb -2.208162 time 2019-02-24 08:39:17.947197
Model ind 685 epoch 229 head B batch: 400 avg loss -2.169445 avg loss no lamb -2.169445 time 2019-02-24 08:40:32.271092
Model ind 685 epoch 229 head A batch: 0 avg loss -2.209632 avg loss no lamb -2.209632 time 2019-02-24 08:41:40.409424
Model ind 685 epoch 229 head A batch: 100 avg loss -2.197639 avg loss no lamb -2.197639 time 2019-02-24 08:42:58.165509
Model ind 685 epoch 229 head A batch: 200 avg loss -2.217073 avg loss no lamb -2.217073 time 2019-02-24 08:44:12.730701
Model ind 685 epoch 229 head A batch: 300 avg loss -2.225347 avg loss no lamb -2.225347 time 2019-02-24 08:45:25.437169
Model ind 685 epoch 229 head A batch: 400 avg loss -2.181186 avg loss no lamb -2.181186 time 2019-02-24 08:46:39.234700
Pre: time 2019-02-24 08:48:08.457440: 
 	std: 0.006498983
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.99244285, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.99244285

Starting e_i: 230
Model ind 685 epoch 230 head B batch: 0 avg loss -2.206219 avg loss no lamb -2.206219 time 2019-02-24 08:48:10.355726
Model ind 685 epoch 230 head B batch: 100 avg loss -2.169312 avg loss no lamb -2.169312 time 2019-02-24 08:49:23.067270
Model ind 685 epoch 230 head B batch: 200 avg loss -2.198285 avg loss no lamb -2.198285 time 2019-02-24 08:50:35.465442
Model ind 685 epoch 230 head B batch: 300 avg loss -2.195617 avg loss no lamb -2.195617 time 2019-02-24 08:51:47.557911
Model ind 685 epoch 230 head B batch: 400 avg loss -2.212198 avg loss no lamb -2.212198 time 2019-02-24 08:53:03.148719
Model ind 685 epoch 230 head B batch: 0 avg loss -2.234529 avg loss no lamb -2.234529 time 2019-02-24 08:54:20.011362
Model ind 685 epoch 230 head B batch: 100 avg loss -2.181416 avg loss no lamb -2.181416 time 2019-02-24 08:55:34.939202
Model ind 685 epoch 230 head B batch: 200 avg loss -2.150782 avg loss no lamb -2.150782 time 2019-02-24 08:56:47.106874
Model ind 685 epoch 230 head B batch: 300 avg loss -2.200699 avg loss no lamb -2.200699 time 2019-02-24 08:58:02.897546
Model ind 685 epoch 230 head B batch: 400 avg loss -2.198805 avg loss no lamb -2.198805 time 2019-02-24 08:59:17.660012
Model ind 685 epoch 230 head A batch: 0 avg loss -2.203777 avg loss no lamb -2.203777 time 2019-02-24 09:00:31.988686
Model ind 685 epoch 230 head A batch: 100 avg loss -2.199295 avg loss no lamb -2.199295 time 2019-02-24 09:01:45.336740
Model ind 685 epoch 230 head A batch: 200 avg loss -2.220703 avg loss no lamb -2.220703 time 2019-02-24 09:02:58.435670
Model ind 685 epoch 230 head A batch: 300 avg loss -2.234323 avg loss no lamb -2.234323 time 2019-02-24 09:04:12.579632
Model ind 685 epoch 230 head A batch: 400 avg loss -2.185617 avg loss no lamb -2.185617 time 2019-02-24 09:05:25.831648
Pre: time 2019-02-24 09:06:59.201018: 
 	std: 0.006487057
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.979, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 231
Model ind 685 epoch 231 head B batch: 0 avg loss -2.209885 avg loss no lamb -2.209885 time 2019-02-24 09:07:01.036418
Model ind 685 epoch 231 head B batch: 100 avg loss -2.168421 avg loss no lamb -2.168421 time 2019-02-24 09:08:15.650575
Model ind 685 epoch 231 head B batch: 200 avg loss -2.190085 avg loss no lamb -2.190085 time 2019-02-24 09:09:27.411386
Model ind 685 epoch 231 head B batch: 300 avg loss -2.191272 avg loss no lamb -2.191272 time 2019-02-24 09:10:40.895717
Model ind 685 epoch 231 head B batch: 400 avg loss -2.175800 avg loss no lamb -2.175800 time 2019-02-24 09:11:53.228920
Model ind 685 epoch 231 head B batch: 0 avg loss -2.186515 avg loss no lamb -2.186515 time 2019-02-24 09:13:06.250936
Model ind 685 epoch 231 head B batch: 100 avg loss -2.234300 avg loss no lamb -2.234300 time 2019-02-24 09:14:20.192500
Model ind 685 epoch 231 head B batch: 200 avg loss -2.218089 avg loss no lamb -2.218089 time 2019-02-24 09:15:32.598785
Model ind 685 epoch 231 head B batch: 300 avg loss -2.196705 avg loss no lamb -2.196705 time 2019-02-24 09:16:45.745440
Model ind 685 epoch 231 head B batch: 400 avg loss -2.214952 avg loss no lamb -2.214952 time 2019-02-24 09:18:00.127368
Model ind 685 epoch 231 head A batch: 0 avg loss -2.212904 avg loss no lamb -2.212904 time 2019-02-24 09:19:15.631847
Model ind 685 epoch 231 head A batch: 100 avg loss -2.212473 avg loss no lamb -2.212473 time 2019-02-24 09:20:27.559063
Model ind 685 epoch 231 head A batch: 200 avg loss -2.202740 avg loss no lamb -2.202740 time 2019-02-24 09:21:44.023975
Model ind 685 epoch 231 head A batch: 300 avg loss -2.230163 avg loss no lamb -2.230163 time 2019-02-24 09:23:00.999250
Model ind 685 epoch 231 head A batch: 400 avg loss -2.190530 avg loss no lamb -2.190530 time 2019-02-24 09:24:12.218168
Pre: time 2019-02-24 09:25:32.527498: 
 	std: 0.006491991
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.99245715, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843429
	best: 0.99245715

Starting e_i: 232
Model ind 685 epoch 232 head B batch: 0 avg loss -2.230323 avg loss no lamb -2.230323 time 2019-02-24 09:25:34.632356
Model ind 685 epoch 232 head B batch: 100 avg loss -2.214813 avg loss no lamb -2.214813 time 2019-02-24 09:26:47.808531
Model ind 685 epoch 232 head B batch: 200 avg loss -2.194065 avg loss no lamb -2.194065 time 2019-02-24 09:28:00.125115
Model ind 685 epoch 232 head B batch: 300 avg loss -2.211948 avg loss no lamb -2.211948 time 2019-02-24 09:29:14.078163
Model ind 685 epoch 232 head B batch: 400 avg loss -2.162595 avg loss no lamb -2.162595 time 2019-02-24 09:30:28.596763
Model ind 685 epoch 232 head B batch: 0 avg loss -2.196654 avg loss no lamb -2.196654 time 2019-02-24 09:31:43.117110
Model ind 685 epoch 232 head B batch: 100 avg loss -2.188627 avg loss no lamb -2.188627 time 2019-02-24 09:32:55.908290
Model ind 685 epoch 232 head B batch: 200 avg loss -2.200958 avg loss no lamb -2.200958 time 2019-02-24 09:34:12.546933
Model ind 685 epoch 232 head B batch: 300 avg loss -2.242666 avg loss no lamb -2.242666 time 2019-02-24 09:35:26.577817
Model ind 685 epoch 232 head B batch: 400 avg loss -2.217718 avg loss no lamb -2.217718 time 2019-02-24 09:36:39.493735
Model ind 685 epoch 232 head A batch: 0 avg loss -2.187882 avg loss no lamb -2.187882 time 2019-02-24 09:37:55.177843
Model ind 685 epoch 232 head A batch: 100 avg loss -2.202858 avg loss no lamb -2.202858 time 2019-02-24 09:39:09.470352
Model ind 685 epoch 232 head A batch: 200 avg loss -2.222393 avg loss no lamb -2.222393 time 2019-02-24 09:40:23.565189
Model ind 685 epoch 232 head A batch: 300 avg loss -2.214336 avg loss no lamb -2.214336 time 2019-02-24 09:41:39.076794
Model ind 685 epoch 232 head A batch: 400 avg loss -2.185317 avg loss no lamb -2.185317 time 2019-02-24 09:42:55.605645
Pre: time 2019-02-24 09:44:25.165783: 
 	std: 0.0064849835
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	train_accs: [0.99245715, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98435146
	best: 0.99245715

Starting e_i: 233
Model ind 685 epoch 233 head B batch: 0 avg loss -2.207923 avg loss no lamb -2.207923 time 2019-02-24 09:44:27.266692
Model ind 685 epoch 233 head B batch: 100 avg loss -2.166838 avg loss no lamb -2.166838 time 2019-02-24 09:45:42.155373
Model ind 685 epoch 233 head B batch: 200 avg loss -2.216307 avg loss no lamb -2.216307 time 2019-02-24 09:46:55.543125
Model ind 685 epoch 233 head B batch: 300 avg loss -2.220778 avg loss no lamb -2.220778 time 2019-02-24 09:48:09.851181
Model ind 685 epoch 233 head B batch: 400 avg loss -2.193746 avg loss no lamb -2.193746 time 2019-02-24 09:49:21.854645
Model ind 685 epoch 233 head B batch: 0 avg loss -2.199372 avg loss no lamb -2.199372 time 2019-02-24 09:50:33.253501
Model ind 685 epoch 233 head B batch: 100 avg loss -2.224408 avg loss no lamb -2.224408 time 2019-02-24 09:51:44.803771
Model ind 685 epoch 233 head B batch: 200 avg loss -2.194870 avg loss no lamb -2.194870 time 2019-02-24 09:52:58.165876
Model ind 685 epoch 233 head B batch: 300 avg loss -2.251129 avg loss no lamb -2.251129 time 2019-02-24 09:54:13.091605
Model ind 685 epoch 233 head B batch: 400 avg loss -2.181254 avg loss no lamb -2.181254 time 2019-02-24 09:55:26.923520
Model ind 685 epoch 233 head A batch: 0 avg loss -2.211832 avg loss no lamb -2.211832 time 2019-02-24 09:56:40.227848
Model ind 685 epoch 233 head A batch: 100 avg loss -2.181489 avg loss no lamb -2.181489 time 2019-02-24 09:57:52.993104
Model ind 685 epoch 233 head A batch: 200 avg loss -2.196520 avg loss no lamb -2.196520 time 2019-02-24 09:59:07.284496
Model ind 685 epoch 233 head A batch: 300 avg loss -2.214947 avg loss no lamb -2.214947 time 2019-02-24 10:00:25.662545
Model ind 685 epoch 233 head A batch: 400 avg loss -2.168621 avg loss no lamb -2.168621 time 2019-02-24 10:01:40.682544
Pre: time 2019-02-24 10:03:10.392842: 
 	std: 0.0064707194
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	train_accs: [0.9923286, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98426855
	best: 0.9923286

Starting e_i: 234
Model ind 685 epoch 234 head B batch: 0 avg loss -2.187190 avg loss no lamb -2.187190 time 2019-02-24 10:03:12.221542
Model ind 685 epoch 234 head B batch: 100 avg loss -2.165909 avg loss no lamb -2.165909 time 2019-02-24 10:04:27.588668
Model ind 685 epoch 234 head B batch: 200 avg loss -2.155303 avg loss no lamb -2.155303 time 2019-02-24 10:05:40.982954
Model ind 685 epoch 234 head B batch: 300 avg loss -2.192549 avg loss no lamb -2.192549 time 2019-02-24 10:06:54.489619
Model ind 685 epoch 234 head B batch: 400 avg loss -2.213283 avg loss no lamb -2.213283 time 2019-02-24 10:08:02.350572
Model ind 685 epoch 234 head B batch: 0 avg loss -2.219891 avg loss no lamb -2.219891 time 2019-02-24 10:09:16.007598
Model ind 685 epoch 234 head B batch: 100 avg loss -2.185023 avg loss no lamb -2.185023 time 2019-02-24 10:10:29.747627
Model ind 685 epoch 234 head B batch: 200 avg loss -2.196400 avg loss no lamb -2.196400 time 2019-02-24 10:11:44.966778
Model ind 685 epoch 234 head B batch: 300 avg loss -2.191603 avg loss no lamb -2.191603 time 2019-02-24 10:12:57.130046
Model ind 685 epoch 234 head B batch: 400 avg loss -2.205870 avg loss no lamb -2.205870 time 2019-02-24 10:14:09.117688
Model ind 685 epoch 234 head A batch: 0 avg loss -2.213937 avg loss no lamb -2.213937 time 2019-02-24 10:15:22.313225
Model ind 685 epoch 234 head A batch: 100 avg loss -2.175839 avg loss no lamb -2.175839 time 2019-02-24 10:16:38.075960
Model ind 685 epoch 234 head A batch: 200 avg loss -2.225523 avg loss no lamb -2.225523 time 2019-02-24 10:17:50.269956
Model ind 685 epoch 234 head A batch: 300 avg loss -2.227904 avg loss no lamb -2.227904 time 2019-02-24 10:19:04.328019
Model ind 685 epoch 234 head A batch: 400 avg loss -2.181126 avg loss no lamb -2.181126 time 2019-02-24 10:20:18.958478
Pre: time 2019-02-24 10:21:48.546096: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	train_accs: [0.99237144, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842771
	best: 0.99237144

Starting e_i: 235
Model ind 685 epoch 235 head B batch: 0 avg loss -2.174073 avg loss no lamb -2.174073 time 2019-02-24 10:21:50.384631
Model ind 685 epoch 235 head B batch: 100 avg loss -2.189445 avg loss no lamb -2.189445 time 2019-02-24 10:23:06.998970
Model ind 685 epoch 235 head B batch: 200 avg loss -2.220644 avg loss no lamb -2.220644 time 2019-02-24 10:24:21.330214
Model ind 685 epoch 235 head B batch: 300 avg loss -2.187804 avg loss no lamb -2.187804 time 2019-02-24 10:25:37.892637
Model ind 685 epoch 235 head B batch: 400 avg loss -2.188807 avg loss no lamb -2.188807 time 2019-02-24 10:26:53.816785
Model ind 685 epoch 235 head B batch: 0 avg loss -2.221440 avg loss no lamb -2.221440 time 2019-02-24 10:28:07.232781
Model ind 685 epoch 235 head B batch: 100 avg loss -2.198162 avg loss no lamb -2.198162 time 2019-02-24 10:29:21.112052
Model ind 685 epoch 235 head B batch: 200 avg loss -2.181606 avg loss no lamb -2.181606 time 2019-02-24 10:30:35.977039
Model ind 685 epoch 235 head B batch: 300 avg loss -2.230485 avg loss no lamb -2.230485 time 2019-02-24 10:31:52.814489
Model ind 685 epoch 235 head B batch: 400 avg loss -2.175218 avg loss no lamb -2.175218 time 2019-02-24 10:33:06.030683
Model ind 685 epoch 235 head A batch: 0 avg loss -2.185752 avg loss no lamb -2.185752 time 2019-02-24 10:34:19.813663
Model ind 685 epoch 235 head A batch: 100 avg loss -2.222191 avg loss no lamb -2.222191 time 2019-02-24 10:35:31.833445
Model ind 685 epoch 235 head A batch: 200 avg loss -2.179826 avg loss no lamb -2.179826 time 2019-02-24 10:36:47.569552
Model ind 685 epoch 235 head A batch: 300 avg loss -2.224167 avg loss no lamb -2.224167 time 2019-02-24 10:38:03.140528
Model ind 685 epoch 235 head A batch: 400 avg loss -2.209240 avg loss no lamb -2.209240 time 2019-02-24 10:39:19.217288
Pre: time 2019-02-24 10:40:49.305489: 
 	std: 0.006483747
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 236
Model ind 685 epoch 236 head B batch: 0 avg loss -2.222885 avg loss no lamb -2.222885 time 2019-02-24 10:40:51.109101
Model ind 685 epoch 236 head B batch: 100 avg loss -2.216040 avg loss no lamb -2.216040 time 2019-02-24 10:42:03.725435
Model ind 685 epoch 236 head B batch: 200 avg loss -2.215370 avg loss no lamb -2.215370 time 2019-02-24 10:43:17.057250
Model ind 685 epoch 236 head B batch: 300 avg loss -2.199292 avg loss no lamb -2.199292 time 2019-02-24 10:44:29.909955
Model ind 685 epoch 236 head B batch: 400 avg loss -2.172335 avg loss no lamb -2.172335 time 2019-02-24 10:45:42.819440
Model ind 685 epoch 236 head B batch: 0 avg loss -2.212896 avg loss no lamb -2.212896 time 2019-02-24 10:46:54.666906
Model ind 685 epoch 236 head B batch: 100 avg loss -2.215176 avg loss no lamb -2.215176 time 2019-02-24 10:48:08.543450
Model ind 685 epoch 236 head B batch: 200 avg loss -2.205160 avg loss no lamb -2.205160 time 2019-02-24 10:49:22.625592
Model ind 685 epoch 236 head B batch: 300 avg loss -2.169250 avg loss no lamb -2.169250 time 2019-02-24 10:50:35.733529
Model ind 685 epoch 236 head B batch: 400 avg loss -2.189795 avg loss no lamb -2.189795 time 2019-02-24 10:51:37.564970
Model ind 685 epoch 236 head A batch: 0 avg loss -2.242506 avg loss no lamb -2.242506 time 2019-02-24 10:52:50.297464
Model ind 685 epoch 236 head A batch: 100 avg loss -2.233950 avg loss no lamb -2.233950 time 2019-02-24 10:54:07.282027
Model ind 685 epoch 236 head A batch: 200 avg loss -2.179433 avg loss no lamb -2.179433 time 2019-02-24 10:55:20.076258
Model ind 685 epoch 236 head A batch: 300 avg loss -2.201960 avg loss no lamb -2.201960 time 2019-02-24 10:56:33.566849
Model ind 685 epoch 236 head A batch: 400 avg loss -2.189890 avg loss no lamb -2.189890 time 2019-02-24 10:57:46.563946
Pre: time 2019-02-24 10:59:18.976508: 
 	std: 0.006513827
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9789857, 0.99215716, 0.9789857, 0.979]
	train_accs: [0.9924143, 0.9789857, 0.99215716, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 237
Model ind 685 epoch 237 head B batch: 0 avg loss -2.189761 avg loss no lamb -2.189761 time 2019-02-24 10:59:20.851775
Model ind 685 epoch 237 head B batch: 100 avg loss -2.243366 avg loss no lamb -2.243366 time 2019-02-24 11:00:33.946896
Model ind 685 epoch 237 head B batch: 200 avg loss -2.181908 avg loss no lamb -2.181908 time 2019-02-24 11:01:50.529624
Model ind 685 epoch 237 head B batch: 300 avg loss -2.207658 avg loss no lamb -2.207658 time 2019-02-24 11:03:06.184960
Model ind 685 epoch 237 head B batch: 400 avg loss -2.210057 avg loss no lamb -2.210057 time 2019-02-24 11:04:20.531504
Model ind 685 epoch 237 head B batch: 0 avg loss -2.179417 avg loss no lamb -2.179417 time 2019-02-24 11:05:33.346862
Model ind 685 epoch 237 head B batch: 100 avg loss -2.207528 avg loss no lamb -2.207528 time 2019-02-24 11:06:48.788578
Model ind 685 epoch 237 head B batch: 200 avg loss -2.223890 avg loss no lamb -2.223890 time 2019-02-24 11:08:00.041347
Model ind 685 epoch 237 head B batch: 300 avg loss -2.192798 avg loss no lamb -2.192798 time 2019-02-24 11:09:12.166932
Model ind 685 epoch 237 head B batch: 400 avg loss -2.186311 avg loss no lamb -2.186311 time 2019-02-24 11:10:26.329242
Model ind 685 epoch 237 head A batch: 0 avg loss -2.202107 avg loss no lamb -2.202107 time 2019-02-24 11:11:38.410920
Model ind 685 epoch 237 head A batch: 100 avg loss -2.193002 avg loss no lamb -2.193002 time 2019-02-24 11:12:52.919161
Model ind 685 epoch 237 head A batch: 200 avg loss -2.197710 avg loss no lamb -2.197710 time 2019-02-24 11:14:06.735518
Model ind 685 epoch 237 head A batch: 300 avg loss -2.184726 avg loss no lamb -2.184726 time 2019-02-24 11:15:20.373271
Model ind 685 epoch 237 head A batch: 400 avg loss -2.174011 avg loss no lamb -2.174011 time 2019-02-24 11:16:34.793052
Pre: time 2019-02-24 11:18:04.395511: 
 	std: 0.006491712
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 238
Model ind 685 epoch 238 head B batch: 0 avg loss -2.211081 avg loss no lamb -2.211081 time 2019-02-24 11:18:06.554574
Model ind 685 epoch 238 head B batch: 100 avg loss -2.202592 avg loss no lamb -2.202592 time 2019-02-24 11:19:19.526844
Model ind 685 epoch 238 head B batch: 200 avg loss -2.205956 avg loss no lamb -2.205956 time 2019-02-24 11:20:32.874416
Model ind 685 epoch 238 head B batch: 300 avg loss -2.217755 avg loss no lamb -2.217755 time 2019-02-24 11:21:46.363701
Model ind 685 epoch 238 head B batch: 400 avg loss -2.156362 avg loss no lamb -2.156362 time 2019-02-24 11:22:58.885559
Model ind 685 epoch 238 head B batch: 0 avg loss -2.227155 avg loss no lamb -2.227155 time 2019-02-24 11:24:13.254047
Model ind 685 epoch 238 head B batch: 100 avg loss -2.228824 avg loss no lamb -2.228824 time 2019-02-24 11:25:29.824657
Model ind 685 epoch 238 head B batch: 200 avg loss -2.194988 avg loss no lamb -2.194988 time 2019-02-24 11:26:45.492100
Model ind 685 epoch 238 head B batch: 300 avg loss -2.230390 avg loss no lamb -2.230390 time 2019-02-24 11:27:59.625192
Model ind 685 epoch 238 head B batch: 400 avg loss -2.212723 avg loss no lamb -2.212723 time 2019-02-24 11:29:13.948715
Model ind 685 epoch 238 head A batch: 0 avg loss -2.231420 avg loss no lamb -2.231420 time 2019-02-24 11:30:27.884477
Model ind 685 epoch 238 head A batch: 100 avg loss -2.214961 avg loss no lamb -2.214961 time 2019-02-24 11:31:40.043779
Model ind 685 epoch 238 head A batch: 200 avg loss -2.198058 avg loss no lamb -2.198058 time 2019-02-24 11:32:51.898458
Model ind 685 epoch 238 head A batch: 300 avg loss -2.176831 avg loss no lamb -2.176831 time 2019-02-24 11:34:04.156409
Model ind 685 epoch 238 head A batch: 400 avg loss -2.170532 avg loss no lamb -2.170532 time 2019-02-24 11:35:07.316940
Pre: time 2019-02-24 11:36:36.348640: 
 	std: 0.006483481
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99212855, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790143, 0.99212855, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9923857

Starting e_i: 239
Model ind 685 epoch 239 head B batch: 0 avg loss -2.179884 avg loss no lamb -2.179884 time 2019-02-24 11:36:38.450611
Model ind 685 epoch 239 head B batch: 100 avg loss -2.185596 avg loss no lamb -2.185596 time 2019-02-24 11:37:51.177447
Model ind 685 epoch 239 head B batch: 200 avg loss -2.224965 avg loss no lamb -2.224965 time 2019-02-24 11:39:04.890699
Model ind 685 epoch 239 head B batch: 300 avg loss -2.224724 avg loss no lamb -2.224724 time 2019-02-24 11:40:19.795923
Model ind 685 epoch 239 head B batch: 400 avg loss -2.196849 avg loss no lamb -2.196849 time 2019-02-24 11:41:31.992008
Model ind 685 epoch 239 head B batch: 0 avg loss -2.210865 avg loss no lamb -2.210865 time 2019-02-24 11:42:45.403232
Model ind 685 epoch 239 head B batch: 100 avg loss -2.181756 avg loss no lamb -2.181756 time 2019-02-24 11:43:57.829276
Model ind 685 epoch 239 head B batch: 200 avg loss -2.214957 avg loss no lamb -2.214957 time 2019-02-24 11:45:12.729885
Model ind 685 epoch 239 head B batch: 300 avg loss -2.168161 avg loss no lamb -2.168161 time 2019-02-24 11:46:29.335487
Model ind 685 epoch 239 head B batch: 400 avg loss -2.200284 avg loss no lamb -2.200284 time 2019-02-24 11:47:47.540953
Model ind 685 epoch 239 head A batch: 0 avg loss -2.187188 avg loss no lamb -2.187188 time 2019-02-24 11:49:04.935697
Model ind 685 epoch 239 head A batch: 100 avg loss -2.217592 avg loss no lamb -2.217592 time 2019-02-24 11:50:19.837775
Model ind 685 epoch 239 head A batch: 200 avg loss -2.132722 avg loss no lamb -2.132722 time 2019-02-24 11:51:35.155295
Model ind 685 epoch 239 head A batch: 300 avg loss -2.206336 avg loss no lamb -2.206336 time 2019-02-24 11:52:51.872248
Model ind 685 epoch 239 head A batch: 400 avg loss -2.191969 avg loss no lamb -2.191969 time 2019-02-24 11:54:09.187055
Pre: time 2019-02-24 11:55:41.406590: 
 	std: 0.006483747
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 240
Model ind 685 epoch 240 head B batch: 0 avg loss -2.196307 avg loss no lamb -2.196307 time 2019-02-24 11:55:43.536793
Model ind 685 epoch 240 head B batch: 100 avg loss -2.183823 avg loss no lamb -2.183823 time 2019-02-24 11:57:02.104261
Model ind 685 epoch 240 head B batch: 200 avg loss -2.217425 avg loss no lamb -2.217425 time 2019-02-24 11:58:16.006911
Model ind 685 epoch 240 head B batch: 300 avg loss -2.183758 avg loss no lamb -2.183758 time 2019-02-24 11:59:30.320798
Model ind 685 epoch 240 head B batch: 400 avg loss -2.191779 avg loss no lamb -2.191779 time 2019-02-24 12:00:43.494314
Model ind 685 epoch 240 head B batch: 0 avg loss -2.225774 avg loss no lamb -2.225774 time 2019-02-24 12:02:03.316940
Model ind 685 epoch 240 head B batch: 100 avg loss -2.220728 avg loss no lamb -2.220728 time 2019-02-24 12:03:19.197519
Model ind 685 epoch 240 head B batch: 200 avg loss -2.213396 avg loss no lamb -2.213396 time 2019-02-24 12:04:35.918003
Model ind 685 epoch 240 head B batch: 300 avg loss -2.210912 avg loss no lamb -2.210912 time 2019-02-24 12:05:51.264951
Model ind 685 epoch 240 head B batch: 400 avg loss -2.192101 avg loss no lamb -2.192101 time 2019-02-24 12:07:10.314728
Model ind 685 epoch 240 head A batch: 0 avg loss -2.196244 avg loss no lamb -2.196244 time 2019-02-24 12:08:23.840000
Model ind 685 epoch 240 head A batch: 100 avg loss -2.209437 avg loss no lamb -2.209437 time 2019-02-24 12:09:39.805132
Model ind 685 epoch 240 head A batch: 200 avg loss -2.220495 avg loss no lamb -2.220495 time 2019-02-24 12:10:54.557461
Model ind 685 epoch 240 head A batch: 300 avg loss -2.226879 avg loss no lamb -2.226879 time 2019-02-24 12:12:10.047791
Model ind 685 epoch 240 head A batch: 400 avg loss -2.161266 avg loss no lamb -2.161266 time 2019-02-24 12:13:26.485695
Pre: time 2019-02-24 12:14:59.830616: 
 	std: 0.0064907395
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789857, 0.99207145, 0.9789857, 0.9789714]
	train_accs: [0.9923857, 0.9789857, 0.99207145, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98428
	best: 0.9923857

Starting e_i: 241
Model ind 685 epoch 241 head B batch: 0 avg loss -2.201399 avg loss no lamb -2.201399 time 2019-02-24 12:15:02.862559
Model ind 685 epoch 241 head B batch: 100 avg loss -2.232793 avg loss no lamb -2.232793 time 2019-02-24 12:16:19.741556
Model ind 685 epoch 241 head B batch: 200 avg loss -2.248634 avg loss no lamb -2.248634 time 2019-02-24 12:17:32.195400
Model ind 685 epoch 241 head B batch: 300 avg loss -2.209972 avg loss no lamb -2.209972 time 2019-02-24 12:18:44.225526
Model ind 685 epoch 241 head B batch: 400 avg loss -2.193247 avg loss no lamb -2.193247 time 2019-02-24 12:20:00.769046
Model ind 685 epoch 241 head B batch: 0 avg loss -2.224389 avg loss no lamb -2.224389 time 2019-02-24 12:21:15.681314
Model ind 685 epoch 241 head B batch: 100 avg loss -2.210195 avg loss no lamb -2.210195 time 2019-02-24 12:22:32.712310
Model ind 685 epoch 241 head B batch: 200 avg loss -2.211801 avg loss no lamb -2.211801 time 2019-02-24 12:23:49.485966
Model ind 685 epoch 241 head B batch: 300 avg loss -2.207101 avg loss no lamb -2.207101 time 2019-02-24 12:25:05.987138
Model ind 685 epoch 241 head B batch: 400 avg loss -2.181790 avg loss no lamb -2.181790 time 2019-02-24 12:26:20.195715
Model ind 685 epoch 241 head A batch: 0 avg loss -2.240543 avg loss no lamb -2.240543 time 2019-02-24 12:27:35.404596
Model ind 685 epoch 241 head A batch: 100 avg loss -2.205589 avg loss no lamb -2.205589 time 2019-02-24 12:28:52.300616
Model ind 685 epoch 241 head A batch: 200 avg loss -2.188140 avg loss no lamb -2.188140 time 2019-02-24 12:30:07.466780
Model ind 685 epoch 241 head A batch: 300 avg loss -2.245423 avg loss no lamb -2.245423 time 2019-02-24 12:31:22.746429
Model ind 685 epoch 241 head A batch: 400 avg loss -2.190712 avg loss no lamb -2.190712 time 2019-02-24 12:32:38.552554
Pre: time 2019-02-24 12:34:14.550875: 
 	std: 0.006505727
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.99215716, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.99215716, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432577
	best: 0.9924286

Starting e_i: 242
Model ind 685 epoch 242 head B batch: 0 avg loss -2.164587 avg loss no lamb -2.164587 time 2019-02-24 12:34:16.420323
Model ind 685 epoch 242 head B batch: 100 avg loss -2.219363 avg loss no lamb -2.219363 time 2019-02-24 12:35:33.409741
Model ind 685 epoch 242 head B batch: 200 avg loss -2.201224 avg loss no lamb -2.201224 time 2019-02-24 12:36:48.528158
Model ind 685 epoch 242 head B batch: 300 avg loss -2.241829 avg loss no lamb -2.241829 time 2019-02-24 12:38:03.011452
Model ind 685 epoch 242 head B batch: 400 avg loss -2.185644 avg loss no lamb -2.185644 time 2019-02-24 12:39:20.061507
Model ind 685 epoch 242 head B batch: 0 avg loss -2.230494 avg loss no lamb -2.230494 time 2019-02-24 12:40:37.778604
Model ind 685 epoch 242 head B batch: 100 avg loss -2.209301 avg loss no lamb -2.209301 time 2019-02-24 12:41:51.552348
Model ind 685 epoch 242 head B batch: 200 avg loss -2.204935 avg loss no lamb -2.204935 time 2019-02-24 12:43:07.107760
Model ind 685 epoch 242 head B batch: 300 avg loss -2.241517 avg loss no lamb -2.241517 time 2019-02-24 12:44:22.290383
Model ind 685 epoch 242 head B batch: 400 avg loss -2.195579 avg loss no lamb -2.195579 time 2019-02-24 12:45:37.525425
Model ind 685 epoch 242 head A batch: 0 avg loss -2.181526 avg loss no lamb -2.181526 time 2019-02-24 12:46:52.928569
Model ind 685 epoch 242 head A batch: 100 avg loss -2.185752 avg loss no lamb -2.185752 time 2019-02-24 12:48:08.615675
Model ind 685 epoch 242 head A batch: 200 avg loss -2.202343 avg loss no lamb -2.202343 time 2019-02-24 12:49:23.316147
Model ind 685 epoch 242 head A batch: 300 avg loss -2.244129 avg loss no lamb -2.244129 time 2019-02-24 12:50:39.135159
Model ind 685 epoch 242 head A batch: 400 avg loss -2.178957 avg loss no lamb -2.178957 time 2019-02-24 12:51:56.666514
Pre: time 2019-02-24 12:53:27.298186: 
 	std: 0.0064778375
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.99205714, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98427427
	best: 0.99235713

Starting e_i: 243
Model ind 685 epoch 243 head B batch: 0 avg loss -2.163539 avg loss no lamb -2.163539 time 2019-02-24 12:53:29.341747
Model ind 685 epoch 243 head B batch: 100 avg loss -2.218381 avg loss no lamb -2.218381 time 2019-02-24 12:54:42.587484
Model ind 685 epoch 243 head B batch: 200 avg loss -2.221669 avg loss no lamb -2.221669 time 2019-02-24 12:55:57.981298
Model ind 685 epoch 243 head B batch: 300 avg loss -2.198529 avg loss no lamb -2.198529 time 2019-02-24 12:57:14.912280
Model ind 685 epoch 243 head B batch: 400 avg loss -2.214382 avg loss no lamb -2.214382 time 2019-02-24 12:58:28.673222
Model ind 685 epoch 243 head B batch: 0 avg loss -2.216136 avg loss no lamb -2.216136 time 2019-02-24 12:59:44.452245
Model ind 685 epoch 243 head B batch: 100 avg loss -2.194429 avg loss no lamb -2.194429 time 2019-02-24 13:00:53.409049
Model ind 685 epoch 243 head B batch: 200 avg loss -2.187365 avg loss no lamb -2.187365 time 2019-02-24 13:02:10.795167
Model ind 685 epoch 243 head B batch: 300 avg loss -2.185591 avg loss no lamb -2.185591 time 2019-02-24 13:03:27.759524
Model ind 685 epoch 243 head B batch: 400 avg loss -2.186845 avg loss no lamb -2.186845 time 2019-02-24 13:04:41.369089
Model ind 685 epoch 243 head A batch: 0 avg loss -2.209320 avg loss no lamb -2.209320 time 2019-02-24 13:05:53.401056
Model ind 685 epoch 243 head A batch: 100 avg loss -2.172413 avg loss no lamb -2.172413 time 2019-02-24 13:07:07.783367
Model ind 685 epoch 243 head A batch: 200 avg loss -2.187681 avg loss no lamb -2.187681 time 2019-02-24 13:08:21.534245
Model ind 685 epoch 243 head A batch: 300 avg loss -2.227368 avg loss no lamb -2.227368 time 2019-02-24 13:09:35.884430
Model ind 685 epoch 243 head A batch: 400 avg loss -2.172298 avg loss no lamb -2.172298 time 2019-02-24 13:10:49.102860
Pre: time 2019-02-24 13:12:20.228309: 
 	std: 0.006482664
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.99207145, 0.9790143, 0.979]
	train_accs: [0.9924, 0.979, 0.99207145, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9924

Starting e_i: 244
Model ind 685 epoch 244 head B batch: 0 avg loss -2.190175 avg loss no lamb -2.190175 time 2019-02-24 13:12:22.284563
Model ind 685 epoch 244 head B batch: 100 avg loss -2.195974 avg loss no lamb -2.195974 time 2019-02-24 13:13:36.793404
Model ind 685 epoch 244 head B batch: 200 avg loss -2.243431 avg loss no lamb -2.243431 time 2019-02-24 13:14:48.934627
Model ind 685 epoch 244 head B batch: 300 avg loss -2.166154 avg loss no lamb -2.166154 time 2019-02-24 13:16:03.559527
Model ind 685 epoch 244 head B batch: 400 avg loss -2.202785 avg loss no lamb -2.202785 time 2019-02-24 13:17:20.282794
Model ind 685 epoch 244 head B batch: 0 avg loss -2.199318 avg loss no lamb -2.199318 time 2019-02-24 13:18:36.110887
Model ind 685 epoch 244 head B batch: 100 avg loss -2.196670 avg loss no lamb -2.196670 time 2019-02-24 13:19:48.655315
Model ind 685 epoch 244 head B batch: 200 avg loss -2.222510 avg loss no lamb -2.222510 time 2019-02-24 13:21:03.696228
Model ind 685 epoch 244 head B batch: 300 avg loss -2.220404 avg loss no lamb -2.220404 time 2019-02-24 13:22:15.322902
Model ind 685 epoch 244 head B batch: 400 avg loss -2.206040 avg loss no lamb -2.206040 time 2019-02-24 13:23:27.798715
Model ind 685 epoch 244 head A batch: 0 avg loss -2.208057 avg loss no lamb -2.208057 time 2019-02-24 13:24:45.317888
Model ind 685 epoch 244 head A batch: 100 avg loss -2.197601 avg loss no lamb -2.197601 time 2019-02-24 13:25:59.400314
Model ind 685 epoch 244 head A batch: 200 avg loss -2.195050 avg loss no lamb -2.195050 time 2019-02-24 13:27:10.957314
Model ind 685 epoch 244 head A batch: 300 avg loss -2.200502 avg loss no lamb -2.200502 time 2019-02-24 13:28:23.243323
Model ind 685 epoch 244 head A batch: 400 avg loss -2.183436 avg loss no lamb -2.183436 time 2019-02-24 13:29:40.813606
Pre: time 2019-02-24 13:31:10.498954: 
 	std: 0.006517534
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789571, 0.9921, 0.9789714, 0.97891426]
	train_accs: [0.9924, 0.9789571, 0.9921, 0.9789714, 0.97891426]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.98426855
	best: 0.9924

Starting e_i: 245
Model ind 685 epoch 245 head B batch: 0 avg loss -2.186810 avg loss no lamb -2.186810 time 2019-02-24 13:31:12.634283
Model ind 685 epoch 245 head B batch: 100 avg loss -2.219506 avg loss no lamb -2.219506 time 2019-02-24 13:32:24.580591
Model ind 685 epoch 245 head B batch: 200 avg loss -2.217597 avg loss no lamb -2.217597 time 2019-02-24 13:33:40.066248
Model ind 685 epoch 245 head B batch: 300 avg loss -2.187947 avg loss no lamb -2.187947 time 2019-02-24 13:34:57.254522
Model ind 685 epoch 245 head B batch: 400 avg loss -2.139561 avg loss no lamb -2.139561 time 2019-02-24 13:36:14.213166
Model ind 685 epoch 245 head B batch: 0 avg loss -2.200190 avg loss no lamb -2.200190 time 2019-02-24 13:37:28.034585
Model ind 685 epoch 245 head B batch: 100 avg loss -2.210574 avg loss no lamb -2.210574 time 2019-02-24 13:38:41.033780
Model ind 685 epoch 245 head B batch: 200 avg loss -2.206010 avg loss no lamb -2.206010 time 2019-02-24 13:39:56.828308
Model ind 685 epoch 245 head B batch: 300 avg loss -2.229692 avg loss no lamb -2.229692 time 2019-02-24 13:41:09.001144
Model ind 685 epoch 245 head B batch: 400 avg loss -2.197523 avg loss no lamb -2.197523 time 2019-02-24 13:42:21.967250
Model ind 685 epoch 245 head A batch: 0 avg loss -2.196312 avg loss no lamb -2.196312 time 2019-02-24 13:43:39.285557
Model ind 685 epoch 245 head A batch: 100 avg loss -2.235217 avg loss no lamb -2.235217 time 2019-02-24 13:44:42.632146
Model ind 685 epoch 245 head A batch: 200 avg loss -2.189200 avg loss no lamb -2.189200 time 2019-02-24 13:45:55.759104
Model ind 685 epoch 245 head A batch: 300 avg loss -2.244817 avg loss no lamb -2.244817 time 2019-02-24 13:47:12.625323
Model ind 685 epoch 245 head A batch: 400 avg loss -2.168250 avg loss no lamb -2.168250 time 2019-02-24 13:48:26.012537
Pre: time 2019-02-24 13:49:53.917699: 
 	std: 0.0064779767
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9920857, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 246
Model ind 685 epoch 246 head B batch: 0 avg loss -2.162292 avg loss no lamb -2.162292 time 2019-02-24 13:49:55.793615
Model ind 685 epoch 246 head B batch: 100 avg loss -2.224385 avg loss no lamb -2.224385 time 2019-02-24 13:51:09.644568
Model ind 685 epoch 246 head B batch: 200 avg loss -2.216137 avg loss no lamb -2.216137 time 2019-02-24 13:52:24.062530
Model ind 685 epoch 246 head B batch: 300 avg loss -2.201038 avg loss no lamb -2.201038 time 2019-02-24 13:53:40.123089
Model ind 685 epoch 246 head B batch: 400 avg loss -2.196201 avg loss no lamb -2.196201 time 2019-02-24 13:54:53.941016
Model ind 685 epoch 246 head B batch: 0 avg loss -2.172969 avg loss no lamb -2.172969 time 2019-02-24 13:56:09.680735
Model ind 685 epoch 246 head B batch: 100 avg loss -2.209141 avg loss no lamb -2.209141 time 2019-02-24 13:57:24.001655
Model ind 685 epoch 246 head B batch: 200 avg loss -2.220576 avg loss no lamb -2.220576 time 2019-02-24 13:58:38.463850
Model ind 685 epoch 246 head B batch: 300 avg loss -2.192965 avg loss no lamb -2.192965 time 2019-02-24 13:59:51.718568
Model ind 685 epoch 246 head B batch: 400 avg loss -2.203398 avg loss no lamb -2.203398 time 2019-02-24 14:01:04.611911
Model ind 685 epoch 246 head A batch: 0 avg loss -2.192199 avg loss no lamb -2.192199 time 2019-02-24 14:02:18.562172
Model ind 685 epoch 246 head A batch: 100 avg loss -2.196558 avg loss no lamb -2.196558 time 2019-02-24 14:03:30.460802
Model ind 685 epoch 246 head A batch: 200 avg loss -2.223625 avg loss no lamb -2.223625 time 2019-02-24 14:04:44.553281
Model ind 685 epoch 246 head A batch: 300 avg loss -2.247561 avg loss no lamb -2.247561 time 2019-02-24 14:05:59.096718
Model ind 685 epoch 246 head A batch: 400 avg loss -2.205240 avg loss no lamb -2.205240 time 2019-02-24 14:07:12.966794
Pre: time 2019-02-24 14:08:43.027644: 
 	std: 0.0064870566
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9921143, 0.979, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9921143, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 247
Model ind 685 epoch 247 head B batch: 0 avg loss -2.209328 avg loss no lamb -2.209328 time 2019-02-24 14:08:44.943749
Model ind 685 epoch 247 head B batch: 100 avg loss -2.197903 avg loss no lamb -2.197903 time 2019-02-24 14:09:59.540960
Model ind 685 epoch 247 head B batch: 200 avg loss -2.172588 avg loss no lamb -2.172588 time 2019-02-24 14:11:13.344129
Model ind 685 epoch 247 head B batch: 300 avg loss -2.208567 avg loss no lamb -2.208567 time 2019-02-24 14:12:26.946204
Model ind 685 epoch 247 head B batch: 400 avg loss -2.186320 avg loss no lamb -2.186320 time 2019-02-24 14:13:40.481497
Model ind 685 epoch 247 head B batch: 0 avg loss -2.201545 avg loss no lamb -2.201545 time 2019-02-24 14:14:55.253254
Model ind 685 epoch 247 head B batch: 100 avg loss -2.175609 avg loss no lamb -2.175609 time 2019-02-24 14:16:08.213493
Model ind 685 epoch 247 head B batch: 200 avg loss -2.157046 avg loss no lamb -2.157046 time 2019-02-24 14:17:21.091282
Model ind 685 epoch 247 head B batch: 300 avg loss -2.189603 avg loss no lamb -2.189603 time 2019-02-24 14:18:33.800609
Model ind 685 epoch 247 head B batch: 400 avg loss -2.145581 avg loss no lamb -2.145581 time 2019-02-24 14:19:47.256950
Model ind 685 epoch 247 head A batch: 0 avg loss -2.224650 avg loss no lamb -2.224650 time 2019-02-24 14:20:59.674701
Model ind 685 epoch 247 head A batch: 100 avg loss -2.165728 avg loss no lamb -2.165728 time 2019-02-24 14:22:14.844920
Model ind 685 epoch 247 head A batch: 200 avg loss -2.215469 avg loss no lamb -2.215469 time 2019-02-24 14:23:28.381169
Model ind 685 epoch 247 head A batch: 300 avg loss -2.247777 avg loss no lamb -2.247777 time 2019-02-24 14:24:42.459664
Model ind 685 epoch 247 head A batch: 400 avg loss -2.212675 avg loss no lamb -2.212675 time 2019-02-24 14:25:55.372111
Pre: time 2019-02-24 14:27:22.531916: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.9924286

Starting e_i: 248
Model ind 685 epoch 248 head B batch: 0 avg loss -2.225826 avg loss no lamb -2.225826 time 2019-02-24 14:27:24.388242
Model ind 685 epoch 248 head B batch: 100 avg loss -2.187889 avg loss no lamb -2.187889 time 2019-02-24 14:28:30.200826
Model ind 685 epoch 248 head B batch: 200 avg loss -2.194746 avg loss no lamb -2.194746 time 2019-02-24 14:29:42.794367
Model ind 685 epoch 248 head B batch: 300 avg loss -2.196740 avg loss no lamb -2.196740 time 2019-02-24 14:30:55.635825
Model ind 685 epoch 248 head B batch: 400 avg loss -2.199074 avg loss no lamb -2.199074 time 2019-02-24 14:32:10.982134
Model ind 685 epoch 248 head B batch: 0 avg loss -2.173394 avg loss no lamb -2.173394 time 2019-02-24 14:33:25.153271
Model ind 685 epoch 248 head B batch: 100 avg loss -2.159629 avg loss no lamb -2.159629 time 2019-02-24 14:34:39.013292
Model ind 685 epoch 248 head B batch: 200 avg loss -2.237449 avg loss no lamb -2.237449 time 2019-02-24 14:35:51.551845
Model ind 685 epoch 248 head B batch: 300 avg loss -2.185237 avg loss no lamb -2.185237 time 2019-02-24 14:37:05.514646
Model ind 685 epoch 248 head B batch: 400 avg loss -2.167989 avg loss no lamb -2.167989 time 2019-02-24 14:38:18.397874
Model ind 685 epoch 248 head A batch: 0 avg loss -2.199656 avg loss no lamb -2.199656 time 2019-02-24 14:39:30.787494
Model ind 685 epoch 248 head A batch: 100 avg loss -2.185685 avg loss no lamb -2.185685 time 2019-02-24 14:40:43.496048
Model ind 685 epoch 248 head A batch: 200 avg loss -2.240671 avg loss no lamb -2.240671 time 2019-02-24 14:41:57.617123
Model ind 685 epoch 248 head A batch: 300 avg loss -2.201358 avg loss no lamb -2.201358 time 2019-02-24 14:43:11.080679
Model ind 685 epoch 248 head A batch: 400 avg loss -2.190776 avg loss no lamb -2.190776 time 2019-02-24 14:44:24.413522
Pre: time 2019-02-24 14:45:54.214762: 
 	std: 0.0065055997
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789714, 0.99212855, 0.9789714, 0.9789714]
	train_accs: [0.99237144, 0.9789714, 0.99212855, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98428285
	best: 0.99237144

Starting e_i: 249
Model ind 685 epoch 249 head B batch: 0 avg loss -2.228735 avg loss no lamb -2.228735 time 2019-02-24 14:45:56.490630
Model ind 685 epoch 249 head B batch: 100 avg loss -2.147679 avg loss no lamb -2.147679 time 2019-02-24 14:47:09.560355
Model ind 685 epoch 249 head B batch: 200 avg loss -2.223444 avg loss no lamb -2.223444 time 2019-02-24 14:48:23.928341
Model ind 685 epoch 249 head B batch: 300 avg loss -2.207006 avg loss no lamb -2.207006 time 2019-02-24 14:49:36.900811
Model ind 685 epoch 249 head B batch: 400 avg loss -2.191552 avg loss no lamb -2.191552 time 2019-02-24 14:50:51.447220
Model ind 685 epoch 249 head B batch: 0 avg loss -2.215629 avg loss no lamb -2.215629 time 2019-02-24 14:52:03.244292
Model ind 685 epoch 249 head B batch: 100 avg loss -2.214645 avg loss no lamb -2.214645 time 2019-02-24 14:53:17.203312
Model ind 685 epoch 249 head B batch: 200 avg loss -2.214676 avg loss no lamb -2.214676 time 2019-02-24 14:54:30.758817
Model ind 685 epoch 249 head B batch: 300 avg loss -2.199720 avg loss no lamb -2.199720 time 2019-02-24 14:55:44.817820
Model ind 685 epoch 249 head B batch: 400 avg loss -2.203552 avg loss no lamb -2.203552 time 2019-02-24 14:56:57.287007
Model ind 685 epoch 249 head A batch: 0 avg loss -2.228152 avg loss no lamb -2.228152 time 2019-02-24 14:58:11.167310
Model ind 685 epoch 249 head A batch: 100 avg loss -2.169441 avg loss no lamb -2.169441 time 2019-02-24 14:59:24.585883
Model ind 685 epoch 249 head A batch: 200 avg loss -2.177536 avg loss no lamb -2.177536 time 2019-02-24 15:00:38.312542
Model ind 685 epoch 249 head A batch: 300 avg loss -2.211626 avg loss no lamb -2.211626 time 2019-02-24 15:01:51.646591
Model ind 685 epoch 249 head A batch: 400 avg loss -2.184242 avg loss no lamb -2.184242 time 2019-02-24 15:03:05.540789
Pre: time 2019-02-24 15:04:34.841411: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 250
Model ind 685 epoch 250 head B batch: 0 avg loss -2.176624 avg loss no lamb -2.176624 time 2019-02-24 15:04:36.994258
Model ind 685 epoch 250 head B batch: 100 avg loss -2.200422 avg loss no lamb -2.200422 time 2019-02-24 15:05:49.143960
Model ind 685 epoch 250 head B batch: 200 avg loss -2.215934 avg loss no lamb -2.215934 time 2019-02-24 15:07:03.009004
Model ind 685 epoch 250 head B batch: 300 avg loss -2.204087 avg loss no lamb -2.204087 time 2019-02-24 15:08:17.508520
Model ind 685 epoch 250 head B batch: 400 avg loss -2.151921 avg loss no lamb -2.151921 time 2019-02-24 15:09:30.175577
Model ind 685 epoch 250 head B batch: 0 avg loss -2.178409 avg loss no lamb -2.178409 time 2019-02-24 15:10:44.816104
Model ind 685 epoch 250 head B batch: 100 avg loss -2.173492 avg loss no lamb -2.173492 time 2019-02-24 15:11:48.084898
Model ind 685 epoch 250 head B batch: 200 avg loss -2.203976 avg loss no lamb -2.203976 time 2019-02-24 15:13:01.555739
Model ind 685 epoch 250 head B batch: 300 avg loss -2.204609 avg loss no lamb -2.204609 time 2019-02-24 15:14:15.613867
Model ind 685 epoch 250 head B batch: 400 avg loss -2.198407 avg loss no lamb -2.198407 time 2019-02-24 15:15:28.301897
Model ind 685 epoch 250 head A batch: 0 avg loss -2.246326 avg loss no lamb -2.246326 time 2019-02-24 15:16:41.971406
Model ind 685 epoch 250 head A batch: 100 avg loss -2.216550 avg loss no lamb -2.216550 time 2019-02-24 15:17:55.050802
Model ind 685 epoch 250 head A batch: 200 avg loss -2.226876 avg loss no lamb -2.226876 time 2019-02-24 15:19:08.552030
Model ind 685 epoch 250 head A batch: 300 avg loss -2.229626 avg loss no lamb -2.229626 time 2019-02-24 15:20:22.154695
Model ind 685 epoch 250 head A batch: 400 avg loss -2.196869 avg loss no lamb -2.196869 time 2019-02-24 15:21:34.831005
Pre: time 2019-02-24 15:23:02.646306: 
 	std: 0.00647674
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.99204284, 0.9789857, 0.9789714]
	train_accs: [0.99235713, 0.9789857, 0.99204284, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98426855
	best: 0.99235713

Starting e_i: 251
Model ind 685 epoch 251 head B batch: 0 avg loss -2.186116 avg loss no lamb -2.186116 time 2019-02-24 15:23:04.641124
Model ind 685 epoch 251 head B batch: 100 avg loss -2.194343 avg loss no lamb -2.194343 time 2019-02-24 15:24:18.290165
Model ind 685 epoch 251 head B batch: 200 avg loss -2.201802 avg loss no lamb -2.201802 time 2019-02-24 15:25:32.688654
Model ind 685 epoch 251 head B batch: 300 avg loss -2.193699 avg loss no lamb -2.193699 time 2019-02-24 15:26:46.012732
Model ind 685 epoch 251 head B batch: 400 avg loss -2.175261 avg loss no lamb -2.175261 time 2019-02-24 15:27:59.139308
Model ind 685 epoch 251 head B batch: 0 avg loss -2.222635 avg loss no lamb -2.222635 time 2019-02-24 15:29:12.156707
Model ind 685 epoch 251 head B batch: 100 avg loss -2.183546 avg loss no lamb -2.183546 time 2019-02-24 15:30:28.022202
Model ind 685 epoch 251 head B batch: 200 avg loss -2.198744 avg loss no lamb -2.198744 time 2019-02-24 15:31:42.570924
Model ind 685 epoch 251 head B batch: 300 avg loss -2.189495 avg loss no lamb -2.189495 time 2019-02-24 15:32:57.298132
Model ind 685 epoch 251 head B batch: 400 avg loss -2.178028 avg loss no lamb -2.178028 time 2019-02-24 15:34:11.891408
Model ind 685 epoch 251 head A batch: 0 avg loss -2.226358 avg loss no lamb -2.226358 time 2019-02-24 15:35:26.703302
Model ind 685 epoch 251 head A batch: 100 avg loss -2.242105 avg loss no lamb -2.242105 time 2019-02-24 15:36:42.847865
Model ind 685 epoch 251 head A batch: 200 avg loss -2.238701 avg loss no lamb -2.238701 time 2019-02-24 15:37:57.148024
Model ind 685 epoch 251 head A batch: 300 avg loss -2.217280 avg loss no lamb -2.217280 time 2019-02-24 15:39:12.785186
Model ind 685 epoch 251 head A batch: 400 avg loss -2.198192 avg loss no lamb -2.198192 time 2019-02-24 15:40:27.390985
Pre: time 2019-02-24 15:41:59.288258: 
 	std: 0.006463852
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789857, 0.9920286, 0.9789857, 0.9789857]
	train_accs: [0.9923286, 0.9789857, 0.9920286, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842628
	best: 0.9923286

Starting e_i: 252
Model ind 685 epoch 252 head B batch: 0 avg loss -2.202631 avg loss no lamb -2.202631 time 2019-02-24 15:42:01.221931
Model ind 685 epoch 252 head B batch: 100 avg loss -2.210631 avg loss no lamb -2.210631 time 2019-02-24 15:43:18.518612
Model ind 685 epoch 252 head B batch: 200 avg loss -2.195352 avg loss no lamb -2.195352 time 2019-02-24 15:44:34.405777
Model ind 685 epoch 252 head B batch: 300 avg loss -2.214132 avg loss no lamb -2.214132 time 2019-02-24 15:45:51.868307
Model ind 685 epoch 252 head B batch: 400 avg loss -2.212573 avg loss no lamb -2.212573 time 2019-02-24 15:47:08.219920
Model ind 685 epoch 252 head B batch: 0 avg loss -2.210992 avg loss no lamb -2.210992 time 2019-02-24 15:48:23.361441
Model ind 685 epoch 252 head B batch: 100 avg loss -2.190316 avg loss no lamb -2.190316 time 2019-02-24 15:49:40.138323
Model ind 685 epoch 252 head B batch: 200 avg loss -2.196561 avg loss no lamb -2.196561 time 2019-02-24 15:50:55.643293
Model ind 685 epoch 252 head B batch: 300 avg loss -2.229627 avg loss no lamb -2.229627 time 2019-02-24 15:52:10.209445
Model ind 685 epoch 252 head B batch: 400 avg loss -2.188953 avg loss no lamb -2.188953 time 2019-02-24 15:53:24.996441
Model ind 685 epoch 252 head A batch: 0 avg loss -2.216537 avg loss no lamb -2.216537 time 2019-02-24 15:54:37.170729
Model ind 685 epoch 252 head A batch: 100 avg loss -2.215271 avg loss no lamb -2.215271 time 2019-02-24 15:55:47.203298
Model ind 685 epoch 252 head A batch: 200 avg loss -2.193032 avg loss no lamb -2.193032 time 2019-02-24 15:57:02.477765
Model ind 685 epoch 252 head A batch: 300 avg loss -2.174569 avg loss no lamb -2.174569 time 2019-02-24 15:58:15.539602
Model ind 685 epoch 252 head A batch: 400 avg loss -2.156168 avg loss no lamb -2.156168 time 2019-02-24 15:59:28.506427
Pre: time 2019-02-24 16:00:59.347002: 
 	std: 0.006476622
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790143, 0.9920857, 0.979, 0.9790143]
	train_accs: [0.99237144, 0.9790143, 0.9920857, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.99237144

Starting e_i: 253
Model ind 685 epoch 253 head B batch: 0 avg loss -2.205740 avg loss no lamb -2.205740 time 2019-02-24 16:01:01.282060
Model ind 685 epoch 253 head B batch: 100 avg loss -2.191209 avg loss no lamb -2.191209 time 2019-02-24 16:02:17.758135
Model ind 685 epoch 253 head B batch: 200 avg loss -2.198060 avg loss no lamb -2.198060 time 2019-02-24 16:03:33.975143
Model ind 685 epoch 253 head B batch: 300 avg loss -2.212724 avg loss no lamb -2.212724 time 2019-02-24 16:04:48.791450
Model ind 685 epoch 253 head B batch: 400 avg loss -2.194435 avg loss no lamb -2.194435 time 2019-02-24 16:06:04.806099
Model ind 685 epoch 253 head B batch: 0 avg loss -2.217188 avg loss no lamb -2.217188 time 2019-02-24 16:07:19.640602
Model ind 685 epoch 253 head B batch: 100 avg loss -2.211199 avg loss no lamb -2.211199 time 2019-02-24 16:08:36.202603
Model ind 685 epoch 253 head B batch: 200 avg loss -2.197226 avg loss no lamb -2.197226 time 2019-02-24 16:09:52.678981
Model ind 685 epoch 253 head B batch: 300 avg loss -2.236683 avg loss no lamb -2.236683 time 2019-02-24 16:11:07.465456
Model ind 685 epoch 253 head B batch: 400 avg loss -2.137608 avg loss no lamb -2.137608 time 2019-02-24 16:12:21.916530
Model ind 685 epoch 253 head A batch: 0 avg loss -2.220946 avg loss no lamb -2.220946 time 2019-02-24 16:13:38.376109
Model ind 685 epoch 253 head A batch: 100 avg loss -2.177490 avg loss no lamb -2.177490 time 2019-02-24 16:14:55.300890
Model ind 685 epoch 253 head A batch: 200 avg loss -2.165329 avg loss no lamb -2.165329 time 2019-02-24 16:16:11.411583
Model ind 685 epoch 253 head A batch: 300 avg loss -2.204451 avg loss no lamb -2.204451 time 2019-02-24 16:17:25.502986
Model ind 685 epoch 253 head A batch: 400 avg loss -2.202364 avg loss no lamb -2.202364 time 2019-02-24 16:18:41.524008
Pre: time 2019-02-24 16:20:13.479922: 
 	std: 0.0064790896
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99207145, 0.979, 0.979]
	train_accs: [0.9923857, 0.9790143, 0.99207145, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 254
Model ind 685 epoch 254 head B batch: 0 avg loss -2.201319 avg loss no lamb -2.201319 time 2019-02-24 16:20:15.388336
Model ind 685 epoch 254 head B batch: 100 avg loss -2.191374 avg loss no lamb -2.191374 time 2019-02-24 16:21:29.552018
Model ind 685 epoch 254 head B batch: 200 avg loss -2.225724 avg loss no lamb -2.225724 time 2019-02-24 16:22:44.261594
Model ind 685 epoch 254 head B batch: 300 avg loss -2.230779 avg loss no lamb -2.230779 time 2019-02-24 16:23:57.482776
Model ind 685 epoch 254 head B batch: 400 avg loss -2.175368 avg loss no lamb -2.175368 time 2019-02-24 16:25:11.569727
Model ind 685 epoch 254 head B batch: 0 avg loss -2.232900 avg loss no lamb -2.232900 time 2019-02-24 16:26:25.581538
Model ind 685 epoch 254 head B batch: 100 avg loss -2.214319 avg loss no lamb -2.214319 time 2019-02-24 16:27:41.825715
Model ind 685 epoch 254 head B batch: 200 avg loss -2.196318 avg loss no lamb -2.196318 time 2019-02-24 16:28:56.531218
Model ind 685 epoch 254 head B batch: 300 avg loss -2.189919 avg loss no lamb -2.189919 time 2019-02-24 16:30:12.188070
Model ind 685 epoch 254 head B batch: 400 avg loss -2.204708 avg loss no lamb -2.204708 time 2019-02-24 16:31:26.399987
Model ind 685 epoch 254 head A batch: 0 avg loss -2.199661 avg loss no lamb -2.199661 time 2019-02-24 16:32:41.330655
Model ind 685 epoch 254 head A batch: 100 avg loss -2.189360 avg loss no lamb -2.189360 time 2019-02-24 16:33:56.008769
Model ind 685 epoch 254 head A batch: 200 avg loss -2.175262 avg loss no lamb -2.175262 time 2019-02-24 16:35:11.630874
Model ind 685 epoch 254 head A batch: 300 avg loss -2.264556 avg loss no lamb -2.264556 time 2019-02-24 16:36:26.290134
Model ind 685 epoch 254 head A batch: 400 avg loss -2.213562 avg loss no lamb -2.213562 time 2019-02-24 16:37:39.299958
Pre: time 2019-02-24 16:39:02.193447: 
 	std: 0.006481277
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 255
Model ind 685 epoch 255 head B batch: 0 avg loss -2.171708 avg loss no lamb -2.171708 time 2019-02-24 16:39:04.892337
Model ind 685 epoch 255 head B batch: 100 avg loss -2.177097 avg loss no lamb -2.177097 time 2019-02-24 16:40:21.041706
Model ind 685 epoch 255 head B batch: 200 avg loss -2.215692 avg loss no lamb -2.215692 time 2019-02-24 16:41:38.925043
Model ind 685 epoch 255 head B batch: 300 avg loss -2.234377 avg loss no lamb -2.234377 time 2019-02-24 16:42:55.118807
Model ind 685 epoch 255 head B batch: 400 avg loss -2.195841 avg loss no lamb -2.195841 time 2019-02-24 16:44:10.422058
Model ind 685 epoch 255 head B batch: 0 avg loss -2.189621 avg loss no lamb -2.189621 time 2019-02-24 16:45:25.459869
Model ind 685 epoch 255 head B batch: 100 avg loss -2.222548 avg loss no lamb -2.222548 time 2019-02-24 16:46:38.890863
Model ind 685 epoch 255 head B batch: 200 avg loss -2.229369 avg loss no lamb -2.229369 time 2019-02-24 16:47:53.422215
Model ind 685 epoch 255 head B batch: 300 avg loss -2.219673 avg loss no lamb -2.219673 time 2019-02-24 16:49:10.329251
Model ind 685 epoch 255 head B batch: 400 avg loss -2.184681 avg loss no lamb -2.184681 time 2019-02-24 16:50:27.501984
Model ind 685 epoch 255 head A batch: 0 avg loss -2.201776 avg loss no lamb -2.201776 time 2019-02-24 16:51:42.846223
Model ind 685 epoch 255 head A batch: 100 avg loss -2.216803 avg loss no lamb -2.216803 time 2019-02-24 16:52:58.417597
Model ind 685 epoch 255 head A batch: 200 avg loss -2.200055 avg loss no lamb -2.200055 time 2019-02-24 16:54:13.221999
Model ind 685 epoch 255 head A batch: 300 avg loss -2.231734 avg loss no lamb -2.231734 time 2019-02-24 16:55:27.854333
Model ind 685 epoch 255 head A batch: 400 avg loss -2.157910 avg loss no lamb -2.157910 time 2019-02-24 16:56:42.268156
Pre: time 2019-02-24 16:58:12.395122: 
 	std: 0.006484705
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98428
	best: 0.99235713

Starting e_i: 256
Model ind 685 epoch 256 head B batch: 0 avg loss -2.187206 avg loss no lamb -2.187206 time 2019-02-24 16:58:14.583800
Model ind 685 epoch 256 head B batch: 100 avg loss -2.216273 avg loss no lamb -2.216273 time 2019-02-24 16:59:30.145769
Model ind 685 epoch 256 head B batch: 200 avg loss -2.167899 avg loss no lamb -2.167899 time 2019-02-24 17:00:45.171315
Model ind 685 epoch 256 head B batch: 300 avg loss -2.225672 avg loss no lamb -2.225672 time 2019-02-24 17:02:00.082738
Model ind 685 epoch 256 head B batch: 400 avg loss -2.187823 avg loss no lamb -2.187823 time 2019-02-24 17:03:15.714656
Model ind 685 epoch 256 head B batch: 0 avg loss -2.167740 avg loss no lamb -2.167740 time 2019-02-24 17:04:30.107020
Model ind 685 epoch 256 head B batch: 100 avg loss -2.232954 avg loss no lamb -2.232954 time 2019-02-24 17:05:47.205967
Model ind 685 epoch 256 head B batch: 200 avg loss -2.202836 avg loss no lamb -2.202836 time 2019-02-24 17:07:02.120158
Model ind 685 epoch 256 head B batch: 300 avg loss -2.221583 avg loss no lamb -2.221583 time 2019-02-24 17:08:17.340003
Model ind 685 epoch 256 head B batch: 400 avg loss -2.176676 avg loss no lamb -2.176676 time 2019-02-24 17:09:31.695131
Model ind 685 epoch 256 head A batch: 0 avg loss -2.235642 avg loss no lamb -2.235642 time 2019-02-24 17:10:46.279607
Model ind 685 epoch 256 head A batch: 100 avg loss -2.192086 avg loss no lamb -2.192086 time 2019-02-24 17:12:02.253844
Model ind 685 epoch 256 head A batch: 200 avg loss -2.225991 avg loss no lamb -2.225991 time 2019-02-24 17:13:17.843955
Model ind 685 epoch 256 head A batch: 300 avg loss -2.221362 avg loss no lamb -2.221362 time 2019-02-24 17:14:35.230669
Model ind 685 epoch 256 head A batch: 400 avg loss -2.195629 avg loss no lamb -2.195629 time 2019-02-24 17:15:51.673616
Pre: time 2019-02-24 17:17:22.026206: 
 	std: 0.0064649577
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.97905713, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9923857, 0.97905713, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843257
	best: 0.9923857

Starting e_i: 257
Model ind 685 epoch 257 head B batch: 0 avg loss -2.229956 avg loss no lamb -2.229956 time 2019-02-24 17:17:26.496039
Model ind 685 epoch 257 head B batch: 100 avg loss -2.219296 avg loss no lamb -2.219296 time 2019-02-24 17:18:41.565639
Model ind 685 epoch 257 head B batch: 200 avg loss -2.228531 avg loss no lamb -2.228531 time 2019-02-24 17:19:54.050366
Model ind 685 epoch 257 head B batch: 300 avg loss -2.203386 avg loss no lamb -2.203386 time 2019-02-24 17:21:01.993432
Model ind 685 epoch 257 head B batch: 400 avg loss -2.172037 avg loss no lamb -2.172037 time 2019-02-24 17:22:13.663963
Model ind 685 epoch 257 head B batch: 0 avg loss -2.242577 avg loss no lamb -2.242577 time 2019-02-24 17:23:30.089629
Model ind 685 epoch 257 head B batch: 100 avg loss -2.195925 avg loss no lamb -2.195925 time 2019-02-24 17:24:44.494920
Model ind 685 epoch 257 head B batch: 200 avg loss -2.177946 avg loss no lamb -2.177946 time 2019-02-24 17:25:58.611569
Model ind 685 epoch 257 head B batch: 300 avg loss -2.243659 avg loss no lamb -2.243659 time 2019-02-24 17:27:12.998822
Model ind 685 epoch 257 head B batch: 400 avg loss -2.197623 avg loss no lamb -2.197623 time 2019-02-24 17:28:26.528040
Model ind 685 epoch 257 head A batch: 0 avg loss -2.220221 avg loss no lamb -2.220221 time 2019-02-24 17:29:41.642828
Model ind 685 epoch 257 head A batch: 100 avg loss -2.239588 avg loss no lamb -2.239588 time 2019-02-24 17:30:55.671176
Model ind 685 epoch 257 head A batch: 200 avg loss -2.180615 avg loss no lamb -2.180615 time 2019-02-24 17:32:10.868317
Model ind 685 epoch 257 head A batch: 300 avg loss -2.231293 avg loss no lamb -2.231293 time 2019-02-24 17:33:24.541196
Model ind 685 epoch 257 head A batch: 400 avg loss -2.171389 avg loss no lamb -2.171389 time 2019-02-24 17:34:37.642133
Pre: time 2019-02-24 17:36:06.732279: 
 	std: 0.006468665
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9920857, 0.9790428, 0.97905713]
	train_accs: [0.9924143, 0.9790428, 0.9920857, 0.9790428, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 258
Model ind 685 epoch 258 head B batch: 0 avg loss -2.221547 avg loss no lamb -2.221547 time 2019-02-24 17:36:08.594467
Model ind 685 epoch 258 head B batch: 100 avg loss -2.216702 avg loss no lamb -2.216702 time 2019-02-24 17:37:20.891395
Model ind 685 epoch 258 head B batch: 200 avg loss -2.185027 avg loss no lamb -2.185027 time 2019-02-24 17:38:34.989440
Model ind 685 epoch 258 head B batch: 300 avg loss -2.201853 avg loss no lamb -2.201853 time 2019-02-24 17:39:49.721829
Model ind 685 epoch 258 head B batch: 400 avg loss -2.213905 avg loss no lamb -2.213905 time 2019-02-24 17:41:05.863679
Model ind 685 epoch 258 head B batch: 0 avg loss -2.222088 avg loss no lamb -2.222088 time 2019-02-24 17:42:21.383507
Model ind 685 epoch 258 head B batch: 100 avg loss -2.170312 avg loss no lamb -2.170312 time 2019-02-24 17:43:34.588128
Model ind 685 epoch 258 head B batch: 200 avg loss -2.193640 avg loss no lamb -2.193640 time 2019-02-24 17:44:49.327018
Model ind 685 epoch 258 head B batch: 300 avg loss -2.235414 avg loss no lamb -2.235414 time 2019-02-24 17:46:02.470109
Model ind 685 epoch 258 head B batch: 400 avg loss -2.170844 avg loss no lamb -2.170844 time 2019-02-24 17:47:17.671834
Model ind 685 epoch 258 head A batch: 0 avg loss -2.198420 avg loss no lamb -2.198420 time 2019-02-24 17:48:31.632312
Model ind 685 epoch 258 head A batch: 100 avg loss -2.181042 avg loss no lamb -2.181042 time 2019-02-24 17:49:46.263222
Model ind 685 epoch 258 head A batch: 200 avg loss -2.226901 avg loss no lamb -2.226901 time 2019-02-24 17:50:59.330753
Model ind 685 epoch 258 head A batch: 300 avg loss -2.191102 avg loss no lamb -2.191102 time 2019-02-24 17:52:14.655927
Model ind 685 epoch 258 head A batch: 400 avg loss -2.183684 avg loss no lamb -2.183684 time 2019-02-24 17:53:29.334814
Pre: time 2019-02-24 17:55:01.027154: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98430574
	best: 0.9924

Starting e_i: 259
Model ind 685 epoch 259 head B batch: 0 avg loss -2.219576 avg loss no lamb -2.219576 time 2019-02-24 17:55:03.230957
Model ind 685 epoch 259 head B batch: 100 avg loss -2.206002 avg loss no lamb -2.206002 time 2019-02-24 17:56:17.535372
Model ind 685 epoch 259 head B batch: 200 avg loss -2.193316 avg loss no lamb -2.193316 time 2019-02-24 17:57:31.263057
Model ind 685 epoch 259 head B batch: 300 avg loss -2.211949 avg loss no lamb -2.211949 time 2019-02-24 17:58:47.359587
Model ind 685 epoch 259 head B batch: 400 avg loss -2.201821 avg loss no lamb -2.201821 time 2019-02-24 18:00:03.246337
Model ind 685 epoch 259 head B batch: 0 avg loss -2.203459 avg loss no lamb -2.203459 time 2019-02-24 18:01:19.384913
Model ind 685 epoch 259 head B batch: 100 avg loss -2.245930 avg loss no lamb -2.245930 time 2019-02-24 18:02:36.013552
Model ind 685 epoch 259 head B batch: 200 avg loss -2.202730 avg loss no lamb -2.202730 time 2019-02-24 18:03:48.228441
Model ind 685 epoch 259 head B batch: 300 avg loss -2.215862 avg loss no lamb -2.215862 time 2019-02-24 18:04:54.722938
Model ind 685 epoch 259 head B batch: 400 avg loss -2.213221 avg loss no lamb -2.213221 time 2019-02-24 18:06:09.882865
Model ind 685 epoch 259 head A batch: 0 avg loss -2.213395 avg loss no lamb -2.213395 time 2019-02-24 18:07:23.321573
Model ind 685 epoch 259 head A batch: 100 avg loss -2.190150 avg loss no lamb -2.190150 time 2019-02-24 18:08:39.897923
Model ind 685 epoch 259 head A batch: 200 avg loss -2.174039 avg loss no lamb -2.174039 time 2019-02-24 18:09:53.613591
Model ind 685 epoch 259 head A batch: 300 avg loss -2.233990 avg loss no lamb -2.233990 time 2019-02-24 18:11:05.883843
Model ind 685 epoch 259 head A batch: 400 avg loss -2.164640 avg loss no lamb -2.164640 time 2019-02-24 18:12:19.581228
Pre: time 2019-02-24 18:13:50.896459: 
 	std: 0.0064720977
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.97905713]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843314
	best: 0.9924143

Starting e_i: 260
Model ind 685 epoch 260 head B batch: 0 avg loss -2.218016 avg loss no lamb -2.218016 time 2019-02-24 18:13:52.763808
Model ind 685 epoch 260 head B batch: 100 avg loss -2.227787 avg loss no lamb -2.227787 time 2019-02-24 18:15:08.779895
Model ind 685 epoch 260 head B batch: 200 avg loss -2.180356 avg loss no lamb -2.180356 time 2019-02-24 18:16:25.513558
Model ind 685 epoch 260 head B batch: 300 avg loss -2.242824 avg loss no lamb -2.242824 time 2019-02-24 18:17:38.880852
Model ind 685 epoch 260 head B batch: 400 avg loss -2.196494 avg loss no lamb -2.196494 time 2019-02-24 18:18:55.893241
Model ind 685 epoch 260 head B batch: 0 avg loss -2.174936 avg loss no lamb -2.174936 time 2019-02-24 18:20:12.241148
Model ind 685 epoch 260 head B batch: 100 avg loss -2.215460 avg loss no lamb -2.215460 time 2019-02-24 18:21:29.683531
Model ind 685 epoch 260 head B batch: 200 avg loss -2.218096 avg loss no lamb -2.218096 time 2019-02-24 18:22:43.804018
Model ind 685 epoch 260 head B batch: 300 avg loss -2.184490 avg loss no lamb -2.184490 time 2019-02-24 18:23:58.541259
Model ind 685 epoch 260 head B batch: 400 avg loss -2.170994 avg loss no lamb -2.170994 time 2019-02-24 18:25:15.991469
Model ind 685 epoch 260 head A batch: 0 avg loss -2.193639 avg loss no lamb -2.193639 time 2019-02-24 18:26:30.812274
Model ind 685 epoch 260 head A batch: 100 avg loss -2.210845 avg loss no lamb -2.210845 time 2019-02-24 18:27:46.787574
Model ind 685 epoch 260 head A batch: 200 avg loss -2.196705 avg loss no lamb -2.196705 time 2019-02-24 18:29:02.512540
Model ind 685 epoch 260 head A batch: 300 avg loss -2.202813 avg loss no lamb -2.202813 time 2019-02-24 18:30:18.498650
Model ind 685 epoch 260 head A batch: 400 avg loss -2.182635 avg loss no lamb -2.182635 time 2019-02-24 18:31:34.205861
Pre: time 2019-02-24 18:33:06.708976: 
 	std: 0.006447533
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99215716, 0.97884285, 0.9918572, 0.97885716, 0.97884285]
	train_accs: [0.99215716, 0.97884285, 0.9918572, 0.97885716, 0.97884285]
	best_train_sub_head: 0
	worst: 0.97884285
	avg: 0.9841114
	best: 0.99215716

Starting e_i: 261
Model ind 685 epoch 261 head B batch: 0 avg loss -2.210415 avg loss no lamb -2.210415 time 2019-02-24 18:33:11.391117
Model ind 685 epoch 261 head B batch: 100 avg loss -2.175309 avg loss no lamb -2.175309 time 2019-02-24 18:34:28.878638
Model ind 685 epoch 261 head B batch: 200 avg loss -2.233225 avg loss no lamb -2.233225 time 2019-02-24 18:35:45.529386
Model ind 685 epoch 261 head B batch: 300 avg loss -2.216963 avg loss no lamb -2.216963 time 2019-02-24 18:37:02.129736
Model ind 685 epoch 261 head B batch: 400 avg loss -2.193421 avg loss no lamb -2.193421 time 2019-02-24 18:38:17.663921
Model ind 685 epoch 261 head B batch: 0 avg loss -2.184853 avg loss no lamb -2.184853 time 2019-02-24 18:39:31.848086
Model ind 685 epoch 261 head B batch: 100 avg loss -2.169176 avg loss no lamb -2.169176 time 2019-02-24 18:40:46.321518
Model ind 685 epoch 261 head B batch: 200 avg loss -2.163190 avg loss no lamb -2.163190 time 2019-02-24 18:42:02.375267
Model ind 685 epoch 261 head B batch: 300 avg loss -2.214490 avg loss no lamb -2.214490 time 2019-02-24 18:43:19.340880
Model ind 685 epoch 261 head B batch: 400 avg loss -2.196683 avg loss no lamb -2.196683 time 2019-02-24 18:44:36.072498
Model ind 685 epoch 261 head A batch: 0 avg loss -2.233427 avg loss no lamb -2.233427 time 2019-02-24 18:45:52.380741
Model ind 685 epoch 261 head A batch: 100 avg loss -2.223159 avg loss no lamb -2.223159 time 2019-02-24 18:47:05.667849
Model ind 685 epoch 261 head A batch: 200 avg loss -2.188800 avg loss no lamb -2.188800 time 2019-02-24 18:48:14.790680
Model ind 685 epoch 261 head A batch: 300 avg loss -2.219793 avg loss no lamb -2.219793 time 2019-02-24 18:49:30.825499
Model ind 685 epoch 261 head A batch: 400 avg loss -2.190413 avg loss no lamb -2.190413 time 2019-02-24 18:50:49.195955
Pre: time 2019-02-24 18:52:19.156775: 
 	std: 0.0064885463
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.99244285, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.99244285

Starting e_i: 262
Model ind 685 epoch 262 head B batch: 0 avg loss -2.223251 avg loss no lamb -2.223251 time 2019-02-24 18:52:21.585214
Model ind 685 epoch 262 head B batch: 100 avg loss -2.194931 avg loss no lamb -2.194931 time 2019-02-24 18:53:37.704775
Model ind 685 epoch 262 head B batch: 200 avg loss -2.195740 avg loss no lamb -2.195740 time 2019-02-24 18:54:53.250665
Model ind 685 epoch 262 head B batch: 300 avg loss -2.220183 avg loss no lamb -2.220183 time 2019-02-24 18:56:08.243194
Model ind 685 epoch 262 head B batch: 400 avg loss -2.212067 avg loss no lamb -2.212067 time 2019-02-24 18:57:25.709121
Model ind 685 epoch 262 head B batch: 0 avg loss -2.210646 avg loss no lamb -2.210646 time 2019-02-24 18:58:40.475069
Model ind 685 epoch 262 head B batch: 100 avg loss -2.206826 avg loss no lamb -2.206826 time 2019-02-24 18:59:55.463728
Model ind 685 epoch 262 head B batch: 200 avg loss -2.180288 avg loss no lamb -2.180288 time 2019-02-24 19:01:11.164672
Model ind 685 epoch 262 head B batch: 300 avg loss -2.228430 avg loss no lamb -2.228430 time 2019-02-24 19:02:27.979614
Model ind 685 epoch 262 head B batch: 400 avg loss -2.181629 avg loss no lamb -2.181629 time 2019-02-24 19:03:43.794937
Model ind 685 epoch 262 head A batch: 0 avg loss -2.239594 avg loss no lamb -2.239594 time 2019-02-24 19:05:01.046276
Model ind 685 epoch 262 head A batch: 100 avg loss -2.208505 avg loss no lamb -2.208505 time 2019-02-24 19:06:16.602234
Model ind 685 epoch 262 head A batch: 200 avg loss -2.213763 avg loss no lamb -2.213763 time 2019-02-24 19:07:31.228117
Model ind 685 epoch 262 head A batch: 300 avg loss -2.214613 avg loss no lamb -2.214613 time 2019-02-24 19:08:48.430812
Model ind 685 epoch 262 head A batch: 400 avg loss -2.196193 avg loss no lamb -2.196193 time 2019-02-24 19:10:04.570247
Pre: time 2019-02-24 19:11:36.304082: 
 	std: 0.006480195
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790286]
	train_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 263
Model ind 685 epoch 263 head B batch: 0 avg loss -2.217608 avg loss no lamb -2.217608 time 2019-02-24 19:11:38.393634
Model ind 685 epoch 263 head B batch: 100 avg loss -2.191006 avg loss no lamb -2.191006 time 2019-02-24 19:12:53.225000
Model ind 685 epoch 263 head B batch: 200 avg loss -2.198654 avg loss no lamb -2.198654 time 2019-02-24 19:14:08.421002
Model ind 685 epoch 263 head B batch: 300 avg loss -2.220230 avg loss no lamb -2.220230 time 2019-02-24 19:15:24.176199
Model ind 685 epoch 263 head B batch: 400 avg loss -2.195781 avg loss no lamb -2.195781 time 2019-02-24 19:16:41.436446
Model ind 685 epoch 263 head B batch: 0 avg loss -2.222571 avg loss no lamb -2.222571 time 2019-02-24 19:17:58.022889
Model ind 685 epoch 263 head B batch: 100 avg loss -2.212913 avg loss no lamb -2.212913 time 2019-02-24 19:19:14.340468
Model ind 685 epoch 263 head B batch: 200 avg loss -2.244551 avg loss no lamb -2.244551 time 2019-02-24 19:20:32.483054
Model ind 685 epoch 263 head B batch: 300 avg loss -2.212005 avg loss no lamb -2.212005 time 2019-02-24 19:21:50.254432
Model ind 685 epoch 263 head B batch: 400 avg loss -2.175054 avg loss no lamb -2.175054 time 2019-02-24 19:23:04.366087
Model ind 685 epoch 263 head A batch: 0 avg loss -2.213217 avg loss no lamb -2.213217 time 2019-02-24 19:24:20.540877
Model ind 685 epoch 263 head A batch: 100 avg loss -2.202403 avg loss no lamb -2.202403 time 2019-02-24 19:25:36.660968
Model ind 685 epoch 263 head A batch: 200 avg loss -2.231120 avg loss no lamb -2.231120 time 2019-02-24 19:26:52.418058
Model ind 685 epoch 263 head A batch: 300 avg loss -2.220000 avg loss no lamb -2.220000 time 2019-02-24 19:28:09.294443
Model ind 685 epoch 263 head A batch: 400 avg loss -2.190470 avg loss no lamb -2.190470 time 2019-02-24 19:29:25.433033
Pre: time 2019-02-24 19:30:47.274794: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 264
Model ind 685 epoch 264 head B batch: 0 avg loss -2.215103 avg loss no lamb -2.215103 time 2019-02-24 19:30:49.134672
Model ind 685 epoch 264 head B batch: 100 avg loss -2.184839 avg loss no lamb -2.184839 time 2019-02-24 19:32:07.536996
Model ind 685 epoch 264 head B batch: 200 avg loss -2.222531 avg loss no lamb -2.222531 time 2019-02-24 19:33:23.569802
Model ind 685 epoch 264 head B batch: 300 avg loss -2.212682 avg loss no lamb -2.212682 time 2019-02-24 19:34:40.156087
Model ind 685 epoch 264 head B batch: 400 avg loss -2.175946 avg loss no lamb -2.175946 time 2019-02-24 19:35:58.234139
Model ind 685 epoch 264 head B batch: 0 avg loss -2.200336 avg loss no lamb -2.200336 time 2019-02-24 19:37:14.828521
Model ind 685 epoch 264 head B batch: 100 avg loss -2.201157 avg loss no lamb -2.201157 time 2019-02-24 19:38:29.591574
Model ind 685 epoch 264 head B batch: 200 avg loss -2.212314 avg loss no lamb -2.212314 time 2019-02-24 19:39:46.804638
Model ind 685 epoch 264 head B batch: 300 avg loss -2.228118 avg loss no lamb -2.228118 time 2019-02-24 19:41:02.761668
Model ind 685 epoch 264 head B batch: 400 avg loss -2.198170 avg loss no lamb -2.198170 time 2019-02-24 19:42:19.216122
Model ind 685 epoch 264 head A batch: 0 avg loss -2.219573 avg loss no lamb -2.219573 time 2019-02-24 19:43:37.036595
Model ind 685 epoch 264 head A batch: 100 avg loss -2.188835 avg loss no lamb -2.188835 time 2019-02-24 19:44:53.603639
Model ind 685 epoch 264 head A batch: 200 avg loss -2.172300 avg loss no lamb -2.172300 time 2019-02-24 19:46:11.050643
Model ind 685 epoch 264 head A batch: 300 avg loss -2.216729 avg loss no lamb -2.216729 time 2019-02-24 19:47:27.909609
Model ind 685 epoch 264 head A batch: 400 avg loss -2.144794 avg loss no lamb -2.144794 time 2019-02-24 19:48:46.797536
Pre: time 2019-02-24 19:50:19.508085: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 265
Model ind 685 epoch 265 head B batch: 0 avg loss -2.211496 avg loss no lamb -2.211496 time 2019-02-24 19:50:21.419457
Model ind 685 epoch 265 head B batch: 100 avg loss -2.169259 avg loss no lamb -2.169259 time 2019-02-24 19:51:37.953448
Model ind 685 epoch 265 head B batch: 200 avg loss -2.183933 avg loss no lamb -2.183933 time 2019-02-24 19:52:52.263194
Model ind 685 epoch 265 head B batch: 300 avg loss -2.193862 avg loss no lamb -2.193862 time 2019-02-24 19:54:09.793397
Model ind 685 epoch 265 head B batch: 400 avg loss -2.198282 avg loss no lamb -2.198282 time 2019-02-24 19:55:25.726681
Model ind 685 epoch 265 head B batch: 0 avg loss -2.240790 avg loss no lamb -2.240790 time 2019-02-24 19:56:43.641756
Model ind 685 epoch 265 head B batch: 100 avg loss -2.183734 avg loss no lamb -2.183734 time 2019-02-24 19:57:58.374607
Model ind 685 epoch 265 head B batch: 200 avg loss -2.175492 avg loss no lamb -2.175492 time 2019-02-24 19:59:14.130908
Model ind 685 epoch 265 head B batch: 300 avg loss -2.224242 avg loss no lamb -2.224242 time 2019-02-24 20:00:30.903425
Model ind 685 epoch 265 head B batch: 400 avg loss -2.197754 avg loss no lamb -2.197754 time 2019-02-24 20:01:44.474436
Model ind 685 epoch 265 head A batch: 0 avg loss -2.174134 avg loss no lamb -2.174134 time 2019-02-24 20:02:57.658800
Model ind 685 epoch 265 head A batch: 100 avg loss -2.198706 avg loss no lamb -2.198706 time 2019-02-24 20:04:12.844249
Model ind 685 epoch 265 head A batch: 200 avg loss -2.202467 avg loss no lamb -2.202467 time 2019-02-24 20:05:28.062887
Model ind 685 epoch 265 head A batch: 300 avg loss -2.220791 avg loss no lamb -2.220791 time 2019-02-24 20:06:41.762056
Model ind 685 epoch 265 head A batch: 400 avg loss -2.198334 avg loss no lamb -2.198334 time 2019-02-24 20:07:57.468192
Pre: time 2019-02-24 20:09:28.476906: 
 	std: 0.006478113
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924857, 0.97908574, 0.99212855, 0.97908574, 0.97908574]
	train_accs: [0.9924857, 0.97908574, 0.99212855, 0.97908574, 0.97908574]
	best_train_sub_head: 0
	worst: 0.97908574
	avg: 0.98437434
	best: 0.9924857

Starting e_i: 266
Model ind 685 epoch 266 head B batch: 0 avg loss -2.146238 avg loss no lamb -2.146238 time 2019-02-24 20:09:30.635367
Model ind 685 epoch 266 head B batch: 100 avg loss -2.234355 avg loss no lamb -2.234355 time 2019-02-24 20:10:44.771361
Model ind 685 epoch 266 head B batch: 200 avg loss -2.185501 avg loss no lamb -2.185501 time 2019-02-24 20:12:04.020300
Model ind 685 epoch 266 head B batch: 300 avg loss -2.203738 avg loss no lamb -2.203738 time 2019-02-24 20:13:12.829899
Model ind 685 epoch 266 head B batch: 400 avg loss -2.223022 avg loss no lamb -2.223022 time 2019-02-24 20:14:28.172547
Model ind 685 epoch 266 head B batch: 0 avg loss -2.207762 avg loss no lamb -2.207762 time 2019-02-24 20:15:44.353753
Model ind 685 epoch 266 head B batch: 100 avg loss -2.182050 avg loss no lamb -2.182050 time 2019-02-24 20:16:58.738405
Model ind 685 epoch 266 head B batch: 200 avg loss -2.204443 avg loss no lamb -2.204443 time 2019-02-24 20:18:14.288154
Model ind 685 epoch 266 head B batch: 300 avg loss -2.221887 avg loss no lamb -2.221887 time 2019-02-24 20:19:27.723505
Model ind 685 epoch 266 head B batch: 400 avg loss -2.153334 avg loss no lamb -2.153334 time 2019-02-24 20:20:41.259472
Model ind 685 epoch 266 head A batch: 0 avg loss -2.182863 avg loss no lamb -2.182863 time 2019-02-24 20:21:56.275337
Model ind 685 epoch 266 head A batch: 100 avg loss -2.158239 avg loss no lamb -2.158239 time 2019-02-24 20:23:10.820561
Model ind 685 epoch 266 head A batch: 200 avg loss -2.205972 avg loss no lamb -2.205972 time 2019-02-24 20:24:25.487317
Model ind 685 epoch 266 head A batch: 300 avg loss -2.227418 avg loss no lamb -2.227418 time 2019-02-24 20:25:38.384515
Model ind 685 epoch 266 head A batch: 400 avg loss -2.161678 avg loss no lamb -2.161678 time 2019-02-24 20:26:55.006882
Pre: time 2019-02-24 20:28:26.870849: 
 	std: 0.006483877
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789857, 0.99204284, 0.9789857, 0.9789714]
	train_accs: [0.9923857, 0.9789857, 0.99204284, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98427427
	best: 0.9923857

Starting e_i: 267
Model ind 685 epoch 267 head B batch: 0 avg loss -2.201455 avg loss no lamb -2.201455 time 2019-02-24 20:28:28.814721
Model ind 685 epoch 267 head B batch: 100 avg loss -2.218117 avg loss no lamb -2.218117 time 2019-02-24 20:29:44.161655
Model ind 685 epoch 267 head B batch: 200 avg loss -2.215951 avg loss no lamb -2.215951 time 2019-02-24 20:31:01.489602
Model ind 685 epoch 267 head B batch: 300 avg loss -2.219321 avg loss no lamb -2.219321 time 2019-02-24 20:32:16.822632
Model ind 685 epoch 267 head B batch: 400 avg loss -2.217928 avg loss no lamb -2.217928 time 2019-02-24 20:33:31.193526
Model ind 685 epoch 267 head B batch: 0 avg loss -2.231711 avg loss no lamb -2.231711 time 2019-02-24 20:34:45.044181
Model ind 685 epoch 267 head B batch: 100 avg loss -2.208017 avg loss no lamb -2.208017 time 2019-02-24 20:36:00.016701
Model ind 685 epoch 267 head B batch: 200 avg loss -2.187500 avg loss no lamb -2.187500 time 2019-02-24 20:37:14.711877
Model ind 685 epoch 267 head B batch: 300 avg loss -2.210892 avg loss no lamb -2.210892 time 2019-02-24 20:38:28.984173
Model ind 685 epoch 267 head B batch: 400 avg loss -2.195848 avg loss no lamb -2.195848 time 2019-02-24 20:39:46.550023
Model ind 685 epoch 267 head A batch: 0 avg loss -2.201243 avg loss no lamb -2.201243 time 2019-02-24 20:41:03.562895
Model ind 685 epoch 267 head A batch: 100 avg loss -2.208509 avg loss no lamb -2.208509 time 2019-02-24 20:42:17.076580
Model ind 685 epoch 267 head A batch: 200 avg loss -2.203647 avg loss no lamb -2.203647 time 2019-02-24 20:43:34.146787
Model ind 685 epoch 267 head A batch: 300 avg loss -2.244907 avg loss no lamb -2.244907 time 2019-02-24 20:44:49.126842
Model ind 685 epoch 267 head A batch: 400 avg loss -2.167078 avg loss no lamb -2.167078 time 2019-02-24 20:46:03.498930
Pre: time 2019-02-24 20:47:33.424021: 
 	std: 0.0064896573
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9920857, 0.979, 0.979]
	train_accs: [0.9924143, 0.9790143, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9924143

Starting e_i: 268
Model ind 685 epoch 268 head B batch: 0 avg loss -2.192536 avg loss no lamb -2.192536 time 2019-02-24 20:47:35.453617
Model ind 685 epoch 268 head B batch: 100 avg loss -2.212261 avg loss no lamb -2.212261 time 2019-02-24 20:48:52.603108
Model ind 685 epoch 268 head B batch: 200 avg loss -2.201197 avg loss no lamb -2.201197 time 2019-02-24 20:50:07.265192
Model ind 685 epoch 268 head B batch: 300 avg loss -2.213450 avg loss no lamb -2.213450 time 2019-02-24 20:51:22.131834
Model ind 685 epoch 268 head B batch: 400 avg loss -2.189331 avg loss no lamb -2.189331 time 2019-02-24 20:52:36.553268
Model ind 685 epoch 268 head B batch: 0 avg loss -2.221357 avg loss no lamb -2.221357 time 2019-02-24 20:53:53.585617
Model ind 685 epoch 268 head B batch: 100 avg loss -2.150673 avg loss no lamb -2.150673 time 2019-02-24 20:55:06.212315
Model ind 685 epoch 268 head B batch: 200 avg loss -2.196534 avg loss no lamb -2.196534 time 2019-02-24 20:56:12.170705
Model ind 685 epoch 268 head B batch: 300 avg loss -2.231134 avg loss no lamb -2.231134 time 2019-02-24 20:57:28.608593
Model ind 685 epoch 268 head B batch: 400 avg loss -2.165605 avg loss no lamb -2.165605 time 2019-02-24 20:58:48.398019
Model ind 685 epoch 268 head A batch: 0 avg loss -2.172038 avg loss no lamb -2.172038 time 2019-02-24 21:00:06.130082
Model ind 685 epoch 268 head A batch: 100 avg loss -2.184898 avg loss no lamb -2.184898 time 2019-02-24 21:01:19.520993
Model ind 685 epoch 268 head A batch: 200 avg loss -2.241549 avg loss no lamb -2.241549 time 2019-02-24 21:02:35.407407
Model ind 685 epoch 268 head A batch: 300 avg loss -2.170439 avg loss no lamb -2.170439 time 2019-02-24 21:03:50.419982
Model ind 685 epoch 268 head A batch: 400 avg loss -2.155501 avg loss no lamb -2.155501 time 2019-02-24 21:05:09.678378
Pre: time 2019-02-24 21:06:42.372210: 
 	std: 0.0064742696
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843143
	best: 0.9923857

Starting e_i: 269
Model ind 685 epoch 269 head B batch: 0 avg loss -2.202606 avg loss no lamb -2.202606 time 2019-02-24 21:06:44.369869
Model ind 685 epoch 269 head B batch: 100 avg loss -2.210713 avg loss no lamb -2.210713 time 2019-02-24 21:07:58.907218
Model ind 685 epoch 269 head B batch: 200 avg loss -2.211703 avg loss no lamb -2.211703 time 2019-02-24 21:09:11.617912
Model ind 685 epoch 269 head B batch: 300 avg loss -2.183307 avg loss no lamb -2.183307 time 2019-02-24 21:10:25.182249
Model ind 685 epoch 269 head B batch: 400 avg loss -2.164611 avg loss no lamb -2.164611 time 2019-02-24 21:11:38.423845
Model ind 685 epoch 269 head B batch: 0 avg loss -2.221624 avg loss no lamb -2.221624 time 2019-02-24 21:12:54.212162
Model ind 685 epoch 269 head B batch: 100 avg loss -2.201818 avg loss no lamb -2.201818 time 2019-02-24 21:14:09.706495
Model ind 685 epoch 269 head B batch: 200 avg loss -2.195357 avg loss no lamb -2.195357 time 2019-02-24 21:15:23.035631
Model ind 685 epoch 269 head B batch: 300 avg loss -2.225298 avg loss no lamb -2.225298 time 2019-02-24 21:16:36.142171
Model ind 685 epoch 269 head B batch: 400 avg loss -2.214387 avg loss no lamb -2.214387 time 2019-02-24 21:17:51.442273
Model ind 685 epoch 269 head A batch: 0 avg loss -2.211139 avg loss no lamb -2.211139 time 2019-02-24 21:19:09.936853
Model ind 685 epoch 269 head A batch: 100 avg loss -2.236834 avg loss no lamb -2.236834 time 2019-02-24 21:20:27.711400
Model ind 685 epoch 269 head A batch: 200 avg loss -2.206441 avg loss no lamb -2.206441 time 2019-02-24 21:21:42.164760
Model ind 685 epoch 269 head A batch: 300 avg loss -2.247766 avg loss no lamb -2.247766 time 2019-02-24 21:23:00.694079
Model ind 685 epoch 269 head A batch: 400 avg loss -2.165040 avg loss no lamb -2.165040 time 2019-02-24 21:24:18.414981
Pre: time 2019-02-24 21:25:48.449233: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 270
Model ind 685 epoch 270 head B batch: 0 avg loss -2.195055 avg loss no lamb -2.195055 time 2019-02-24 21:25:50.386039
Model ind 685 epoch 270 head B batch: 100 avg loss -2.246225 avg loss no lamb -2.246225 time 2019-02-24 21:27:04.412988
Model ind 685 epoch 270 head B batch: 200 avg loss -2.210425 avg loss no lamb -2.210425 time 2019-02-24 21:28:19.415297
Model ind 685 epoch 270 head B batch: 300 avg loss -2.199957 avg loss no lamb -2.199957 time 2019-02-24 21:29:31.524131
Model ind 685 epoch 270 head B batch: 400 avg loss -2.184044 avg loss no lamb -2.184044 time 2019-02-24 21:30:44.951097
Model ind 685 epoch 270 head B batch: 0 avg loss -2.234503 avg loss no lamb -2.234503 time 2019-02-24 21:32:00.466147
Model ind 685 epoch 270 head B batch: 100 avg loss -2.221081 avg loss no lamb -2.221081 time 2019-02-24 21:33:15.965933
Model ind 685 epoch 270 head B batch: 200 avg loss -2.184326 avg loss no lamb -2.184326 time 2019-02-24 21:34:33.549373
Model ind 685 epoch 270 head B batch: 300 avg loss -2.251413 avg loss no lamb -2.251413 time 2019-02-24 21:35:52.007423
Model ind 685 epoch 270 head B batch: 400 avg loss -2.208749 avg loss no lamb -2.208749 time 2019-02-24 21:37:07.737294
Model ind 685 epoch 270 head A batch: 0 avg loss -2.169448 avg loss no lamb -2.169448 time 2019-02-24 21:38:25.361746
Model ind 685 epoch 270 head A batch: 100 avg loss -2.207782 avg loss no lamb -2.207782 time 2019-02-24 21:39:33.512831
Model ind 685 epoch 270 head A batch: 200 avg loss -2.174930 avg loss no lamb -2.174930 time 2019-02-24 21:40:47.519546
Model ind 685 epoch 270 head A batch: 300 avg loss -2.217532 avg loss no lamb -2.217532 time 2019-02-24 21:42:06.099745
Model ind 685 epoch 270 head A batch: 400 avg loss -2.206647 avg loss no lamb -2.206647 time 2019-02-24 21:43:23.267709
Pre: time 2019-02-24 21:44:56.231436: 
 	std: 0.0064580827
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9790286, 0.99204284, 0.9790143, 0.9790143]
	train_accs: [0.99235713, 0.9790286, 0.99204284, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98429143
	best: 0.99235713

Starting e_i: 271
Model ind 685 epoch 271 head B batch: 0 avg loss -2.232539 avg loss no lamb -2.232539 time 2019-02-24 21:44:58.341838
Model ind 685 epoch 271 head B batch: 100 avg loss -2.190445 avg loss no lamb -2.190445 time 2019-02-24 21:46:14.372229
Model ind 685 epoch 271 head B batch: 200 avg loss -2.200425 avg loss no lamb -2.200425 time 2019-02-24 21:47:27.680602
Model ind 685 epoch 271 head B batch: 300 avg loss -2.222918 avg loss no lamb -2.222918 time 2019-02-24 21:48:41.718996
Model ind 685 epoch 271 head B batch: 400 avg loss -2.208953 avg loss no lamb -2.208953 time 2019-02-24 21:49:55.584983
Model ind 685 epoch 271 head B batch: 0 avg loss -2.217497 avg loss no lamb -2.217497 time 2019-02-24 21:51:13.284911
Model ind 685 epoch 271 head B batch: 100 avg loss -2.207671 avg loss no lamb -2.207671 time 2019-02-24 21:52:27.452509
Model ind 685 epoch 271 head B batch: 200 avg loss -2.206514 avg loss no lamb -2.206514 time 2019-02-24 21:53:42.724966
Model ind 685 epoch 271 head B batch: 300 avg loss -2.215170 avg loss no lamb -2.215170 time 2019-02-24 21:54:57.419067
Model ind 685 epoch 271 head B batch: 400 avg loss -2.184906 avg loss no lamb -2.184906 time 2019-02-24 21:56:12.331959
Model ind 685 epoch 271 head A batch: 0 avg loss -2.248190 avg loss no lamb -2.248190 time 2019-02-24 21:57:27.528308
Model ind 685 epoch 271 head A batch: 100 avg loss -2.213676 avg loss no lamb -2.213676 time 2019-02-24 21:58:43.819966
Model ind 685 epoch 271 head A batch: 200 avg loss -2.197790 avg loss no lamb -2.197790 time 2019-02-24 22:00:00.453513
Model ind 685 epoch 271 head A batch: 300 avg loss -2.228277 avg loss no lamb -2.228277 time 2019-02-24 22:01:15.213854
Model ind 685 epoch 271 head A batch: 400 avg loss -2.192310 avg loss no lamb -2.192310 time 2019-02-24 22:02:31.176568
Pre: time 2019-02-24 22:04:02.993210: 
 	std: 0.006477844
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9920857, 0.9790143, 0.9790286]
	train_accs: [0.9923857, 0.979, 0.9920857, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 272
Model ind 685 epoch 272 head B batch: 0 avg loss -2.240470 avg loss no lamb -2.240470 time 2019-02-24 22:04:05.022480
Model ind 685 epoch 272 head B batch: 100 avg loss -2.182218 avg loss no lamb -2.182218 time 2019-02-24 22:05:23.606455
Model ind 685 epoch 272 head B batch: 200 avg loss -2.218446 avg loss no lamb -2.218446 time 2019-02-24 22:06:40.616684
Model ind 685 epoch 272 head B batch: 300 avg loss -2.222986 avg loss no lamb -2.222986 time 2019-02-24 22:07:57.934875
Model ind 685 epoch 272 head B batch: 400 avg loss -2.169707 avg loss no lamb -2.169707 time 2019-02-24 22:09:13.261942
Model ind 685 epoch 272 head B batch: 0 avg loss -2.189692 avg loss no lamb -2.189692 time 2019-02-24 22:10:29.242171
Model ind 685 epoch 272 head B batch: 100 avg loss -2.243145 avg loss no lamb -2.243145 time 2019-02-24 22:11:45.582203
Model ind 685 epoch 272 head B batch: 200 avg loss -2.162015 avg loss no lamb -2.162015 time 2019-02-24 22:13:02.115871
Model ind 685 epoch 272 head B batch: 300 avg loss -2.257962 avg loss no lamb -2.257962 time 2019-02-24 22:14:18.828923
Model ind 685 epoch 272 head B batch: 400 avg loss -2.150736 avg loss no lamb -2.150736 time 2019-02-24 22:15:36.856950
Model ind 685 epoch 272 head A batch: 0 avg loss -2.222248 avg loss no lamb -2.222248 time 2019-02-24 22:16:54.264268
Model ind 685 epoch 272 head A batch: 100 avg loss -2.224604 avg loss no lamb -2.224604 time 2019-02-24 22:18:11.438256
Model ind 685 epoch 272 head A batch: 200 avg loss -2.186844 avg loss no lamb -2.186844 time 2019-02-24 22:19:27.989242
Model ind 685 epoch 272 head A batch: 300 avg loss -2.236631 avg loss no lamb -2.236631 time 2019-02-24 22:20:45.426211
Model ind 685 epoch 272 head A batch: 400 avg loss -2.212067 avg loss no lamb -2.212067 time 2019-02-24 22:21:53.648577
Pre: time 2019-02-24 22:23:29.146339: 
 	std: 0.006487188
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843343
	best: 0.9924286

Starting e_i: 273
Model ind 685 epoch 273 head B batch: 0 avg loss -2.236716 avg loss no lamb -2.236716 time 2019-02-24 22:23:31.297484
Model ind 685 epoch 273 head B batch: 100 avg loss -2.204875 avg loss no lamb -2.204875 time 2019-02-24 22:24:50.738797
Model ind 685 epoch 273 head B batch: 200 avg loss -2.164045 avg loss no lamb -2.164045 time 2019-02-24 22:26:09.805693
Model ind 685 epoch 273 head B batch: 300 avg loss -2.232201 avg loss no lamb -2.232201 time 2019-02-24 22:27:26.536063
Model ind 685 epoch 273 head B batch: 400 avg loss -2.192632 avg loss no lamb -2.192632 time 2019-02-24 22:28:44.256699
Model ind 685 epoch 273 head B batch: 0 avg loss -2.192019 avg loss no lamb -2.192019 time 2019-02-24 22:30:02.651725
Model ind 685 epoch 273 head B batch: 100 avg loss -2.171416 avg loss no lamb -2.171416 time 2019-02-24 22:31:19.303333
Model ind 685 epoch 273 head B batch: 200 avg loss -2.219471 avg loss no lamb -2.219471 time 2019-02-24 22:32:35.249172
Model ind 685 epoch 273 head B batch: 300 avg loss -2.214049 avg loss no lamb -2.214049 time 2019-02-24 22:33:53.513634
Model ind 685 epoch 273 head B batch: 400 avg loss -2.204333 avg loss no lamb -2.204333 time 2019-02-24 22:35:09.253030
Model ind 685 epoch 273 head A batch: 0 avg loss -2.221886 avg loss no lamb -2.221886 time 2019-02-24 22:36:26.430910
Model ind 685 epoch 273 head A batch: 100 avg loss -2.156188 avg loss no lamb -2.156188 time 2019-02-24 22:37:41.398362
Model ind 685 epoch 273 head A batch: 200 avg loss -2.199215 avg loss no lamb -2.199215 time 2019-02-24 22:38:59.454246
Model ind 685 epoch 273 head A batch: 300 avg loss -2.190751 avg loss no lamb -2.190751 time 2019-02-24 22:40:16.229383
Model ind 685 epoch 273 head A batch: 400 avg loss -2.205678 avg loss no lamb -2.205678 time 2019-02-24 22:41:33.014924
Pre: time 2019-02-24 22:43:06.336126: 
 	std: 0.0064951577
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.99212855, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.99212855, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 274
Model ind 685 epoch 274 head B batch: 0 avg loss -2.220931 avg loss no lamb -2.220931 time 2019-02-24 22:43:08.369191
Model ind 685 epoch 274 head B batch: 100 avg loss -2.210820 avg loss no lamb -2.210820 time 2019-02-24 22:44:26.284454
Model ind 685 epoch 274 head B batch: 200 avg loss -2.167182 avg loss no lamb -2.167182 time 2019-02-24 22:45:43.581249
Model ind 685 epoch 274 head B batch: 300 avg loss -2.260297 avg loss no lamb -2.260297 time 2019-02-24 22:47:01.941336
Model ind 685 epoch 274 head B batch: 400 avg loss -2.136210 avg loss no lamb -2.136210 time 2019-02-24 22:48:20.675092
Model ind 685 epoch 274 head B batch: 0 avg loss -2.197652 avg loss no lamb -2.197652 time 2019-02-24 22:49:35.997458
Model ind 685 epoch 274 head B batch: 100 avg loss -2.231758 avg loss no lamb -2.231758 time 2019-02-24 22:50:51.739667
Model ind 685 epoch 274 head B batch: 200 avg loss -2.223884 avg loss no lamb -2.223884 time 2019-02-24 22:52:09.859433
Model ind 685 epoch 274 head B batch: 300 avg loss -2.234645 avg loss no lamb -2.234645 time 2019-02-24 22:53:27.787349
Model ind 685 epoch 274 head B batch: 400 avg loss -2.224770 avg loss no lamb -2.224770 time 2019-02-24 22:54:44.747412
Model ind 685 epoch 274 head A batch: 0 avg loss -2.206720 avg loss no lamb -2.206720 time 2019-02-24 22:56:03.034013
Model ind 685 epoch 274 head A batch: 100 avg loss -2.146791 avg loss no lamb -2.146791 time 2019-02-24 22:57:20.801900
Model ind 685 epoch 274 head A batch: 200 avg loss -2.149812 avg loss no lamb -2.149812 time 2019-02-24 22:58:38.348223
Model ind 685 epoch 274 head A batch: 300 avg loss -2.220751 avg loss no lamb -2.220751 time 2019-02-24 22:59:53.979698
Model ind 685 epoch 274 head A batch: 400 avg loss -2.213061 avg loss no lamb -2.213061 time 2019-02-24 23:01:11.885630
Pre: time 2019-02-24 23:02:45.736828: 
 	std: 0.0064835004
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790428, 0.99214286, 0.9790428, 0.9790286]
	train_accs: [0.9924, 0.9790428, 0.99214286, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843314
	best: 0.9924

Starting e_i: 275
Model ind 685 epoch 275 head B batch: 0 avg loss -2.245641 avg loss no lamb -2.245641 time 2019-02-24 23:02:47.623958
Model ind 685 epoch 275 head B batch: 100 avg loss -2.197708 avg loss no lamb -2.197708 time 2019-02-24 23:04:04.248980
Model ind 685 epoch 275 head B batch: 200 avg loss -2.205072 avg loss no lamb -2.205072 time 2019-02-24 23:05:13.620249
Model ind 685 epoch 275 head B batch: 300 avg loss -2.219225 avg loss no lamb -2.219225 time 2019-02-24 23:06:30.247339
Model ind 685 epoch 275 head B batch: 400 avg loss -2.195117 avg loss no lamb -2.195117 time 2019-02-24 23:07:47.814358
Model ind 685 epoch 275 head B batch: 0 avg loss -2.219151 avg loss no lamb -2.219151 time 2019-02-24 23:09:04.949816
Model ind 685 epoch 275 head B batch: 100 avg loss -2.238811 avg loss no lamb -2.238811 time 2019-02-24 23:10:22.874923
Model ind 685 epoch 275 head B batch: 200 avg loss -2.206514 avg loss no lamb -2.206514 time 2019-02-24 23:11:40.970522
Model ind 685 epoch 275 head B batch: 300 avg loss -2.157786 avg loss no lamb -2.157786 time 2019-02-24 23:12:59.024021
Model ind 685 epoch 275 head B batch: 400 avg loss -2.181571 avg loss no lamb -2.181571 time 2019-02-24 23:14:17.327813
Model ind 685 epoch 275 head A batch: 0 avg loss -2.200647 avg loss no lamb -2.200647 time 2019-02-24 23:15:36.593539
Model ind 685 epoch 275 head A batch: 100 avg loss -2.235718 avg loss no lamb -2.235718 time 2019-02-24 23:16:53.937008
Model ind 685 epoch 275 head A batch: 200 avg loss -2.205281 avg loss no lamb -2.205281 time 2019-02-24 23:18:09.290428
Model ind 685 epoch 275 head A batch: 300 avg loss -2.227135 avg loss no lamb -2.227135 time 2019-02-24 23:19:26.228303
Model ind 685 epoch 275 head A batch: 400 avg loss -2.158336 avg loss no lamb -2.158336 time 2019-02-24 23:20:43.781646
Pre: time 2019-02-24 23:22:17.634784: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 276
Model ind 685 epoch 276 head B batch: 0 avg loss -2.200652 avg loss no lamb -2.200652 time 2019-02-24 23:22:20.066654
Model ind 685 epoch 276 head B batch: 100 avg loss -2.200176 avg loss no lamb -2.200176 time 2019-02-24 23:23:38.133175
Model ind 685 epoch 276 head B batch: 200 avg loss -2.199247 avg loss no lamb -2.199247 time 2019-02-24 23:24:55.551503
Model ind 685 epoch 276 head B batch: 300 avg loss -2.204765 avg loss no lamb -2.204765 time 2019-02-24 23:26:14.874130
Model ind 685 epoch 276 head B batch: 400 avg loss -2.179320 avg loss no lamb -2.179320 time 2019-02-24 23:27:32.487054
Model ind 685 epoch 276 head B batch: 0 avg loss -2.193209 avg loss no lamb -2.193209 time 2019-02-24 23:28:50.542473
Model ind 685 epoch 276 head B batch: 100 avg loss -2.208202 avg loss no lamb -2.208202 time 2019-02-24 23:30:07.123671
Model ind 685 epoch 276 head B batch: 200 avg loss -2.190379 avg loss no lamb -2.190379 time 2019-02-24 23:31:22.925891
Model ind 685 epoch 276 head B batch: 300 avg loss -2.229490 avg loss no lamb -2.229490 time 2019-02-24 23:32:38.914983
Model ind 685 epoch 276 head B batch: 400 avg loss -2.182351 avg loss no lamb -2.182351 time 2019-02-24 23:33:55.689062
Model ind 685 epoch 276 head A batch: 0 avg loss -2.197662 avg loss no lamb -2.197662 time 2019-02-24 23:35:12.583071
Model ind 685 epoch 276 head A batch: 100 avg loss -2.170241 avg loss no lamb -2.170241 time 2019-02-24 23:36:30.914902
Model ind 685 epoch 276 head A batch: 200 avg loss -2.201868 avg loss no lamb -2.201868 time 2019-02-24 23:37:49.833159
Model ind 685 epoch 276 head A batch: 300 avg loss -2.213602 avg loss no lamb -2.213602 time 2019-02-24 23:39:08.025258
Model ind 685 epoch 276 head A batch: 400 avg loss -2.186193 avg loss no lamb -2.186193 time 2019-02-24 23:40:25.781288
Pre: time 2019-02-24 23:42:00.336491: 
 	std: 0.0064963857
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99214286, 0.9790143, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.99214286, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 277
Model ind 685 epoch 277 head B batch: 0 avg loss -2.166515 avg loss no lamb -2.166515 time 2019-02-24 23:42:02.585244
Model ind 685 epoch 277 head B batch: 100 avg loss -2.192405 avg loss no lamb -2.192405 time 2019-02-24 23:43:20.590638
Model ind 685 epoch 277 head B batch: 200 avg loss -2.221899 avg loss no lamb -2.221899 time 2019-02-24 23:44:38.986007
Model ind 685 epoch 277 head B batch: 300 avg loss -2.227191 avg loss no lamb -2.227191 time 2019-02-24 23:45:53.884951
Model ind 685 epoch 277 head B batch: 400 avg loss -2.133706 avg loss no lamb -2.133706 time 2019-02-24 23:47:07.729304
Model ind 685 epoch 277 head B batch: 0 avg loss -2.206079 avg loss no lamb -2.206079 time 2019-02-24 23:48:20.163845
Model ind 685 epoch 277 head B batch: 100 avg loss -2.175292 avg loss no lamb -2.175292 time 2019-02-24 23:49:37.125811
Model ind 685 epoch 277 head B batch: 200 avg loss -2.224644 avg loss no lamb -2.224644 time 2019-02-24 23:50:54.527006
Model ind 685 epoch 277 head B batch: 300 avg loss -2.190638 avg loss no lamb -2.190638 time 2019-02-24 23:52:12.025335
Model ind 685 epoch 277 head B batch: 400 avg loss -2.178028 avg loss no lamb -2.178028 time 2019-02-24 23:53:28.692251
Model ind 685 epoch 277 head A batch: 0 avg loss -2.226250 avg loss no lamb -2.226250 time 2019-02-24 23:54:45.302235
Model ind 685 epoch 277 head A batch: 100 avg loss -2.203607 avg loss no lamb -2.203607 time 2019-02-24 23:56:02.723048
Model ind 685 epoch 277 head A batch: 200 avg loss -2.214014 avg loss no lamb -2.214014 time 2019-02-24 23:57:20.474392
Model ind 685 epoch 277 head A batch: 300 avg loss -2.221950 avg loss no lamb -2.221950 time 2019-02-24 23:58:38.570192
Model ind 685 epoch 277 head A batch: 400 avg loss -2.233365 avg loss no lamb -2.233365 time 2019-02-24 23:59:56.013796
Pre: time 2019-02-25 00:01:30.558756: 
 	std: 0.006480192
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.97905713, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.97905713, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843286
	best: 0.9924143

Starting e_i: 278
Model ind 685 epoch 278 head B batch: 0 avg loss -2.191566 avg loss no lamb -2.191566 time 2019-02-25 00:01:32.947833
Model ind 685 epoch 278 head B batch: 100 avg loss -2.189951 avg loss no lamb -2.189951 time 2019-02-25 00:02:49.136631
Model ind 685 epoch 278 head B batch: 200 avg loss -2.156219 avg loss no lamb -2.156219 time 2019-02-25 00:04:06.640984
Model ind 685 epoch 278 head B batch: 300 avg loss -2.207989 avg loss no lamb -2.207989 time 2019-02-25 00:05:25.943675
Model ind 685 epoch 278 head B batch: 400 avg loss -2.184215 avg loss no lamb -2.184215 time 2019-02-25 00:06:44.020171
Model ind 685 epoch 278 head B batch: 0 avg loss -2.217664 avg loss no lamb -2.217664 time 2019-02-25 00:08:01.578167
Model ind 685 epoch 278 head B batch: 100 avg loss -2.202103 avg loss no lamb -2.202103 time 2019-02-25 00:09:15.339353
Model ind 685 epoch 278 head B batch: 200 avg loss -2.228618 avg loss no lamb -2.228618 time 2019-02-25 00:10:31.714631
Model ind 685 epoch 278 head B batch: 300 avg loss -2.208575 avg loss no lamb -2.208575 time 2019-02-25 00:11:49.391272
Model ind 685 epoch 278 head B batch: 400 avg loss -2.227759 avg loss no lamb -2.227759 time 2019-02-25 00:13:07.116155
Model ind 685 epoch 278 head A batch: 0 avg loss -2.201875 avg loss no lamb -2.201875 time 2019-02-25 00:14:24.139443
Model ind 685 epoch 278 head A batch: 100 avg loss -2.218634 avg loss no lamb -2.218634 time 2019-02-25 00:15:42.026462
Model ind 685 epoch 278 head A batch: 200 avg loss -2.228185 avg loss no lamb -2.228185 time 2019-02-25 00:17:00.470645
Model ind 685 epoch 278 head A batch: 300 avg loss -2.215368 avg loss no lamb -2.215368 time 2019-02-25 00:18:17.555779
Model ind 685 epoch 278 head A batch: 400 avg loss -2.164447 avg loss no lamb -2.164447 time 2019-02-25 00:19:34.040968
Pre: time 2019-02-25 00:21:08.321710: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.99244285

Starting e_i: 279
Model ind 685 epoch 279 head B batch: 0 avg loss -2.194963 avg loss no lamb -2.194963 time 2019-02-25 00:21:10.495342
Model ind 685 epoch 279 head B batch: 100 avg loss -2.191814 avg loss no lamb -2.191814 time 2019-02-25 00:22:28.077100
Model ind 685 epoch 279 head B batch: 200 avg loss -2.167781 avg loss no lamb -2.167781 time 2019-02-25 00:23:44.787364
Model ind 685 epoch 279 head B batch: 300 avg loss -2.209434 avg loss no lamb -2.209434 time 2019-02-25 00:25:01.253960
Model ind 685 epoch 279 head B batch: 400 avg loss -2.193227 avg loss no lamb -2.193227 time 2019-02-25 00:26:18.202371
Model ind 685 epoch 279 head B batch: 0 avg loss -2.224592 avg loss no lamb -2.224592 time 2019-02-25 00:27:34.941155
Model ind 685 epoch 279 head B batch: 100 avg loss -2.220117 avg loss no lamb -2.220117 time 2019-02-25 00:28:51.391550
Model ind 685 epoch 279 head B batch: 200 avg loss -2.178965 avg loss no lamb -2.178965 time 2019-02-25 00:30:05.239934
Model ind 685 epoch 279 head B batch: 300 avg loss -2.210007 avg loss no lamb -2.210007 time 2019-02-25 00:31:18.815624
Model ind 685 epoch 279 head B batch: 400 avg loss -2.176547 avg loss no lamb -2.176547 time 2019-02-25 00:32:37.184662
Model ind 685 epoch 279 head A batch: 0 avg loss -2.187847 avg loss no lamb -2.187847 time 2019-02-25 00:33:54.499705
Model ind 685 epoch 279 head A batch: 100 avg loss -2.200944 avg loss no lamb -2.200944 time 2019-02-25 00:35:12.949960
Model ind 685 epoch 279 head A batch: 200 avg loss -2.190242 avg loss no lamb -2.190242 time 2019-02-25 00:36:30.245883
Model ind 685 epoch 279 head A batch: 300 avg loss -2.226550 avg loss no lamb -2.226550 time 2019-02-25 00:37:48.491836
Model ind 685 epoch 279 head A batch: 400 avg loss -2.200965 avg loss no lamb -2.200965 time 2019-02-25 00:39:05.196186
Pre: time 2019-02-25 00:40:39.236614: 
 	std: 0.006496526
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790428, 0.99215716, 0.9790428, 0.97905713]
	train_accs: [0.99245715, 0.9790428, 0.99215716, 0.9790428, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98435146
	best: 0.99245715

Starting e_i: 280
Model ind 685 epoch 280 head B batch: 0 avg loss -2.230655 avg loss no lamb -2.230655 time 2019-02-25 00:40:41.316901
Model ind 685 epoch 280 head B batch: 100 avg loss -2.166785 avg loss no lamb -2.166785 time 2019-02-25 00:41:58.968838
Model ind 685 epoch 280 head B batch: 200 avg loss -2.218490 avg loss no lamb -2.218490 time 2019-02-25 00:43:15.815967
Model ind 685 epoch 280 head B batch: 300 avg loss -2.245395 avg loss no lamb -2.245395 time 2019-02-25 00:44:32.451439
Model ind 685 epoch 280 head B batch: 400 avg loss -2.196565 avg loss no lamb -2.196565 time 2019-02-25 00:45:51.220691
Model ind 685 epoch 280 head B batch: 0 avg loss -2.217379 avg loss no lamb -2.217379 time 2019-02-25 00:47:06.854233
Model ind 685 epoch 280 head B batch: 100 avg loss -2.188083 avg loss no lamb -2.188083 time 2019-02-25 00:48:22.056781
Model ind 685 epoch 280 head B batch: 200 avg loss -2.169620 avg loss no lamb -2.169620 time 2019-02-25 00:49:39.329696
Model ind 685 epoch 280 head B batch: 300 avg loss -2.150338 avg loss no lamb -2.150338 time 2019-02-25 00:50:55.403579
Model ind 685 epoch 280 head B batch: 400 avg loss -2.188843 avg loss no lamb -2.188843 time 2019-02-25 00:52:09.025357
Model ind 685 epoch 280 head A batch: 0 avg loss -2.222160 avg loss no lamb -2.222160 time 2019-02-25 00:53:26.910089
Model ind 685 epoch 280 head A batch: 100 avg loss -2.212925 avg loss no lamb -2.212925 time 2019-02-25 00:54:43.635714
Model ind 685 epoch 280 head A batch: 200 avg loss -2.209458 avg loss no lamb -2.209458 time 2019-02-25 00:56:00.182354
Model ind 685 epoch 280 head A batch: 300 avg loss -2.214177 avg loss no lamb -2.214177 time 2019-02-25 00:57:17.461797
Model ind 685 epoch 280 head A batch: 400 avg loss -2.192075 avg loss no lamb -2.192075 time 2019-02-25 00:58:34.564978
Pre: time 2019-02-25 01:00:07.276975: 
 	std: 0.0065011876
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789429, 0.99207145, 0.9789571, 0.9789571]
	train_accs: [0.99237144, 0.9789429, 0.99207145, 0.9789571, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98425996
	best: 0.99237144

Starting e_i: 281
Model ind 685 epoch 281 head B batch: 0 avg loss -2.193485 avg loss no lamb -2.193485 time 2019-02-25 01:00:11.325196
Model ind 685 epoch 281 head B batch: 100 avg loss -2.217452 avg loss no lamb -2.217452 time 2019-02-25 01:01:29.395312
Model ind 685 epoch 281 head B batch: 200 avg loss -2.198832 avg loss no lamb -2.198832 time 2019-02-25 01:02:44.277775
Model ind 685 epoch 281 head B batch: 300 avg loss -2.182465 avg loss no lamb -2.182465 time 2019-02-25 01:04:02.821793
Model ind 685 epoch 281 head B batch: 400 avg loss -2.199908 avg loss no lamb -2.199908 time 2019-02-25 01:05:20.373456
Model ind 685 epoch 281 head B batch: 0 avg loss -2.217020 avg loss no lamb -2.217020 time 2019-02-25 01:06:39.320575
Model ind 685 epoch 281 head B batch: 100 avg loss -2.179656 avg loss no lamb -2.179656 time 2019-02-25 01:07:54.529525
Model ind 685 epoch 281 head B batch: 200 avg loss -2.222308 avg loss no lamb -2.222308 time 2019-02-25 01:09:10.123245
Model ind 685 epoch 281 head B batch: 300 avg loss -2.217097 avg loss no lamb -2.217097 time 2019-02-25 01:10:24.760732
Model ind 685 epoch 281 head B batch: 400 avg loss -2.203907 avg loss no lamb -2.203907 time 2019-02-25 01:11:38.587049
Model ind 685 epoch 281 head A batch: 0 avg loss -2.215972 avg loss no lamb -2.215972 time 2019-02-25 01:12:49.749955
Model ind 685 epoch 281 head A batch: 100 avg loss -2.226668 avg loss no lamb -2.226668 time 2019-02-25 01:14:00.596990
Model ind 685 epoch 281 head A batch: 200 avg loss -2.177393 avg loss no lamb -2.177393 time 2019-02-25 01:15:14.595100
Model ind 685 epoch 281 head A batch: 300 avg loss -2.236276 avg loss no lamb -2.236276 time 2019-02-25 01:16:28.386010
Model ind 685 epoch 281 head A batch: 400 avg loss -2.181267 avg loss no lamb -2.181267 time 2019-02-25 01:17:44.292600
Pre: time 2019-02-25 01:19:16.882675: 
 	std: 0.0064906124
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9921, 0.9789857, 0.979]
	train_accs: [0.9923857, 0.979, 0.9921, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 282
Model ind 685 epoch 282 head B batch: 0 avg loss -2.229898 avg loss no lamb -2.229898 time 2019-02-25 01:19:19.089652
Model ind 685 epoch 282 head B batch: 100 avg loss -2.213056 avg loss no lamb -2.213056 time 2019-02-25 01:20:36.211303
Model ind 685 epoch 282 head B batch: 200 avg loss -2.173692 avg loss no lamb -2.173692 time 2019-02-25 01:21:49.914193
Model ind 685 epoch 282 head B batch: 300 avg loss -2.210495 avg loss no lamb -2.210495 time 2019-02-25 01:23:07.280986
Model ind 685 epoch 282 head B batch: 400 avg loss -2.189102 avg loss no lamb -2.189102 time 2019-02-25 01:24:23.117823
Model ind 685 epoch 282 head B batch: 0 avg loss -2.220914 avg loss no lamb -2.220914 time 2019-02-25 01:25:38.773508
Model ind 685 epoch 282 head B batch: 100 avg loss -2.179111 avg loss no lamb -2.179111 time 2019-02-25 01:26:56.607655
Model ind 685 epoch 282 head B batch: 200 avg loss -2.217942 avg loss no lamb -2.217942 time 2019-02-25 01:28:13.520516
Model ind 685 epoch 282 head B batch: 300 avg loss -2.246209 avg loss no lamb -2.246209 time 2019-02-25 01:29:28.573212
Model ind 685 epoch 282 head B batch: 400 avg loss -2.167764 avg loss no lamb -2.167764 time 2019-02-25 01:30:44.507943
Model ind 685 epoch 282 head A batch: 0 avg loss -2.155247 avg loss no lamb -2.155247 time 2019-02-25 01:32:00.809330
Model ind 685 epoch 282 head A batch: 100 avg loss -2.146024 avg loss no lamb -2.146024 time 2019-02-25 01:33:16.195424
Model ind 685 epoch 282 head A batch: 200 avg loss -2.191959 avg loss no lamb -2.191959 time 2019-02-25 01:34:33.362133
Model ind 685 epoch 282 head A batch: 300 avg loss -2.196334 avg loss no lamb -2.196334 time 2019-02-25 01:35:51.982247
Model ind 685 epoch 282 head A batch: 400 avg loss -2.194800 avg loss no lamb -2.194800 time 2019-02-25 01:37:10.731400
Pre: time 2019-02-25 01:38:44.323477: 
 	std: 0.006469615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.97907144, 0.99212855, 0.97907144, 0.97905713]
	train_accs: [0.9924143, 0.97907144, 0.99212855, 0.97907144, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.9843486
	best: 0.9924143

Starting e_i: 283
Model ind 685 epoch 283 head B batch: 0 avg loss -2.204060 avg loss no lamb -2.204060 time 2019-02-25 01:38:46.211287
Model ind 685 epoch 283 head B batch: 100 avg loss -2.223385 avg loss no lamb -2.223385 time 2019-02-25 01:40:03.507735
Model ind 685 epoch 283 head B batch: 200 avg loss -2.219929 avg loss no lamb -2.219929 time 2019-02-25 01:41:18.420671
Model ind 685 epoch 283 head B batch: 300 avg loss -2.209763 avg loss no lamb -2.209763 time 2019-02-25 01:42:31.908929
Model ind 685 epoch 283 head B batch: 400 avg loss -2.177930 avg loss no lamb -2.177930 time 2019-02-25 01:43:48.808202
Model ind 685 epoch 283 head B batch: 0 avg loss -2.201659 avg loss no lamb -2.201659 time 2019-02-25 01:45:06.039794
Model ind 685 epoch 283 head B batch: 100 avg loss -2.207002 avg loss no lamb -2.207002 time 2019-02-25 01:46:23.643540
Model ind 685 epoch 283 head B batch: 200 avg loss -2.194793 avg loss no lamb -2.194793 time 2019-02-25 01:47:39.969054
Model ind 685 epoch 283 head B batch: 300 avg loss -2.219394 avg loss no lamb -2.219394 time 2019-02-25 01:48:57.405563
Model ind 685 epoch 283 head B batch: 400 avg loss -2.188242 avg loss no lamb -2.188242 time 2019-02-25 01:50:14.634524
Model ind 685 epoch 283 head A batch: 0 avg loss -2.224718 avg loss no lamb -2.224718 time 2019-02-25 01:51:32.069520
Model ind 685 epoch 283 head A batch: 100 avg loss -2.207320 avg loss no lamb -2.207320 time 2019-02-25 01:52:49.900779
Model ind 685 epoch 283 head A batch: 200 avg loss -2.166201 avg loss no lamb -2.166201 time 2019-02-25 01:54:08.311493
Model ind 685 epoch 283 head A batch: 300 avg loss -2.204244 avg loss no lamb -2.204244 time 2019-02-25 01:55:24.592284
Model ind 685 epoch 283 head A batch: 400 avg loss -2.164299 avg loss no lamb -2.164299 time 2019-02-25 01:56:27.149101
Pre: time 2019-02-25 01:57:57.671279: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 284
Model ind 685 epoch 284 head B batch: 0 avg loss -2.212427 avg loss no lamb -2.212427 time 2019-02-25 01:58:00.126595
Model ind 685 epoch 284 head B batch: 100 avg loss -2.202423 avg loss no lamb -2.202423 time 2019-02-25 01:59:15.268545
Model ind 685 epoch 284 head B batch: 200 avg loss -2.202559 avg loss no lamb -2.202559 time 2019-02-25 02:00:29.876673
Model ind 685 epoch 284 head B batch: 300 avg loss -2.210122 avg loss no lamb -2.210122 time 2019-02-25 02:01:46.508775
Model ind 685 epoch 284 head B batch: 400 avg loss -2.172984 avg loss no lamb -2.172984 time 2019-02-25 02:03:05.511479
Model ind 685 epoch 284 head B batch: 0 avg loss -2.198074 avg loss no lamb -2.198074 time 2019-02-25 02:04:23.312208
Model ind 685 epoch 284 head B batch: 100 avg loss -2.238924 avg loss no lamb -2.238924 time 2019-02-25 02:05:37.516587
Model ind 685 epoch 284 head B batch: 200 avg loss -2.223445 avg loss no lamb -2.223445 time 2019-02-25 02:06:53.447664
Model ind 685 epoch 284 head B batch: 300 avg loss -2.226012 avg loss no lamb -2.226012 time 2019-02-25 02:08:08.943581
Model ind 685 epoch 284 head B batch: 400 avg loss -2.172047 avg loss no lamb -2.172047 time 2019-02-25 02:09:25.388474
Model ind 685 epoch 284 head A batch: 0 avg loss -2.210872 avg loss no lamb -2.210872 time 2019-02-25 02:10:42.264631
Model ind 685 epoch 284 head A batch: 100 avg loss -2.223546 avg loss no lamb -2.223546 time 2019-02-25 02:11:59.384837
Model ind 685 epoch 284 head A batch: 200 avg loss -2.205841 avg loss no lamb -2.205841 time 2019-02-25 02:13:17.170063
Model ind 685 epoch 284 head A batch: 300 avg loss -2.218641 avg loss no lamb -2.218641 time 2019-02-25 02:14:32.400345
Model ind 685 epoch 284 head A batch: 400 avg loss -2.183133 avg loss no lamb -2.183133 time 2019-02-25 02:15:50.239586
Pre: time 2019-02-25 02:17:21.852151: 
 	std: 0.0064859455
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.9920857, 0.979, 0.9789857]
	train_accs: [0.99237144, 0.9789857, 0.9920857, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.99237144

Starting e_i: 285
Model ind 685 epoch 285 head B batch: 0 avg loss -2.207532 avg loss no lamb -2.207532 time 2019-02-25 02:17:23.736735
Model ind 685 epoch 285 head B batch: 100 avg loss -2.164648 avg loss no lamb -2.164648 time 2019-02-25 02:18:36.051632
Model ind 685 epoch 285 head B batch: 200 avg loss -2.203027 avg loss no lamb -2.203027 time 2019-02-25 02:19:55.947827
Model ind 685 epoch 285 head B batch: 300 avg loss -2.214117 avg loss no lamb -2.214117 time 2019-02-25 02:21:14.037677
Model ind 685 epoch 285 head B batch: 400 avg loss -2.174641 avg loss no lamb -2.174641 time 2019-02-25 02:22:28.383470
Model ind 685 epoch 285 head B batch: 0 avg loss -2.192205 avg loss no lamb -2.192205 time 2019-02-25 02:23:46.906070
Model ind 685 epoch 285 head B batch: 100 avg loss -2.211705 avg loss no lamb -2.211705 time 2019-02-25 02:25:03.070032
Model ind 685 epoch 285 head B batch: 200 avg loss -2.224590 avg loss no lamb -2.224590 time 2019-02-25 02:26:16.486423
Model ind 685 epoch 285 head B batch: 300 avg loss -2.216718 avg loss no lamb -2.216718 time 2019-02-25 02:27:34.876275
Model ind 685 epoch 285 head B batch: 400 avg loss -2.183017 avg loss no lamb -2.183017 time 2019-02-25 02:28:54.870485
Model ind 685 epoch 285 head A batch: 0 avg loss -2.198950 avg loss no lamb -2.198950 time 2019-02-25 02:30:11.337540
Model ind 685 epoch 285 head A batch: 100 avg loss -2.207206 avg loss no lamb -2.207206 time 2019-02-25 02:31:28.849406
Model ind 685 epoch 285 head A batch: 200 avg loss -2.190076 avg loss no lamb -2.190076 time 2019-02-25 02:32:42.358943
Model ind 685 epoch 285 head A batch: 300 avg loss -2.238598 avg loss no lamb -2.238598 time 2019-02-25 02:34:01.368505
Model ind 685 epoch 285 head A batch: 400 avg loss -2.181908 avg loss no lamb -2.181908 time 2019-02-25 02:35:14.179597
Pre: time 2019-02-25 02:36:47.670102: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 286
Model ind 685 epoch 286 head B batch: 0 avg loss -2.175168 avg loss no lamb -2.175168 time 2019-02-25 02:36:49.616579
Model ind 685 epoch 286 head B batch: 100 avg loss -2.201790 avg loss no lamb -2.201790 time 2019-02-25 02:38:06.892109
Model ind 685 epoch 286 head B batch: 200 avg loss -2.217570 avg loss no lamb -2.217570 time 2019-02-25 02:39:18.481812
Model ind 685 epoch 286 head B batch: 300 avg loss -2.224185 avg loss no lamb -2.224185 time 2019-02-25 02:40:34.234726
Model ind 685 epoch 286 head B batch: 400 avg loss -2.190763 avg loss no lamb -2.190763 time 2019-02-25 02:41:50.394243
Model ind 685 epoch 286 head B batch: 0 avg loss -2.225427 avg loss no lamb -2.225427 time 2019-02-25 02:43:08.650264
Model ind 685 epoch 286 head B batch: 100 avg loss -2.190356 avg loss no lamb -2.190356 time 2019-02-25 02:44:25.643527
Model ind 685 epoch 286 head B batch: 200 avg loss -2.135709 avg loss no lamb -2.135709 time 2019-02-25 02:45:41.525184
Model ind 685 epoch 286 head B batch: 300 avg loss -2.247932 avg loss no lamb -2.247932 time 2019-02-25 02:46:59.513365
Model ind 685 epoch 286 head B batch: 400 avg loss -2.185256 avg loss no lamb -2.185256 time 2019-02-25 02:48:18.305397
Model ind 685 epoch 286 head A batch: 0 avg loss -2.218624 avg loss no lamb -2.218624 time 2019-02-25 02:49:36.914760
Model ind 685 epoch 286 head A batch: 100 avg loss -2.193082 avg loss no lamb -2.193082 time 2019-02-25 02:50:56.907620
Model ind 685 epoch 286 head A batch: 200 avg loss -2.167461 avg loss no lamb -2.167461 time 2019-02-25 02:52:15.894914
Model ind 685 epoch 286 head A batch: 300 avg loss -2.222227 avg loss no lamb -2.222227 time 2019-02-25 02:53:35.361539
Model ind 685 epoch 286 head A batch: 400 avg loss -2.185865 avg loss no lamb -2.185865 time 2019-02-25 02:54:55.815809
Pre: time 2019-02-25 02:56:33.978359: 
 	std: 0.006463852
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	train_accs: [0.9923143, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842485
	best: 0.9923143

Starting e_i: 287
Model ind 685 epoch 287 head B batch: 0 avg loss -2.215957 avg loss no lamb -2.215957 time 2019-02-25 02:56:36.184151
Model ind 685 epoch 287 head B batch: 100 avg loss -2.178292 avg loss no lamb -2.178292 time 2019-02-25 02:57:53.844379
Model ind 685 epoch 287 head B batch: 200 avg loss -2.221444 avg loss no lamb -2.221444 time 2019-02-25 02:59:07.415475
Model ind 685 epoch 287 head B batch: 300 avg loss -2.186071 avg loss no lamb -2.186071 time 2019-02-25 03:00:22.648010
Model ind 685 epoch 287 head B batch: 400 avg loss -2.202872 avg loss no lamb -2.202872 time 2019-02-25 03:01:36.815571
Model ind 685 epoch 287 head B batch: 0 avg loss -2.198130 avg loss no lamb -2.198130 time 2019-02-25 03:02:55.841296
Model ind 685 epoch 287 head B batch: 100 avg loss -2.193009 avg loss no lamb -2.193009 time 2019-02-25 03:04:15.401882
Model ind 685 epoch 287 head B batch: 200 avg loss -2.192554 avg loss no lamb -2.192554 time 2019-02-25 03:05:33.407448
Model ind 685 epoch 287 head B batch: 300 avg loss -2.250251 avg loss no lamb -2.250251 time 2019-02-25 03:06:51.385984
Model ind 685 epoch 287 head B batch: 400 avg loss -2.168772 avg loss no lamb -2.168772 time 2019-02-25 03:08:05.886597
Model ind 685 epoch 287 head A batch: 0 avg loss -2.201093 avg loss no lamb -2.201093 time 2019-02-25 03:09:24.930528
Model ind 685 epoch 287 head A batch: 100 avg loss -2.149482 avg loss no lamb -2.149482 time 2019-02-25 03:10:41.526761
Model ind 685 epoch 287 head A batch: 200 avg loss -2.194417 avg loss no lamb -2.194417 time 2019-02-25 03:12:01.267509
Model ind 685 epoch 287 head A batch: 300 avg loss -2.218211 avg loss no lamb -2.218211 time 2019-02-25 03:13:18.044169
Model ind 685 epoch 287 head A batch: 400 avg loss -2.178777 avg loss no lamb -2.178777 time 2019-02-25 03:14:30.301192
Pre: time 2019-02-25 03:16:05.107592: 
 	std: 0.006478819
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790428, 0.99212855, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790428, 0.99212855, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432285
	best: 0.9923857

Starting e_i: 288
Model ind 685 epoch 288 head B batch: 0 avg loss -2.220749 avg loss no lamb -2.220749 time 2019-02-25 03:16:07.324469
Model ind 685 epoch 288 head B batch: 100 avg loss -2.209359 avg loss no lamb -2.209359 time 2019-02-25 03:17:24.219575
Model ind 685 epoch 288 head B batch: 200 avg loss -2.206462 avg loss no lamb -2.206462 time 2019-02-25 03:18:40.244887
Model ind 685 epoch 288 head B batch: 300 avg loss -2.220959 avg loss no lamb -2.220959 time 2019-02-25 03:19:59.010951
Model ind 685 epoch 288 head B batch: 400 avg loss -2.236105 avg loss no lamb -2.236105 time 2019-02-25 03:21:03.778792
Model ind 685 epoch 288 head B batch: 0 avg loss -2.216145 avg loss no lamb -2.216145 time 2019-02-25 03:22:17.333532
Model ind 685 epoch 288 head B batch: 100 avg loss -2.195448 avg loss no lamb -2.195448 time 2019-02-25 03:23:34.222083
Model ind 685 epoch 288 head B batch: 200 avg loss -2.239669 avg loss no lamb -2.239669 time 2019-02-25 03:24:51.821342
Model ind 685 epoch 288 head B batch: 300 avg loss -2.231802 avg loss no lamb -2.231802 time 2019-02-25 03:26:09.877115
Model ind 685 epoch 288 head B batch: 400 avg loss -2.153778 avg loss no lamb -2.153778 time 2019-02-25 03:27:27.958000
Model ind 685 epoch 288 head A batch: 0 avg loss -2.191979 avg loss no lamb -2.191979 time 2019-02-25 03:28:44.928660
Model ind 685 epoch 288 head A batch: 100 avg loss -2.218359 avg loss no lamb -2.218359 time 2019-02-25 03:29:59.353016
Model ind 685 epoch 288 head A batch: 200 avg loss -2.208024 avg loss no lamb -2.208024 time 2019-02-25 03:31:17.152581
Model ind 685 epoch 288 head A batch: 300 avg loss -2.243413 avg loss no lamb -2.243413 time 2019-02-25 03:32:36.954381
Model ind 685 epoch 288 head A batch: 400 avg loss -2.190201 avg loss no lamb -2.190201 time 2019-02-25 03:33:50.310030
Pre: time 2019-02-25 03:35:22.337926: 
 	std: 0.0064930897
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790286]
	train_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.9924286

Starting e_i: 289
Model ind 685 epoch 289 head B batch: 0 avg loss -2.178147 avg loss no lamb -2.178147 time 2019-02-25 03:35:24.489167
Model ind 685 epoch 289 head B batch: 100 avg loss -2.209533 avg loss no lamb -2.209533 time 2019-02-25 03:36:37.169979
Model ind 685 epoch 289 head B batch: 200 avg loss -2.163416 avg loss no lamb -2.163416 time 2019-02-25 03:37:49.401188
Model ind 685 epoch 289 head B batch: 300 avg loss -2.162371 avg loss no lamb -2.162371 time 2019-02-25 03:39:05.993914
Model ind 685 epoch 289 head B batch: 400 avg loss -2.173978 avg loss no lamb -2.173978 time 2019-02-25 03:40:25.086431
Model ind 685 epoch 289 head B batch: 0 avg loss -2.221097 avg loss no lamb -2.221097 time 2019-02-25 03:41:43.721512
Model ind 685 epoch 289 head B batch: 100 avg loss -2.219756 avg loss no lamb -2.219756 time 2019-02-25 03:43:01.280806
Model ind 685 epoch 289 head B batch: 200 avg loss -2.189844 avg loss no lamb -2.189844 time 2019-02-25 03:44:21.694602
Model ind 685 epoch 289 head B batch: 300 avg loss -2.237273 avg loss no lamb -2.237273 time 2019-02-25 03:45:41.992812
Model ind 685 epoch 289 head B batch: 400 avg loss -2.203108 avg loss no lamb -2.203108 time 2019-02-25 03:47:02.742970
Model ind 685 epoch 289 head A batch: 0 avg loss -2.218123 avg loss no lamb -2.218123 time 2019-02-25 03:48:23.337135
Model ind 685 epoch 289 head A batch: 100 avg loss -2.215128 avg loss no lamb -2.215128 time 2019-02-25 03:49:42.751352
Model ind 685 epoch 289 head A batch: 200 avg loss -2.224936 avg loss no lamb -2.224936 time 2019-02-25 03:50:56.532623
Model ind 685 epoch 289 head A batch: 300 avg loss -2.185507 avg loss no lamb -2.185507 time 2019-02-25 03:52:09.098789
Model ind 685 epoch 289 head A batch: 400 avg loss -2.179260 avg loss no lamb -2.179260 time 2019-02-25 03:53:21.201824
Pre: time 2019-02-25 03:54:54.024594: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 290
Model ind 685 epoch 290 head B batch: 0 avg loss -2.229317 avg loss no lamb -2.229317 time 2019-02-25 03:54:55.904637
Model ind 685 epoch 290 head B batch: 100 avg loss -2.194827 avg loss no lamb -2.194827 time 2019-02-25 03:56:14.222042
Model ind 685 epoch 290 head B batch: 200 avg loss -2.200543 avg loss no lamb -2.200543 time 2019-02-25 03:57:28.712198
Model ind 685 epoch 290 head B batch: 300 avg loss -2.205021 avg loss no lamb -2.205021 time 2019-02-25 03:58:45.965371
Model ind 685 epoch 290 head B batch: 400 avg loss -2.217088 avg loss no lamb -2.217088 time 2019-02-25 04:00:06.318406
Model ind 685 epoch 290 head B batch: 0 avg loss -2.206876 avg loss no lamb -2.206876 time 2019-02-25 04:01:27.850840
Model ind 685 epoch 290 head B batch: 100 avg loss -2.202963 avg loss no lamb -2.202963 time 2019-02-25 04:02:47.743106
Model ind 685 epoch 290 head B batch: 200 avg loss -2.196487 avg loss no lamb -2.196487 time 2019-02-25 04:03:59.108546
Model ind 685 epoch 290 head B batch: 300 avg loss -2.255090 avg loss no lamb -2.255090 time 2019-02-25 04:05:21.232854
Model ind 685 epoch 290 head B batch: 400 avg loss -2.200187 avg loss no lamb -2.200187 time 2019-02-25 04:06:39.309586
Model ind 685 epoch 290 head A batch: 0 avg loss -2.206770 avg loss no lamb -2.206770 time 2019-02-25 04:07:55.718942
Model ind 685 epoch 290 head A batch: 100 avg loss -2.236192 avg loss no lamb -2.236192 time 2019-02-25 04:09:10.968941
Model ind 685 epoch 290 head A batch: 200 avg loss -2.211418 avg loss no lamb -2.211418 time 2019-02-25 04:10:28.154469
Model ind 685 epoch 290 head A batch: 300 avg loss -2.199790 avg loss no lamb -2.199790 time 2019-02-25 04:11:46.878296
Model ind 685 epoch 290 head A batch: 400 avg loss -2.191649 avg loss no lamb -2.191649 time 2019-02-25 04:13:04.358218
Pre: time 2019-02-25 04:14:41.095061: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 291
Model ind 685 epoch 291 head B batch: 0 avg loss -2.211937 avg loss no lamb -2.211937 time 2019-02-25 04:14:43.012177
Model ind 685 epoch 291 head B batch: 100 avg loss -2.146534 avg loss no lamb -2.146534 time 2019-02-25 04:16:02.666067
Model ind 685 epoch 291 head B batch: 200 avg loss -2.205866 avg loss no lamb -2.205866 time 2019-02-25 04:17:22.648644
Model ind 685 epoch 291 head B batch: 300 avg loss -2.249286 avg loss no lamb -2.249286 time 2019-02-25 04:18:37.824490
Model ind 685 epoch 291 head B batch: 400 avg loss -2.229840 avg loss no lamb -2.229840 time 2019-02-25 04:19:55.273934
Model ind 685 epoch 291 head B batch: 0 avg loss -2.240708 avg loss no lamb -2.240708 time 2019-02-25 04:21:13.213060
Model ind 685 epoch 291 head B batch: 100 avg loss -2.210482 avg loss no lamb -2.210482 time 2019-02-25 04:22:33.301058
Model ind 685 epoch 291 head B batch: 200 avg loss -2.191787 avg loss no lamb -2.191787 time 2019-02-25 04:23:53.231328
Model ind 685 epoch 291 head B batch: 300 avg loss -2.223395 avg loss no lamb -2.223395 time 2019-02-25 04:25:11.258069
Model ind 685 epoch 291 head B batch: 400 avg loss -2.173389 avg loss no lamb -2.173389 time 2019-02-25 04:26:29.591221
Model ind 685 epoch 291 head A batch: 0 avg loss -2.211908 avg loss no lamb -2.211908 time 2019-02-25 04:27:49.878018
Model ind 685 epoch 291 head A batch: 100 avg loss -2.178208 avg loss no lamb -2.178208 time 2019-02-25 04:29:03.179852
Model ind 685 epoch 291 head A batch: 200 avg loss -2.232854 avg loss no lamb -2.232854 time 2019-02-25 04:30:19.114155
Model ind 685 epoch 291 head A batch: 300 avg loss -2.186356 avg loss no lamb -2.186356 time 2019-02-25 04:31:39.115242
Model ind 685 epoch 291 head A batch: 400 avg loss -2.183582 avg loss no lamb -2.183582 time 2019-02-25 04:32:53.229740
Pre: time 2019-02-25 04:34:22.198018: 
 	std: 0.0064918525
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924143

Starting e_i: 292
Model ind 685 epoch 292 head B batch: 0 avg loss -2.189177 avg loss no lamb -2.189177 time 2019-02-25 04:34:24.338973
Model ind 685 epoch 292 head B batch: 100 avg loss -2.180358 avg loss no lamb -2.180358 time 2019-02-25 04:35:37.439011
Model ind 685 epoch 292 head B batch: 200 avg loss -2.186890 avg loss no lamb -2.186890 time 2019-02-25 04:36:53.035644
Model ind 685 epoch 292 head B batch: 300 avg loss -2.215992 avg loss no lamb -2.215992 time 2019-02-25 04:38:04.929966
Model ind 685 epoch 292 head B batch: 400 avg loss -2.217130 avg loss no lamb -2.217130 time 2019-02-25 04:39:23.166922
Model ind 685 epoch 292 head B batch: 0 avg loss -2.192219 avg loss no lamb -2.192219 time 2019-02-25 04:40:36.131317
Model ind 685 epoch 292 head B batch: 100 avg loss -2.136329 avg loss no lamb -2.136329 time 2019-02-25 04:41:47.257062
Model ind 685 epoch 292 head B batch: 200 avg loss -2.193392 avg loss no lamb -2.193392 time 2019-02-25 04:43:03.104738
Model ind 685 epoch 292 head B batch: 300 avg loss -2.204621 avg loss no lamb -2.204621 time 2019-02-25 04:44:21.259560
Model ind 685 epoch 292 head B batch: 400 avg loss -2.137894 avg loss no lamb -2.137894 time 2019-02-25 04:45:37.498251
Model ind 685 epoch 292 head A batch: 0 avg loss -2.215750 avg loss no lamb -2.215750 time 2019-02-25 04:46:53.249747
Model ind 685 epoch 292 head A batch: 100 avg loss -2.185913 avg loss no lamb -2.185913 time 2019-02-25 04:48:12.494734
Model ind 685 epoch 292 head A batch: 200 avg loss -2.203892 avg loss no lamb -2.203892 time 2019-02-25 04:49:30.803121
Model ind 685 epoch 292 head A batch: 300 avg loss -2.221122 avg loss no lamb -2.221122 time 2019-02-25 04:50:46.770877
Model ind 685 epoch 292 head A batch: 400 avg loss -2.197889 avg loss no lamb -2.197889 time 2019-02-25 04:52:07.370607
Pre: time 2019-02-25 04:53:44.418586: 
 	std: 0.006511615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789571, 0.9921143, 0.9789714, 0.9789714]
	train_accs: [0.9924, 0.9789571, 0.9921143, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.98428285
	best: 0.9924

Starting e_i: 293
Model ind 685 epoch 293 head B batch: 0 avg loss -2.244413 avg loss no lamb -2.244413 time 2019-02-25 04:53:46.651289
Model ind 685 epoch 293 head B batch: 100 avg loss -2.172843 avg loss no lamb -2.172843 time 2019-02-25 04:55:08.553960
Model ind 685 epoch 293 head B batch: 200 avg loss -2.194530 avg loss no lamb -2.194530 time 2019-02-25 04:56:28.989400
Model ind 685 epoch 293 head B batch: 300 avg loss -2.247500 avg loss no lamb -2.247500 time 2019-02-25 04:57:47.267057
Model ind 685 epoch 293 head B batch: 400 avg loss -2.184083 avg loss no lamb -2.184083 time 2019-02-25 04:59:06.241951
Model ind 685 epoch 293 head B batch: 0 avg loss -2.243107 avg loss no lamb -2.243107 time 2019-02-25 05:00:24.815794
Model ind 685 epoch 293 head B batch: 100 avg loss -2.192987 avg loss no lamb -2.192987 time 2019-02-25 05:01:38.264876
Model ind 685 epoch 293 head B batch: 200 avg loss -2.220967 avg loss no lamb -2.220967 time 2019-02-25 05:02:54.788280
Model ind 685 epoch 293 head B batch: 300 avg loss -2.179342 avg loss no lamb -2.179342 time 2019-02-25 05:04:12.324644
Model ind 685 epoch 293 head B batch: 400 avg loss -2.195703 avg loss no lamb -2.195703 time 2019-02-25 05:05:30.346887
Model ind 685 epoch 293 head A batch: 0 avg loss -2.214155 avg loss no lamb -2.214155 time 2019-02-25 05:06:46.182155
Model ind 685 epoch 293 head A batch: 100 avg loss -2.208333 avg loss no lamb -2.208333 time 2019-02-25 05:08:03.542983
Model ind 685 epoch 293 head A batch: 200 avg loss -2.182419 avg loss no lamb -2.182419 time 2019-02-25 05:09:16.520048
Model ind 685 epoch 293 head A batch: 300 avg loss -2.194764 avg loss no lamb -2.194764 time 2019-02-25 05:10:28.296474
Model ind 685 epoch 293 head A batch: 400 avg loss -2.188128 avg loss no lamb -2.188128 time 2019-02-25 05:11:43.106445
Pre: time 2019-02-25 05:13:15.941394: 
 	std: 0.006480315
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9920857, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.9920857, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924143

Starting e_i: 294
Model ind 685 epoch 294 head B batch: 0 avg loss -2.202978 avg loss no lamb -2.202978 time 2019-02-25 05:13:17.858371
Model ind 685 epoch 294 head B batch: 100 avg loss -2.191513 avg loss no lamb -2.191513 time 2019-02-25 05:14:35.713662
Model ind 685 epoch 294 head B batch: 200 avg loss -2.230800 avg loss no lamb -2.230800 time 2019-02-25 05:15:53.474748
Model ind 685 epoch 294 head B batch: 300 avg loss -2.217819 avg loss no lamb -2.217819 time 2019-02-25 05:17:13.465527
Model ind 685 epoch 294 head B batch: 400 avg loss -2.202396 avg loss no lamb -2.202396 time 2019-02-25 05:18:32.815161
Model ind 685 epoch 294 head B batch: 0 avg loss -2.213665 avg loss no lamb -2.213665 time 2019-02-25 05:19:51.875704
Model ind 685 epoch 294 head B batch: 100 avg loss -2.141959 avg loss no lamb -2.141959 time 2019-02-25 05:21:12.932746
Model ind 685 epoch 294 head B batch: 200 avg loss -2.198268 avg loss no lamb -2.198268 time 2019-02-25 05:22:34.890922
Model ind 685 epoch 294 head B batch: 300 avg loss -2.252095 avg loss no lamb -2.252095 time 2019-02-25 05:23:56.997562
Model ind 685 epoch 294 head B batch: 400 avg loss -2.180542 avg loss no lamb -2.180542 time 2019-02-25 05:25:18.295315
Model ind 685 epoch 294 head A batch: 0 avg loss -2.211966 avg loss no lamb -2.211966 time 2019-02-25 05:26:35.099051
Model ind 685 epoch 294 head A batch: 100 avg loss -2.222956 avg loss no lamb -2.222956 time 2019-02-25 05:27:49.764115
Model ind 685 epoch 294 head A batch: 200 avg loss -2.220139 avg loss no lamb -2.220139 time 2019-02-25 05:29:03.305896
Model ind 685 epoch 294 head A batch: 300 avg loss -2.247865 avg loss no lamb -2.247865 time 2019-02-25 05:30:23.915535
Model ind 685 epoch 294 head A batch: 400 avg loss -2.174953 avg loss no lamb -2.174953 time 2019-02-25 05:31:45.783075
Pre: time 2019-02-25 05:33:19.916242: 
 	std: 0.00650227
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	train_accs: [0.9924, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842943
	best: 0.9924

Starting e_i: 295
Model ind 685 epoch 295 head B batch: 0 avg loss -2.133821 avg loss no lamb -2.133821 time 2019-02-25 05:33:21.787969
Model ind 685 epoch 295 head B batch: 100 avg loss -2.205315 avg loss no lamb -2.205315 time 2019-02-25 05:34:36.512221
Model ind 685 epoch 295 head B batch: 200 avg loss -2.209996 avg loss no lamb -2.209996 time 2019-02-25 05:35:53.840674
Model ind 685 epoch 295 head B batch: 300 avg loss -2.218610 avg loss no lamb -2.218610 time 2019-02-25 05:37:14.054301
Model ind 685 epoch 295 head B batch: 400 avg loss -2.181531 avg loss no lamb -2.181531 time 2019-02-25 05:38:32.643079
Model ind 685 epoch 295 head B batch: 0 avg loss -2.218411 avg loss no lamb -2.218411 time 2019-02-25 05:39:50.692092
Model ind 685 epoch 295 head B batch: 100 avg loss -2.216193 avg loss no lamb -2.216193 time 2019-02-25 05:41:11.558138
Model ind 685 epoch 295 head B batch: 200 avg loss -2.191381 avg loss no lamb -2.191381 time 2019-02-25 05:42:27.420261
Model ind 685 epoch 295 head B batch: 300 avg loss -2.215827 avg loss no lamb -2.215827 time 2019-02-25 05:43:40.869906
Model ind 685 epoch 295 head B batch: 400 avg loss -2.190574 avg loss no lamb -2.190574 time 2019-02-25 05:44:59.517730
Model ind 685 epoch 295 head A batch: 0 avg loss -2.181351 avg loss no lamb -2.181351 time 2019-02-25 05:46:13.182319
Model ind 685 epoch 295 head A batch: 100 avg loss -2.179148 avg loss no lamb -2.179148 time 2019-02-25 05:47:30.634279
Model ind 685 epoch 295 head A batch: 200 avg loss -2.200892 avg loss no lamb -2.200892 time 2019-02-25 05:48:49.943874
Model ind 685 epoch 295 head A batch: 300 avg loss -2.223406 avg loss no lamb -2.223406 time 2019-02-25 05:50:05.035452
Model ind 685 epoch 295 head A batch: 400 avg loss -2.201310 avg loss no lamb -2.201310 time 2019-02-25 05:51:21.688851
Pre: time 2019-02-25 05:52:53.119610: 
 	std: 0.006491823
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	train_accs: [0.9923857, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.9923857

Starting e_i: 296
Model ind 685 epoch 296 head B batch: 0 avg loss -2.247363 avg loss no lamb -2.247363 time 2019-02-25 05:52:55.000220
Model ind 685 epoch 296 head B batch: 100 avg loss -2.210890 avg loss no lamb -2.210890 time 2019-02-25 05:54:08.391337
Model ind 685 epoch 296 head B batch: 200 avg loss -2.177083 avg loss no lamb -2.177083 time 2019-02-25 05:55:23.305547
Model ind 685 epoch 296 head B batch: 300 avg loss -2.251204 avg loss no lamb -2.251204 time 2019-02-25 05:56:35.830183
Model ind 685 epoch 296 head B batch: 400 avg loss -2.179024 avg loss no lamb -2.179024 time 2019-02-25 05:57:49.948253
Model ind 685 epoch 296 head B batch: 0 avg loss -2.196774 avg loss no lamb -2.196774 time 2019-02-25 05:59:08.634458
Model ind 685 epoch 296 head B batch: 100 avg loss -2.209349 avg loss no lamb -2.209349 time 2019-02-25 06:00:27.988205
Model ind 685 epoch 296 head B batch: 200 avg loss -2.176666 avg loss no lamb -2.176666 time 2019-02-25 06:01:41.446778
Model ind 685 epoch 296 head B batch: 300 avg loss -2.227234 avg loss no lamb -2.227234 time 2019-02-25 06:02:57.497179
Model ind 685 epoch 296 head B batch: 400 avg loss -2.201732 avg loss no lamb -2.201732 time 2019-02-25 06:04:16.351395
Model ind 685 epoch 296 head A batch: 0 avg loss -2.199615 avg loss no lamb -2.199615 time 2019-02-25 06:05:31.956100
Model ind 685 epoch 296 head A batch: 100 avg loss -2.169005 avg loss no lamb -2.169005 time 2019-02-25 06:06:47.507115
Model ind 685 epoch 296 head A batch: 200 avg loss -2.194963 avg loss no lamb -2.194963 time 2019-02-25 06:08:01.424517
Model ind 685 epoch 296 head A batch: 300 avg loss -2.201905 avg loss no lamb -2.201905 time 2019-02-25 06:09:17.088225
Model ind 685 epoch 296 head A batch: 400 avg loss -2.202014 avg loss no lamb -2.202014 time 2019-02-25 06:10:26.939404
Pre: time 2019-02-25 06:11:57.053729: 
 	std: 0.0064932345
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99247146, 0.97905713, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.99247146, 0.97905713, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843486
	best: 0.99247146

Starting e_i: 297
Model ind 685 epoch 297 head B batch: 0 avg loss -2.228072 avg loss no lamb -2.228072 time 2019-02-25 06:11:58.979802
Model ind 685 epoch 297 head B batch: 100 avg loss -2.206223 avg loss no lamb -2.206223 time 2019-02-25 06:13:15.034324
Model ind 685 epoch 297 head B batch: 200 avg loss -2.255757 avg loss no lamb -2.255757 time 2019-02-25 06:14:32.209646
Model ind 685 epoch 297 head B batch: 300 avg loss -2.206554 avg loss no lamb -2.206554 time 2019-02-25 06:15:45.619514
Model ind 685 epoch 297 head B batch: 400 avg loss -2.160665 avg loss no lamb -2.160665 time 2019-02-25 06:16:57.374116
Model ind 685 epoch 297 head B batch: 0 avg loss -2.214908 avg loss no lamb -2.214908 time 2019-02-25 06:18:10.910801
Model ind 685 epoch 297 head B batch: 100 avg loss -2.201065 avg loss no lamb -2.201065 time 2019-02-25 06:19:26.293433
Model ind 685 epoch 297 head B batch: 200 avg loss -2.200655 avg loss no lamb -2.200655 time 2019-02-25 06:20:43.391057
Model ind 685 epoch 297 head B batch: 300 avg loss -2.225724 avg loss no lamb -2.225724 time 2019-02-25 06:21:55.368730
Model ind 685 epoch 297 head B batch: 400 avg loss -2.229944 avg loss no lamb -2.229944 time 2019-02-25 06:23:08.590651
Model ind 685 epoch 297 head A batch: 0 avg loss -2.239726 avg loss no lamb -2.239726 time 2019-02-25 06:24:26.513304
Model ind 685 epoch 297 head A batch: 100 avg loss -2.180457 avg loss no lamb -2.180457 time 2019-02-25 06:25:43.890240
Model ind 685 epoch 297 head A batch: 200 avg loss -2.192894 avg loss no lamb -2.192894 time 2019-02-25 06:27:04.015189
Model ind 685 epoch 297 head A batch: 300 avg loss -2.215518 avg loss no lamb -2.215518 time 2019-02-25 06:28:25.221668
Model ind 685 epoch 297 head A batch: 400 avg loss -2.187365 avg loss no lamb -2.187365 time 2019-02-25 06:29:45.861672
Pre: time 2019-02-25 06:31:22.191642: 
 	std: 0.0064881565
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.99212855, 0.979, 0.9790143]
	train_accs: [0.9923857, 0.9790286, 0.99212855, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98431146
	best: 0.9923857

Starting e_i: 298
Model ind 685 epoch 298 head B batch: 0 avg loss -2.225162 avg loss no lamb -2.225162 time 2019-02-25 06:31:24.004117
Model ind 685 epoch 298 head B batch: 100 avg loss -2.187213 avg loss no lamb -2.187213 time 2019-02-25 06:32:44.731212
Model ind 685 epoch 298 head B batch: 200 avg loss -2.202008 avg loss no lamb -2.202008 time 2019-02-25 06:34:03.971624
Model ind 685 epoch 298 head B batch: 300 avg loss -2.202667 avg loss no lamb -2.202667 time 2019-02-25 06:35:24.072332
Model ind 685 epoch 298 head B batch: 400 avg loss -2.193297 avg loss no lamb -2.193297 time 2019-02-25 06:36:45.234105
Model ind 685 epoch 298 head B batch: 0 avg loss -2.200716 avg loss no lamb -2.200716 time 2019-02-25 06:38:07.341236
Model ind 685 epoch 298 head B batch: 100 avg loss -2.232013 avg loss no lamb -2.232013 time 2019-02-25 06:39:27.607246
Model ind 685 epoch 298 head B batch: 200 avg loss -2.183603 avg loss no lamb -2.183603 time 2019-02-25 06:40:48.357289
Model ind 685 epoch 298 head B batch: 300 avg loss -2.203480 avg loss no lamb -2.203480 time 2019-02-25 06:42:10.815848
Model ind 685 epoch 298 head B batch: 400 avg loss -2.175389 avg loss no lamb -2.175389 time 2019-02-25 06:43:32.628388
Model ind 685 epoch 298 head A batch: 0 avg loss -2.207240 avg loss no lamb -2.207240 time 2019-02-25 06:44:54.446024
Model ind 685 epoch 298 head A batch: 100 avg loss -2.210785 avg loss no lamb -2.210785 time 2019-02-25 06:46:16.401564
Model ind 685 epoch 298 head A batch: 200 avg loss -2.225441 avg loss no lamb -2.225441 time 2019-02-25 06:47:38.623081
Model ind 685 epoch 298 head A batch: 300 avg loss -2.211489 avg loss no lamb -2.211489 time 2019-02-25 06:48:58.639139
Model ind 685 epoch 298 head A batch: 400 avg loss -2.194477 avg loss no lamb -2.194477 time 2019-02-25 06:50:18.163527
Pre: time 2019-02-25 06:51:55.345258: 
 	std: 0.0064767543
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99207145, 0.9790143, 0.979]
	train_accs: [0.9923857, 0.9790143, 0.99207145, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9923857

Starting e_i: 299
Model ind 685 epoch 299 head B batch: 0 avg loss -2.198750 avg loss no lamb -2.198750 time 2019-02-25 06:51:57.226796
Model ind 685 epoch 299 head B batch: 100 avg loss -2.222738 avg loss no lamb -2.222738 time 2019-02-25 06:53:05.166519
Model ind 685 epoch 299 head B batch: 200 avg loss -2.182027 avg loss no lamb -2.182027 time 2019-02-25 06:54:26.356410
Model ind 685 epoch 299 head B batch: 300 avg loss -2.200975 avg loss no lamb -2.200975 time 2019-02-25 06:55:49.390107
Model ind 685 epoch 299 head B batch: 400 avg loss -2.189114 avg loss no lamb -2.189114 time 2019-02-25 06:57:08.909595
Model ind 685 epoch 299 head B batch: 0 avg loss -2.211561 avg loss no lamb -2.211561 time 2019-02-25 06:58:29.330384
Model ind 685 epoch 299 head B batch: 100 avg loss -2.195828 avg loss no lamb -2.195828 time 2019-02-25 06:59:51.052778
Model ind 685 epoch 299 head B batch: 200 avg loss -2.209841 avg loss no lamb -2.209841 time 2019-02-25 07:01:10.171115
Model ind 685 epoch 299 head B batch: 300 avg loss -2.229199 avg loss no lamb -2.229199 time 2019-02-25 07:02:29.477430
Model ind 685 epoch 299 head B batch: 400 avg loss -2.143738 avg loss no lamb -2.143738 time 2019-02-25 07:03:50.704071
Model ind 685 epoch 299 head A batch: 0 avg loss -2.229621 avg loss no lamb -2.229621 time 2019-02-25 07:05:11.295023
Model ind 685 epoch 299 head A batch: 100 avg loss -2.195019 avg loss no lamb -2.195019 time 2019-02-25 07:06:31.465238
Model ind 685 epoch 299 head A batch: 200 avg loss -2.207813 avg loss no lamb -2.207813 time 2019-02-25 07:07:51.367721
Model ind 685 epoch 299 head A batch: 300 avg loss -2.201876 avg loss no lamb -2.201876 time 2019-02-25 07:09:10.493399
Model ind 685 epoch 299 head A batch: 400 avg loss -2.219768 avg loss no lamb -2.219768 time 2019-02-25 07:10:28.981316
Pre: time 2019-02-25 07:12:09.345523: 
 	std: 0.006476769
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9921143, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.9790428, 0.9921143, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.9924286

Starting e_i: 300
Model ind 685 epoch 300 head B batch: 0 avg loss -2.195847 avg loss no lamb -2.195847 time 2019-02-25 07:12:11.550717
Model ind 685 epoch 300 head B batch: 100 avg loss -2.181472 avg loss no lamb -2.181472 time 2019-02-25 07:13:30.657169
Model ind 685 epoch 300 head B batch: 200 avg loss -2.220889 avg loss no lamb -2.220889 time 2019-02-25 07:14:44.083209
Model ind 685 epoch 300 head B batch: 300 avg loss -2.206959 avg loss no lamb -2.206959 time 2019-02-25 07:16:00.484446
Model ind 685 epoch 300 head B batch: 400 avg loss -2.178909 avg loss no lamb -2.178909 time 2019-02-25 07:17:17.336808
Model ind 685 epoch 300 head B batch: 0 avg loss -2.226751 avg loss no lamb -2.226751 time 2019-02-25 07:18:32.180884
Model ind 685 epoch 300 head B batch: 100 avg loss -2.213268 avg loss no lamb -2.213268 time 2019-02-25 07:19:51.146305
Model ind 685 epoch 300 head B batch: 200 avg loss -2.176435 avg loss no lamb -2.176435 time 2019-02-25 07:21:07.871458
Model ind 685 epoch 300 head B batch: 300 avg loss -2.197678 avg loss no lamb -2.197678 time 2019-02-25 07:22:27.562112
Model ind 685 epoch 300 head B batch: 400 avg loss -2.175308 avg loss no lamb -2.175308 time 2019-02-25 07:23:46.433074
Model ind 685 epoch 300 head A batch: 0 avg loss -2.201683 avg loss no lamb -2.201683 time 2019-02-25 07:25:00.757469
Model ind 685 epoch 300 head A batch: 100 avg loss -2.245891 avg loss no lamb -2.245891 time 2019-02-25 07:26:15.515038
Model ind 685 epoch 300 head A batch: 200 avg loss -2.245795 avg loss no lamb -2.245795 time 2019-02-25 07:27:33.416080
Model ind 685 epoch 300 head A batch: 300 avg loss -2.213276 avg loss no lamb -2.213276 time 2019-02-25 07:28:48.745564
Model ind 685 epoch 300 head A batch: 400 avg loss -2.203557 avg loss no lamb -2.203557 time 2019-02-25 07:30:05.226320
Pre: time 2019-02-25 07:31:40.289783: 
 	std: 0.006497615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99247146, 0.97907144, 0.9921857, 0.97907144, 0.97905713]
	train_accs: [0.99247146, 0.97907144, 0.9921857, 0.97907144, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98437136
	best: 0.99247146

Starting e_i: 301
Model ind 685 epoch 301 head B batch: 0 avg loss -2.181605 avg loss no lamb -2.181605 time 2019-02-25 07:31:43.506623
Model ind 685 epoch 301 head B batch: 100 avg loss -2.150694 avg loss no lamb -2.150694 time 2019-02-25 07:33:02.129853
Model ind 685 epoch 301 head B batch: 200 avg loss -2.224768 avg loss no lamb -2.224768 time 2019-02-25 07:34:22.340614
Model ind 685 epoch 301 head B batch: 300 avg loss -2.257624 avg loss no lamb -2.257624 time 2019-02-25 07:35:33.636027
Model ind 685 epoch 301 head B batch: 400 avg loss -2.213113 avg loss no lamb -2.213113 time 2019-02-25 07:36:55.958171
Model ind 685 epoch 301 head B batch: 0 avg loss -2.229170 avg loss no lamb -2.229170 time 2019-02-25 07:38:16.649903
Model ind 685 epoch 301 head B batch: 100 avg loss -2.189168 avg loss no lamb -2.189168 time 2019-02-25 07:39:37.261864
Model ind 685 epoch 301 head B batch: 200 avg loss -2.202251 avg loss no lamb -2.202251 time 2019-02-25 07:40:57.608065
Model ind 685 epoch 301 head B batch: 300 avg loss -2.218170 avg loss no lamb -2.218170 time 2019-02-25 07:42:16.698235
Model ind 685 epoch 301 head B batch: 400 avg loss -2.203549 avg loss no lamb -2.203549 time 2019-02-25 07:43:36.511732
Model ind 685 epoch 301 head A batch: 0 avg loss -2.207166 avg loss no lamb -2.207166 time 2019-02-25 07:44:59.333251
Model ind 685 epoch 301 head A batch: 100 avg loss -2.254353 avg loss no lamb -2.254353 time 2019-02-25 07:46:19.082046
Model ind 685 epoch 301 head A batch: 200 avg loss -2.176752 avg loss no lamb -2.176752 time 2019-02-25 07:47:40.768715
Model ind 685 epoch 301 head A batch: 300 avg loss -2.194171 avg loss no lamb -2.194171 time 2019-02-25 07:49:02.909175
Model ind 685 epoch 301 head A batch: 400 avg loss -2.213019 avg loss no lamb -2.213019 time 2019-02-25 07:50:23.359766
Pre: time 2019-02-25 07:52:00.670390: 
 	std: 0.0065059764
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99247146, 0.9790286, 0.99214286, 0.9790286, 0.9790286]
	train_accs: [0.99247146, 0.9790286, 0.99214286, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98434
	best: 0.99247146

Starting e_i: 302
Model ind 685 epoch 302 head B batch: 0 avg loss -2.190845 avg loss no lamb -2.190845 time 2019-02-25 07:52:02.886609
Model ind 685 epoch 302 head B batch: 100 avg loss -2.183376 avg loss no lamb -2.183376 time 2019-02-25 07:53:17.216545
Model ind 685 epoch 302 head B batch: 200 avg loss -2.177834 avg loss no lamb -2.177834 time 2019-02-25 07:54:32.511527
Model ind 685 epoch 302 head B batch: 300 avg loss -2.194924 avg loss no lamb -2.194924 time 2019-02-25 07:55:49.176451
Model ind 685 epoch 302 head B batch: 400 avg loss -2.186551 avg loss no lamb -2.186551 time 2019-02-25 07:57:00.448338
Model ind 685 epoch 302 head B batch: 0 avg loss -2.223434 avg loss no lamb -2.223434 time 2019-02-25 07:58:16.414759
Model ind 685 epoch 302 head B batch: 100 avg loss -2.200198 avg loss no lamb -2.200198 time 2019-02-25 07:59:32.425583
Model ind 685 epoch 302 head B batch: 200 avg loss -2.221898 avg loss no lamb -2.221898 time 2019-02-25 08:00:46.526604
Model ind 685 epoch 302 head B batch: 300 avg loss -2.212065 avg loss no lamb -2.212065 time 2019-02-25 08:01:58.740650
Model ind 685 epoch 302 head B batch: 400 avg loss -2.168781 avg loss no lamb -2.168781 time 2019-02-25 08:03:12.093940
Model ind 685 epoch 302 head A batch: 0 avg loss -2.237440 avg loss no lamb -2.237440 time 2019-02-25 08:04:29.306917
Model ind 685 epoch 302 head A batch: 100 avg loss -2.195707 avg loss no lamb -2.195707 time 2019-02-25 08:05:45.387782
Model ind 685 epoch 302 head A batch: 200 avg loss -2.186034 avg loss no lamb -2.186034 time 2019-02-25 08:07:01.685983
Model ind 685 epoch 302 head A batch: 300 avg loss -2.259649 avg loss no lamb -2.259649 time 2019-02-25 08:08:13.564586
Model ind 685 epoch 302 head A batch: 400 avg loss -2.190180 avg loss no lamb -2.190180 time 2019-02-25 08:09:25.693093
Pre: time 2019-02-25 08:10:56.322620: 
 	std: 0.006496526
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9921143, 0.9790143, 0.979]
	train_accs: [0.9924143, 0.979, 0.9921143, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 303
Model ind 685 epoch 303 head B batch: 0 avg loss -2.238971 avg loss no lamb -2.238971 time 2019-02-25 08:10:58.535131
Model ind 685 epoch 303 head B batch: 100 avg loss -2.180696 avg loss no lamb -2.180696 time 2019-02-25 08:12:16.588893
Model ind 685 epoch 303 head B batch: 200 avg loss -2.215605 avg loss no lamb -2.215605 time 2019-02-25 08:13:31.517054
Model ind 685 epoch 303 head B batch: 300 avg loss -2.183971 avg loss no lamb -2.183971 time 2019-02-25 08:14:45.103991
Model ind 685 epoch 303 head B batch: 400 avg loss -2.168597 avg loss no lamb -2.168597 time 2019-02-25 08:16:01.669791
Model ind 685 epoch 303 head B batch: 0 avg loss -2.222689 avg loss no lamb -2.222689 time 2019-02-25 08:17:14.852842
Model ind 685 epoch 303 head B batch: 100 avg loss -2.226665 avg loss no lamb -2.226665 time 2019-02-25 08:18:29.131324
Model ind 685 epoch 303 head B batch: 200 avg loss -2.232132 avg loss no lamb -2.232132 time 2019-02-25 08:19:49.750129
Model ind 685 epoch 303 head B batch: 300 avg loss -2.210782 avg loss no lamb -2.210782 time 2019-02-25 08:21:11.723331
Model ind 685 epoch 303 head B batch: 400 avg loss -2.209561 avg loss no lamb -2.209561 time 2019-02-25 08:22:32.132240
Model ind 685 epoch 303 head A batch: 0 avg loss -2.216077 avg loss no lamb -2.216077 time 2019-02-25 08:23:55.165298
Model ind 685 epoch 303 head A batch: 100 avg loss -2.217314 avg loss no lamb -2.217314 time 2019-02-25 08:25:18.547809
Model ind 685 epoch 303 head A batch: 200 avg loss -2.212043 avg loss no lamb -2.212043 time 2019-02-25 08:26:36.919806
Model ind 685 epoch 303 head A batch: 300 avg loss -2.205379 avg loss no lamb -2.205379 time 2019-02-25 08:27:57.307258
Model ind 685 epoch 303 head A batch: 400 avg loss -2.143543 avg loss no lamb -2.143543 time 2019-02-25 08:29:18.197404
Pre: time 2019-02-25 08:30:51.407664: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 304
Model ind 685 epoch 304 head B batch: 0 avg loss -2.238008 avg loss no lamb -2.238008 time 2019-02-25 08:30:53.380460
Model ind 685 epoch 304 head B batch: 100 avg loss -2.229753 avg loss no lamb -2.229753 time 2019-02-25 08:32:11.461258
Model ind 685 epoch 304 head B batch: 200 avg loss -2.206687 avg loss no lamb -2.206687 time 2019-02-25 08:33:29.379322
Model ind 685 epoch 304 head B batch: 300 avg loss -2.229918 avg loss no lamb -2.229918 time 2019-02-25 08:34:44.568897
Model ind 685 epoch 304 head B batch: 400 avg loss -2.168118 avg loss no lamb -2.168118 time 2019-02-25 08:36:03.662810
Model ind 685 epoch 304 head B batch: 0 avg loss -2.186999 avg loss no lamb -2.186999 time 2019-02-25 08:37:20.526070
Model ind 685 epoch 304 head B batch: 100 avg loss -2.208548 avg loss no lamb -2.208548 time 2019-02-25 08:38:36.765697
Model ind 685 epoch 304 head B batch: 200 avg loss -2.237980 avg loss no lamb -2.237980 time 2019-02-25 08:39:53.189030
Model ind 685 epoch 304 head B batch: 300 avg loss -2.224957 avg loss no lamb -2.224957 time 2019-02-25 08:41:09.511140
Model ind 685 epoch 304 head B batch: 400 avg loss -2.213726 avg loss no lamb -2.213726 time 2019-02-25 08:42:24.602999
Model ind 685 epoch 304 head A batch: 0 avg loss -2.199964 avg loss no lamb -2.199964 time 2019-02-25 08:43:38.787799
Model ind 685 epoch 304 head A batch: 100 avg loss -2.202038 avg loss no lamb -2.202038 time 2019-02-25 08:44:55.474163
Model ind 685 epoch 304 head A batch: 200 avg loss -2.194851 avg loss no lamb -2.194851 time 2019-02-25 08:46:13.080977
Model ind 685 epoch 304 head A batch: 300 avg loss -2.198270 avg loss no lamb -2.198270 time 2019-02-25 08:47:30.126286
Model ind 685 epoch 304 head A batch: 400 avg loss -2.219372 avg loss no lamb -2.219372 time 2019-02-25 08:48:49.263313
Pre: time 2019-02-25 08:50:26.963680: 
 	std: 0.0064812917
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.99237144, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429143
	best: 0.99237144

Starting e_i: 305
Model ind 685 epoch 305 head B batch: 0 avg loss -2.184701 avg loss no lamb -2.184701 time 2019-02-25 08:50:29.110908
Model ind 685 epoch 305 head B batch: 100 avg loss -2.188348 avg loss no lamb -2.188348 time 2019-02-25 08:51:44.920788
Model ind 685 epoch 305 head B batch: 200 avg loss -2.226924 avg loss no lamb -2.226924 time 2019-02-25 08:53:03.351509
Model ind 685 epoch 305 head B batch: 300 avg loss -2.212301 avg loss no lamb -2.212301 time 2019-02-25 08:54:18.957483
Model ind 685 epoch 305 head B batch: 400 avg loss -2.183712 avg loss no lamb -2.183712 time 2019-02-25 08:55:32.734972
Model ind 685 epoch 305 head B batch: 0 avg loss -2.212775 avg loss no lamb -2.212775 time 2019-02-25 08:56:49.065076
Model ind 685 epoch 305 head B batch: 100 avg loss -2.186278 avg loss no lamb -2.186278 time 2019-02-25 08:58:02.687075
Model ind 685 epoch 305 head B batch: 200 avg loss -2.216551 avg loss no lamb -2.216551 time 2019-02-25 08:59:19.907512
Model ind 685 epoch 305 head B batch: 300 avg loss -2.193257 avg loss no lamb -2.193257 time 2019-02-25 09:00:26.933553
Model ind 685 epoch 305 head B batch: 400 avg loss -2.183577 avg loss no lamb -2.183577 time 2019-02-25 09:01:45.181268
Model ind 685 epoch 305 head A batch: 0 avg loss -2.209029 avg loss no lamb -2.209029 time 2019-02-25 09:03:00.547427
Model ind 685 epoch 305 head A batch: 100 avg loss -2.209432 avg loss no lamb -2.209432 time 2019-02-25 09:04:16.410118
Model ind 685 epoch 305 head A batch: 200 avg loss -2.191706 avg loss no lamb -2.191706 time 2019-02-25 09:05:35.245033
Model ind 685 epoch 305 head A batch: 300 avg loss -2.208709 avg loss no lamb -2.208709 time 2019-02-25 09:06:50.943163
Model ind 685 epoch 305 head A batch: 400 avg loss -2.187935 avg loss no lamb -2.187935 time 2019-02-25 09:08:07.448029
Pre: time 2019-02-25 09:09:37.690085: 
 	std: 0.0064986125
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789429, 0.99207145, 0.97891426, 0.97892857]
	train_accs: [0.9923143, 0.9789429, 0.99207145, 0.97891426, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97891426
	avg: 0.9842342
	best: 0.9923143

Starting e_i: 306
Model ind 685 epoch 306 head B batch: 0 avg loss -2.229824 avg loss no lamb -2.229824 time 2019-02-25 09:09:39.793226
Model ind 685 epoch 306 head B batch: 100 avg loss -2.160607 avg loss no lamb -2.160607 time 2019-02-25 09:10:51.828641
Model ind 685 epoch 306 head B batch: 200 avg loss -2.194934 avg loss no lamb -2.194934 time 2019-02-25 09:12:06.817687
Model ind 685 epoch 306 head B batch: 300 avg loss -2.222089 avg loss no lamb -2.222089 time 2019-02-25 09:13:22.441415
Model ind 685 epoch 306 head B batch: 400 avg loss -2.208221 avg loss no lamb -2.208221 time 2019-02-25 09:14:35.109098
Model ind 685 epoch 306 head B batch: 0 avg loss -2.222316 avg loss no lamb -2.222316 time 2019-02-25 09:15:47.321683
Model ind 685 epoch 306 head B batch: 100 avg loss -2.202197 avg loss no lamb -2.202197 time 2019-02-25 09:17:03.592849
Model ind 685 epoch 306 head B batch: 200 avg loss -2.172574 avg loss no lamb -2.172574 time 2019-02-25 09:18:17.325842
Model ind 685 epoch 306 head B batch: 300 avg loss -2.188405 avg loss no lamb -2.188405 time 2019-02-25 09:19:31.195809
Model ind 685 epoch 306 head B batch: 400 avg loss -2.174405 avg loss no lamb -2.174405 time 2019-02-25 09:20:45.082114
Model ind 685 epoch 306 head A batch: 0 avg loss -2.184316 avg loss no lamb -2.184316 time 2019-02-25 09:21:56.600959
Model ind 685 epoch 306 head A batch: 100 avg loss -2.223404 avg loss no lamb -2.223404 time 2019-02-25 09:23:11.080117
Model ind 685 epoch 306 head A batch: 200 avg loss -2.223542 avg loss no lamb -2.223542 time 2019-02-25 09:24:24.446469
Model ind 685 epoch 306 head A batch: 300 avg loss -2.218799 avg loss no lamb -2.218799 time 2019-02-25 09:25:39.543338
Model ind 685 epoch 306 head A batch: 400 avg loss -2.204401 avg loss no lamb -2.204401 time 2019-02-25 09:26:57.232513
Pre: time 2019-02-25 09:28:27.405646: 
 	std: 0.0064779907
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97905713, 0.9921143, 0.97905713, 0.97905713]
	train_accs: [0.99244285, 0.97905713, 0.9921143, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434573
	best: 0.99244285

Starting e_i: 307
Model ind 685 epoch 307 head B batch: 0 avg loss -2.206421 avg loss no lamb -2.206421 time 2019-02-25 09:28:29.396669
Model ind 685 epoch 307 head B batch: 100 avg loss -2.163844 avg loss no lamb -2.163844 time 2019-02-25 09:29:46.956885
Model ind 685 epoch 307 head B batch: 200 avg loss -2.203248 avg loss no lamb -2.203248 time 2019-02-25 09:31:02.650318
Model ind 685 epoch 307 head B batch: 300 avg loss -2.237806 avg loss no lamb -2.237806 time 2019-02-25 09:32:21.100547
Model ind 685 epoch 307 head B batch: 400 avg loss -2.188396 avg loss no lamb -2.188396 time 2019-02-25 09:33:37.657060
Model ind 685 epoch 307 head B batch: 0 avg loss -2.226218 avg loss no lamb -2.226218 time 2019-02-25 09:34:55.437634
Model ind 685 epoch 307 head B batch: 100 avg loss -2.215633 avg loss no lamb -2.215633 time 2019-02-25 09:36:09.221858
Model ind 685 epoch 307 head B batch: 200 avg loss -2.221532 avg loss no lamb -2.221532 time 2019-02-25 09:37:25.282040
Model ind 685 epoch 307 head B batch: 300 avg loss -2.226104 avg loss no lamb -2.226104 time 2019-02-25 09:38:39.631691
Model ind 685 epoch 307 head B batch: 400 avg loss -2.207550 avg loss no lamb -2.207550 time 2019-02-25 09:39:58.856599
Model ind 685 epoch 307 head A batch: 0 avg loss -2.184701 avg loss no lamb -2.184701 time 2019-02-25 09:41:12.272237
Model ind 685 epoch 307 head A batch: 100 avg loss -2.244706 avg loss no lamb -2.244706 time 2019-02-25 09:42:24.743615
Model ind 685 epoch 307 head A batch: 200 avg loss -2.214962 avg loss no lamb -2.214962 time 2019-02-25 09:43:26.911257
Model ind 685 epoch 307 head A batch: 300 avg loss -2.202393 avg loss no lamb -2.202393 time 2019-02-25 09:44:44.486474
Model ind 685 epoch 307 head A batch: 400 avg loss -2.165615 avg loss no lamb -2.165615 time 2019-02-25 09:46:05.136943
Pre: time 2019-02-25 09:47:43.968640: 
 	std: 0.0064929435
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 308
Model ind 685 epoch 308 head B batch: 0 avg loss -2.233268 avg loss no lamb -2.233268 time 2019-02-25 09:47:46.069924
Model ind 685 epoch 308 head B batch: 100 avg loss -2.185006 avg loss no lamb -2.185006 time 2019-02-25 09:49:05.563436
Model ind 685 epoch 308 head B batch: 200 avg loss -2.178051 avg loss no lamb -2.178051 time 2019-02-25 09:50:26.371091
Model ind 685 epoch 308 head B batch: 300 avg loss -2.198548 avg loss no lamb -2.198548 time 2019-02-25 09:51:47.632202
Model ind 685 epoch 308 head B batch: 400 avg loss -2.195100 avg loss no lamb -2.195100 time 2019-02-25 09:53:07.117597
Model ind 685 epoch 308 head B batch: 0 avg loss -2.186362 avg loss no lamb -2.186362 time 2019-02-25 09:54:26.576966
Model ind 685 epoch 308 head B batch: 100 avg loss -2.215137 avg loss no lamb -2.215137 time 2019-02-25 09:55:48.345641
Model ind 685 epoch 308 head B batch: 200 avg loss -2.237085 avg loss no lamb -2.237085 time 2019-02-25 09:57:10.044537
Model ind 685 epoch 308 head B batch: 300 avg loss -2.204731 avg loss no lamb -2.204731 time 2019-02-25 09:58:31.600318
Model ind 685 epoch 308 head B batch: 400 avg loss -2.231409 avg loss no lamb -2.231409 time 2019-02-25 09:59:52.305082
Model ind 685 epoch 308 head A batch: 0 avg loss -2.186796 avg loss no lamb -2.186796 time 2019-02-25 10:01:10.101231
Model ind 685 epoch 308 head A batch: 100 avg loss -2.216801 avg loss no lamb -2.216801 time 2019-02-25 10:02:30.909935
Model ind 685 epoch 308 head A batch: 200 avg loss -2.248665 avg loss no lamb -2.248665 time 2019-02-25 10:03:52.577545
Model ind 685 epoch 308 head A batch: 300 avg loss -2.218522 avg loss no lamb -2.218522 time 2019-02-25 10:05:14.007294
Model ind 685 epoch 308 head A batch: 400 avg loss -2.178352 avg loss no lamb -2.178352 time 2019-02-25 10:06:33.637149
Pre: time 2019-02-25 10:08:13.225968: 
 	std: 0.006497496
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789714, 0.9921143, 0.9789857, 0.9789857]
	train_accs: [0.99237144, 0.9789714, 0.9921143, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842857
	best: 0.99237144

Starting e_i: 309
Model ind 685 epoch 309 head B batch: 0 avg loss -2.203050 avg loss no lamb -2.203050 time 2019-02-25 10:08:15.250184
Model ind 685 epoch 309 head B batch: 100 avg loss -2.230270 avg loss no lamb -2.230270 time 2019-02-25 10:09:35.960359
Model ind 685 epoch 309 head B batch: 200 avg loss -2.208035 avg loss no lamb -2.208035 time 2019-02-25 10:10:56.650106
Model ind 685 epoch 309 head B batch: 300 avg loss -2.223078 avg loss no lamb -2.223078 time 2019-02-25 10:12:18.258159
Model ind 685 epoch 309 head B batch: 400 avg loss -2.216015 avg loss no lamb -2.216015 time 2019-02-25 10:13:41.664831
Model ind 685 epoch 309 head B batch: 0 avg loss -2.203559 avg loss no lamb -2.203559 time 2019-02-25 10:15:01.787996
Model ind 685 epoch 309 head B batch: 100 avg loss -2.187374 avg loss no lamb -2.187374 time 2019-02-25 10:16:23.233725
Model ind 685 epoch 309 head B batch: 200 avg loss -2.202060 avg loss no lamb -2.202060 time 2019-02-25 10:17:41.571240
Model ind 685 epoch 309 head B batch: 300 avg loss -2.205073 avg loss no lamb -2.205073 time 2019-02-25 10:18:58.930658
Model ind 685 epoch 309 head B batch: 400 avg loss -2.170709 avg loss no lamb -2.170709 time 2019-02-25 10:20:14.840877
Model ind 685 epoch 309 head A batch: 0 avg loss -2.207512 avg loss no lamb -2.207512 time 2019-02-25 10:21:30.415517
Model ind 685 epoch 309 head A batch: 100 avg loss -2.179982 avg loss no lamb -2.179982 time 2019-02-25 10:22:48.907703
Model ind 685 epoch 309 head A batch: 200 avg loss -2.221531 avg loss no lamb -2.221531 time 2019-02-25 10:24:04.807265
Model ind 685 epoch 309 head A batch: 300 avg loss -2.215019 avg loss no lamb -2.215019 time 2019-02-25 10:25:10.468629
Model ind 685 epoch 309 head A batch: 400 avg loss -2.176950 avg loss no lamb -2.176950 time 2019-02-25 10:26:29.199846
Pre: time 2019-02-25 10:28:04.260498: 
 	std: 0.0064896573
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921, 0.9790143, 0.9790286]
	train_accs: [0.9924286, 0.9790143, 0.9921, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924286

Starting e_i: 310
Model ind 685 epoch 310 head B batch: 0 avg loss -2.210007 avg loss no lamb -2.210007 time 2019-02-25 10:28:06.357702
Model ind 685 epoch 310 head B batch: 100 avg loss -2.197650 avg loss no lamb -2.197650 time 2019-02-25 10:29:23.150240
Model ind 685 epoch 310 head B batch: 200 avg loss -2.208448 avg loss no lamb -2.208448 time 2019-02-25 10:30:37.854407
Model ind 685 epoch 310 head B batch: 300 avg loss -2.239161 avg loss no lamb -2.239161 time 2019-02-25 10:31:53.063329
Model ind 685 epoch 310 head B batch: 400 avg loss -2.201635 avg loss no lamb -2.201635 time 2019-02-25 10:33:08.719596
Model ind 685 epoch 310 head B batch: 0 avg loss -2.200504 avg loss no lamb -2.200504 time 2019-02-25 10:34:24.558489
Model ind 685 epoch 310 head B batch: 100 avg loss -2.232152 avg loss no lamb -2.232152 time 2019-02-25 10:35:41.851590
Model ind 685 epoch 310 head B batch: 200 avg loss -2.169080 avg loss no lamb -2.169080 time 2019-02-25 10:36:59.921795
Model ind 685 epoch 310 head B batch: 300 avg loss -2.200219 avg loss no lamb -2.200219 time 2019-02-25 10:38:18.275561
Model ind 685 epoch 310 head B batch: 400 avg loss -2.218199 avg loss no lamb -2.218199 time 2019-02-25 10:39:33.967256
Model ind 685 epoch 310 head A batch: 0 avg loss -2.228607 avg loss no lamb -2.228607 time 2019-02-25 10:40:52.090200
Model ind 685 epoch 310 head A batch: 100 avg loss -2.179000 avg loss no lamb -2.179000 time 2019-02-25 10:42:09.059918
Model ind 685 epoch 310 head A batch: 200 avg loss -2.211931 avg loss no lamb -2.211931 time 2019-02-25 10:43:24.858306
Model ind 685 epoch 310 head A batch: 300 avg loss -2.250442 avg loss no lamb -2.250442 time 2019-02-25 10:44:40.105542
Model ind 685 epoch 310 head A batch: 400 avg loss -2.186709 avg loss no lamb -2.186709 time 2019-02-25 10:45:53.063309
Pre: time 2019-02-25 10:47:25.742745: 
 	std: 0.0064568613
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789714, 0.9920143, 0.979, 0.9789857]
	train_accs: [0.9923143, 0.9789714, 0.9920143, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842571
	best: 0.9923143

Starting e_i: 311
Model ind 685 epoch 311 head B batch: 0 avg loss -2.196359 avg loss no lamb -2.196359 time 2019-02-25 10:47:27.723813
Model ind 685 epoch 311 head B batch: 100 avg loss -2.175626 avg loss no lamb -2.175626 time 2019-02-25 10:48:45.042364
Model ind 685 epoch 311 head B batch: 200 avg loss -2.205389 avg loss no lamb -2.205389 time 2019-02-25 10:50:01.597047
Model ind 685 epoch 311 head B batch: 300 avg loss -2.204562 avg loss no lamb -2.204562 time 2019-02-25 10:51:20.209111
Model ind 685 epoch 311 head B batch: 400 avg loss -2.205691 avg loss no lamb -2.205691 time 2019-02-25 10:52:39.680681
Model ind 685 epoch 311 head B batch: 0 avg loss -2.198898 avg loss no lamb -2.198898 time 2019-02-25 10:53:55.120852
Model ind 685 epoch 311 head B batch: 100 avg loss -2.232590 avg loss no lamb -2.232590 time 2019-02-25 10:55:12.365417
Model ind 685 epoch 311 head B batch: 200 avg loss -2.206080 avg loss no lamb -2.206080 time 2019-02-25 10:56:29.041122
Model ind 685 epoch 311 head B batch: 300 avg loss -2.173961 avg loss no lamb -2.173961 time 2019-02-25 10:57:41.564809
Model ind 685 epoch 311 head B batch: 400 avg loss -2.218392 avg loss no lamb -2.218392 time 2019-02-25 10:58:58.888812
Model ind 685 epoch 311 head A batch: 0 avg loss -2.199252 avg loss no lamb -2.199252 time 2019-02-25 11:00:15.231861
Model ind 685 epoch 311 head A batch: 100 avg loss -2.216259 avg loss no lamb -2.216259 time 2019-02-25 11:01:34.109798
Model ind 685 epoch 311 head A batch: 200 avg loss -2.231732 avg loss no lamb -2.231732 time 2019-02-25 11:02:52.866739
Model ind 685 epoch 311 head A batch: 300 avg loss -2.199533 avg loss no lamb -2.199533 time 2019-02-25 11:04:11.893550
Model ind 685 epoch 311 head A batch: 400 avg loss -2.177247 avg loss no lamb -2.177247 time 2019-02-25 11:05:31.895837
Pre: time 2019-02-25 11:07:07.051190: 
 	std: 0.0064931004
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.97885716, 0.9919429, 0.9788714, 0.9788143]
	train_accs: [0.9922571, 0.97885716, 0.9919429, 0.9788714, 0.9788143]
	best_train_sub_head: 0
	worst: 0.9788143
	avg: 0.9841485
	best: 0.9922571

Starting e_i: 312
Model ind 685 epoch 312 head B batch: 0 avg loss -2.180175 avg loss no lamb -2.180175 time 2019-02-25 11:07:09.300597
Model ind 685 epoch 312 head B batch: 100 avg loss -2.209931 avg loss no lamb -2.209931 time 2019-02-25 11:08:20.201983
Model ind 685 epoch 312 head B batch: 200 avg loss -2.188392 avg loss no lamb -2.188392 time 2019-02-25 11:09:36.283335
Model ind 685 epoch 312 head B batch: 300 avg loss -2.234717 avg loss no lamb -2.234717 time 2019-02-25 11:10:54.268301
Model ind 685 epoch 312 head B batch: 400 avg loss -2.190474 avg loss no lamb -2.190474 time 2019-02-25 11:12:10.798608
Model ind 685 epoch 312 head B batch: 0 avg loss -2.220797 avg loss no lamb -2.220797 time 2019-02-25 11:13:26.402225
Model ind 685 epoch 312 head B batch: 100 avg loss -2.231761 avg loss no lamb -2.231761 time 2019-02-25 11:14:43.444024
Model ind 685 epoch 312 head B batch: 200 avg loss -2.179399 avg loss no lamb -2.179399 time 2019-02-25 11:16:02.262577
Model ind 685 epoch 312 head B batch: 300 avg loss -2.234902 avg loss no lamb -2.234902 time 2019-02-25 11:17:20.025642
Model ind 685 epoch 312 head B batch: 400 avg loss -2.170492 avg loss no lamb -2.170492 time 2019-02-25 11:18:37.287577
Model ind 685 epoch 312 head A batch: 0 avg loss -2.208657 avg loss no lamb -2.208657 time 2019-02-25 11:19:56.950182
Model ind 685 epoch 312 head A batch: 100 avg loss -2.202408 avg loss no lamb -2.202408 time 2019-02-25 11:21:16.741832
Model ind 685 epoch 312 head A batch: 200 avg loss -2.212412 avg loss no lamb -2.212412 time 2019-02-25 11:22:35.456274
Model ind 685 epoch 312 head A batch: 300 avg loss -2.217016 avg loss no lamb -2.217016 time 2019-02-25 11:23:53.599332
Model ind 685 epoch 312 head A batch: 400 avg loss -2.188828 avg loss no lamb -2.188828 time 2019-02-25 11:25:14.582410
Pre: time 2019-02-25 11:26:47.737919: 
 	std: 0.0064987196
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.99212855, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.99212855, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9924

Starting e_i: 313
Model ind 685 epoch 313 head B batch: 0 avg loss -2.232270 avg loss no lamb -2.232270 time 2019-02-25 11:26:49.718243
Model ind 685 epoch 313 head B batch: 100 avg loss -2.172757 avg loss no lamb -2.172757 time 2019-02-25 11:28:05.621275
Model ind 685 epoch 313 head B batch: 200 avg loss -2.221968 avg loss no lamb -2.221968 time 2019-02-25 11:29:20.772497
Model ind 685 epoch 313 head B batch: 300 avg loss -2.236247 avg loss no lamb -2.236247 time 2019-02-25 11:30:39.205814
Model ind 685 epoch 313 head B batch: 400 avg loss -2.197026 avg loss no lamb -2.197026 time 2019-02-25 11:31:56.606912
Model ind 685 epoch 313 head B batch: 0 avg loss -2.188797 avg loss no lamb -2.188797 time 2019-02-25 11:33:14.227727
Model ind 685 epoch 313 head B batch: 100 avg loss -2.235520 avg loss no lamb -2.235520 time 2019-02-25 11:34:30.195401
Model ind 685 epoch 313 head B batch: 200 avg loss -2.212940 avg loss no lamb -2.212940 time 2019-02-25 11:35:46.071679
Model ind 685 epoch 313 head B batch: 300 avg loss -2.229407 avg loss no lamb -2.229407 time 2019-02-25 11:37:06.278501
Model ind 685 epoch 313 head B batch: 400 avg loss -2.187777 avg loss no lamb -2.187777 time 2019-02-25 11:38:23.633582
Model ind 685 epoch 313 head A batch: 0 avg loss -2.225032 avg loss no lamb -2.225032 time 2019-02-25 11:39:42.721580
Model ind 685 epoch 313 head A batch: 100 avg loss -2.188108 avg loss no lamb -2.188108 time 2019-02-25 11:40:59.648520
Model ind 685 epoch 313 head A batch: 200 avg loss -2.176521 avg loss no lamb -2.176521 time 2019-02-25 11:42:17.176325
Model ind 685 epoch 313 head A batch: 300 avg loss -2.191664 avg loss no lamb -2.191664 time 2019-02-25 11:43:35.325917
Model ind 685 epoch 313 head A batch: 400 avg loss -2.192446 avg loss no lamb -2.192446 time 2019-02-25 11:44:53.108245
Pre: time 2019-02-25 11:46:26.275216: 
 	std: 0.0064744162
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99207145, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.99207145, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843
	best: 0.9923857

Starting e_i: 314
Model ind 685 epoch 314 head B batch: 0 avg loss -2.227751 avg loss no lamb -2.227751 time 2019-02-25 11:46:28.629122
Model ind 685 epoch 314 head B batch: 100 avg loss -2.173539 avg loss no lamb -2.173539 time 2019-02-25 11:47:45.801578
Model ind 685 epoch 314 head B batch: 200 avg loss -2.190482 avg loss no lamb -2.190482 time 2019-02-25 11:49:03.980732
Model ind 685 epoch 314 head B batch: 300 avg loss -2.224698 avg loss no lamb -2.224698 time 2019-02-25 11:50:15.444307
Model ind 685 epoch 314 head B batch: 400 avg loss -2.186221 avg loss no lamb -2.186221 time 2019-02-25 11:51:30.427277
Model ind 685 epoch 314 head B batch: 0 avg loss -2.199429 avg loss no lamb -2.199429 time 2019-02-25 11:52:48.963242
Model ind 685 epoch 314 head B batch: 100 avg loss -2.198080 avg loss no lamb -2.198080 time 2019-02-25 11:54:08.057924
Model ind 685 epoch 314 head B batch: 200 avg loss -2.174112 avg loss no lamb -2.174112 time 2019-02-25 11:55:27.185286
Model ind 685 epoch 314 head B batch: 300 avg loss -2.208838 avg loss no lamb -2.208838 time 2019-02-25 11:56:47.945877
Model ind 685 epoch 314 head B batch: 400 avg loss -2.205344 avg loss no lamb -2.205344 time 2019-02-25 11:58:05.859839
Model ind 685 epoch 314 head A batch: 0 avg loss -2.191581 avg loss no lamb -2.191581 time 2019-02-25 11:59:22.916114
Model ind 685 epoch 314 head A batch: 100 avg loss -2.212469 avg loss no lamb -2.212469 time 2019-02-25 12:00:39.538369
Model ind 685 epoch 314 head A batch: 200 avg loss -2.198336 avg loss no lamb -2.198336 time 2019-02-25 12:01:58.064415
Model ind 685 epoch 314 head A batch: 300 avg loss -2.214278 avg loss no lamb -2.214278 time 2019-02-25 12:03:14.276610
Model ind 685 epoch 314 head A batch: 400 avg loss -2.179697 avg loss no lamb -2.179697 time 2019-02-25 12:04:33.341225
Pre: time 2019-02-25 12:06:07.666062: 
 	std: 0.0064812917
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843343
	best: 0.9924143

Starting e_i: 315
Model ind 685 epoch 315 head B batch: 0 avg loss -2.245589 avg loss no lamb -2.245589 time 2019-02-25 12:06:09.781014
Model ind 685 epoch 315 head B batch: 100 avg loss -2.227901 avg loss no lamb -2.227901 time 2019-02-25 12:07:25.686659
Model ind 685 epoch 315 head B batch: 200 avg loss -2.221687 avg loss no lamb -2.221687 time 2019-02-25 12:08:42.937045
Model ind 685 epoch 315 head B batch: 300 avg loss -2.238089 avg loss no lamb -2.238089 time 2019-02-25 12:10:00.618091
Model ind 685 epoch 315 head B batch: 400 avg loss -2.207952 avg loss no lamb -2.207952 time 2019-02-25 12:11:15.686850
Model ind 685 epoch 315 head B batch: 0 avg loss -2.186560 avg loss no lamb -2.186560 time 2019-02-25 12:12:31.021515
Model ind 685 epoch 315 head B batch: 100 avg loss -2.203739 avg loss no lamb -2.203739 time 2019-02-25 12:13:44.688563
Model ind 685 epoch 315 head B batch: 200 avg loss -2.202896 avg loss no lamb -2.202896 time 2019-02-25 12:15:00.849623
Model ind 685 epoch 315 head B batch: 300 avg loss -2.155573 avg loss no lamb -2.155573 time 2019-02-25 12:16:16.195136
Model ind 685 epoch 315 head B batch: 400 avg loss -2.173156 avg loss no lamb -2.173156 time 2019-02-25 12:17:33.009696
Model ind 685 epoch 315 head A batch: 0 avg loss -2.201054 avg loss no lamb -2.201054 time 2019-02-25 12:18:48.994486
Model ind 685 epoch 315 head A batch: 100 avg loss -2.219035 avg loss no lamb -2.219035 time 2019-02-25 12:20:04.831734
Model ind 685 epoch 315 head A batch: 200 avg loss -2.187076 avg loss no lamb -2.187076 time 2019-02-25 12:21:19.920859
Model ind 685 epoch 315 head A batch: 300 avg loss -2.212263 avg loss no lamb -2.212263 time 2019-02-25 12:22:34.858309
Model ind 685 epoch 315 head A batch: 400 avg loss -2.213384 avg loss no lamb -2.213384 time 2019-02-25 12:23:51.896752
Pre: time 2019-02-25 12:25:23.531676: 
 	std: 0.006463852
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923286, 0.9789857, 0.9920286, 0.9789857, 0.9789857]
	train_accs: [0.9923286, 0.9789857, 0.9920286, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842628
	best: 0.9923286

Starting e_i: 316
Model ind 685 epoch 316 head B batch: 0 avg loss -2.184595 avg loss no lamb -2.184595 time 2019-02-25 12:25:25.448828
Model ind 685 epoch 316 head B batch: 100 avg loss -2.206076 avg loss no lamb -2.206076 time 2019-02-25 12:26:39.934013
Model ind 685 epoch 316 head B batch: 200 avg loss -2.216357 avg loss no lamb -2.216357 time 2019-02-25 12:27:51.931519
Model ind 685 epoch 316 head B batch: 300 avg loss -2.207369 avg loss no lamb -2.207369 time 2019-02-25 12:29:06.971076
Model ind 685 epoch 316 head B batch: 400 avg loss -2.198747 avg loss no lamb -2.198747 time 2019-02-25 12:30:18.681779
Model ind 685 epoch 316 head B batch: 0 avg loss -2.198375 avg loss no lamb -2.198375 time 2019-02-25 12:31:33.793760
Model ind 685 epoch 316 head B batch: 100 avg loss -2.208303 avg loss no lamb -2.208303 time 2019-02-25 12:32:47.448494
Model ind 685 epoch 316 head B batch: 200 avg loss -2.159952 avg loss no lamb -2.159952 time 2019-02-25 12:33:55.110673
Model ind 685 epoch 316 head B batch: 300 avg loss -2.222901 avg loss no lamb -2.222901 time 2019-02-25 12:35:10.582647
Model ind 685 epoch 316 head B batch: 400 avg loss -2.160581 avg loss no lamb -2.160581 time 2019-02-25 12:36:24.703464
Model ind 685 epoch 316 head A batch: 0 avg loss -2.199499 avg loss no lamb -2.199499 time 2019-02-25 12:37:41.526002
Model ind 685 epoch 316 head A batch: 100 avg loss -2.209689 avg loss no lamb -2.209689 time 2019-02-25 12:38:57.011094
Model ind 685 epoch 316 head A batch: 200 avg loss -2.196120 avg loss no lamb -2.196120 time 2019-02-25 12:40:09.973579
Model ind 685 epoch 316 head A batch: 300 avg loss -2.240093 avg loss no lamb -2.240093 time 2019-02-25 12:41:23.954873
Model ind 685 epoch 316 head A batch: 400 avg loss -2.162157 avg loss no lamb -2.162157 time 2019-02-25 12:42:39.440124
Pre: time 2019-02-25 12:44:10.334625: 
 	std: 0.0064709987
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434
	best: 0.9924286

Starting e_i: 317
Model ind 685 epoch 317 head B batch: 0 avg loss -2.242972 avg loss no lamb -2.242972 time 2019-02-25 12:44:12.295296
Model ind 685 epoch 317 head B batch: 100 avg loss -2.214769 avg loss no lamb -2.214769 time 2019-02-25 12:45:28.917446
Model ind 685 epoch 317 head B batch: 200 avg loss -2.224928 avg loss no lamb -2.224928 time 2019-02-25 12:46:42.594369
Model ind 685 epoch 317 head B batch: 300 avg loss -2.236105 avg loss no lamb -2.236105 time 2019-02-25 12:47:58.189367
Model ind 685 epoch 317 head B batch: 400 avg loss -2.211944 avg loss no lamb -2.211944 time 2019-02-25 12:49:15.196948
Model ind 685 epoch 317 head B batch: 0 avg loss -2.210691 avg loss no lamb -2.210691 time 2019-02-25 12:50:32.543828
Model ind 685 epoch 317 head B batch: 100 avg loss -2.201471 avg loss no lamb -2.201471 time 2019-02-25 12:51:48.238860
Model ind 685 epoch 317 head B batch: 200 avg loss -2.185623 avg loss no lamb -2.185623 time 2019-02-25 12:53:04.329935
Model ind 685 epoch 317 head B batch: 300 avg loss -2.177908 avg loss no lamb -2.177908 time 2019-02-25 12:54:24.527218
Model ind 685 epoch 317 head B batch: 400 avg loss -2.197260 avg loss no lamb -2.197260 time 2019-02-25 12:55:42.655989
Model ind 685 epoch 317 head A batch: 0 avg loss -2.219450 avg loss no lamb -2.219450 time 2019-02-25 12:57:01.175077
Model ind 685 epoch 317 head A batch: 100 avg loss -2.192831 avg loss no lamb -2.192831 time 2019-02-25 12:58:18.363380
Model ind 685 epoch 317 head A batch: 200 avg loss -2.236443 avg loss no lamb -2.236443 time 2019-02-25 12:59:34.963780
Model ind 685 epoch 317 head A batch: 300 avg loss -2.200993 avg loss no lamb -2.200993 time 2019-02-25 13:00:50.245180
Model ind 685 epoch 317 head A batch: 400 avg loss -2.202538 avg loss no lamb -2.202538 time 2019-02-25 13:02:07.769405
Pre: time 2019-02-25 13:03:37.301403: 
 	std: 0.006480195
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.979, 0.99207145, 0.9789857, 0.979]
	train_accs: [0.99237144, 0.979, 0.99207145, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.99237144

Starting e_i: 318
Model ind 685 epoch 318 head B batch: 0 avg loss -2.206414 avg loss no lamb -2.206414 time 2019-02-25 13:03:39.212167
Model ind 685 epoch 318 head B batch: 100 avg loss -2.216547 avg loss no lamb -2.216547 time 2019-02-25 13:04:54.520071
Model ind 685 epoch 318 head B batch: 200 avg loss -2.226924 avg loss no lamb -2.226924 time 2019-02-25 13:06:10.088226
Model ind 685 epoch 318 head B batch: 300 avg loss -2.254591 avg loss no lamb -2.254591 time 2019-02-25 13:07:28.722818
Model ind 685 epoch 318 head B batch: 400 avg loss -2.189935 avg loss no lamb -2.189935 time 2019-02-25 13:08:44.762606
Model ind 685 epoch 318 head B batch: 0 avg loss -2.234239 avg loss no lamb -2.234239 time 2019-02-25 13:10:00.594736
Model ind 685 epoch 318 head B batch: 100 avg loss -2.215600 avg loss no lamb -2.215600 time 2019-02-25 13:11:17.804402
Model ind 685 epoch 318 head B batch: 200 avg loss -2.195350 avg loss no lamb -2.195350 time 2019-02-25 13:12:30.247629
Model ind 685 epoch 318 head B batch: 300 avg loss -2.226995 avg loss no lamb -2.226995 time 2019-02-25 13:13:47.584889
Model ind 685 epoch 318 head B batch: 400 avg loss -2.193628 avg loss no lamb -2.193628 time 2019-02-25 13:15:04.082455
Model ind 685 epoch 318 head A batch: 0 avg loss -2.192683 avg loss no lamb -2.192683 time 2019-02-25 13:16:11.768129
Model ind 685 epoch 318 head A batch: 100 avg loss -2.168724 avg loss no lamb -2.168724 time 2019-02-25 13:17:26.915166
Model ind 685 epoch 318 head A batch: 200 avg loss -2.188138 avg loss no lamb -2.188138 time 2019-02-25 13:18:41.621562
Model ind 685 epoch 318 head A batch: 300 avg loss -2.240445 avg loss no lamb -2.240445 time 2019-02-25 13:19:58.624043
Model ind 685 epoch 318 head A batch: 400 avg loss -2.179198 avg loss no lamb -2.179198 time 2019-02-25 13:21:13.382822
Pre: time 2019-02-25 13:22:47.556802: 
 	std: 0.0064929575
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921143, 0.979, 0.9790143]
	train_accs: [0.9924, 0.979, 0.9921143, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9924

Starting e_i: 319
Model ind 685 epoch 319 head B batch: 0 avg loss -2.200245 avg loss no lamb -2.200245 time 2019-02-25 13:22:49.464041
Model ind 685 epoch 319 head B batch: 100 avg loss -2.210573 avg loss no lamb -2.210573 time 2019-02-25 13:24:07.878546
Model ind 685 epoch 319 head B batch: 200 avg loss -2.245089 avg loss no lamb -2.245089 time 2019-02-25 13:25:23.630548
Model ind 685 epoch 319 head B batch: 300 avg loss -2.201010 avg loss no lamb -2.201010 time 2019-02-25 13:26:40.505143
Model ind 685 epoch 319 head B batch: 400 avg loss -2.153062 avg loss no lamb -2.153062 time 2019-02-25 13:27:58.031594
Model ind 685 epoch 319 head B batch: 0 avg loss -2.239991 avg loss no lamb -2.239991 time 2019-02-25 13:29:12.846013
Model ind 685 epoch 319 head B batch: 100 avg loss -2.188120 avg loss no lamb -2.188120 time 2019-02-25 13:30:30.361229
Model ind 685 epoch 319 head B batch: 200 avg loss -2.204214 avg loss no lamb -2.204214 time 2019-02-25 13:31:43.168808
Model ind 685 epoch 319 head B batch: 300 avg loss -2.242985 avg loss no lamb -2.242985 time 2019-02-25 13:32:59.529621
Model ind 685 epoch 319 head B batch: 400 avg loss -2.208640 avg loss no lamb -2.208640 time 2019-02-25 13:34:14.026447
Model ind 685 epoch 319 head A batch: 0 avg loss -2.162971 avg loss no lamb -2.162971 time 2019-02-25 13:35:27.011458
Model ind 685 epoch 319 head A batch: 100 avg loss -2.175145 avg loss no lamb -2.175145 time 2019-02-25 13:36:42.798578
Model ind 685 epoch 319 head A batch: 200 avg loss -2.181463 avg loss no lamb -2.181463 time 2019-02-25 13:37:59.227361
Model ind 685 epoch 319 head A batch: 300 avg loss -2.256334 avg loss no lamb -2.256334 time 2019-02-25 13:39:12.149365
Model ind 685 epoch 319 head A batch: 400 avg loss -2.185475 avg loss no lamb -2.185475 time 2019-02-25 13:40:30.329795
Pre: time 2019-02-25 13:42:03.238646: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 320
Model ind 685 epoch 320 head B batch: 0 avg loss -2.191071 avg loss no lamb -2.191071 time 2019-02-25 13:42:05.518668
Model ind 685 epoch 320 head B batch: 100 avg loss -2.216888 avg loss no lamb -2.216888 time 2019-02-25 13:43:20.559179
Model ind 685 epoch 320 head B batch: 200 avg loss -2.226988 avg loss no lamb -2.226988 time 2019-02-25 13:44:36.055425
Model ind 685 epoch 320 head B batch: 300 avg loss -2.216169 avg loss no lamb -2.216169 time 2019-02-25 13:45:49.602607
Model ind 685 epoch 320 head B batch: 400 avg loss -2.143828 avg loss no lamb -2.143828 time 2019-02-25 13:47:05.639017
Model ind 685 epoch 320 head B batch: 0 avg loss -2.187001 avg loss no lamb -2.187001 time 2019-02-25 13:48:22.737283
Model ind 685 epoch 320 head B batch: 100 avg loss -2.225798 avg loss no lamb -2.225798 time 2019-02-25 13:49:38.169843
Model ind 685 epoch 320 head B batch: 200 avg loss -2.220102 avg loss no lamb -2.220102 time 2019-02-25 13:50:54.228348
Model ind 685 epoch 320 head B batch: 300 avg loss -2.229977 avg loss no lamb -2.229977 time 2019-02-25 13:52:08.736619
Model ind 685 epoch 320 head B batch: 400 avg loss -2.208132 avg loss no lamb -2.208132 time 2019-02-25 13:53:26.853205
Model ind 685 epoch 320 head A batch: 0 avg loss -2.211725 avg loss no lamb -2.211725 time 2019-02-25 13:54:43.732329
Model ind 685 epoch 320 head A batch: 100 avg loss -2.194001 avg loss no lamb -2.194001 time 2019-02-25 13:55:59.778606
Model ind 685 epoch 320 head A batch: 200 avg loss -2.238380 avg loss no lamb -2.238380 time 2019-02-25 13:57:13.911974
Model ind 685 epoch 320 head A batch: 300 avg loss -2.227838 avg loss no lamb -2.227838 time 2019-02-25 13:58:32.442417
Model ind 685 epoch 320 head A batch: 400 avg loss -2.179549 avg loss no lamb -2.179549 time 2019-02-25 13:59:41.708636
Pre: time 2019-02-25 14:01:12.945999: 
 	std: 0.00648486
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98433715
	best: 0.9924286

Starting e_i: 321
Model ind 685 epoch 321 head B batch: 0 avg loss -2.186122 avg loss no lamb -2.186122 time 2019-02-25 14:01:16.221055
Model ind 685 epoch 321 head B batch: 100 avg loss -2.195526 avg loss no lamb -2.195526 time 2019-02-25 14:02:30.685180
Model ind 685 epoch 321 head B batch: 200 avg loss -2.209167 avg loss no lamb -2.209167 time 2019-02-25 14:03:45.676339
Model ind 685 epoch 321 head B batch: 300 avg loss -2.217348 avg loss no lamb -2.217348 time 2019-02-25 14:05:02.820682
Model ind 685 epoch 321 head B batch: 400 avg loss -2.202238 avg loss no lamb -2.202238 time 2019-02-25 14:06:17.840299
Model ind 685 epoch 321 head B batch: 0 avg loss -2.219951 avg loss no lamb -2.219951 time 2019-02-25 14:07:31.052464
Model ind 685 epoch 321 head B batch: 100 avg loss -2.181525 avg loss no lamb -2.181525 time 2019-02-25 14:08:45.386953
Model ind 685 epoch 321 head B batch: 200 avg loss -2.203976 avg loss no lamb -2.203976 time 2019-02-25 14:10:01.994462
Model ind 685 epoch 321 head B batch: 300 avg loss -2.237541 avg loss no lamb -2.237541 time 2019-02-25 14:11:16.567935
Model ind 685 epoch 321 head B batch: 400 avg loss -2.144276 avg loss no lamb -2.144276 time 2019-02-25 14:12:32.475299
Model ind 685 epoch 321 head A batch: 0 avg loss -2.210066 avg loss no lamb -2.210066 time 2019-02-25 14:13:45.917360
Model ind 685 epoch 321 head A batch: 100 avg loss -2.214928 avg loss no lamb -2.214928 time 2019-02-25 14:15:01.711571
Model ind 685 epoch 321 head A batch: 200 avg loss -2.212439 avg loss no lamb -2.212439 time 2019-02-25 14:16:14.738349
Model ind 685 epoch 321 head A batch: 300 avg loss -2.175722 avg loss no lamb -2.175722 time 2019-02-25 14:17:29.892666
Model ind 685 epoch 321 head A batch: 400 avg loss -2.163121 avg loss no lamb -2.163121 time 2019-02-25 14:18:44.755215
Pre: time 2019-02-25 14:20:16.682893: 
 	std: 0.0064915987
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.99235713

Starting e_i: 322
Model ind 685 epoch 322 head B batch: 0 avg loss -2.199031 avg loss no lamb -2.199031 time 2019-02-25 14:20:18.669125
Model ind 685 epoch 322 head B batch: 100 avg loss -2.206476 avg loss no lamb -2.206476 time 2019-02-25 14:21:33.707969
Model ind 685 epoch 322 head B batch: 200 avg loss -2.234002 avg loss no lamb -2.234002 time 2019-02-25 14:22:48.141871
Model ind 685 epoch 322 head B batch: 300 avg loss -2.240366 avg loss no lamb -2.240366 time 2019-02-25 14:24:04.478019
Model ind 685 epoch 322 head B batch: 400 avg loss -2.189146 avg loss no lamb -2.189146 time 2019-02-25 14:25:18.311436
Model ind 685 epoch 322 head B batch: 0 avg loss -2.198572 avg loss no lamb -2.198572 time 2019-02-25 14:26:36.774178
Model ind 685 epoch 322 head B batch: 100 avg loss -2.181692 avg loss no lamb -2.181692 time 2019-02-25 14:27:54.629771
Model ind 685 epoch 322 head B batch: 200 avg loss -2.209548 avg loss no lamb -2.209548 time 2019-02-25 14:29:08.899358
Model ind 685 epoch 322 head B batch: 300 avg loss -2.246548 avg loss no lamb -2.246548 time 2019-02-25 14:30:24.308605
Model ind 685 epoch 322 head B batch: 400 avg loss -2.154806 avg loss no lamb -2.154806 time 2019-02-25 14:31:36.930614
Model ind 685 epoch 322 head A batch: 0 avg loss -2.235549 avg loss no lamb -2.235549 time 2019-02-25 14:32:52.465966
Model ind 685 epoch 322 head A batch: 100 avg loss -2.189131 avg loss no lamb -2.189131 time 2019-02-25 14:34:06.213974
Model ind 685 epoch 322 head A batch: 200 avg loss -2.185754 avg loss no lamb -2.185754 time 2019-02-25 14:35:18.932118
Model ind 685 epoch 322 head A batch: 300 avg loss -2.266080 avg loss no lamb -2.266080 time 2019-02-25 14:36:32.949104
Model ind 685 epoch 322 head A batch: 400 avg loss -2.223321 avg loss no lamb -2.223321 time 2019-02-25 14:37:47.089127
Pre: time 2019-02-25 14:39:18.252882: 
 	std: 0.006469752
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790428, 0.9920857, 0.9790286, 0.9790428]
	train_accs: [0.9924, 0.9790428, 0.9920857, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924

Starting e_i: 323
Model ind 685 epoch 323 head B batch: 0 avg loss -2.218863 avg loss no lamb -2.218863 time 2019-02-25 14:39:20.218310
Model ind 685 epoch 323 head B batch: 100 avg loss -2.194287 avg loss no lamb -2.194287 time 2019-02-25 14:40:35.882364
Model ind 685 epoch 323 head B batch: 200 avg loss -2.231284 avg loss no lamb -2.231284 time 2019-02-25 14:41:51.356894
Model ind 685 epoch 323 head B batch: 300 avg loss -2.204792 avg loss no lamb -2.204792 time 2019-02-25 14:42:56.769072
Model ind 685 epoch 323 head B batch: 400 avg loss -2.234360 avg loss no lamb -2.234360 time 2019-02-25 14:44:11.684358
Model ind 685 epoch 323 head B batch: 0 avg loss -2.229462 avg loss no lamb -2.229462 time 2019-02-25 14:45:28.702527
Model ind 685 epoch 323 head B batch: 100 avg loss -2.173388 avg loss no lamb -2.173388 time 2019-02-25 14:46:45.582783
Model ind 685 epoch 323 head B batch: 200 avg loss -2.215314 avg loss no lamb -2.215314 time 2019-02-25 14:48:00.425639
Model ind 685 epoch 323 head B batch: 300 avg loss -2.238520 avg loss no lamb -2.238520 time 2019-02-25 14:49:16.275963
Model ind 685 epoch 323 head B batch: 400 avg loss -2.213051 avg loss no lamb -2.213051 time 2019-02-25 14:50:29.715215
Model ind 685 epoch 323 head A batch: 0 avg loss -2.187475 avg loss no lamb -2.187475 time 2019-02-25 14:51:44.008363
Model ind 685 epoch 323 head A batch: 100 avg loss -2.238568 avg loss no lamb -2.238568 time 2019-02-25 14:52:58.334914
Model ind 685 epoch 323 head A batch: 200 avg loss -2.191295 avg loss no lamb -2.191295 time 2019-02-25 14:54:12.173218
Model ind 685 epoch 323 head A batch: 300 avg loss -2.184418 avg loss no lamb -2.184418 time 2019-02-25 14:55:27.671747
Model ind 685 epoch 323 head A batch: 400 avg loss -2.170808 avg loss no lamb -2.170808 time 2019-02-25 14:56:40.737650
Pre: time 2019-02-25 14:58:13.816720: 
 	std: 0.006491838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.99212855, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.99212855, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843286
	best: 0.9924286

Starting e_i: 324
Model ind 685 epoch 324 head B batch: 0 avg loss -2.217106 avg loss no lamb -2.217106 time 2019-02-25 14:58:15.898488
Model ind 685 epoch 324 head B batch: 100 avg loss -2.184617 avg loss no lamb -2.184617 time 2019-02-25 14:59:30.458329
Model ind 685 epoch 324 head B batch: 200 avg loss -2.178542 avg loss no lamb -2.178542 time 2019-02-25 15:00:46.771076
Model ind 685 epoch 324 head B batch: 300 avg loss -2.185508 avg loss no lamb -2.185508 time 2019-02-25 15:02:00.885940
Model ind 685 epoch 324 head B batch: 400 avg loss -2.156227 avg loss no lamb -2.156227 time 2019-02-25 15:03:13.971211
Model ind 685 epoch 324 head B batch: 0 avg loss -2.231123 avg loss no lamb -2.231123 time 2019-02-25 15:04:28.630413
Model ind 685 epoch 324 head B batch: 100 avg loss -2.208965 avg loss no lamb -2.208965 time 2019-02-25 15:05:46.291568
Model ind 685 epoch 324 head B batch: 200 avg loss -2.239753 avg loss no lamb -2.239753 time 2019-02-25 15:07:01.661937
Model ind 685 epoch 324 head B batch: 300 avg loss -2.191968 avg loss no lamb -2.191968 time 2019-02-25 15:08:19.307373
Model ind 685 epoch 324 head B batch: 400 avg loss -2.232416 avg loss no lamb -2.232416 time 2019-02-25 15:09:35.765065
Model ind 685 epoch 324 head A batch: 0 avg loss -2.201414 avg loss no lamb -2.201414 time 2019-02-25 15:10:50.224903
Model ind 685 epoch 324 head A batch: 100 avg loss -2.198657 avg loss no lamb -2.198657 time 2019-02-25 15:12:04.089147
Model ind 685 epoch 324 head A batch: 200 avg loss -2.224918 avg loss no lamb -2.224918 time 2019-02-25 15:13:17.251414
Model ind 685 epoch 324 head A batch: 300 avg loss -2.193331 avg loss no lamb -2.193331 time 2019-02-25 15:14:31.820575
Model ind 685 epoch 324 head A batch: 400 avg loss -2.196444 avg loss no lamb -2.196444 time 2019-02-25 15:15:48.483617
Pre: time 2019-02-25 15:17:17.766470: 
 	std: 0.0064792302
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9920857, 0.9790428, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9920857, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432285
	best: 0.9924286

Starting e_i: 325
Model ind 685 epoch 325 head B batch: 0 avg loss -2.225721 avg loss no lamb -2.225721 time 2019-02-25 15:17:19.628183
Model ind 685 epoch 325 head B batch: 100 avg loss -2.217424 avg loss no lamb -2.217424 time 2019-02-25 15:18:37.457565
Model ind 685 epoch 325 head B batch: 200 avg loss -2.195925 avg loss no lamb -2.195925 time 2019-02-25 15:19:55.053117
Model ind 685 epoch 325 head B batch: 300 avg loss -2.171383 avg loss no lamb -2.171383 time 2019-02-25 15:21:11.955688
Model ind 685 epoch 325 head B batch: 400 avg loss -2.194310 avg loss no lamb -2.194310 time 2019-02-25 15:22:25.214025
Model ind 685 epoch 325 head B batch: 0 avg loss -2.218003 avg loss no lamb -2.218003 time 2019-02-25 15:23:36.425000
Model ind 685 epoch 325 head B batch: 100 avg loss -2.228672 avg loss no lamb -2.228672 time 2019-02-25 15:24:53.077063
Model ind 685 epoch 325 head B batch: 200 avg loss -2.196245 avg loss no lamb -2.196245 time 2019-02-25 15:26:04.629353
Model ind 685 epoch 325 head B batch: 300 avg loss -2.205470 avg loss no lamb -2.205470 time 2019-02-25 15:27:19.641419
Model ind 685 epoch 325 head B batch: 400 avg loss -2.220030 avg loss no lamb -2.220030 time 2019-02-25 15:28:34.444379
Model ind 685 epoch 325 head A batch: 0 avg loss -2.234327 avg loss no lamb -2.234327 time 2019-02-25 15:29:47.130036
Model ind 685 epoch 325 head A batch: 100 avg loss -2.208647 avg loss no lamb -2.208647 time 2019-02-25 15:31:02.396884
Model ind 685 epoch 325 head A batch: 200 avg loss -2.232517 avg loss no lamb -2.232517 time 2019-02-25 15:32:18.262898
Model ind 685 epoch 325 head A batch: 300 avg loss -2.192846 avg loss no lamb -2.192846 time 2019-02-25 15:33:34.856129
Model ind 685 epoch 325 head A batch: 400 avg loss -2.215803 avg loss no lamb -2.215803 time 2019-02-25 15:34:50.580017
Pre: time 2019-02-25 15:36:26.407154: 
 	std: 0.006495569
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9924286, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9924286

Starting e_i: 326
Model ind 685 epoch 326 head B batch: 0 avg loss -2.212242 avg loss no lamb -2.212242 time 2019-02-25 15:36:28.668699
Model ind 685 epoch 326 head B batch: 100 avg loss -2.201918 avg loss no lamb -2.201918 time 2019-02-25 15:37:46.557819
Model ind 685 epoch 326 head B batch: 200 avg loss -2.218498 avg loss no lamb -2.218498 time 2019-02-25 15:39:03.619090
Model ind 685 epoch 326 head B batch: 300 avg loss -2.237106 avg loss no lamb -2.237106 time 2019-02-25 15:40:19.409266
Model ind 685 epoch 326 head B batch: 400 avg loss -2.198475 avg loss no lamb -2.198475 time 2019-02-25 15:41:36.877625
Model ind 685 epoch 326 head B batch: 0 avg loss -2.211798 avg loss no lamb -2.211798 time 2019-02-25 15:42:51.621673
Model ind 685 epoch 326 head B batch: 100 avg loss -2.185525 avg loss no lamb -2.185525 time 2019-02-25 15:44:07.534103
Model ind 685 epoch 326 head B batch: 200 avg loss -2.187632 avg loss no lamb -2.187632 time 2019-02-25 15:45:25.040809
Model ind 685 epoch 326 head B batch: 300 avg loss -2.218840 avg loss no lamb -2.218840 time 2019-02-25 15:46:39.689133
Model ind 685 epoch 326 head B batch: 400 avg loss -2.197603 avg loss no lamb -2.197603 time 2019-02-25 15:47:51.778340
Model ind 685 epoch 326 head A batch: 0 avg loss -2.216603 avg loss no lamb -2.216603 time 2019-02-25 15:49:07.743959
Model ind 685 epoch 326 head A batch: 100 avg loss -2.230143 avg loss no lamb -2.230143 time 2019-02-25 15:50:22.805406
Model ind 685 epoch 326 head A batch: 200 avg loss -2.178531 avg loss no lamb -2.178531 time 2019-02-25 15:51:38.585504
Model ind 685 epoch 326 head A batch: 300 avg loss -2.227625 avg loss no lamb -2.227625 time 2019-02-25 15:52:56.251532
Model ind 685 epoch 326 head A batch: 400 avg loss -2.149132 avg loss no lamb -2.149132 time 2019-02-25 15:54:12.278109
Pre: time 2019-02-25 15:55:46.654198: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 327
Model ind 685 epoch 327 head B batch: 0 avg loss -2.221316 avg loss no lamb -2.221316 time 2019-02-25 15:55:48.881219
Model ind 685 epoch 327 head B batch: 100 avg loss -2.207604 avg loss no lamb -2.207604 time 2019-02-25 15:57:06.226408
Model ind 685 epoch 327 head B batch: 200 avg loss -2.174716 avg loss no lamb -2.174716 time 2019-02-25 15:58:20.346710
Model ind 685 epoch 327 head B batch: 300 avg loss -2.213393 avg loss no lamb -2.213393 time 2019-02-25 15:59:33.384338
Model ind 685 epoch 327 head B batch: 400 avg loss -2.177779 avg loss no lamb -2.177779 time 2019-02-25 16:00:46.186833
Model ind 685 epoch 327 head B batch: 0 avg loss -2.242916 avg loss no lamb -2.242916 time 2019-02-25 16:02:02.556890
Model ind 685 epoch 327 head B batch: 100 avg loss -2.197778 avg loss no lamb -2.197778 time 2019-02-25 16:03:19.829403
Model ind 685 epoch 327 head B batch: 200 avg loss -2.188768 avg loss no lamb -2.188768 time 2019-02-25 16:04:30.711407
Model ind 685 epoch 327 head B batch: 300 avg loss -2.184785 avg loss no lamb -2.184785 time 2019-02-25 16:05:41.523146
Model ind 685 epoch 327 head B batch: 400 avg loss -2.153723 avg loss no lamb -2.153723 time 2019-02-25 16:06:58.162344
Model ind 685 epoch 327 head A batch: 0 avg loss -2.197466 avg loss no lamb -2.197466 time 2019-02-25 16:08:14.756226
Model ind 685 epoch 327 head A batch: 100 avg loss -2.170588 avg loss no lamb -2.170588 time 2019-02-25 16:09:21.608049
Model ind 685 epoch 327 head A batch: 200 avg loss -2.190962 avg loss no lamb -2.190962 time 2019-02-25 16:10:39.814327
Model ind 685 epoch 327 head A batch: 300 avg loss -2.214388 avg loss no lamb -2.214388 time 2019-02-25 16:11:58.067267
Model ind 685 epoch 327 head A batch: 400 avg loss -2.200594 avg loss no lamb -2.200594 time 2019-02-25 16:13:14.724608
Pre: time 2019-02-25 16:14:44.601491: 
 	std: 0.006467424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.9924143, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98433715
	best: 0.9924143

Starting e_i: 328
Model ind 685 epoch 328 head B batch: 0 avg loss -2.214966 avg loss no lamb -2.214966 time 2019-02-25 16:14:46.576979
Model ind 685 epoch 328 head B batch: 100 avg loss -2.202236 avg loss no lamb -2.202236 time 2019-02-25 16:16:03.066539
Model ind 685 epoch 328 head B batch: 200 avg loss -2.197326 avg loss no lamb -2.197326 time 2019-02-25 16:17:17.191583
Model ind 685 epoch 328 head B batch: 300 avg loss -2.208374 avg loss no lamb -2.208374 time 2019-02-25 16:18:39.203031
Model ind 685 epoch 328 head B batch: 400 avg loss -2.169238 avg loss no lamb -2.169238 time 2019-02-25 16:19:55.949868
Model ind 685 epoch 328 head B batch: 0 avg loss -2.215482 avg loss no lamb -2.215482 time 2019-02-25 16:21:11.115511
Model ind 685 epoch 328 head B batch: 100 avg loss -2.193898 avg loss no lamb -2.193898 time 2019-02-25 16:22:27.683455
Model ind 685 epoch 328 head B batch: 200 avg loss -2.186497 avg loss no lamb -2.186497 time 2019-02-25 16:23:44.419603
Model ind 685 epoch 328 head B batch: 300 avg loss -2.224077 avg loss no lamb -2.224077 time 2019-02-25 16:25:01.261892
Model ind 685 epoch 328 head B batch: 400 avg loss -2.161421 avg loss no lamb -2.161421 time 2019-02-25 16:26:12.111460
Model ind 685 epoch 328 head A batch: 0 avg loss -2.229596 avg loss no lamb -2.229596 time 2019-02-25 16:27:27.348281
Model ind 685 epoch 328 head A batch: 100 avg loss -2.192395 avg loss no lamb -2.192395 time 2019-02-25 16:28:46.531096
Model ind 685 epoch 328 head A batch: 200 avg loss -2.227772 avg loss no lamb -2.227772 time 2019-02-25 16:30:05.687697
Model ind 685 epoch 328 head A batch: 300 avg loss -2.212491 avg loss no lamb -2.212491 time 2019-02-25 16:31:23.854433
Model ind 685 epoch 328 head A batch: 400 avg loss -2.185836 avg loss no lamb -2.185836 time 2019-02-25 16:32:40.312166
Pre: time 2019-02-25 16:34:11.800410: 
 	std: 0.0064918525
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.9921, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843
	best: 0.9924

Starting e_i: 329
Model ind 685 epoch 329 head B batch: 0 avg loss -2.223811 avg loss no lamb -2.223811 time 2019-02-25 16:34:13.751235
Model ind 685 epoch 329 head B batch: 100 avg loss -2.205926 avg loss no lamb -2.205926 time 2019-02-25 16:35:28.620965
Model ind 685 epoch 329 head B batch: 200 avg loss -2.208295 avg loss no lamb -2.208295 time 2019-02-25 16:36:43.154399
Model ind 685 epoch 329 head B batch: 300 avg loss -2.238469 avg loss no lamb -2.238469 time 2019-02-25 16:37:57.686233
Model ind 685 epoch 329 head B batch: 400 avg loss -2.197709 avg loss no lamb -2.197709 time 2019-02-25 16:39:16.375417
Model ind 685 epoch 329 head B batch: 0 avg loss -2.240687 avg loss no lamb -2.240687 time 2019-02-25 16:40:35.157553
Model ind 685 epoch 329 head B batch: 100 avg loss -2.193165 avg loss no lamb -2.193165 time 2019-02-25 16:41:51.464694
Model ind 685 epoch 329 head B batch: 200 avg loss -2.167868 avg loss no lamb -2.167868 time 2019-02-25 16:43:08.778192
Model ind 685 epoch 329 head B batch: 300 avg loss -2.196270 avg loss no lamb -2.196270 time 2019-02-25 16:44:27.255633
Model ind 685 epoch 329 head B batch: 400 avg loss -2.185220 avg loss no lamb -2.185220 time 2019-02-25 16:45:43.220201
Model ind 685 epoch 329 head A batch: 0 avg loss -2.218904 avg loss no lamb -2.218904 time 2019-02-25 16:47:03.883615
Model ind 685 epoch 329 head A batch: 100 avg loss -2.186536 avg loss no lamb -2.186536 time 2019-02-25 16:48:23.537064
Model ind 685 epoch 329 head A batch: 200 avg loss -2.229901 avg loss no lamb -2.229901 time 2019-02-25 16:49:41.574815
Model ind 685 epoch 329 head A batch: 300 avg loss -2.212694 avg loss no lamb -2.212694 time 2019-02-25 16:50:59.876905
Model ind 685 epoch 329 head A batch: 400 avg loss -2.207125 avg loss no lamb -2.207125 time 2019-02-25 16:52:09.764892
Pre: time 2019-02-25 16:53:45.637744: 
 	std: 0.006495424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924286

Starting e_i: 330
Model ind 685 epoch 330 head B batch: 0 avg loss -2.190358 avg loss no lamb -2.190358 time 2019-02-25 16:53:47.754185
Model ind 685 epoch 330 head B batch: 100 avg loss -2.216830 avg loss no lamb -2.216830 time 2019-02-25 16:55:06.269656
Model ind 685 epoch 330 head B batch: 200 avg loss -2.191114 avg loss no lamb -2.191114 time 2019-02-25 16:56:24.021625
Model ind 685 epoch 330 head B batch: 300 avg loss -2.219756 avg loss no lamb -2.219756 time 2019-02-25 16:57:43.488964
Model ind 685 epoch 330 head B batch: 400 avg loss -2.190718 avg loss no lamb -2.190718 time 2019-02-25 16:59:02.430193
Model ind 685 epoch 330 head B batch: 0 avg loss -2.227193 avg loss no lamb -2.227193 time 2019-02-25 17:00:22.889576
Model ind 685 epoch 330 head B batch: 100 avg loss -2.222901 avg loss no lamb -2.222901 time 2019-02-25 17:01:40.140928
Model ind 685 epoch 330 head B batch: 200 avg loss -2.207800 avg loss no lamb -2.207800 time 2019-02-25 17:03:01.410287
Model ind 685 epoch 330 head B batch: 300 avg loss -2.204082 avg loss no lamb -2.204082 time 2019-02-25 17:04:21.929451
Model ind 685 epoch 330 head B batch: 400 avg loss -2.148044 avg loss no lamb -2.148044 time 2019-02-25 17:05:44.620483
Model ind 685 epoch 330 head A batch: 0 avg loss -2.225405 avg loss no lamb -2.225405 time 2019-02-25 17:07:05.894673
Model ind 685 epoch 330 head A batch: 100 avg loss -2.195864 avg loss no lamb -2.195864 time 2019-02-25 17:08:26.921594
Model ind 685 epoch 330 head A batch: 200 avg loss -2.221277 avg loss no lamb -2.221277 time 2019-02-25 17:09:45.284791
Model ind 685 epoch 330 head A batch: 300 avg loss -2.226096 avg loss no lamb -2.226096 time 2019-02-25 17:11:02.285186
Model ind 685 epoch 330 head A batch: 400 avg loss -2.212467 avg loss no lamb -2.212467 time 2019-02-25 17:12:21.409532
Pre: time 2019-02-25 17:13:59.230526: 
 	std: 0.0064952774
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 331
Model ind 685 epoch 331 head B batch: 0 avg loss -2.232503 avg loss no lamb -2.232503 time 2019-02-25 17:14:01.424645
Model ind 685 epoch 331 head B batch: 100 avg loss -2.201555 avg loss no lamb -2.201555 time 2019-02-25 17:15:21.865713
Model ind 685 epoch 331 head B batch: 200 avg loss -2.220125 avg loss no lamb -2.220125 time 2019-02-25 17:16:42.801144
Model ind 685 epoch 331 head B batch: 300 avg loss -2.260416 avg loss no lamb -2.260416 time 2019-02-25 17:18:03.743394
Model ind 685 epoch 331 head B batch: 400 avg loss -2.147681 avg loss no lamb -2.147681 time 2019-02-25 17:19:26.009896
Model ind 685 epoch 331 head B batch: 0 avg loss -2.228574 avg loss no lamb -2.228574 time 2019-02-25 17:20:47.693167
Model ind 685 epoch 331 head B batch: 100 avg loss -2.165547 avg loss no lamb -2.165547 time 2019-02-25 17:22:08.775658
Model ind 685 epoch 331 head B batch: 200 avg loss -2.244180 avg loss no lamb -2.244180 time 2019-02-25 17:23:29.216357
Model ind 685 epoch 331 head B batch: 300 avg loss -2.229072 avg loss no lamb -2.229072 time 2019-02-25 17:24:49.434260
Model ind 685 epoch 331 head B batch: 400 avg loss -2.159571 avg loss no lamb -2.159571 time 2019-02-25 17:26:11.234784
Model ind 685 epoch 331 head A batch: 0 avg loss -2.200965 avg loss no lamb -2.200965 time 2019-02-25 17:27:33.146257
Model ind 685 epoch 331 head A batch: 100 avg loss -2.170155 avg loss no lamb -2.170155 time 2019-02-25 17:28:53.383143
Model ind 685 epoch 331 head A batch: 200 avg loss -2.218754 avg loss no lamb -2.218754 time 2019-02-25 17:30:13.698473
Model ind 685 epoch 331 head A batch: 300 avg loss -2.193501 avg loss no lamb -2.193501 time 2019-02-25 17:31:32.490421
Model ind 685 epoch 331 head A batch: 400 avg loss -2.209351 avg loss no lamb -2.209351 time 2019-02-25 17:32:51.230525
Pre: time 2019-02-25 17:34:20.107960: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 332
Model ind 685 epoch 332 head B batch: 0 avg loss -2.214289 avg loss no lamb -2.214289 time 2019-02-25 17:34:22.377923
Model ind 685 epoch 332 head B batch: 100 avg loss -2.221750 avg loss no lamb -2.221750 time 2019-02-25 17:35:38.377706
Model ind 685 epoch 332 head B batch: 200 avg loss -2.228903 avg loss no lamb -2.228903 time 2019-02-25 17:36:57.255051
Model ind 685 epoch 332 head B batch: 300 avg loss -2.226731 avg loss no lamb -2.226731 time 2019-02-25 17:38:15.409738
Model ind 685 epoch 332 head B batch: 400 avg loss -2.177204 avg loss no lamb -2.177204 time 2019-02-25 17:39:34.021734
Model ind 685 epoch 332 head B batch: 0 avg loss -2.218272 avg loss no lamb -2.218272 time 2019-02-25 17:40:51.775941
Model ind 685 epoch 332 head B batch: 100 avg loss -2.203234 avg loss no lamb -2.203234 time 2019-02-25 17:42:08.871517
Model ind 685 epoch 332 head B batch: 200 avg loss -2.233528 avg loss no lamb -2.233528 time 2019-02-25 17:43:26.215329
Model ind 685 epoch 332 head B batch: 300 avg loss -2.244605 avg loss no lamb -2.244605 time 2019-02-25 17:44:42.732582
Model ind 685 epoch 332 head B batch: 400 avg loss -2.193017 avg loss no lamb -2.193017 time 2019-02-25 17:45:59.183347
Model ind 685 epoch 332 head A batch: 0 avg loss -2.238150 avg loss no lamb -2.238150 time 2019-02-25 17:47:17.927743
Model ind 685 epoch 332 head A batch: 100 avg loss -2.181283 avg loss no lamb -2.181283 time 2019-02-25 17:48:33.372248
Model ind 685 epoch 332 head A batch: 200 avg loss -2.169882 avg loss no lamb -2.169882 time 2019-02-25 17:49:51.337902
Model ind 685 epoch 332 head A batch: 300 avg loss -2.223714 avg loss no lamb -2.223714 time 2019-02-25 17:51:11.928423
Model ind 685 epoch 332 head A batch: 400 avg loss -2.203139 avg loss no lamb -2.203139 time 2019-02-25 17:52:28.179939
Pre: time 2019-02-25 17:53:59.866350: 
 	std: 0.006502567
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790286, 0.9921143, 0.979, 0.9790143]
	train_accs: [0.99245715, 0.9790286, 0.9921143, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843229
	best: 0.99245715

Starting e_i: 333
Model ind 685 epoch 333 head B batch: 0 avg loss -2.191093 avg loss no lamb -2.191093 time 2019-02-25 17:54:01.802379
Model ind 685 epoch 333 head B batch: 100 avg loss -2.205372 avg loss no lamb -2.205372 time 2019-02-25 17:55:18.970956
Model ind 685 epoch 333 head B batch: 200 avg loss -2.196097 avg loss no lamb -2.196097 time 2019-02-25 17:56:36.054063
Model ind 685 epoch 333 head B batch: 300 avg loss -2.228184 avg loss no lamb -2.228184 time 2019-02-25 17:57:52.531775
Model ind 685 epoch 333 head B batch: 400 avg loss -2.213894 avg loss no lamb -2.213894 time 2019-02-25 17:59:06.030434
Model ind 685 epoch 333 head B batch: 0 avg loss -2.196589 avg loss no lamb -2.196589 time 2019-02-25 18:00:16.824213
Model ind 685 epoch 333 head B batch: 100 avg loss -2.183551 avg loss no lamb -2.183551 time 2019-02-25 18:01:27.553362
Model ind 685 epoch 333 head B batch: 200 avg loss -2.240437 avg loss no lamb -2.240437 time 2019-02-25 18:02:38.285066
Model ind 685 epoch 333 head B batch: 300 avg loss -2.245495 avg loss no lamb -2.245495 time 2019-02-25 18:03:49.016976
Model ind 685 epoch 333 head B batch: 400 avg loss -2.221052 avg loss no lamb -2.221052 time 2019-02-25 18:05:02.022636
Model ind 685 epoch 333 head A batch: 0 avg loss -2.181767 avg loss no lamb -2.181767 time 2019-02-25 18:06:19.765117
Model ind 685 epoch 333 head A batch: 100 avg loss -2.206401 avg loss no lamb -2.206401 time 2019-02-25 18:07:35.928428
Model ind 685 epoch 333 head A batch: 200 avg loss -2.233904 avg loss no lamb -2.233904 time 2019-02-25 18:09:00.373940
Model ind 685 epoch 333 head A batch: 300 avg loss -2.203445 avg loss no lamb -2.203445 time 2019-02-25 18:10:21.713889
Model ind 685 epoch 333 head A batch: 400 avg loss -2.172230 avg loss no lamb -2.172230 time 2019-02-25 18:11:38.704887
Pre: time 2019-02-25 18:13:19.049269: 
 	std: 0.0064988593
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790428, 0.99215716, 0.9790428, 0.9790428]
	train_accs: [0.99245715, 0.9790428, 0.99215716, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843486
	best: 0.99245715

Starting e_i: 334
Model ind 685 epoch 334 head B batch: 0 avg loss -2.221439 avg loss no lamb -2.221439 time 2019-02-25 18:13:21.156307
Model ind 685 epoch 334 head B batch: 100 avg loss -2.229220 avg loss no lamb -2.229220 time 2019-02-25 18:14:43.117398
Model ind 685 epoch 334 head B batch: 200 avg loss -2.204919 avg loss no lamb -2.204919 time 2019-02-25 18:16:03.252644
Model ind 685 epoch 334 head B batch: 300 avg loss -2.252798 avg loss no lamb -2.252798 time 2019-02-25 18:17:15.130783
Model ind 685 epoch 334 head B batch: 400 avg loss -2.170833 avg loss no lamb -2.170833 time 2019-02-25 18:18:32.731473
Model ind 685 epoch 334 head B batch: 0 avg loss -2.216372 avg loss no lamb -2.216372 time 2019-02-25 18:19:49.470286
Model ind 685 epoch 334 head B batch: 100 avg loss -2.193178 avg loss no lamb -2.193178 time 2019-02-25 18:21:07.773116
Model ind 685 epoch 334 head B batch: 200 avg loss -2.212857 avg loss no lamb -2.212857 time 2019-02-25 18:22:26.101297
Model ind 685 epoch 334 head B batch: 300 avg loss -2.208686 avg loss no lamb -2.208686 time 2019-02-25 18:23:43.928055
Model ind 685 epoch 334 head B batch: 400 avg loss -2.222546 avg loss no lamb -2.222546 time 2019-02-25 18:25:01.763661
Model ind 685 epoch 334 head A batch: 0 avg loss -2.188519 avg loss no lamb -2.188519 time 2019-02-25 18:26:19.629579
Model ind 685 epoch 334 head A batch: 100 avg loss -2.226645 avg loss no lamb -2.226645 time 2019-02-25 18:27:36.363112
Model ind 685 epoch 334 head A batch: 200 avg loss -2.182662 avg loss no lamb -2.182662 time 2019-02-25 18:28:56.013652
Model ind 685 epoch 334 head A batch: 300 avg loss -2.248172 avg loss no lamb -2.248172 time 2019-02-25 18:30:12.830468
Model ind 685 epoch 334 head A batch: 400 avg loss -2.196055 avg loss no lamb -2.196055 time 2019-02-25 18:31:26.321756
Pre: time 2019-02-25 18:32:57.892647: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 335
Model ind 685 epoch 335 head B batch: 0 avg loss -2.221188 avg loss no lamb -2.221188 time 2019-02-25 18:33:00.118877
Model ind 685 epoch 335 head B batch: 100 avg loss -2.236305 avg loss no lamb -2.236305 time 2019-02-25 18:34:16.802224
Model ind 685 epoch 335 head B batch: 200 avg loss -2.209041 avg loss no lamb -2.209041 time 2019-02-25 18:35:32.947288
Model ind 685 epoch 335 head B batch: 300 avg loss -2.251827 avg loss no lamb -2.251827 time 2019-02-25 18:36:49.784625
Model ind 685 epoch 335 head B batch: 400 avg loss -2.201751 avg loss no lamb -2.201751 time 2019-02-25 18:38:10.173449
Model ind 685 epoch 335 head B batch: 0 avg loss -2.204636 avg loss no lamb -2.204636 time 2019-02-25 18:39:32.841994
Model ind 685 epoch 335 head B batch: 100 avg loss -2.182949 avg loss no lamb -2.182949 time 2019-02-25 18:40:51.804964
Model ind 685 epoch 335 head B batch: 200 avg loss -2.217759 avg loss no lamb -2.217759 time 2019-02-25 18:42:07.846913
Model ind 685 epoch 335 head B batch: 300 avg loss -2.166570 avg loss no lamb -2.166570 time 2019-02-25 18:43:22.438460
Model ind 685 epoch 335 head B batch: 400 avg loss -2.175881 avg loss no lamb -2.175881 time 2019-02-25 18:44:39.578779
Model ind 685 epoch 335 head A batch: 0 avg loss -2.236850 avg loss no lamb -2.236850 time 2019-02-25 18:45:55.680700
Model ind 685 epoch 335 head A batch: 100 avg loss -2.206147 avg loss no lamb -2.206147 time 2019-02-25 18:47:12.616608
Model ind 685 epoch 335 head A batch: 200 avg loss -2.251250 avg loss no lamb -2.251250 time 2019-02-25 18:48:27.443098
Model ind 685 epoch 335 head A batch: 300 avg loss -2.171103 avg loss no lamb -2.171103 time 2019-02-25 18:49:42.665355
Model ind 685 epoch 335 head A batch: 400 avg loss -2.198883 avg loss no lamb -2.198883 time 2019-02-25 18:50:58.439417
Pre: time 2019-02-25 18:52:32.066140: 
 	std: 0.0064745764
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9920857, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.9920857, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924286

Starting e_i: 336
Model ind 685 epoch 336 head B batch: 0 avg loss -2.190277 avg loss no lamb -2.190277 time 2019-02-25 18:52:34.281012
Model ind 685 epoch 336 head B batch: 100 avg loss -2.188985 avg loss no lamb -2.188985 time 2019-02-25 18:53:49.776525
Model ind 685 epoch 336 head B batch: 200 avg loss -2.186264 avg loss no lamb -2.186264 time 2019-02-25 18:55:05.311770
Model ind 685 epoch 336 head B batch: 300 avg loss -2.214962 avg loss no lamb -2.214962 time 2019-02-25 18:56:22.414285
Model ind 685 epoch 336 head B batch: 400 avg loss -2.203872 avg loss no lamb -2.203872 time 2019-02-25 18:57:39.625407
Model ind 685 epoch 336 head B batch: 0 avg loss -2.236168 avg loss no lamb -2.236168 time 2019-02-25 18:58:53.097011
Model ind 685 epoch 336 head B batch: 100 avg loss -2.206016 avg loss no lamb -2.206016 time 2019-02-25 18:59:57.265275
Model ind 685 epoch 336 head B batch: 200 avg loss -2.210990 avg loss no lamb -2.210990 time 2019-02-25 19:01:09.674133
Model ind 685 epoch 336 head B batch: 300 avg loss -2.226076 avg loss no lamb -2.226076 time 2019-02-25 19:02:24.714210
Model ind 685 epoch 336 head B batch: 400 avg loss -2.203283 avg loss no lamb -2.203283 time 2019-02-25 19:03:41.489482
Model ind 685 epoch 336 head A batch: 0 avg loss -2.240093 avg loss no lamb -2.240093 time 2019-02-25 19:04:57.698522
Model ind 685 epoch 336 head A batch: 100 avg loss -2.199692 avg loss no lamb -2.199692 time 2019-02-25 19:06:14.127404
Model ind 685 epoch 336 head A batch: 200 avg loss -2.221358 avg loss no lamb -2.221358 time 2019-02-25 19:07:30.587280
Model ind 685 epoch 336 head A batch: 300 avg loss -2.212057 avg loss no lamb -2.212057 time 2019-02-25 19:08:45.981249
Model ind 685 epoch 336 head A batch: 400 avg loss -2.184198 avg loss no lamb -2.184198 time 2019-02-25 19:10:03.315220
Pre: time 2019-02-25 19:11:33.224436: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.99244285

Starting e_i: 337
Model ind 685 epoch 337 head B batch: 0 avg loss -2.220703 avg loss no lamb -2.220703 time 2019-02-25 19:11:35.384661
Model ind 685 epoch 337 head B batch: 100 avg loss -2.209490 avg loss no lamb -2.209490 time 2019-02-25 19:12:48.568046
Model ind 685 epoch 337 head B batch: 200 avg loss -2.235994 avg loss no lamb -2.235994 time 2019-02-25 19:14:03.711076
Model ind 685 epoch 337 head B batch: 300 avg loss -2.226996 avg loss no lamb -2.226996 time 2019-02-25 19:15:19.097390
Model ind 685 epoch 337 head B batch: 400 avg loss -2.195061 avg loss no lamb -2.195061 time 2019-02-25 19:16:35.819031
Model ind 685 epoch 337 head B batch: 0 avg loss -2.238213 avg loss no lamb -2.238213 time 2019-02-25 19:17:50.183327
Model ind 685 epoch 337 head B batch: 100 avg loss -2.228949 avg loss no lamb -2.228949 time 2019-02-25 19:19:04.958077
Model ind 685 epoch 337 head B batch: 200 avg loss -2.212836 avg loss no lamb -2.212836 time 2019-02-25 19:20:21.497007
Model ind 685 epoch 337 head B batch: 300 avg loss -2.216900 avg loss no lamb -2.216900 time 2019-02-25 19:21:38.033541
Model ind 685 epoch 337 head B batch: 400 avg loss -2.175086 avg loss no lamb -2.175086 time 2019-02-25 19:22:54.381216
Model ind 685 epoch 337 head A batch: 0 avg loss -2.178025 avg loss no lamb -2.178025 time 2019-02-25 19:24:13.175194
Model ind 685 epoch 337 head A batch: 100 avg loss -2.180602 avg loss no lamb -2.180602 time 2019-02-25 19:25:26.544862
Model ind 685 epoch 337 head A batch: 200 avg loss -2.199786 avg loss no lamb -2.199786 time 2019-02-25 19:26:43.981959
Model ind 685 epoch 337 head A batch: 300 avg loss -2.256679 avg loss no lamb -2.256679 time 2019-02-25 19:27:59.818956
Model ind 685 epoch 337 head A batch: 400 avg loss -2.226537 avg loss no lamb -2.226537 time 2019-02-25 19:29:14.119666
Pre: time 2019-02-25 19:30:44.268261: 
 	std: 0.006486083
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924143

Starting e_i: 338
Model ind 685 epoch 338 head B batch: 0 avg loss -2.193656 avg loss no lamb -2.193656 time 2019-02-25 19:30:46.422808
Model ind 685 epoch 338 head B batch: 100 avg loss -2.209791 avg loss no lamb -2.209791 time 2019-02-25 19:32:05.246520
Model ind 685 epoch 338 head B batch: 200 avg loss -2.216642 avg loss no lamb -2.216642 time 2019-02-25 19:33:25.930353
Model ind 685 epoch 338 head B batch: 300 avg loss -2.205563 avg loss no lamb -2.205563 time 2019-02-25 19:34:44.601989
Model ind 685 epoch 338 head B batch: 400 avg loss -2.208168 avg loss no lamb -2.208168 time 2019-02-25 19:36:04.449108
Model ind 685 epoch 338 head B batch: 0 avg loss -2.192108 avg loss no lamb -2.192108 time 2019-02-25 19:37:23.140803
Model ind 685 epoch 338 head B batch: 100 avg loss -2.193416 avg loss no lamb -2.193416 time 2019-02-25 19:38:40.872900
Model ind 685 epoch 338 head B batch: 200 avg loss -2.168568 avg loss no lamb -2.168568 time 2019-02-25 19:39:58.562309
Model ind 685 epoch 338 head B batch: 300 avg loss -2.187809 avg loss no lamb -2.187809 time 2019-02-25 19:41:14.724532
Model ind 685 epoch 338 head B batch: 400 avg loss -2.199654 avg loss no lamb -2.199654 time 2019-02-25 19:42:23.683569
Model ind 685 epoch 338 head A batch: 0 avg loss -2.212724 avg loss no lamb -2.212724 time 2019-02-25 19:43:39.359842
Model ind 685 epoch 338 head A batch: 100 avg loss -2.177184 avg loss no lamb -2.177184 time 2019-02-25 19:44:56.525239
Model ind 685 epoch 338 head A batch: 200 avg loss -2.199861 avg loss no lamb -2.199861 time 2019-02-25 19:46:10.012551
Model ind 685 epoch 338 head A batch: 300 avg loss -2.240311 avg loss no lamb -2.240311 time 2019-02-25 19:47:22.728605
Model ind 685 epoch 338 head A batch: 400 avg loss -2.190593 avg loss no lamb -2.190593 time 2019-02-25 19:48:36.993178
Pre: time 2019-02-25 19:50:07.139517: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 339
Model ind 685 epoch 339 head B batch: 0 avg loss -2.209583 avg loss no lamb -2.209583 time 2019-02-25 19:50:08.990096
Model ind 685 epoch 339 head B batch: 100 avg loss -2.205168 avg loss no lamb -2.205168 time 2019-02-25 19:51:25.228927
Model ind 685 epoch 339 head B batch: 200 avg loss -2.212339 avg loss no lamb -2.212339 time 2019-02-25 19:52:42.875697
Model ind 685 epoch 339 head B batch: 300 avg loss -2.207330 avg loss no lamb -2.207330 time 2019-02-25 19:53:59.394054
Model ind 685 epoch 339 head B batch: 400 avg loss -2.186822 avg loss no lamb -2.186822 time 2019-02-25 19:55:11.480779
Model ind 685 epoch 339 head B batch: 0 avg loss -2.215162 avg loss no lamb -2.215162 time 2019-02-25 19:56:25.340343
Model ind 685 epoch 339 head B batch: 100 avg loss -2.191081 avg loss no lamb -2.191081 time 2019-02-25 19:57:39.108209
Model ind 685 epoch 339 head B batch: 200 avg loss -2.232573 avg loss no lamb -2.232573 time 2019-02-25 19:58:54.909325
Model ind 685 epoch 339 head B batch: 300 avg loss -2.199186 avg loss no lamb -2.199186 time 2019-02-25 20:00:07.978772
Model ind 685 epoch 339 head B batch: 400 avg loss -2.178029 avg loss no lamb -2.178029 time 2019-02-25 20:01:21.095986
Model ind 685 epoch 339 head A batch: 0 avg loss -2.224640 avg loss no lamb -2.224640 time 2019-02-25 20:02:36.422317
Model ind 685 epoch 339 head A batch: 100 avg loss -2.212083 avg loss no lamb -2.212083 time 2019-02-25 20:03:49.764446
Model ind 685 epoch 339 head A batch: 200 avg loss -2.198557 avg loss no lamb -2.198557 time 2019-02-25 20:05:03.421259
Model ind 685 epoch 339 head A batch: 300 avg loss -2.238728 avg loss no lamb -2.238728 time 2019-02-25 20:06:18.564369
Model ind 685 epoch 339 head A batch: 400 avg loss -2.221345 avg loss no lamb -2.221345 time 2019-02-25 20:07:33.950769
Pre: time 2019-02-25 20:09:05.477539: 
 	std: 0.006464128
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99222857, 0.97885716, 0.9918714, 0.97885716, 0.97885716]
	train_accs: [0.99222857, 0.97885716, 0.9918714, 0.97885716, 0.97885716]
	best_train_sub_head: 0
	worst: 0.97885716
	avg: 0.9841343
	best: 0.99222857

Starting e_i: 340
Model ind 685 epoch 340 head B batch: 0 avg loss -2.243270 avg loss no lamb -2.243270 time 2019-02-25 20:09:07.586713
Model ind 685 epoch 340 head B batch: 100 avg loss -2.192407 avg loss no lamb -2.192407 time 2019-02-25 20:10:22.403580
Model ind 685 epoch 340 head B batch: 200 avg loss -2.189720 avg loss no lamb -2.189720 time 2019-02-25 20:11:41.570936
Model ind 685 epoch 340 head B batch: 300 avg loss -2.242575 avg loss no lamb -2.242575 time 2019-02-25 20:12:58.382272
Model ind 685 epoch 340 head B batch: 400 avg loss -2.207552 avg loss no lamb -2.207552 time 2019-02-25 20:14:15.140833
Model ind 685 epoch 340 head B batch: 0 avg loss -2.164465 avg loss no lamb -2.164465 time 2019-02-25 20:15:30.005529
Model ind 685 epoch 340 head B batch: 100 avg loss -2.248654 avg loss no lamb -2.248654 time 2019-02-25 20:16:44.963120
Model ind 685 epoch 340 head B batch: 200 avg loss -2.194994 avg loss no lamb -2.194994 time 2019-02-25 20:18:01.930286
Model ind 685 epoch 340 head B batch: 300 avg loss -2.206042 avg loss no lamb -2.206042 time 2019-02-25 20:19:15.498371
Model ind 685 epoch 340 head B batch: 400 avg loss -2.153586 avg loss no lamb -2.153586 time 2019-02-25 20:20:32.482424
Model ind 685 epoch 340 head A batch: 0 avg loss -2.228128 avg loss no lamb -2.228128 time 2019-02-25 20:21:47.345307
Model ind 685 epoch 340 head A batch: 100 avg loss -2.203498 avg loss no lamb -2.203498 time 2019-02-25 20:23:06.922381
Model ind 685 epoch 340 head A batch: 200 avg loss -2.202335 avg loss no lamb -2.202335 time 2019-02-25 20:24:19.973693
Model ind 685 epoch 340 head A batch: 300 avg loss -2.228462 avg loss no lamb -2.228462 time 2019-02-25 20:25:24.150930
Model ind 685 epoch 340 head A batch: 400 avg loss -2.184233 avg loss no lamb -2.184233 time 2019-02-25 20:26:42.238839
Pre: time 2019-02-25 20:28:16.535432: 
 	std: 0.006476757
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789429, 0.9920286, 0.9789857, 0.9789714]
	train_accs: [0.9923428, 0.9789429, 0.9920286, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98425424
	best: 0.9923428

Starting e_i: 341
Model ind 685 epoch 341 head B batch: 0 avg loss -2.218979 avg loss no lamb -2.218979 time 2019-02-25 20:28:20.051035
Model ind 685 epoch 341 head B batch: 100 avg loss -2.251670 avg loss no lamb -2.251670 time 2019-02-25 20:29:37.828673
Model ind 685 epoch 341 head B batch: 200 avg loss -2.213592 avg loss no lamb -2.213592 time 2019-02-25 20:30:55.575130
Model ind 685 epoch 341 head B batch: 300 avg loss -2.216187 avg loss no lamb -2.216187 time 2019-02-25 20:32:13.171560
Model ind 685 epoch 341 head B batch: 400 avg loss -2.163069 avg loss no lamb -2.163069 time 2019-02-25 20:33:31.196191
Model ind 685 epoch 341 head B batch: 0 avg loss -2.242114 avg loss no lamb -2.242114 time 2019-02-25 20:34:49.182892
Model ind 685 epoch 341 head B batch: 100 avg loss -2.223581 avg loss no lamb -2.223581 time 2019-02-25 20:36:03.027978
Model ind 685 epoch 341 head B batch: 200 avg loss -2.204964 avg loss no lamb -2.204964 time 2019-02-25 20:37:14.469299
Model ind 685 epoch 341 head B batch: 300 avg loss -2.246684 avg loss no lamb -2.246684 time 2019-02-25 20:38:25.742908
Model ind 685 epoch 341 head B batch: 400 avg loss -2.166029 avg loss no lamb -2.166029 time 2019-02-25 20:39:41.194394
Model ind 685 epoch 341 head A batch: 0 avg loss -2.211461 avg loss no lamb -2.211461 time 2019-02-25 20:40:58.200881
Model ind 685 epoch 341 head A batch: 100 avg loss -2.221571 avg loss no lamb -2.221571 time 2019-02-25 20:42:18.857928
Model ind 685 epoch 341 head A batch: 200 avg loss -2.189122 avg loss no lamb -2.189122 time 2019-02-25 20:43:37.910409
Model ind 685 epoch 341 head A batch: 300 avg loss -2.222130 avg loss no lamb -2.222130 time 2019-02-25 20:44:54.398085
Model ind 685 epoch 341 head A batch: 400 avg loss -2.198256 avg loss no lamb -2.198256 time 2019-02-25 20:46:13.658472
Pre: time 2019-02-25 20:47:47.298067: 
 	std: 0.0064952774
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.99212855, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 342
Model ind 685 epoch 342 head B batch: 0 avg loss -2.200286 avg loss no lamb -2.200286 time 2019-02-25 20:47:49.732704
Model ind 685 epoch 342 head B batch: 100 avg loss -2.239130 avg loss no lamb -2.239130 time 2019-02-25 20:49:06.783714
Model ind 685 epoch 342 head B batch: 200 avg loss -2.174847 avg loss no lamb -2.174847 time 2019-02-25 20:50:22.742697
Model ind 685 epoch 342 head B batch: 300 avg loss -2.174031 avg loss no lamb -2.174031 time 2019-02-25 20:51:40.027314
Model ind 685 epoch 342 head B batch: 400 avg loss -2.180661 avg loss no lamb -2.180661 time 2019-02-25 20:52:56.005683
Model ind 685 epoch 342 head B batch: 0 avg loss -2.226778 avg loss no lamb -2.226778 time 2019-02-25 20:54:13.039025
Model ind 685 epoch 342 head B batch: 100 avg loss -2.207820 avg loss no lamb -2.207820 time 2019-02-25 20:55:26.970864
Model ind 685 epoch 342 head B batch: 200 avg loss -2.216697 avg loss no lamb -2.216697 time 2019-02-25 20:56:43.896792
Model ind 685 epoch 342 head B batch: 300 avg loss -2.208696 avg loss no lamb -2.208696 time 2019-02-25 20:58:00.553511
Model ind 685 epoch 342 head B batch: 400 avg loss -2.209002 avg loss no lamb -2.209002 time 2019-02-25 20:59:19.854097
Model ind 685 epoch 342 head A batch: 0 avg loss -2.213415 avg loss no lamb -2.213415 time 2019-02-25 21:00:36.888157
Model ind 685 epoch 342 head A batch: 100 avg loss -2.206296 avg loss no lamb -2.206296 time 2019-02-25 21:01:52.881995
Model ind 685 epoch 342 head A batch: 200 avg loss -2.236915 avg loss no lamb -2.236915 time 2019-02-25 21:03:09.175766
Model ind 685 epoch 342 head A batch: 300 avg loss -2.191234 avg loss no lamb -2.191234 time 2019-02-25 21:04:25.947628
Model ind 685 epoch 342 head A batch: 400 avg loss -2.192609 avg loss no lamb -2.192609 time 2019-02-25 21:05:39.347151
Pre: time 2019-02-25 21:07:10.684847: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 343
Model ind 685 epoch 343 head B batch: 0 avg loss -2.219558 avg loss no lamb -2.219558 time 2019-02-25 21:07:13.135497
Model ind 685 epoch 343 head B batch: 100 avg loss -2.201204 avg loss no lamb -2.201204 time 2019-02-25 21:08:23.179926
Model ind 685 epoch 343 head B batch: 200 avg loss -2.201793 avg loss no lamb -2.201793 time 2019-02-25 21:09:38.764287
Model ind 685 epoch 343 head B batch: 300 avg loss -2.195680 avg loss no lamb -2.195680 time 2019-02-25 21:10:49.925945
Model ind 685 epoch 343 head B batch: 400 avg loss -2.192965 avg loss no lamb -2.192965 time 2019-02-25 21:12:01.146739
Model ind 685 epoch 343 head B batch: 0 avg loss -2.222355 avg loss no lamb -2.222355 time 2019-02-25 21:13:15.927030
Model ind 685 epoch 343 head B batch: 100 avg loss -2.214496 avg loss no lamb -2.214496 time 2019-02-25 21:14:27.208797
Model ind 685 epoch 343 head B batch: 200 avg loss -2.229893 avg loss no lamb -2.229893 time 2019-02-25 21:15:38.941517
Model ind 685 epoch 343 head B batch: 300 avg loss -2.193626 avg loss no lamb -2.193626 time 2019-02-25 21:16:55.609053
Model ind 685 epoch 343 head B batch: 400 avg loss -2.194354 avg loss no lamb -2.194354 time 2019-02-25 21:18:13.490448
Model ind 685 epoch 343 head A batch: 0 avg loss -2.251590 avg loss no lamb -2.251590 time 2019-02-25 21:19:30.102081
Model ind 685 epoch 343 head A batch: 100 avg loss -2.207218 avg loss no lamb -2.207218 time 2019-02-25 21:20:41.346622
Model ind 685 epoch 343 head A batch: 200 avg loss -2.196506 avg loss no lamb -2.196506 time 2019-02-25 21:21:52.585051
Model ind 685 epoch 343 head A batch: 300 avg loss -2.191848 avg loss no lamb -2.191848 time 2019-02-25 21:23:03.330467
Model ind 685 epoch 343 head A batch: 400 avg loss -2.174173 avg loss no lamb -2.174173 time 2019-02-25 21:24:14.157471
Pre: time 2019-02-25 21:25:40.140389: 
 	std: 0.006468507
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790428]
	train_accs: [0.9923857, 0.9790286, 0.9920857, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843143
	best: 0.9923857

Starting e_i: 344
Model ind 685 epoch 344 head B batch: 0 avg loss -2.228040 avg loss no lamb -2.228040 time 2019-02-25 21:25:42.264905
Model ind 685 epoch 344 head B batch: 100 avg loss -2.244722 avg loss no lamb -2.244722 time 2019-02-25 21:26:55.703014
Model ind 685 epoch 344 head B batch: 200 avg loss -2.178437 avg loss no lamb -2.178437 time 2019-02-25 21:28:11.657238
Model ind 685 epoch 344 head B batch: 300 avg loss -2.249704 avg loss no lamb -2.249704 time 2019-02-25 21:29:29.360325
Model ind 685 epoch 344 head B batch: 400 avg loss -2.225325 avg loss no lamb -2.225325 time 2019-02-25 21:30:46.610908
Model ind 685 epoch 344 head B batch: 0 avg loss -2.203459 avg loss no lamb -2.203459 time 2019-02-25 21:32:02.670994
Model ind 685 epoch 344 head B batch: 100 avg loss -2.212898 avg loss no lamb -2.212898 time 2019-02-25 21:33:18.297338
Model ind 685 epoch 344 head B batch: 200 avg loss -2.181601 avg loss no lamb -2.181601 time 2019-02-25 21:34:30.935345
Model ind 685 epoch 344 head B batch: 300 avg loss -2.215601 avg loss no lamb -2.215601 time 2019-02-25 21:35:44.603514
Model ind 685 epoch 344 head B batch: 400 avg loss -2.195080 avg loss no lamb -2.195080 time 2019-02-25 21:36:55.425429
Model ind 685 epoch 344 head A batch: 0 avg loss -2.254946 avg loss no lamb -2.254946 time 2019-02-25 21:38:06.217510
Model ind 685 epoch 344 head A batch: 100 avg loss -2.220286 avg loss no lamb -2.220286 time 2019-02-25 21:39:17.030184
Model ind 685 epoch 344 head A batch: 200 avg loss -2.203257 avg loss no lamb -2.203257 time 2019-02-25 21:40:27.829875
Model ind 685 epoch 344 head A batch: 300 avg loss -2.209814 avg loss no lamb -2.209814 time 2019-02-25 21:41:39.617892
Model ind 685 epoch 344 head A batch: 400 avg loss -2.192729 avg loss no lamb -2.192729 time 2019-02-25 21:42:54.245422
Pre: time 2019-02-25 21:44:27.654614: 
 	std: 0.006480192
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790428, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790428, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 345
Model ind 685 epoch 345 head B batch: 0 avg loss -2.197603 avg loss no lamb -2.197603 time 2019-02-25 21:44:30.104160
Model ind 685 epoch 345 head B batch: 100 avg loss -2.211143 avg loss no lamb -2.211143 time 2019-02-25 21:45:45.047649
Model ind 685 epoch 345 head B batch: 200 avg loss -2.205921 avg loss no lamb -2.205921 time 2019-02-25 21:47:01.339007
Model ind 685 epoch 345 head B batch: 300 avg loss -2.203820 avg loss no lamb -2.203820 time 2019-02-25 21:48:17.868598
Model ind 685 epoch 345 head B batch: 400 avg loss -2.180634 avg loss no lamb -2.180634 time 2019-02-25 21:49:34.023740
Model ind 685 epoch 345 head B batch: 0 avg loss -2.226912 avg loss no lamb -2.226912 time 2019-02-25 21:50:42.954318
Model ind 685 epoch 345 head B batch: 100 avg loss -2.195478 avg loss no lamb -2.195478 time 2019-02-25 21:51:59.055961
Model ind 685 epoch 345 head B batch: 200 avg loss -2.226325 avg loss no lamb -2.226325 time 2019-02-25 21:53:13.920401
Model ind 685 epoch 345 head B batch: 300 avg loss -2.199897 avg loss no lamb -2.199897 time 2019-02-25 21:54:25.799510
Model ind 685 epoch 345 head B batch: 400 avg loss -2.217695 avg loss no lamb -2.217695 time 2019-02-25 21:55:36.990679
Model ind 685 epoch 345 head A batch: 0 avg loss -2.219821 avg loss no lamb -2.219821 time 2019-02-25 21:56:48.240122
Model ind 685 epoch 345 head A batch: 100 avg loss -2.198218 avg loss no lamb -2.198218 time 2019-02-25 21:58:01.722227
Model ind 685 epoch 345 head A batch: 200 avg loss -2.215909 avg loss no lamb -2.215909 time 2019-02-25 21:59:18.426587
Model ind 685 epoch 345 head A batch: 300 avg loss -2.200078 avg loss no lamb -2.200078 time 2019-02-25 22:00:35.656250
Model ind 685 epoch 345 head A batch: 400 avg loss -2.168011 avg loss no lamb -2.168011 time 2019-02-25 22:01:51.102302
Pre: time 2019-02-25 22:03:22.906297: 
 	std: 0.006487183
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.979]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9924

Starting e_i: 346
Model ind 685 epoch 346 head B batch: 0 avg loss -2.207176 avg loss no lamb -2.207176 time 2019-02-25 22:03:24.917014
Model ind 685 epoch 346 head B batch: 100 avg loss -2.212375 avg loss no lamb -2.212375 time 2019-02-25 22:04:42.185129
Model ind 685 epoch 346 head B batch: 200 avg loss -2.226169 avg loss no lamb -2.226169 time 2019-02-25 22:05:58.663131
Model ind 685 epoch 346 head B batch: 300 avg loss -2.220534 avg loss no lamb -2.220534 time 2019-02-25 22:07:11.157360
Model ind 685 epoch 346 head B batch: 400 avg loss -2.210299 avg loss no lamb -2.210299 time 2019-02-25 22:08:23.360505
Model ind 685 epoch 346 head B batch: 0 avg loss -2.253001 avg loss no lamb -2.253001 time 2019-02-25 22:09:39.074885
Model ind 685 epoch 346 head B batch: 100 avg loss -2.199638 avg loss no lamb -2.199638 time 2019-02-25 22:10:55.554931
Model ind 685 epoch 346 head B batch: 200 avg loss -2.182778 avg loss no lamb -2.182778 time 2019-02-25 22:12:10.744951
Model ind 685 epoch 346 head B batch: 300 avg loss -2.243451 avg loss no lamb -2.243451 time 2019-02-25 22:13:26.043322
Model ind 685 epoch 346 head B batch: 400 avg loss -2.225063 avg loss no lamb -2.225063 time 2019-02-25 22:14:42.485118
Model ind 685 epoch 346 head A batch: 0 avg loss -2.215097 avg loss no lamb -2.215097 time 2019-02-25 22:15:59.213017
Model ind 685 epoch 346 head A batch: 100 avg loss -2.217040 avg loss no lamb -2.217040 time 2019-02-25 22:17:16.337270
Model ind 685 epoch 346 head A batch: 200 avg loss -2.178391 avg loss no lamb -2.178391 time 2019-02-25 22:18:33.839052
Model ind 685 epoch 346 head A batch: 300 avg loss -2.226025 avg loss no lamb -2.226025 time 2019-02-25 22:19:50.913230
Model ind 685 epoch 346 head A batch: 400 avg loss -2.158309 avg loss no lamb -2.158309 time 2019-02-25 22:21:03.658110
Pre: time 2019-02-25 22:22:31.076283: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 347
Model ind 685 epoch 347 head B batch: 0 avg loss -2.231050 avg loss no lamb -2.231050 time 2019-02-25 22:22:33.012343
Model ind 685 epoch 347 head B batch: 100 avg loss -2.219459 avg loss no lamb -2.219459 time 2019-02-25 22:23:47.656033
Model ind 685 epoch 347 head B batch: 200 avg loss -2.192812 avg loss no lamb -2.192812 time 2019-02-25 22:24:59.752591
Model ind 685 epoch 347 head B batch: 300 avg loss -2.225937 avg loss no lamb -2.225937 time 2019-02-25 22:26:12.509052
Model ind 685 epoch 347 head B batch: 400 avg loss -2.219086 avg loss no lamb -2.219086 time 2019-02-25 22:27:27.050823
Model ind 685 epoch 347 head B batch: 0 avg loss -2.221682 avg loss no lamb -2.221682 time 2019-02-25 22:28:42.525062
Model ind 685 epoch 347 head B batch: 100 avg loss -2.202449 avg loss no lamb -2.202449 time 2019-02-25 22:29:53.262142
Model ind 685 epoch 347 head B batch: 200 avg loss -2.196837 avg loss no lamb -2.196837 time 2019-02-25 22:31:09.360774
Model ind 685 epoch 347 head B batch: 300 avg loss -2.244471 avg loss no lamb -2.244471 time 2019-02-25 22:32:24.965278
Model ind 685 epoch 347 head B batch: 400 avg loss -2.175888 avg loss no lamb -2.175888 time 2019-02-25 22:33:30.535786
Model ind 685 epoch 347 head A batch: 0 avg loss -2.220584 avg loss no lamb -2.220584 time 2019-02-25 22:34:45.202312
Model ind 685 epoch 347 head A batch: 100 avg loss -2.201097 avg loss no lamb -2.201097 time 2019-02-25 22:36:00.134018
Model ind 685 epoch 347 head A batch: 200 avg loss -2.217431 avg loss no lamb -2.217431 time 2019-02-25 22:37:13.180833
Model ind 685 epoch 347 head A batch: 300 avg loss -2.220140 avg loss no lamb -2.220140 time 2019-02-25 22:38:26.539341
Model ind 685 epoch 347 head A batch: 400 avg loss -2.218509 avg loss no lamb -2.218509 time 2019-02-25 22:39:40.006862
Pre: time 2019-02-25 22:41:11.002879: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.99244285, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.99244285

Starting e_i: 348
Model ind 685 epoch 348 head B batch: 0 avg loss -2.227388 avg loss no lamb -2.227388 time 2019-02-25 22:41:13.134319
Model ind 685 epoch 348 head B batch: 100 avg loss -2.200644 avg loss no lamb -2.200644 time 2019-02-25 22:42:27.719532
Model ind 685 epoch 348 head B batch: 200 avg loss -2.209415 avg loss no lamb -2.209415 time 2019-02-25 22:43:39.089018
Model ind 685 epoch 348 head B batch: 300 avg loss -2.259565 avg loss no lamb -2.259565 time 2019-02-25 22:44:50.416179
Model ind 685 epoch 348 head B batch: 400 avg loss -2.214123 avg loss no lamb -2.214123 time 2019-02-25 22:46:01.704124
Model ind 685 epoch 348 head B batch: 0 avg loss -2.216210 avg loss no lamb -2.216210 time 2019-02-25 22:47:12.919286
Model ind 685 epoch 348 head B batch: 100 avg loss -2.196044 avg loss no lamb -2.196044 time 2019-02-25 22:48:25.369794
Model ind 685 epoch 348 head B batch: 200 avg loss -2.192564 avg loss no lamb -2.192564 time 2019-02-25 22:49:39.721672
Model ind 685 epoch 348 head B batch: 300 avg loss -2.227017 avg loss no lamb -2.227017 time 2019-02-25 22:50:55.152316
Model ind 685 epoch 348 head B batch: 400 avg loss -2.187088 avg loss no lamb -2.187088 time 2019-02-25 22:52:11.677007
Model ind 685 epoch 348 head A batch: 0 avg loss -2.212832 avg loss no lamb -2.212832 time 2019-02-25 22:53:27.910873
Model ind 685 epoch 348 head A batch: 100 avg loss -2.185742 avg loss no lamb -2.185742 time 2019-02-25 22:54:44.772340
Model ind 685 epoch 348 head A batch: 200 avg loss -2.226897 avg loss no lamb -2.226897 time 2019-02-25 22:56:01.111755
Model ind 685 epoch 348 head A batch: 300 avg loss -2.209658 avg loss no lamb -2.209658 time 2019-02-25 22:57:16.393973
Model ind 685 epoch 348 head A batch: 400 avg loss -2.225363 avg loss no lamb -2.225363 time 2019-02-25 22:58:32.458462
Pre: time 2019-02-25 23:00:05.648520: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9924

Starting e_i: 349
Model ind 685 epoch 349 head B batch: 0 avg loss -2.114259 avg loss no lamb -2.114259 time 2019-02-25 23:00:08.025924
Model ind 685 epoch 349 head B batch: 100 avg loss -2.192214 avg loss no lamb -2.192214 time 2019-02-25 23:01:23.785142
Model ind 685 epoch 349 head B batch: 200 avg loss -2.200836 avg loss no lamb -2.200836 time 2019-02-25 23:02:37.878762
Model ind 685 epoch 349 head B batch: 300 avg loss -2.243213 avg loss no lamb -2.243213 time 2019-02-25 23:03:51.670297
Model ind 685 epoch 349 head B batch: 400 avg loss -2.196589 avg loss no lamb -2.196589 time 2019-02-25 23:05:03.056200
Model ind 685 epoch 349 head B batch: 0 avg loss -2.221954 avg loss no lamb -2.221954 time 2019-02-25 23:06:17.262541
Model ind 685 epoch 349 head B batch: 100 avg loss -2.184474 avg loss no lamb -2.184474 time 2019-02-25 23:07:32.866217
Model ind 685 epoch 349 head B batch: 200 avg loss -2.212109 avg loss no lamb -2.212109 time 2019-02-25 23:08:49.177425
Model ind 685 epoch 349 head B batch: 300 avg loss -2.233398 avg loss no lamb -2.233398 time 2019-02-25 23:10:06.763714
Model ind 685 epoch 349 head B batch: 400 avg loss -2.208406 avg loss no lamb -2.208406 time 2019-02-25 23:11:22.349692
Model ind 685 epoch 349 head A batch: 0 avg loss -2.222157 avg loss no lamb -2.222157 time 2019-02-25 23:12:37.335822
Model ind 685 epoch 349 head A batch: 100 avg loss -2.196397 avg loss no lamb -2.196397 time 2019-02-25 23:13:53.206701
Model ind 685 epoch 349 head A batch: 200 avg loss -2.239389 avg loss no lamb -2.239389 time 2019-02-25 23:15:10.424819
Model ind 685 epoch 349 head A batch: 300 avg loss -2.169827 avg loss no lamb -2.169827 time 2019-02-25 23:16:12.912866
Model ind 685 epoch 349 head A batch: 400 avg loss -2.185818 avg loss no lamb -2.185818 time 2019-02-25 23:17:18.294847
Pre: time 2019-02-25 23:18:47.644846: 
 	std: 0.006472078
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.979, 0.99204284, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.979, 0.99204284, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98427427
	best: 0.99235713

Starting e_i: 350
Model ind 685 epoch 350 head B batch: 0 avg loss -2.230388 avg loss no lamb -2.230388 time 2019-02-25 23:18:49.827636
Model ind 685 epoch 350 head B batch: 100 avg loss -2.216451 avg loss no lamb -2.216451 time 2019-02-25 23:20:03.081494
Model ind 685 epoch 350 head B batch: 200 avg loss -2.206767 avg loss no lamb -2.206767 time 2019-02-25 23:21:14.403334
Model ind 685 epoch 350 head B batch: 300 avg loss -2.239073 avg loss no lamb -2.239073 time 2019-02-25 23:22:26.484693
Model ind 685 epoch 350 head B batch: 400 avg loss -2.200811 avg loss no lamb -2.200811 time 2019-02-25 23:23:41.089447
Model ind 685 epoch 350 head B batch: 0 avg loss -2.230536 avg loss no lamb -2.230536 time 2019-02-25 23:24:54.777606
Model ind 685 epoch 350 head B batch: 100 avg loss -2.209513 avg loss no lamb -2.209513 time 2019-02-25 23:26:08.529110
Model ind 685 epoch 350 head B batch: 200 avg loss -2.201915 avg loss no lamb -2.201915 time 2019-02-25 23:27:19.765653
Model ind 685 epoch 350 head B batch: 300 avg loss -2.249670 avg loss no lamb -2.249670 time 2019-02-25 23:28:31.175560
Model ind 685 epoch 350 head B batch: 400 avg loss -2.191077 avg loss no lamb -2.191077 time 2019-02-25 23:29:42.424134
Model ind 685 epoch 350 head A batch: 0 avg loss -2.199740 avg loss no lamb -2.199740 time 2019-02-25 23:30:53.970705
Model ind 685 epoch 350 head A batch: 100 avg loss -2.196507 avg loss no lamb -2.196507 time 2019-02-25 23:32:11.327558
Model ind 685 epoch 350 head A batch: 200 avg loss -2.227127 avg loss no lamb -2.227127 time 2019-02-25 23:33:27.208525
Model ind 685 epoch 350 head A batch: 300 avg loss -2.236505 avg loss no lamb -2.236505 time 2019-02-25 23:34:46.348950
Model ind 685 epoch 350 head A batch: 400 avg loss -2.176773 avg loss no lamb -2.176773 time 2019-02-25 23:35:59.034678
Pre: time 2019-02-25 23:37:29.169665: 
 	std: 0.0064733215
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.99207145, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790143, 0.99207145, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 351
Model ind 685 epoch 351 head B batch: 0 avg loss -2.186113 avg loss no lamb -2.186113 time 2019-02-25 23:37:31.350921
Model ind 685 epoch 351 head B batch: 100 avg loss -2.176564 avg loss no lamb -2.176564 time 2019-02-25 23:38:47.259528
Model ind 685 epoch 351 head B batch: 200 avg loss -2.238030 avg loss no lamb -2.238030 time 2019-02-25 23:39:58.079838
Model ind 685 epoch 351 head B batch: 300 avg loss -2.237958 avg loss no lamb -2.237958 time 2019-02-25 23:41:09.294971
Model ind 685 epoch 351 head B batch: 400 avg loss -2.172514 avg loss no lamb -2.172514 time 2019-02-25 23:42:24.873086
Model ind 685 epoch 351 head B batch: 0 avg loss -2.206920 avg loss no lamb -2.206920 time 2019-02-25 23:43:39.211396
Model ind 685 epoch 351 head B batch: 100 avg loss -2.183168 avg loss no lamb -2.183168 time 2019-02-25 23:44:58.612711
Model ind 685 epoch 351 head B batch: 200 avg loss -2.175665 avg loss no lamb -2.175665 time 2019-02-25 23:46:15.668274
Model ind 685 epoch 351 head B batch: 300 avg loss -2.172951 avg loss no lamb -2.172951 time 2019-02-25 23:47:31.079130
Model ind 685 epoch 351 head B batch: 400 avg loss -2.199526 avg loss no lamb -2.199526 time 2019-02-25 23:48:45.856662
Model ind 685 epoch 351 head A batch: 0 avg loss -2.212006 avg loss no lamb -2.212006 time 2019-02-25 23:50:03.842787
Model ind 685 epoch 351 head A batch: 100 avg loss -2.186820 avg loss no lamb -2.186820 time 2019-02-25 23:51:21.057389
Model ind 685 epoch 351 head A batch: 200 avg loss -2.198606 avg loss no lamb -2.198606 time 2019-02-25 23:52:35.313292
Model ind 685 epoch 351 head A batch: 300 avg loss -2.201866 avg loss no lamb -2.201866 time 2019-02-25 23:53:51.303060
Model ind 685 epoch 351 head A batch: 400 avg loss -2.198159 avg loss no lamb -2.198159 time 2019-02-25 23:55:09.040520
Pre: time 2019-02-25 23:56:40.428337: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 352
Model ind 685 epoch 352 head B batch: 0 avg loss -2.182127 avg loss no lamb -2.182127 time 2019-02-25 23:56:42.414898
Model ind 685 epoch 352 head B batch: 100 avg loss -2.180355 avg loss no lamb -2.180355 time 2019-02-25 23:57:58.022996
Model ind 685 epoch 352 head B batch: 200 avg loss -2.203906 avg loss no lamb -2.203906 time 2019-02-25 23:59:07.402860
Model ind 685 epoch 352 head B batch: 300 avg loss -2.183784 avg loss no lamb -2.183784 time 2019-02-26 00:00:22.026108
Model ind 685 epoch 352 head B batch: 400 avg loss -2.208249 avg loss no lamb -2.208249 time 2019-02-26 00:01:39.296775
Model ind 685 epoch 352 head B batch: 0 avg loss -2.231085 avg loss no lamb -2.231085 time 2019-02-26 00:02:56.337432
Model ind 685 epoch 352 head B batch: 100 avg loss -2.196105 avg loss no lamb -2.196105 time 2019-02-26 00:04:15.205735
Model ind 685 epoch 352 head B batch: 200 avg loss -2.215548 avg loss no lamb -2.215548 time 2019-02-26 00:05:31.291428
Model ind 685 epoch 352 head B batch: 300 avg loss -2.220688 avg loss no lamb -2.220688 time 2019-02-26 00:06:47.950632
Model ind 685 epoch 352 head B batch: 400 avg loss -2.146158 avg loss no lamb -2.146158 time 2019-02-26 00:08:03.605978
Model ind 685 epoch 352 head A batch: 0 avg loss -2.207221 avg loss no lamb -2.207221 time 2019-02-26 00:09:21.248892
Model ind 685 epoch 352 head A batch: 100 avg loss -2.222296 avg loss no lamb -2.222296 time 2019-02-26 00:10:37.016285
Model ind 685 epoch 352 head A batch: 200 avg loss -2.174540 avg loss no lamb -2.174540 time 2019-02-26 00:11:56.658627
Model ind 685 epoch 352 head A batch: 300 avg loss -2.224636 avg loss no lamb -2.224636 time 2019-02-26 00:13:11.833524
Model ind 685 epoch 352 head A batch: 400 avg loss -2.195261 avg loss no lamb -2.195261 time 2019-02-26 00:14:27.658820
Pre: time 2019-02-26 00:16:00.858670: 
 	std: 0.0064918525
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789714, 0.99207145, 0.9789714, 0.9789714]
	train_accs: [0.99237144, 0.9789714, 0.99207145, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842714
	best: 0.99237144

Starting e_i: 353
Model ind 685 epoch 353 head B batch: 0 avg loss -2.222582 avg loss no lamb -2.222582 time 2019-02-26 00:16:03.231077
Model ind 685 epoch 353 head B batch: 100 avg loss -2.207943 avg loss no lamb -2.207943 time 2019-02-26 00:17:19.025323
Model ind 685 epoch 353 head B batch: 200 avg loss -2.193162 avg loss no lamb -2.193162 time 2019-02-26 00:18:34.165663
Model ind 685 epoch 353 head B batch: 300 avg loss -2.245491 avg loss no lamb -2.245491 time 2019-02-26 00:19:49.372215
Model ind 685 epoch 353 head B batch: 400 avg loss -2.208570 avg loss no lamb -2.208570 time 2019-02-26 00:21:07.184661
Model ind 685 epoch 353 head B batch: 0 avg loss -2.245426 avg loss no lamb -2.245426 time 2019-02-26 00:22:24.259047
Model ind 685 epoch 353 head B batch: 100 avg loss -2.223742 avg loss no lamb -2.223742 time 2019-02-26 00:23:41.422204
Model ind 685 epoch 353 head B batch: 200 avg loss -2.231122 avg loss no lamb -2.231122 time 2019-02-26 00:24:57.070128
Model ind 685 epoch 353 head B batch: 300 avg loss -2.219886 avg loss no lamb -2.219886 time 2019-02-26 00:26:13.720656
Model ind 685 epoch 353 head B batch: 400 avg loss -2.196250 avg loss no lamb -2.196250 time 2019-02-26 00:27:28.626301
Model ind 685 epoch 353 head A batch: 0 avg loss -2.220344 avg loss no lamb -2.220344 time 2019-02-26 00:28:44.427158
Model ind 685 epoch 353 head A batch: 100 avg loss -2.240052 avg loss no lamb -2.240052 time 2019-02-26 00:29:59.908532
Model ind 685 epoch 353 head A batch: 200 avg loss -2.174037 avg loss no lamb -2.174037 time 2019-02-26 00:31:16.808398
Model ind 685 epoch 353 head A batch: 300 avg loss -2.187388 avg loss no lamb -2.187388 time 2019-02-26 00:32:34.784901
Model ind 685 epoch 353 head A batch: 400 avg loss -2.175993 avg loss no lamb -2.175993 time 2019-02-26 00:33:53.654399
Pre: time 2019-02-26 00:35:25.443806: 
 	std: 0.0064837667
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9921143, 0.9790286, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.9921143, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843315
	best: 0.9924286

Starting e_i: 354
Model ind 685 epoch 354 head B batch: 0 avg loss -2.191177 avg loss no lamb -2.191177 time 2019-02-26 00:35:27.555552
Model ind 685 epoch 354 head B batch: 100 avg loss -2.221259 avg loss no lamb -2.221259 time 2019-02-26 00:36:43.076715
Model ind 685 epoch 354 head B batch: 200 avg loss -2.211668 avg loss no lamb -2.211668 time 2019-02-26 00:38:01.577526
Model ind 685 epoch 354 head B batch: 300 avg loss -2.259432 avg loss no lamb -2.259432 time 2019-02-26 00:39:17.433817
Model ind 685 epoch 354 head B batch: 400 avg loss -2.173833 avg loss no lamb -2.173833 time 2019-02-26 00:40:36.230879
Model ind 685 epoch 354 head B batch: 0 avg loss -2.228754 avg loss no lamb -2.228754 time 2019-02-26 00:41:53.351745
Model ind 685 epoch 354 head B batch: 100 avg loss -2.212047 avg loss no lamb -2.212047 time 2019-02-26 00:43:05.592611
Model ind 685 epoch 354 head B batch: 200 avg loss -2.182292 avg loss no lamb -2.182292 time 2019-02-26 00:44:23.863224
Model ind 685 epoch 354 head B batch: 300 avg loss -2.225682 avg loss no lamb -2.225682 time 2019-02-26 00:45:39.674653
Model ind 685 epoch 354 head B batch: 400 avg loss -2.172538 avg loss no lamb -2.172538 time 2019-02-26 00:46:56.734131
Model ind 685 epoch 354 head A batch: 0 avg loss -2.205844 avg loss no lamb -2.205844 time 2019-02-26 00:48:14.942536
Model ind 685 epoch 354 head A batch: 100 avg loss -2.203549 avg loss no lamb -2.203549 time 2019-02-26 00:49:31.880060
Model ind 685 epoch 354 head A batch: 200 avg loss -2.200786 avg loss no lamb -2.200786 time 2019-02-26 00:50:45.719189
Model ind 685 epoch 354 head A batch: 300 avg loss -2.196607 avg loss no lamb -2.196607 time 2019-02-26 00:52:02.880760
Model ind 685 epoch 354 head A batch: 400 avg loss -2.200949 avg loss no lamb -2.200949 time 2019-02-26 00:53:19.340393
Pre: time 2019-02-26 00:54:56.346451: 
 	std: 0.0064790854
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.979, 0.99205714, 0.9789857, 0.9789857]
	train_accs: [0.99237144, 0.979, 0.99205714, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98428
	best: 0.99237144

Starting e_i: 355
Model ind 685 epoch 355 head B batch: 0 avg loss -2.215528 avg loss no lamb -2.215528 time 2019-02-26 00:54:58.499949
Model ind 685 epoch 355 head B batch: 100 avg loss -2.206661 avg loss no lamb -2.206661 time 2019-02-26 00:56:15.032726
Model ind 685 epoch 355 head B batch: 200 avg loss -2.211418 avg loss no lamb -2.211418 time 2019-02-26 00:57:29.681198
Model ind 685 epoch 355 head B batch: 300 avg loss -2.219216 avg loss no lamb -2.219216 time 2019-02-26 00:58:45.380051
Model ind 685 epoch 355 head B batch: 400 avg loss -2.179625 avg loss no lamb -2.179625 time 2019-02-26 01:00:03.049531
Model ind 685 epoch 355 head B batch: 0 avg loss -2.179453 avg loss no lamb -2.179453 time 2019-02-26 01:01:19.653205
Model ind 685 epoch 355 head B batch: 100 avg loss -2.192450 avg loss no lamb -2.192450 time 2019-02-26 01:02:37.672799
Model ind 685 epoch 355 head B batch: 200 avg loss -2.230505 avg loss no lamb -2.230505 time 2019-02-26 01:03:56.201190
Model ind 685 epoch 355 head B batch: 300 avg loss -2.217050 avg loss no lamb -2.217050 time 2019-02-26 01:05:14.350761
Model ind 685 epoch 355 head B batch: 400 avg loss -2.247770 avg loss no lamb -2.247770 time 2019-02-26 01:06:29.280089
Model ind 685 epoch 355 head A batch: 0 avg loss -2.230652 avg loss no lamb -2.230652 time 2019-02-26 01:07:48.005786
Model ind 685 epoch 355 head A batch: 100 avg loss -2.206715 avg loss no lamb -2.206715 time 2019-02-26 01:09:02.560749
Model ind 685 epoch 355 head A batch: 200 avg loss -2.223831 avg loss no lamb -2.223831 time 2019-02-26 01:10:18.055413
Model ind 685 epoch 355 head A batch: 300 avg loss -2.235889 avg loss no lamb -2.235889 time 2019-02-26 01:11:37.383817
Model ind 685 epoch 355 head A batch: 400 avg loss -2.181418 avg loss no lamb -2.181418 time 2019-02-26 01:12:55.916993
Pre: time 2019-02-26 01:14:28.309010: 
 	std: 0.0064742696
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843143
	best: 0.9923857

Starting e_i: 356
Model ind 685 epoch 356 head B batch: 0 avg loss -2.239115 avg loss no lamb -2.239115 time 2019-02-26 01:14:30.419858
Model ind 685 epoch 356 head B batch: 100 avg loss -2.213643 avg loss no lamb -2.213643 time 2019-02-26 01:15:47.663346
Model ind 685 epoch 356 head B batch: 200 avg loss -2.223690 avg loss no lamb -2.223690 time 2019-02-26 01:17:02.041648
Model ind 685 epoch 356 head B batch: 300 avg loss -2.245023 avg loss no lamb -2.245023 time 2019-02-26 01:18:18.386010
Model ind 685 epoch 356 head B batch: 400 avg loss -2.200591 avg loss no lamb -2.200591 time 2019-02-26 01:19:35.354772
Model ind 685 epoch 356 head B batch: 0 avg loss -2.233571 avg loss no lamb -2.233571 time 2019-02-26 01:20:52.146089
Model ind 685 epoch 356 head B batch: 100 avg loss -2.218581 avg loss no lamb -2.218581 time 2019-02-26 01:22:10.599275
Model ind 685 epoch 356 head B batch: 200 avg loss -2.196419 avg loss no lamb -2.196419 time 2019-02-26 01:23:25.429786
Model ind 685 epoch 356 head B batch: 300 avg loss -2.201826 avg loss no lamb -2.201826 time 2019-02-26 01:24:37.958481
Model ind 685 epoch 356 head B batch: 400 avg loss -2.178820 avg loss no lamb -2.178820 time 2019-02-26 01:25:41.018288
Model ind 685 epoch 356 head A batch: 0 avg loss -2.231454 avg loss no lamb -2.231454 time 2019-02-26 01:26:55.472969
Model ind 685 epoch 356 head A batch: 100 avg loss -2.184691 avg loss no lamb -2.184691 time 2019-02-26 01:28:12.302978
Model ind 685 epoch 356 head A batch: 200 avg loss -2.188793 avg loss no lamb -2.188793 time 2019-02-26 01:29:29.194712
Model ind 685 epoch 356 head A batch: 300 avg loss -2.213057 avg loss no lamb -2.213057 time 2019-02-26 01:30:45.094328
Model ind 685 epoch 356 head A batch: 400 avg loss -2.203304 avg loss no lamb -2.203304 time 2019-02-26 01:32:01.736252
Pre: time 2019-02-26 01:33:34.320272: 
 	std: 0.0064906217
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.979, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924

Starting e_i: 357
Model ind 685 epoch 357 head B batch: 0 avg loss -2.209203 avg loss no lamb -2.209203 time 2019-02-26 01:33:36.343458
Model ind 685 epoch 357 head B batch: 100 avg loss -2.222458 avg loss no lamb -2.222458 time 2019-02-26 01:34:49.799469
Model ind 685 epoch 357 head B batch: 200 avg loss -2.222212 avg loss no lamb -2.222212 time 2019-02-26 01:36:06.116807
Model ind 685 epoch 357 head B batch: 300 avg loss -2.188298 avg loss no lamb -2.188298 time 2019-02-26 01:37:21.378137
Model ind 685 epoch 357 head B batch: 400 avg loss -2.228782 avg loss no lamb -2.228782 time 2019-02-26 01:38:38.848733
Model ind 685 epoch 357 head B batch: 0 avg loss -2.189951 avg loss no lamb -2.189951 time 2019-02-26 01:39:57.661394
Model ind 685 epoch 357 head B batch: 100 avg loss -2.207593 avg loss no lamb -2.207593 time 2019-02-26 01:41:13.963952
Model ind 685 epoch 357 head B batch: 200 avg loss -2.195698 avg loss no lamb -2.195698 time 2019-02-26 01:42:29.482507
Model ind 685 epoch 357 head B batch: 300 avg loss -2.217470 avg loss no lamb -2.217470 time 2019-02-26 01:43:44.188712
Model ind 685 epoch 357 head B batch: 400 avg loss -2.215233 avg loss no lamb -2.215233 time 2019-02-26 01:44:57.711650
Model ind 685 epoch 357 head A batch: 0 avg loss -2.201688 avg loss no lamb -2.201688 time 2019-02-26 01:46:10.142745
Model ind 685 epoch 357 head A batch: 100 avg loss -2.183487 avg loss no lamb -2.183487 time 2019-02-26 01:47:27.193563
Model ind 685 epoch 357 head A batch: 200 avg loss -2.191183 avg loss no lamb -2.191183 time 2019-02-26 01:48:45.478361
Model ind 685 epoch 357 head A batch: 300 avg loss -2.202155 avg loss no lamb -2.202155 time 2019-02-26 01:50:01.310898
Model ind 685 epoch 357 head A batch: 400 avg loss -2.214911 avg loss no lamb -2.214911 time 2019-02-26 01:51:18.491085
Pre: time 2019-02-26 01:52:47.723974: 
 	std: 0.0064778673
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843314
	best: 0.9924143

Starting e_i: 358
Model ind 685 epoch 358 head B batch: 0 avg loss -2.219502 avg loss no lamb -2.219502 time 2019-02-26 01:52:49.611438
Model ind 685 epoch 358 head B batch: 100 avg loss -2.209925 avg loss no lamb -2.209925 time 2019-02-26 01:54:01.446452
Model ind 685 epoch 358 head B batch: 200 avg loss -2.212634 avg loss no lamb -2.212634 time 2019-02-26 01:55:18.179429
Model ind 685 epoch 358 head B batch: 300 avg loss -2.235353 avg loss no lamb -2.235353 time 2019-02-26 01:56:35.814304
Model ind 685 epoch 358 head B batch: 400 avg loss -2.223763 avg loss no lamb -2.223763 time 2019-02-26 01:57:50.497351
Model ind 685 epoch 358 head B batch: 0 avg loss -2.216662 avg loss no lamb -2.216662 time 2019-02-26 01:59:06.769495
Model ind 685 epoch 358 head B batch: 100 avg loss -2.214481 avg loss no lamb -2.214481 time 2019-02-26 02:00:20.434960
Model ind 685 epoch 358 head B batch: 200 avg loss -2.223628 avg loss no lamb -2.223628 time 2019-02-26 02:01:33.809231
Model ind 685 epoch 358 head B batch: 300 avg loss -2.221903 avg loss no lamb -2.221903 time 2019-02-26 02:02:47.775300
Model ind 685 epoch 358 head B batch: 400 avg loss -2.211102 avg loss no lamb -2.211102 time 2019-02-26 02:04:04.323362
Model ind 685 epoch 358 head A batch: 0 avg loss -2.232975 avg loss no lamb -2.232975 time 2019-02-26 02:05:18.405844
Model ind 685 epoch 358 head A batch: 100 avg loss -2.221593 avg loss no lamb -2.221593 time 2019-02-26 02:06:33.332256
Model ind 685 epoch 358 head A batch: 200 avg loss -2.194657 avg loss no lamb -2.194657 time 2019-02-26 02:07:47.039490
Model ind 685 epoch 358 head A batch: 300 avg loss -2.257258 avg loss no lamb -2.257258 time 2019-02-26 02:08:53.932980
Model ind 685 epoch 358 head A batch: 400 avg loss -2.199585 avg loss no lamb -2.199585 time 2019-02-26 02:10:05.734637
Pre: time 2019-02-26 02:11:40.546482: 
 	std: 0.006495424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9921, 0.979, 0.979]
	train_accs: [0.9924143, 0.979, 0.9921, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9924143

Starting e_i: 359
Model ind 685 epoch 359 head B batch: 0 avg loss -2.183509 avg loss no lamb -2.183509 time 2019-02-26 02:11:42.699816
Model ind 685 epoch 359 head B batch: 100 avg loss -2.228025 avg loss no lamb -2.228025 time 2019-02-26 02:12:59.017410
Model ind 685 epoch 359 head B batch: 200 avg loss -2.164201 avg loss no lamb -2.164201 time 2019-02-26 02:14:13.476182
Model ind 685 epoch 359 head B batch: 300 avg loss -2.251189 avg loss no lamb -2.251189 time 2019-02-26 02:15:31.157948
Model ind 685 epoch 359 head B batch: 400 avg loss -2.204312 avg loss no lamb -2.204312 time 2019-02-26 02:16:48.471553
Model ind 685 epoch 359 head B batch: 0 avg loss -2.215887 avg loss no lamb -2.215887 time 2019-02-26 02:18:06.802590
Model ind 685 epoch 359 head B batch: 100 avg loss -2.187961 avg loss no lamb -2.187961 time 2019-02-26 02:19:20.492201
Model ind 685 epoch 359 head B batch: 200 avg loss -2.188855 avg loss no lamb -2.188855 time 2019-02-26 02:20:39.585115
Model ind 685 epoch 359 head B batch: 300 avg loss -2.203522 avg loss no lamb -2.203522 time 2019-02-26 02:21:54.785397
Model ind 685 epoch 359 head B batch: 400 avg loss -2.218396 avg loss no lamb -2.218396 time 2019-02-26 02:23:07.444632
Model ind 685 epoch 359 head A batch: 0 avg loss -2.217399 avg loss no lamb -2.217399 time 2019-02-26 02:24:24.002112
Model ind 685 epoch 359 head A batch: 100 avg loss -2.216218 avg loss no lamb -2.216218 time 2019-02-26 02:25:41.626553
Model ind 685 epoch 359 head A batch: 200 avg loss -2.197683 avg loss no lamb -2.197683 time 2019-02-26 02:26:57.870967
Model ind 685 epoch 359 head A batch: 300 avg loss -2.208841 avg loss no lamb -2.208841 time 2019-02-26 02:28:13.423131
Model ind 685 epoch 359 head A batch: 400 avg loss -2.212334 avg loss no lamb -2.212334 time 2019-02-26 02:29:31.936014
Pre: time 2019-02-26 02:31:04.550986: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9924

Starting e_i: 360
Model ind 685 epoch 360 head B batch: 0 avg loss -2.215239 avg loss no lamb -2.215239 time 2019-02-26 02:31:06.600490
Model ind 685 epoch 360 head B batch: 100 avg loss -2.218544 avg loss no lamb -2.218544 time 2019-02-26 02:32:22.803878
Model ind 685 epoch 360 head B batch: 200 avg loss -2.231125 avg loss no lamb -2.231125 time 2019-02-26 02:33:39.087883
Model ind 685 epoch 360 head B batch: 300 avg loss -2.221102 avg loss no lamb -2.221102 time 2019-02-26 02:34:53.941950
Model ind 685 epoch 360 head B batch: 400 avg loss -2.164406 avg loss no lamb -2.164406 time 2019-02-26 02:36:09.562201
Model ind 685 epoch 360 head B batch: 0 avg loss -2.202181 avg loss no lamb -2.202181 time 2019-02-26 02:37:27.188876
Model ind 685 epoch 360 head B batch: 100 avg loss -2.224998 avg loss no lamb -2.224998 time 2019-02-26 02:38:41.342945
Model ind 685 epoch 360 head B batch: 200 avg loss -2.213389 avg loss no lamb -2.213389 time 2019-02-26 02:39:53.845751
Model ind 685 epoch 360 head B batch: 300 avg loss -2.205085 avg loss no lamb -2.205085 time 2019-02-26 02:41:07.360868
Model ind 685 epoch 360 head B batch: 400 avg loss -2.176483 avg loss no lamb -2.176483 time 2019-02-26 02:42:20.567007
Model ind 685 epoch 360 head A batch: 0 avg loss -2.231175 avg loss no lamb -2.231175 time 2019-02-26 02:43:35.410920
Model ind 685 epoch 360 head A batch: 100 avg loss -2.199008 avg loss no lamb -2.199008 time 2019-02-26 02:44:53.459334
Model ind 685 epoch 360 head A batch: 200 avg loss -2.225273 avg loss no lamb -2.225273 time 2019-02-26 02:46:11.277475
Model ind 685 epoch 360 head A batch: 300 avg loss -2.245173 avg loss no lamb -2.245173 time 2019-02-26 02:47:29.664555
Model ind 685 epoch 360 head A batch: 400 avg loss -2.159692 avg loss no lamb -2.159692 time 2019-02-26 02:48:45.230693
Pre: time 2019-02-26 02:50:20.969475: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98430574
	best: 0.9924

Starting e_i: 361
Model ind 685 epoch 361 head B batch: 0 avg loss -2.236145 avg loss no lamb -2.236145 time 2019-02-26 02:50:24.267564
Model ind 685 epoch 361 head B batch: 100 avg loss -2.212008 avg loss no lamb -2.212008 time 2019-02-26 02:51:31.804475
Model ind 685 epoch 361 head B batch: 200 avg loss -2.221527 avg loss no lamb -2.221527 time 2019-02-26 02:52:47.954326
Model ind 685 epoch 361 head B batch: 300 avg loss -2.209176 avg loss no lamb -2.209176 time 2019-02-26 02:54:05.197402
Model ind 685 epoch 361 head B batch: 400 avg loss -2.204507 avg loss no lamb -2.204507 time 2019-02-26 02:55:23.288771
Model ind 685 epoch 361 head B batch: 0 avg loss -2.201457 avg loss no lamb -2.201457 time 2019-02-26 02:56:36.070235
Model ind 685 epoch 361 head B batch: 100 avg loss -2.214506 avg loss no lamb -2.214506 time 2019-02-26 02:57:48.012602
Model ind 685 epoch 361 head B batch: 200 avg loss -2.174015 avg loss no lamb -2.174015 time 2019-02-26 02:58:59.756364
Model ind 685 epoch 361 head B batch: 300 avg loss -2.239336 avg loss no lamb -2.239336 time 2019-02-26 03:00:14.290083
Model ind 685 epoch 361 head B batch: 400 avg loss -2.193818 avg loss no lamb -2.193818 time 2019-02-26 03:01:32.133776
Model ind 685 epoch 361 head A batch: 0 avg loss -2.237915 avg loss no lamb -2.237915 time 2019-02-26 03:02:46.836459
Model ind 685 epoch 361 head A batch: 100 avg loss -2.222241 avg loss no lamb -2.222241 time 2019-02-26 03:04:01.246538
Model ind 685 epoch 361 head A batch: 200 avg loss -2.212870 avg loss no lamb -2.212870 time 2019-02-26 03:05:16.709280
Model ind 685 epoch 361 head A batch: 300 avg loss -2.222282 avg loss no lamb -2.222282 time 2019-02-26 03:06:34.514056
Model ind 685 epoch 361 head A batch: 400 avg loss -2.192143 avg loss no lamb -2.192143 time 2019-02-26 03:07:54.082827
Pre: time 2019-02-26 03:09:26.406321: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 362
Model ind 685 epoch 362 head B batch: 0 avg loss -2.210729 avg loss no lamb -2.210729 time 2019-02-26 03:09:28.475976
Model ind 685 epoch 362 head B batch: 100 avg loss -2.211740 avg loss no lamb -2.211740 time 2019-02-26 03:10:43.455344
Model ind 685 epoch 362 head B batch: 200 avg loss -2.202373 avg loss no lamb -2.202373 time 2019-02-26 03:12:01.468490
Model ind 685 epoch 362 head B batch: 300 avg loss -2.251070 avg loss no lamb -2.251070 time 2019-02-26 03:13:15.028621
Model ind 685 epoch 362 head B batch: 400 avg loss -2.156317 avg loss no lamb -2.156317 time 2019-02-26 03:14:28.555840
Model ind 685 epoch 362 head B batch: 0 avg loss -2.187788 avg loss no lamb -2.187788 time 2019-02-26 03:15:44.818322
Model ind 685 epoch 362 head B batch: 100 avg loss -2.228958 avg loss no lamb -2.228958 time 2019-02-26 03:17:02.064788
Model ind 685 epoch 362 head B batch: 200 avg loss -2.214181 avg loss no lamb -2.214181 time 2019-02-26 03:18:18.298134
Model ind 685 epoch 362 head B batch: 300 avg loss -2.208166 avg loss no lamb -2.208166 time 2019-02-26 03:19:34.839167
Model ind 685 epoch 362 head B batch: 400 avg loss -2.186725 avg loss no lamb -2.186725 time 2019-02-26 03:20:51.756416
Model ind 685 epoch 362 head A batch: 0 avg loss -2.196068 avg loss no lamb -2.196068 time 2019-02-26 03:22:03.754508
Model ind 685 epoch 362 head A batch: 100 avg loss -2.207220 avg loss no lamb -2.207220 time 2019-02-26 03:23:15.244847
Model ind 685 epoch 362 head A batch: 200 avg loss -2.184412 avg loss no lamb -2.184412 time 2019-02-26 03:24:26.383741
Model ind 685 epoch 362 head A batch: 300 avg loss -2.255642 avg loss no lamb -2.255642 time 2019-02-26 03:25:38.260581
Model ind 685 epoch 362 head A batch: 400 avg loss -2.206316 avg loss no lamb -2.206316 time 2019-02-26 03:26:53.445434
Pre: time 2019-02-26 03:28:27.294746: 
 	std: 0.00645715
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.97905713, 0.99205714, 0.97905713, 0.97905713]
	train_accs: [0.9924143, 0.97905713, 0.99205714, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 363
Model ind 685 epoch 363 head B batch: 0 avg loss -2.220569 avg loss no lamb -2.220569 time 2019-02-26 03:28:29.404138
Model ind 685 epoch 363 head B batch: 100 avg loss -2.235400 avg loss no lamb -2.235400 time 2019-02-26 03:29:47.885630
Model ind 685 epoch 363 head B batch: 200 avg loss -2.179881 avg loss no lamb -2.179881 time 2019-02-26 03:31:02.349559
Model ind 685 epoch 363 head B batch: 300 avg loss -2.261903 avg loss no lamb -2.261903 time 2019-02-26 03:32:13.485174
Model ind 685 epoch 363 head B batch: 400 avg loss -2.162891 avg loss no lamb -2.162891 time 2019-02-26 03:33:28.616721
Model ind 685 epoch 363 head B batch: 0 avg loss -2.231204 avg loss no lamb -2.231204 time 2019-02-26 03:34:38.262094
Model ind 685 epoch 363 head B batch: 100 avg loss -2.215278 avg loss no lamb -2.215278 time 2019-02-26 03:35:51.251797
Model ind 685 epoch 363 head B batch: 200 avg loss -2.211996 avg loss no lamb -2.211996 time 2019-02-26 03:37:06.938169
Model ind 685 epoch 363 head B batch: 300 avg loss -2.211088 avg loss no lamb -2.211088 time 2019-02-26 03:38:25.360294
Model ind 685 epoch 363 head B batch: 400 avg loss -2.212372 avg loss no lamb -2.212372 time 2019-02-26 03:39:41.518184
Model ind 685 epoch 363 head A batch: 0 avg loss -2.213144 avg loss no lamb -2.213144 time 2019-02-26 03:40:57.426955
Model ind 685 epoch 363 head A batch: 100 avg loss -2.216753 avg loss no lamb -2.216753 time 2019-02-26 03:42:11.001075
Model ind 685 epoch 363 head A batch: 200 avg loss -2.200714 avg loss no lamb -2.200714 time 2019-02-26 03:43:26.956852
Model ind 685 epoch 363 head A batch: 300 avg loss -2.239437 avg loss no lamb -2.239437 time 2019-02-26 03:44:43.255403
Model ind 685 epoch 363 head A batch: 400 avg loss -2.201336 avg loss no lamb -2.201336 time 2019-02-26 03:46:01.455825
Pre: time 2019-02-26 03:47:35.392775: 
 	std: 0.006491976
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.99244285, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843286
	best: 0.99244285

Starting e_i: 364
Model ind 685 epoch 364 head B batch: 0 avg loss -2.181003 avg loss no lamb -2.181003 time 2019-02-26 03:47:37.638898
Model ind 685 epoch 364 head B batch: 100 avg loss -2.212132 avg loss no lamb -2.212132 time 2019-02-26 03:48:53.964612
Model ind 685 epoch 364 head B batch: 200 avg loss -2.205040 avg loss no lamb -2.205040 time 2019-02-26 03:50:07.863140
Model ind 685 epoch 364 head B batch: 300 avg loss -2.191427 avg loss no lamb -2.191427 time 2019-02-26 03:51:23.433334
Model ind 685 epoch 364 head B batch: 400 avg loss -2.191665 avg loss no lamb -2.191665 time 2019-02-26 03:52:41.071745
Model ind 685 epoch 364 head B batch: 0 avg loss -2.229855 avg loss no lamb -2.229855 time 2019-02-26 03:54:00.218135
Model ind 685 epoch 364 head B batch: 100 avg loss -2.191492 avg loss no lamb -2.191492 time 2019-02-26 03:55:18.670056
Model ind 685 epoch 364 head B batch: 200 avg loss -2.196108 avg loss no lamb -2.196108 time 2019-02-26 03:56:36.495347
Model ind 685 epoch 364 head B batch: 300 avg loss -2.229366 avg loss no lamb -2.229366 time 2019-02-26 03:57:54.871909
Model ind 685 epoch 364 head B batch: 400 avg loss -2.230841 avg loss no lamb -2.230841 time 2019-02-26 03:59:13.836240
Model ind 685 epoch 364 head A batch: 0 avg loss -2.196912 avg loss no lamb -2.196912 time 2019-02-26 04:00:29.836802
Model ind 685 epoch 364 head A batch: 100 avg loss -2.222320 avg loss no lamb -2.222320 time 2019-02-26 04:01:43.677642
Model ind 685 epoch 364 head A batch: 200 avg loss -2.215259 avg loss no lamb -2.215259 time 2019-02-26 04:03:00.395136
Model ind 685 epoch 364 head A batch: 300 avg loss -2.211303 avg loss no lamb -2.211303 time 2019-02-26 04:04:12.744608
Model ind 685 epoch 364 head A batch: 400 avg loss -2.180205 avg loss no lamb -2.180205 time 2019-02-26 04:05:23.542907
Pre: time 2019-02-26 04:06:53.841491: 
 	std: 0.00648486
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98433715
	best: 0.9924286

Starting e_i: 365
Model ind 685 epoch 365 head B batch: 0 avg loss -2.216496 avg loss no lamb -2.216496 time 2019-02-26 04:06:56.017718
Model ind 685 epoch 365 head B batch: 100 avg loss -2.218392 avg loss no lamb -2.218392 time 2019-02-26 04:08:15.699920
Model ind 685 epoch 365 head B batch: 200 avg loss -2.171589 avg loss no lamb -2.171589 time 2019-02-26 04:09:33.390963
Model ind 685 epoch 365 head B batch: 300 avg loss -2.233042 avg loss no lamb -2.233042 time 2019-02-26 04:10:49.905448
Model ind 685 epoch 365 head B batch: 400 avg loss -2.198921 avg loss no lamb -2.198921 time 2019-02-26 04:12:07.307455
Model ind 685 epoch 365 head B batch: 0 avg loss -2.205630 avg loss no lamb -2.205630 time 2019-02-26 04:13:25.752699
Model ind 685 epoch 365 head B batch: 100 avg loss -2.174957 avg loss no lamb -2.174957 time 2019-02-26 04:14:43.738449
Model ind 685 epoch 365 head B batch: 200 avg loss -2.185833 avg loss no lamb -2.185833 time 2019-02-26 04:16:01.283395
Model ind 685 epoch 365 head B batch: 300 avg loss -2.202935 avg loss no lamb -2.202935 time 2019-02-26 04:17:09.377663
Model ind 685 epoch 365 head B batch: 400 avg loss -2.214128 avg loss no lamb -2.214128 time 2019-02-26 04:18:25.295800
Model ind 685 epoch 365 head A batch: 0 avg loss -2.218327 avg loss no lamb -2.218327 time 2019-02-26 04:19:43.175620
Model ind 685 epoch 365 head A batch: 100 avg loss -2.234038 avg loss no lamb -2.234038 time 2019-02-26 04:21:02.902916
Model ind 685 epoch 365 head A batch: 200 avg loss -2.184938 avg loss no lamb -2.184938 time 2019-02-26 04:22:20.518614
Model ind 685 epoch 365 head A batch: 300 avg loss -2.221885 avg loss no lamb -2.221885 time 2019-02-26 04:23:37.203672
Model ind 685 epoch 365 head A batch: 400 avg loss -2.167824 avg loss no lamb -2.167824 time 2019-02-26 04:24:54.089992
Pre: time 2019-02-26 04:26:27.248145: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.9924286

Starting e_i: 366
Model ind 685 epoch 366 head B batch: 0 avg loss -2.213819 avg loss no lamb -2.213819 time 2019-02-26 04:26:29.184296
Model ind 685 epoch 366 head B batch: 100 avg loss -2.188330 avg loss no lamb -2.188330 time 2019-02-26 04:27:45.292081
Model ind 685 epoch 366 head B batch: 200 avg loss -2.194422 avg loss no lamb -2.194422 time 2019-02-26 04:28:57.532913
Model ind 685 epoch 366 head B batch: 300 avg loss -2.228561 avg loss no lamb -2.228561 time 2019-02-26 04:30:10.463303
Model ind 685 epoch 366 head B batch: 400 avg loss -2.192789 avg loss no lamb -2.192789 time 2019-02-26 04:31:26.173855
Model ind 685 epoch 366 head B batch: 0 avg loss -2.220843 avg loss no lamb -2.220843 time 2019-02-26 04:32:41.355299
Model ind 685 epoch 366 head B batch: 100 avg loss -2.236118 avg loss no lamb -2.236118 time 2019-02-26 04:33:58.295114
Model ind 685 epoch 366 head B batch: 200 avg loss -2.200072 avg loss no lamb -2.200072 time 2019-02-26 04:35:14.807205
Model ind 685 epoch 366 head B batch: 300 avg loss -2.231504 avg loss no lamb -2.231504 time 2019-02-26 04:36:33.271233
Model ind 685 epoch 366 head B batch: 400 avg loss -2.201223 avg loss no lamb -2.201223 time 2019-02-26 04:37:50.637043
Model ind 685 epoch 366 head A batch: 0 avg loss -2.223661 avg loss no lamb -2.223661 time 2019-02-26 04:39:08.931217
Model ind 685 epoch 366 head A batch: 100 avg loss -2.210249 avg loss no lamb -2.210249 time 2019-02-26 04:40:25.061616
Model ind 685 epoch 366 head A batch: 200 avg loss -2.208781 avg loss no lamb -2.208781 time 2019-02-26 04:41:43.072657
Model ind 685 epoch 366 head A batch: 300 avg loss -2.209970 avg loss no lamb -2.209970 time 2019-02-26 04:43:01.859845
Model ind 685 epoch 366 head A batch: 400 avg loss -2.191468 avg loss no lamb -2.191468 time 2019-02-26 04:44:21.198496
Pre: time 2019-02-26 04:45:53.900799: 
 	std: 0.0064884126
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9920857, 0.9790143, 0.979]
	train_accs: [0.9924, 0.9789857, 0.9920857, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98429716
	best: 0.9924

Starting e_i: 367
Model ind 685 epoch 367 head B batch: 0 avg loss -2.226435 avg loss no lamb -2.226435 time 2019-02-26 04:45:55.991579
Model ind 685 epoch 367 head B batch: 100 avg loss -2.226737 avg loss no lamb -2.226737 time 2019-02-26 04:47:10.908401
Model ind 685 epoch 367 head B batch: 200 avg loss -2.213309 avg loss no lamb -2.213309 time 2019-02-26 04:48:26.462623
Model ind 685 epoch 367 head B batch: 300 avg loss -2.202428 avg loss no lamb -2.202428 time 2019-02-26 04:49:43.839149
Model ind 685 epoch 367 head B batch: 400 avg loss -2.216794 avg loss no lamb -2.216794 time 2019-02-26 04:50:54.856284
Model ind 685 epoch 367 head B batch: 0 avg loss -2.228441 avg loss no lamb -2.228441 time 2019-02-26 04:52:06.040835
Model ind 685 epoch 367 head B batch: 100 avg loss -2.231939 avg loss no lamb -2.231939 time 2019-02-26 04:53:19.760728
Model ind 685 epoch 367 head B batch: 200 avg loss -2.184714 avg loss no lamb -2.184714 time 2019-02-26 04:54:35.391199
Model ind 685 epoch 367 head B batch: 300 avg loss -2.238039 avg loss no lamb -2.238039 time 2019-02-26 04:55:51.548183
Model ind 685 epoch 367 head B batch: 400 avg loss -2.194914 avg loss no lamb -2.194914 time 2019-02-26 04:57:09.495416
Model ind 685 epoch 367 head A batch: 0 avg loss -2.225343 avg loss no lamb -2.225343 time 2019-02-26 04:58:28.113329
Model ind 685 epoch 367 head A batch: 100 avg loss -2.211919 avg loss no lamb -2.211919 time 2019-02-26 04:59:37.313065
Model ind 685 epoch 367 head A batch: 200 avg loss -2.229647 avg loss no lamb -2.229647 time 2019-02-26 05:00:52.793571
Model ind 685 epoch 367 head A batch: 300 avg loss -2.210413 avg loss no lamb -2.210413 time 2019-02-26 05:02:08.646045
Model ind 685 epoch 367 head A batch: 400 avg loss -2.190343 avg loss no lamb -2.190343 time 2019-02-26 05:03:24.255367
Pre: time 2019-02-26 05:04:54.285053: 
 	std: 0.006478142
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.99245715, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434573
	best: 0.99245715

Starting e_i: 368
Model ind 685 epoch 368 head B batch: 0 avg loss -2.206573 avg loss no lamb -2.206573 time 2019-02-26 05:04:56.077646
Model ind 685 epoch 368 head B batch: 100 avg loss -2.231802 avg loss no lamb -2.231802 time 2019-02-26 05:06:07.767597
Model ind 685 epoch 368 head B batch: 200 avg loss -2.219662 avg loss no lamb -2.219662 time 2019-02-26 05:07:19.566403
Model ind 685 epoch 368 head B batch: 300 avg loss -2.231634 avg loss no lamb -2.231634 time 2019-02-26 05:08:34.046390
Model ind 685 epoch 368 head B batch: 400 avg loss -2.173264 avg loss no lamb -2.173264 time 2019-02-26 05:09:52.340210
Model ind 685 epoch 368 head B batch: 0 avg loss -2.229504 avg loss no lamb -2.229504 time 2019-02-26 05:11:06.248729
Model ind 685 epoch 368 head B batch: 100 avg loss -2.218978 avg loss no lamb -2.218978 time 2019-02-26 05:12:18.052544
Model ind 685 epoch 368 head B batch: 200 avg loss -2.202573 avg loss no lamb -2.202573 time 2019-02-26 05:13:34.807532
Model ind 685 epoch 368 head B batch: 300 avg loss -2.199207 avg loss no lamb -2.199207 time 2019-02-26 05:14:51.894031
Model ind 685 epoch 368 head B batch: 400 avg loss -2.179567 avg loss no lamb -2.179567 time 2019-02-26 05:16:07.291010
Model ind 685 epoch 368 head A batch: 0 avg loss -2.186965 avg loss no lamb -2.186965 time 2019-02-26 05:17:21.702788
Model ind 685 epoch 368 head A batch: 100 avg loss -2.227766 avg loss no lamb -2.227766 time 2019-02-26 05:18:32.896134
Model ind 685 epoch 368 head A batch: 200 avg loss -2.234241 avg loss no lamb -2.234241 time 2019-02-26 05:19:45.717179
Model ind 685 epoch 368 head A batch: 300 avg loss -2.190226 avg loss no lamb -2.190226 time 2019-02-26 05:20:57.221704
Model ind 685 epoch 368 head A batch: 400 avg loss -2.200428 avg loss no lamb -2.200428 time 2019-02-26 05:22:11.558446
Pre: time 2019-02-26 05:23:41.284476: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 369
Model ind 685 epoch 369 head B batch: 0 avg loss -2.238196 avg loss no lamb -2.238196 time 2019-02-26 05:23:43.304974
Model ind 685 epoch 369 head B batch: 100 avg loss -2.217589 avg loss no lamb -2.217589 time 2019-02-26 05:25:00.206651
Model ind 685 epoch 369 head B batch: 200 avg loss -2.216728 avg loss no lamb -2.216728 time 2019-02-26 05:26:15.949731
Model ind 685 epoch 369 head B batch: 300 avg loss -2.228365 avg loss no lamb -2.228365 time 2019-02-26 05:27:31.403801
Model ind 685 epoch 369 head B batch: 400 avg loss -2.186899 avg loss no lamb -2.186899 time 2019-02-26 05:28:48.578669
Model ind 685 epoch 369 head B batch: 0 avg loss -2.238717 avg loss no lamb -2.238717 time 2019-02-26 05:30:06.798625
Model ind 685 epoch 369 head B batch: 100 avg loss -2.206077 avg loss no lamb -2.206077 time 2019-02-26 05:31:21.288691
Model ind 685 epoch 369 head B batch: 200 avg loss -2.200070 avg loss no lamb -2.200070 time 2019-02-26 05:32:38.287246
Model ind 685 epoch 369 head B batch: 300 avg loss -2.210132 avg loss no lamb -2.210132 time 2019-02-26 05:33:54.979296
Model ind 685 epoch 369 head B batch: 400 avg loss -2.173892 avg loss no lamb -2.173892 time 2019-02-26 05:35:12.753204
Model ind 685 epoch 369 head A batch: 0 avg loss -2.223173 avg loss no lamb -2.223173 time 2019-02-26 05:36:29.218077
Model ind 685 epoch 369 head A batch: 100 avg loss -2.232136 avg loss no lamb -2.232136 time 2019-02-26 05:37:45.367910
Model ind 685 epoch 369 head A batch: 200 avg loss -2.231875 avg loss no lamb -2.231875 time 2019-02-26 05:39:03.362815
Model ind 685 epoch 369 head A batch: 300 avg loss -2.224992 avg loss no lamb -2.224992 time 2019-02-26 05:40:16.441194
Model ind 685 epoch 369 head A batch: 400 avg loss -2.230505 avg loss no lamb -2.230505 time 2019-02-26 05:41:32.538956
Pre: time 2019-02-26 05:42:59.421832: 
 	std: 0.0064871847
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.99212855, 0.9790286, 0.97905713]
	train_accs: [0.9924286, 0.9790286, 0.99212855, 0.9790286, 0.97905713]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98433435
	best: 0.9924286

Starting e_i: 370
Model ind 685 epoch 370 head B batch: 0 avg loss -2.202605 avg loss no lamb -2.202605 time 2019-02-26 05:43:01.447023
Model ind 685 epoch 370 head B batch: 100 avg loss -2.174948 avg loss no lamb -2.174948 time 2019-02-26 05:44:15.870853
Model ind 685 epoch 370 head B batch: 200 avg loss -2.222765 avg loss no lamb -2.222765 time 2019-02-26 05:45:28.120015
Model ind 685 epoch 370 head B batch: 300 avg loss -2.201380 avg loss no lamb -2.201380 time 2019-02-26 05:46:45.085549
Model ind 685 epoch 370 head B batch: 400 avg loss -2.140918 avg loss no lamb -2.140918 time 2019-02-26 05:48:01.083504
Model ind 685 epoch 370 head B batch: 0 avg loss -2.168224 avg loss no lamb -2.168224 time 2019-02-26 05:49:15.213212
Model ind 685 epoch 370 head B batch: 100 avg loss -2.202123 avg loss no lamb -2.202123 time 2019-02-26 05:50:27.340399
Model ind 685 epoch 370 head B batch: 200 avg loss -2.216658 avg loss no lamb -2.216658 time 2019-02-26 05:51:41.153754
Model ind 685 epoch 370 head B batch: 300 avg loss -2.218419 avg loss no lamb -2.218419 time 2019-02-26 05:52:56.393185
Model ind 685 epoch 370 head B batch: 400 avg loss -2.217237 avg loss no lamb -2.217237 time 2019-02-26 05:54:11.551471
Model ind 685 epoch 370 head A batch: 0 avg loss -2.202249 avg loss no lamb -2.202249 time 2019-02-26 05:55:27.821828
Model ind 685 epoch 370 head A batch: 100 avg loss -2.132242 avg loss no lamb -2.132242 time 2019-02-26 05:56:43.440304
Model ind 685 epoch 370 head A batch: 200 avg loss -2.190263 avg loss no lamb -2.190263 time 2019-02-26 05:57:58.503167
Model ind 685 epoch 370 head A batch: 300 avg loss -2.231245 avg loss no lamb -2.231245 time 2019-02-26 05:59:13.404357
Model ind 685 epoch 370 head A batch: 400 avg loss -2.214651 avg loss no lamb -2.214651 time 2019-02-26 06:00:31.626834
Pre: time 2019-02-26 06:02:02.250945: 
 	std: 0.0064742845
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.99237144, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843
	best: 0.99237144

Starting e_i: 371
Model ind 685 epoch 371 head B batch: 0 avg loss -2.187282 avg loss no lamb -2.187282 time 2019-02-26 06:02:04.310010
Model ind 685 epoch 371 head B batch: 100 avg loss -2.196527 avg loss no lamb -2.196527 time 2019-02-26 06:03:18.880096
Model ind 685 epoch 371 head B batch: 200 avg loss -2.201065 avg loss no lamb -2.201065 time 2019-02-26 06:04:35.147841
Model ind 685 epoch 371 head B batch: 300 avg loss -2.217357 avg loss no lamb -2.217357 time 2019-02-26 06:05:53.236070
Model ind 685 epoch 371 head B batch: 400 avg loss -2.183203 avg loss no lamb -2.183203 time 2019-02-26 06:07:11.128473
Model ind 685 epoch 371 head B batch: 0 avg loss -2.244040 avg loss no lamb -2.244040 time 2019-02-26 06:08:28.024860
Model ind 685 epoch 371 head B batch: 100 avg loss -2.213713 avg loss no lamb -2.213713 time 2019-02-26 06:09:39.885011
Model ind 685 epoch 371 head B batch: 200 avg loss -2.205264 avg loss no lamb -2.205264 time 2019-02-26 06:10:57.553967
Model ind 685 epoch 371 head B batch: 300 avg loss -2.219406 avg loss no lamb -2.219406 time 2019-02-26 06:12:15.523808
Model ind 685 epoch 371 head B batch: 400 avg loss -2.210244 avg loss no lamb -2.210244 time 2019-02-26 06:13:35.294305
Model ind 685 epoch 371 head A batch: 0 avg loss -2.219207 avg loss no lamb -2.219207 time 2019-02-26 06:14:51.880217
Model ind 685 epoch 371 head A batch: 100 avg loss -2.247791 avg loss no lamb -2.247791 time 2019-02-26 06:16:10.026604
Model ind 685 epoch 371 head A batch: 200 avg loss -2.203788 avg loss no lamb -2.203788 time 2019-02-26 06:17:24.661068
Model ind 685 epoch 371 head A batch: 300 avg loss -2.219911 avg loss no lamb -2.219911 time 2019-02-26 06:18:40.274475
Model ind 685 epoch 371 head A batch: 400 avg loss -2.199279 avg loss no lamb -2.199279 time 2019-02-26 06:19:56.050079
Pre: time 2019-02-26 06:21:29.826089: 
 	std: 0.00648499
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9920857, 0.9790286, 0.9790143]
	train_accs: [0.9924143, 0.979, 0.9920857, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 372
Model ind 685 epoch 372 head B batch: 0 avg loss -2.208083 avg loss no lamb -2.208083 time 2019-02-26 06:21:31.836328
Model ind 685 epoch 372 head B batch: 100 avg loss -2.161919 avg loss no lamb -2.161919 time 2019-02-26 06:22:45.862448
Model ind 685 epoch 372 head B batch: 200 avg loss -2.208717 avg loss no lamb -2.208717 time 2019-02-26 06:24:01.596414
Model ind 685 epoch 372 head B batch: 300 avg loss -2.234351 avg loss no lamb -2.234351 time 2019-02-26 06:25:10.910657
Model ind 685 epoch 372 head B batch: 400 avg loss -2.183459 avg loss no lamb -2.183459 time 2019-02-26 06:26:14.571996
Model ind 685 epoch 372 head B batch: 0 avg loss -2.211464 avg loss no lamb -2.211464 time 2019-02-26 06:27:28.309901
Model ind 685 epoch 372 head B batch: 100 avg loss -2.198923 avg loss no lamb -2.198923 time 2019-02-26 06:28:47.229971
Model ind 685 epoch 372 head B batch: 200 avg loss -2.209998 avg loss no lamb -2.209998 time 2019-02-26 06:30:00.722824
Model ind 685 epoch 372 head B batch: 300 avg loss -2.237853 avg loss no lamb -2.237853 time 2019-02-26 06:31:13.921757
Model ind 685 epoch 372 head B batch: 400 avg loss -2.247374 avg loss no lamb -2.247374 time 2019-02-26 06:32:28.444224
Model ind 685 epoch 372 head A batch: 0 avg loss -2.223341 avg loss no lamb -2.223341 time 2019-02-26 06:33:43.645401
Model ind 685 epoch 372 head A batch: 100 avg loss -2.200120 avg loss no lamb -2.200120 time 2019-02-26 06:35:00.943709
Model ind 685 epoch 372 head A batch: 200 avg loss -2.195654 avg loss no lamb -2.195654 time 2019-02-26 06:36:15.426850
Model ind 685 epoch 372 head A batch: 300 avg loss -2.213887 avg loss no lamb -2.213887 time 2019-02-26 06:37:33.306707
Model ind 685 epoch 372 head A batch: 400 avg loss -2.200254 avg loss no lamb -2.200254 time 2019-02-26 06:38:48.927827
Pre: time 2019-02-26 06:40:18.773690: 
 	std: 0.0064789494
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789857, 0.99205714, 0.9789857, 0.9789571]
	train_accs: [0.9923428, 0.9789857, 0.99205714, 0.9789857, 0.9789571]
	best_train_sub_head: 0
	worst: 0.9789571
	avg: 0.9842657
	best: 0.9923428

Starting e_i: 373
Model ind 685 epoch 373 head B batch: 0 avg loss -2.247163 avg loss no lamb -2.247163 time 2019-02-26 06:40:21.020949
Model ind 685 epoch 373 head B batch: 100 avg loss -2.196300 avg loss no lamb -2.196300 time 2019-02-26 06:41:37.540277
Model ind 685 epoch 373 head B batch: 200 avg loss -2.228219 avg loss no lamb -2.228219 time 2019-02-26 06:42:55.476757
Model ind 685 epoch 373 head B batch: 300 avg loss -2.179590 avg loss no lamb -2.179590 time 2019-02-26 06:44:12.359816
Model ind 685 epoch 373 head B batch: 400 avg loss -2.192988 avg loss no lamb -2.192988 time 2019-02-26 06:45:27.804379
Model ind 685 epoch 373 head B batch: 0 avg loss -2.240089 avg loss no lamb -2.240089 time 2019-02-26 06:46:45.438174
Model ind 685 epoch 373 head B batch: 100 avg loss -2.221159 avg loss no lamb -2.221159 time 2019-02-26 06:47:59.602306
Model ind 685 epoch 373 head B batch: 200 avg loss -2.204704 avg loss no lamb -2.204704 time 2019-02-26 06:49:11.900408
Model ind 685 epoch 373 head B batch: 300 avg loss -2.219722 avg loss no lamb -2.219722 time 2019-02-26 06:50:26.896362
Model ind 685 epoch 373 head B batch: 400 avg loss -2.178825 avg loss no lamb -2.178825 time 2019-02-26 06:51:44.388924
Model ind 685 epoch 373 head A batch: 0 avg loss -2.226504 avg loss no lamb -2.226504 time 2019-02-26 06:53:01.054291
Model ind 685 epoch 373 head A batch: 100 avg loss -2.193416 avg loss no lamb -2.193416 time 2019-02-26 06:54:19.919148
Model ind 685 epoch 373 head A batch: 200 avg loss -2.209311 avg loss no lamb -2.209311 time 2019-02-26 06:55:36.274225
Model ind 685 epoch 373 head A batch: 300 avg loss -2.210009 avg loss no lamb -2.210009 time 2019-02-26 06:56:54.108327
Model ind 685 epoch 373 head A batch: 400 avg loss -2.226552 avg loss no lamb -2.226552 time 2019-02-26 06:58:11.096488
Pre: time 2019-02-26 06:59:42.605264: 
 	std: 0.0064895186
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790286]
	train_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924143

Starting e_i: 374
Model ind 685 epoch 374 head B batch: 0 avg loss -2.217699 avg loss no lamb -2.217699 time 2019-02-26 06:59:44.541523
Model ind 685 epoch 374 head B batch: 100 avg loss -2.203148 avg loss no lamb -2.203148 time 2019-02-26 07:01:01.150848
Model ind 685 epoch 374 head B batch: 200 avg loss -2.208847 avg loss no lamb -2.208847 time 2019-02-26 07:02:18.447554
Model ind 685 epoch 374 head B batch: 300 avg loss -2.223868 avg loss no lamb -2.223868 time 2019-02-26 07:03:34.943389
Model ind 685 epoch 374 head B batch: 400 avg loss -2.237769 avg loss no lamb -2.237769 time 2019-02-26 07:04:51.682116
Model ind 685 epoch 374 head B batch: 0 avg loss -2.206379 avg loss no lamb -2.206379 time 2019-02-26 07:06:02.802397
Model ind 685 epoch 374 head B batch: 100 avg loss -2.193606 avg loss no lamb -2.193606 time 2019-02-26 07:07:17.963520
Model ind 685 epoch 374 head B batch: 200 avg loss -2.238398 avg loss no lamb -2.238398 time 2019-02-26 07:08:26.198013
Model ind 685 epoch 374 head B batch: 300 avg loss -2.231435 avg loss no lamb -2.231435 time 2019-02-26 07:09:40.952417
Model ind 685 epoch 374 head B batch: 400 avg loss -2.175946 avg loss no lamb -2.175946 time 2019-02-26 07:10:59.315779
Model ind 685 epoch 374 head A batch: 0 avg loss -2.212885 avg loss no lamb -2.212885 time 2019-02-26 07:12:18.024253
Model ind 685 epoch 374 head A batch: 100 avg loss -2.201820 avg loss no lamb -2.201820 time 2019-02-26 07:13:34.709099
Model ind 685 epoch 374 head A batch: 200 avg loss -2.187848 avg loss no lamb -2.187848 time 2019-02-26 07:14:52.985182
Model ind 685 epoch 374 head A batch: 300 avg loss -2.224625 avg loss no lamb -2.224625 time 2019-02-26 07:16:11.835989
Model ind 685 epoch 374 head A batch: 400 avg loss -2.218896 avg loss no lamb -2.218896 time 2019-02-26 07:17:27.394857
Pre: time 2019-02-26 07:18:59.178937: 
 	std: 0.006488575
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.99245715, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98434
	best: 0.99245715

Starting e_i: 375
Model ind 685 epoch 375 head B batch: 0 avg loss -2.217166 avg loss no lamb -2.217166 time 2019-02-26 07:19:01.259680
Model ind 685 epoch 375 head B batch: 100 avg loss -2.158580 avg loss no lamb -2.158580 time 2019-02-26 07:20:18.102019
Model ind 685 epoch 375 head B batch: 200 avg loss -2.178062 avg loss no lamb -2.178062 time 2019-02-26 07:21:29.849501
Model ind 685 epoch 375 head B batch: 300 avg loss -2.243186 avg loss no lamb -2.243186 time 2019-02-26 07:22:47.731051
Model ind 685 epoch 375 head B batch: 400 avg loss -2.192379 avg loss no lamb -2.192379 time 2019-02-26 07:24:02.901624
Model ind 685 epoch 375 head B batch: 0 avg loss -2.203416 avg loss no lamb -2.203416 time 2019-02-26 07:25:14.624753
Model ind 685 epoch 375 head B batch: 100 avg loss -2.210109 avg loss no lamb -2.210109 time 2019-02-26 07:26:25.801820
Model ind 685 epoch 375 head B batch: 200 avg loss -2.192500 avg loss no lamb -2.192500 time 2019-02-26 07:27:38.168596
Model ind 685 epoch 375 head B batch: 300 avg loss -2.211847 avg loss no lamb -2.211847 time 2019-02-26 07:28:54.379645
Model ind 685 epoch 375 head B batch: 400 avg loss -2.222398 avg loss no lamb -2.222398 time 2019-02-26 07:30:09.427427
Model ind 685 epoch 375 head A batch: 0 avg loss -2.203784 avg loss no lamb -2.203784 time 2019-02-26 07:31:24.238094
Model ind 685 epoch 375 head A batch: 100 avg loss -2.213105 avg loss no lamb -2.213105 time 2019-02-26 07:32:40.125181
Model ind 685 epoch 375 head A batch: 200 avg loss -2.221659 avg loss no lamb -2.221659 time 2019-02-26 07:33:58.332760
Model ind 685 epoch 375 head A batch: 300 avg loss -2.216169 avg loss no lamb -2.216169 time 2019-02-26 07:35:15.212967
Model ind 685 epoch 375 head A batch: 400 avg loss -2.209658 avg loss no lamb -2.209658 time 2019-02-26 07:36:33.860326
Pre: time 2019-02-26 07:38:03.756450: 
 	std: 0.00648486
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98433715
	best: 0.9924286

Starting e_i: 376
Model ind 685 epoch 376 head B batch: 0 avg loss -2.205995 avg loss no lamb -2.205995 time 2019-02-26 07:38:06.925900
Model ind 685 epoch 376 head B batch: 100 avg loss -2.220608 avg loss no lamb -2.220608 time 2019-02-26 07:39:23.002134
Model ind 685 epoch 376 head B batch: 200 avg loss -2.207633 avg loss no lamb -2.207633 time 2019-02-26 07:40:38.405160
Model ind 685 epoch 376 head B batch: 300 avg loss -2.222931 avg loss no lamb -2.222931 time 2019-02-26 07:41:51.584976
Model ind 685 epoch 376 head B batch: 400 avg loss -2.214819 avg loss no lamb -2.214819 time 2019-02-26 07:43:03.754187
Model ind 685 epoch 376 head B batch: 0 avg loss -2.209407 avg loss no lamb -2.209407 time 2019-02-26 07:44:21.978576
Model ind 685 epoch 376 head B batch: 100 avg loss -2.187134 avg loss no lamb -2.187134 time 2019-02-26 07:45:33.412302
Model ind 685 epoch 376 head B batch: 200 avg loss -2.204232 avg loss no lamb -2.204232 time 2019-02-26 07:46:44.494891
Model ind 685 epoch 376 head B batch: 300 avg loss -2.215359 avg loss no lamb -2.215359 time 2019-02-26 07:47:55.681461
Model ind 685 epoch 376 head B batch: 400 avg loss -2.210493 avg loss no lamb -2.210493 time 2019-02-26 07:49:12.133009
Model ind 685 epoch 376 head A batch: 0 avg loss -2.226888 avg loss no lamb -2.226888 time 2019-02-26 07:50:31.188176
Model ind 685 epoch 376 head A batch: 100 avg loss -2.176490 avg loss no lamb -2.176490 time 2019-02-26 07:51:40.314403
Model ind 685 epoch 376 head A batch: 200 avg loss -2.233632 avg loss no lamb -2.233632 time 2019-02-26 07:52:55.815361
Model ind 685 epoch 376 head A batch: 300 avg loss -2.216030 avg loss no lamb -2.216030 time 2019-02-26 07:54:10.840529
Model ind 685 epoch 376 head A batch: 400 avg loss -2.202874 avg loss no lamb -2.202874 time 2019-02-26 07:55:29.109778
Pre: time 2019-02-26 07:56:57.826672: 
 	std: 0.0064929533
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.99214286, 0.9790428, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.99214286, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843343
	best: 0.9924286

Starting e_i: 377
Model ind 685 epoch 377 head B batch: 0 avg loss -2.199815 avg loss no lamb -2.199815 time 2019-02-26 07:56:59.857412
Model ind 685 epoch 377 head B batch: 100 avg loss -2.210725 avg loss no lamb -2.210725 time 2019-02-26 07:58:13.572004
Model ind 685 epoch 377 head B batch: 200 avg loss -2.209763 avg loss no lamb -2.209763 time 2019-02-26 07:59:29.384899
Model ind 685 epoch 377 head B batch: 300 avg loss -2.215647 avg loss no lamb -2.215647 time 2019-02-26 08:00:46.657153
Model ind 685 epoch 377 head B batch: 400 avg loss -2.215572 avg loss no lamb -2.215572 time 2019-02-26 08:02:03.095168
Model ind 685 epoch 377 head B batch: 0 avg loss -2.198572 avg loss no lamb -2.198572 time 2019-02-26 08:03:20.352349
Model ind 685 epoch 377 head B batch: 100 avg loss -2.222667 avg loss no lamb -2.222667 time 2019-02-26 08:04:34.091553
Model ind 685 epoch 377 head B batch: 200 avg loss -2.233813 avg loss no lamb -2.233813 time 2019-02-26 08:05:46.524619
Model ind 685 epoch 377 head B batch: 300 avg loss -2.228268 avg loss no lamb -2.228268 time 2019-02-26 08:07:02.502354
Model ind 685 epoch 377 head B batch: 400 avg loss -2.215407 avg loss no lamb -2.215407 time 2019-02-26 08:08:19.854840
Model ind 685 epoch 377 head A batch: 0 avg loss -2.223564 avg loss no lamb -2.223564 time 2019-02-26 08:09:36.228020
Model ind 685 epoch 377 head A batch: 100 avg loss -2.204689 avg loss no lamb -2.204689 time 2019-02-26 08:10:49.740940
Model ind 685 epoch 377 head A batch: 200 avg loss -2.189460 avg loss no lamb -2.189460 time 2019-02-26 08:12:02.902415
Model ind 685 epoch 377 head A batch: 300 avg loss -2.213714 avg loss no lamb -2.213714 time 2019-02-26 08:13:19.856151
Model ind 685 epoch 377 head A batch: 400 avg loss -2.170642 avg loss no lamb -2.170642 time 2019-02-26 08:14:31.835105
Pre: time 2019-02-26 08:16:00.496873: 
 	std: 0.0064801755
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790143]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 378
Model ind 685 epoch 378 head B batch: 0 avg loss -2.201311 avg loss no lamb -2.201311 time 2019-02-26 08:16:02.807678
Model ind 685 epoch 378 head B batch: 100 avg loss -2.235385 avg loss no lamb -2.235385 time 2019-02-26 08:17:16.347923
Model ind 685 epoch 378 head B batch: 200 avg loss -2.196342 avg loss no lamb -2.196342 time 2019-02-26 08:18:33.828921
Model ind 685 epoch 378 head B batch: 300 avg loss -2.234585 avg loss no lamb -2.234585 time 2019-02-26 08:19:49.462067
Model ind 685 epoch 378 head B batch: 400 avg loss -2.195525 avg loss no lamb -2.195525 time 2019-02-26 08:21:02.579421
Model ind 685 epoch 378 head B batch: 0 avg loss -2.230952 avg loss no lamb -2.230952 time 2019-02-26 08:22:14.770013
Model ind 685 epoch 378 head B batch: 100 avg loss -2.182187 avg loss no lamb -2.182187 time 2019-02-26 08:23:32.605138
Model ind 685 epoch 378 head B batch: 200 avg loss -2.136432 avg loss no lamb -2.136432 time 2019-02-26 08:24:45.771891
Model ind 685 epoch 378 head B batch: 300 avg loss -2.223349 avg loss no lamb -2.223349 time 2019-02-26 08:26:01.658009
Model ind 685 epoch 378 head B batch: 400 avg loss -2.180277 avg loss no lamb -2.180277 time 2019-02-26 08:27:13.844943
Model ind 685 epoch 378 head A batch: 0 avg loss -2.198220 avg loss no lamb -2.198220 time 2019-02-26 08:28:29.512645
Model ind 685 epoch 378 head A batch: 100 avg loss -2.232153 avg loss no lamb -2.232153 time 2019-02-26 08:29:44.293897
Model ind 685 epoch 378 head A batch: 200 avg loss -2.211118 avg loss no lamb -2.211118 time 2019-02-26 08:31:03.113867
Model ind 685 epoch 378 head A batch: 300 avg loss -2.242903 avg loss no lamb -2.242903 time 2019-02-26 08:32:22.256058
Model ind 685 epoch 378 head A batch: 400 avg loss -2.170165 avg loss no lamb -2.170165 time 2019-02-26 08:33:39.033250
Pre: time 2019-02-26 08:34:55.671449: 
 	std: 0.0064930706
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9920857, 0.9789857, 0.979]
	train_accs: [0.9924, 0.9789857, 0.9920857, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98429143
	best: 0.9924

Starting e_i: 379
Model ind 685 epoch 379 head B batch: 0 avg loss -2.221529 avg loss no lamb -2.221529 time 2019-02-26 08:34:57.746214
Model ind 685 epoch 379 head B batch: 100 avg loss -2.211052 avg loss no lamb -2.211052 time 2019-02-26 08:36:11.257257
Model ind 685 epoch 379 head B batch: 200 avg loss -2.222498 avg loss no lamb -2.222498 time 2019-02-26 08:37:23.510203
Model ind 685 epoch 379 head B batch: 300 avg loss -2.235795 avg loss no lamb -2.235795 time 2019-02-26 08:51:47.551802
Model ind 685 epoch 379 head B batch: 400 avg loss -2.177190 avg loss no lamb -2.177190 time 2019-02-26 08:53:06.051995
Model ind 685 epoch 379 head B batch: 0 avg loss -2.247197 avg loss no lamb -2.247197 time 2019-02-26 08:54:24.067510
Model ind 685 epoch 379 head B batch: 100 avg loss -2.207273 avg loss no lamb -2.207273 time 2019-02-26 08:55:41.349913
Model ind 685 epoch 379 head B batch: 200 avg loss -2.232641 avg loss no lamb -2.232641 time 2019-02-26 08:56:57.868463
Model ind 685 epoch 379 head B batch: 300 avg loss -2.224710 avg loss no lamb -2.224710 time 2019-02-26 08:58:16.000910
Model ind 685 epoch 379 head B batch: 400 avg loss -2.179965 avg loss no lamb -2.179965 time 2019-02-26 08:59:31.821648
Model ind 685 epoch 379 head A batch: 0 avg loss -2.207501 avg loss no lamb -2.207501 time 2019-02-26 09:00:45.740958
Model ind 685 epoch 379 head A batch: 100 avg loss -2.220707 avg loss no lamb -2.220707 time 2019-02-26 09:01:57.010219
Model ind 685 epoch 379 head A batch: 200 avg loss -2.199569 avg loss no lamb -2.199569 time 2019-02-26 09:03:10.845161
Model ind 685 epoch 379 head A batch: 300 avg loss -2.231649 avg loss no lamb -2.231649 time 2019-02-26 09:04:27.851273
Model ind 685 epoch 379 head A batch: 400 avg loss -2.209684 avg loss no lamb -2.209684 time 2019-02-26 09:05:44.114709
Pre: time 2019-02-26 09:07:16.711910: 
 	std: 0.006481277
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98430574
	best: 0.9923857

Starting e_i: 380
Model ind 685 epoch 380 head B batch: 0 avg loss -2.201691 avg loss no lamb -2.201691 time 2019-02-26 09:07:18.657950
Model ind 685 epoch 380 head B batch: 100 avg loss -2.229342 avg loss no lamb -2.229342 time 2019-02-26 09:08:36.143129
Model ind 685 epoch 380 head B batch: 200 avg loss -2.211391 avg loss no lamb -2.211391 time 2019-02-26 09:09:52.431256
Model ind 685 epoch 380 head B batch: 300 avg loss -2.193662 avg loss no lamb -2.193662 time 2019-02-26 09:11:04.728634
Model ind 685 epoch 380 head B batch: 400 avg loss -2.192789 avg loss no lamb -2.192789 time 2019-02-26 09:12:19.252027
Model ind 685 epoch 380 head B batch: 0 avg loss -2.253561 avg loss no lamb -2.253561 time 2019-02-26 09:13:37.145206
Model ind 685 epoch 380 head B batch: 100 avg loss -2.193257 avg loss no lamb -2.193257 time 2019-02-26 09:14:53.323605
Model ind 685 epoch 380 head B batch: 200 avg loss -2.234096 avg loss no lamb -2.234096 time 2019-02-26 09:16:11.293859
Model ind 685 epoch 380 head B batch: 300 avg loss -2.225773 avg loss no lamb -2.225773 time 2019-02-26 09:17:30.947084
Model ind 685 epoch 380 head B batch: 400 avg loss -2.229184 avg loss no lamb -2.229184 time 2019-02-26 09:18:42.802986
Model ind 685 epoch 380 head A batch: 0 avg loss -2.223759 avg loss no lamb -2.223759 time 2019-02-26 09:19:58.059862
Model ind 685 epoch 380 head A batch: 100 avg loss -2.235759 avg loss no lamb -2.235759 time 2019-02-26 09:21:15.419472
Model ind 685 epoch 380 head A batch: 200 avg loss -2.186942 avg loss no lamb -2.186942 time 2019-02-26 09:22:30.535419
Model ind 685 epoch 380 head A batch: 300 avg loss -2.202600 avg loss no lamb -2.202600 time 2019-02-26 09:23:39.092178
Model ind 685 epoch 380 head A batch: 400 avg loss -2.184086 avg loss no lamb -2.184086 time 2019-02-26 09:24:44.988723
Pre: time 2019-02-26 09:26:12.597290: 
 	std: 0.0064719454
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790428, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790428, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9923857

Starting e_i: 381
Model ind 685 epoch 381 head B batch: 0 avg loss -2.210191 avg loss no lamb -2.210191 time 2019-02-26 09:26:15.907121
Model ind 685 epoch 381 head B batch: 100 avg loss -2.194704 avg loss no lamb -2.194704 time 2019-02-26 09:27:30.515880
Model ind 685 epoch 381 head B batch: 200 avg loss -2.198599 avg loss no lamb -2.198599 time 2019-02-26 09:28:46.625328
Model ind 685 epoch 381 head B batch: 300 avg loss -2.218222 avg loss no lamb -2.218222 time 2019-02-26 09:30:03.447895
Model ind 685 epoch 381 head B batch: 400 avg loss -2.198466 avg loss no lamb -2.198466 time 2019-02-26 09:31:20.805735
Model ind 685 epoch 381 head B batch: 0 avg loss -2.214953 avg loss no lamb -2.214953 time 2019-02-26 09:32:39.281783
Model ind 685 epoch 381 head B batch: 100 avg loss -2.202781 avg loss no lamb -2.202781 time 2019-02-26 09:33:56.372875
Model ind 685 epoch 381 head B batch: 200 avg loss -2.207077 avg loss no lamb -2.207077 time 2019-02-26 09:35:11.771946
Model ind 685 epoch 381 head B batch: 300 avg loss -2.251383 avg loss no lamb -2.251383 time 2019-02-26 09:36:27.049493
Model ind 685 epoch 381 head B batch: 400 avg loss -2.206218 avg loss no lamb -2.206218 time 2019-02-26 09:37:43.424272
Model ind 685 epoch 381 head A batch: 0 avg loss -2.232412 avg loss no lamb -2.232412 time 2019-02-26 09:39:00.060263
Model ind 685 epoch 381 head A batch: 100 avg loss -2.190116 avg loss no lamb -2.190116 time 2019-02-26 09:40:16.418914
Model ind 685 epoch 381 head A batch: 200 avg loss -2.212155 avg loss no lamb -2.212155 time 2019-02-26 09:41:27.603468
Model ind 685 epoch 381 head A batch: 300 avg loss -2.228389 avg loss no lamb -2.228389 time 2019-02-26 09:42:39.345243
Model ind 685 epoch 381 head A batch: 400 avg loss -2.220891 avg loss no lamb -2.220891 time 2019-02-26 09:43:52.204061
Pre: time 2019-02-26 09:45:28.062115: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 382
Model ind 685 epoch 382 head B batch: 0 avg loss -2.232399 avg loss no lamb -2.232399 time 2019-02-26 09:45:30.142031
Model ind 685 epoch 382 head B batch: 100 avg loss -2.186194 avg loss no lamb -2.186194 time 2019-02-26 09:46:46.497587
Model ind 685 epoch 382 head B batch: 200 avg loss -2.210709 avg loss no lamb -2.210709 time 2019-02-26 09:48:04.522259
Model ind 685 epoch 382 head B batch: 300 avg loss -2.204829 avg loss no lamb -2.204829 time 2019-02-26 09:49:22.872309
Model ind 685 epoch 382 head B batch: 400 avg loss -2.216218 avg loss no lamb -2.216218 time 2019-02-26 09:50:39.408583
Model ind 685 epoch 382 head B batch: 0 avg loss -2.182578 avg loss no lamb -2.182578 time 2019-02-26 09:51:57.229086
Model ind 685 epoch 382 head B batch: 100 avg loss -2.188705 avg loss no lamb -2.188705 time 2019-02-26 09:53:15.440781
Model ind 685 epoch 382 head B batch: 200 avg loss -2.222581 avg loss no lamb -2.222581 time 2019-02-26 09:54:34.790104
Model ind 685 epoch 382 head B batch: 300 avg loss -2.226520 avg loss no lamb -2.226520 time 2019-02-26 09:55:49.805161
Model ind 685 epoch 382 head B batch: 400 avg loss -2.235362 avg loss no lamb -2.235362 time 2019-02-26 09:57:04.120365
Model ind 685 epoch 382 head A batch: 0 avg loss -2.215459 avg loss no lamb -2.215459 time 2019-02-26 09:58:16.831430
Model ind 685 epoch 382 head A batch: 100 avg loss -2.156926 avg loss no lamb -2.156926 time 2019-02-26 09:59:31.739614
Model ind 685 epoch 382 head A batch: 200 avg loss -2.202761 avg loss no lamb -2.202761 time 2019-02-26 10:00:48.562567
Model ind 685 epoch 382 head A batch: 300 avg loss -2.232006 avg loss no lamb -2.232006 time 2019-02-26 10:02:01.826164
Model ind 685 epoch 382 head A batch: 400 avg loss -2.177757 avg loss no lamb -2.177757 time 2019-02-26 10:03:14.655095
Pre: time 2019-02-26 10:04:46.731000: 
 	std: 0.006488284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924

Starting e_i: 383
Model ind 685 epoch 383 head B batch: 0 avg loss -2.226778 avg loss no lamb -2.226778 time 2019-02-26 10:04:48.815444
Model ind 685 epoch 383 head B batch: 100 avg loss -2.234518 avg loss no lamb -2.234518 time 2019-02-26 10:06:00.403118
Model ind 685 epoch 383 head B batch: 200 avg loss -2.241969 avg loss no lamb -2.241969 time 2019-02-26 10:07:09.250904
Model ind 685 epoch 383 head B batch: 300 avg loss -2.204440 avg loss no lamb -2.204440 time 2019-02-26 10:08:26.193910
Model ind 685 epoch 383 head B batch: 400 avg loss -2.219907 avg loss no lamb -2.219907 time 2019-02-26 10:09:38.984567
Model ind 685 epoch 383 head B batch: 0 avg loss -2.216825 avg loss no lamb -2.216825 time 2019-02-26 10:10:51.324660
Model ind 685 epoch 383 head B batch: 100 avg loss -2.227826 avg loss no lamb -2.227826 time 2019-02-26 10:12:03.560371
Model ind 685 epoch 383 head B batch: 200 avg loss -2.167397 avg loss no lamb -2.167397 time 2019-02-26 10:13:17.704173
Model ind 685 epoch 383 head B batch: 300 avg loss -2.228865 avg loss no lamb -2.228865 time 2019-02-26 10:14:33.880260
Model ind 685 epoch 383 head B batch: 400 avg loss -2.152886 avg loss no lamb -2.152886 time 2019-02-26 10:15:53.912195
Model ind 685 epoch 383 head A batch: 0 avg loss -2.230388 avg loss no lamb -2.230388 time 2019-02-26 10:17:12.085403
Model ind 685 epoch 383 head A batch: 100 avg loss -2.233443 avg loss no lamb -2.233443 time 2019-02-26 10:18:30.414004
Model ind 685 epoch 383 head A batch: 200 avg loss -2.215002 avg loss no lamb -2.215002 time 2019-02-26 10:19:49.700808
Model ind 685 epoch 383 head A batch: 300 avg loss -2.226074 avg loss no lamb -2.226074 time 2019-02-26 10:21:07.098264
Model ind 685 epoch 383 head A batch: 400 avg loss -2.210975 avg loss no lamb -2.210975 time 2019-02-26 10:22:25.885269
Pre: time 2019-02-26 10:24:02.133925: 
 	std: 0.006450646
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922714, 0.9789857, 0.99207145, 0.9790143, 0.9790143]
	train_accs: [0.9922714, 0.9789857, 0.99207145, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842714
	best: 0.9922714

Starting e_i: 384
Model ind 685 epoch 384 head B batch: 0 avg loss -2.202903 avg loss no lamb -2.202903 time 2019-02-26 10:24:04.323853
Model ind 685 epoch 384 head B batch: 100 avg loss -2.208605 avg loss no lamb -2.208605 time 2019-02-26 10:25:19.100422
Model ind 685 epoch 384 head B batch: 200 avg loss -2.206812 avg loss no lamb -2.206812 time 2019-02-26 10:26:32.218857
Model ind 685 epoch 384 head B batch: 300 avg loss -2.241726 avg loss no lamb -2.241726 time 2019-02-26 10:27:49.625828
Model ind 685 epoch 384 head B batch: 400 avg loss -2.200974 avg loss no lamb -2.200974 time 2019-02-26 10:29:06.747084
Model ind 685 epoch 384 head B batch: 0 avg loss -2.249514 avg loss no lamb -2.249514 time 2019-02-26 10:30:24.986853
Model ind 685 epoch 384 head B batch: 100 avg loss -2.221478 avg loss no lamb -2.221478 time 2019-02-26 10:31:43.134578
Model ind 685 epoch 384 head B batch: 200 avg loss -2.164812 avg loss no lamb -2.164812 time 2019-02-26 10:32:59.674729
Model ind 685 epoch 384 head B batch: 300 avg loss -2.214247 avg loss no lamb -2.214247 time 2019-02-26 10:34:15.501144
Model ind 685 epoch 384 head B batch: 400 avg loss -2.220294 avg loss no lamb -2.220294 time 2019-02-26 10:35:32.774621
Model ind 685 epoch 384 head A batch: 0 avg loss -2.197210 avg loss no lamb -2.197210 time 2019-02-26 10:36:48.583524
Model ind 685 epoch 384 head A batch: 100 avg loss -2.215942 avg loss no lamb -2.215942 time 2019-02-26 10:38:04.224124
Model ind 685 epoch 384 head A batch: 200 avg loss -2.210352 avg loss no lamb -2.210352 time 2019-02-26 10:39:20.742802
Model ind 685 epoch 384 head A batch: 300 avg loss -2.235523 avg loss no lamb -2.235523 time 2019-02-26 10:40:36.553502
Model ind 685 epoch 384 head A batch: 400 avg loss -2.174402 avg loss no lamb -2.174402 time 2019-02-26 10:41:52.474640
Pre: time 2019-02-26 10:43:24.523847: 
 	std: 0.0064534238
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922571, 0.97892857, 0.9919429, 0.97892857, 0.97892857]
	train_accs: [0.9922571, 0.97892857, 0.9919429, 0.97892857, 0.97892857]
	best_train_sub_head: 0
	worst: 0.97892857
	avg: 0.98419714
	best: 0.9922571

Starting e_i: 385
Model ind 685 epoch 385 head B batch: 0 avg loss -2.236130 avg loss no lamb -2.236130 time 2019-02-26 10:43:26.751239
Model ind 685 epoch 385 head B batch: 100 avg loss -2.189973 avg loss no lamb -2.189973 time 2019-02-26 10:44:42.924593
Model ind 685 epoch 385 head B batch: 200 avg loss -2.203577 avg loss no lamb -2.203577 time 2019-02-26 10:46:00.500154
Model ind 685 epoch 385 head B batch: 300 avg loss -2.213020 avg loss no lamb -2.213020 time 2019-02-26 10:47:15.761474
Model ind 685 epoch 385 head B batch: 400 avg loss -2.184494 avg loss no lamb -2.184494 time 2019-02-26 10:48:31.075504
Model ind 685 epoch 385 head B batch: 0 avg loss -2.210548 avg loss no lamb -2.210548 time 2019-02-26 10:49:39.275227
Model ind 685 epoch 385 head B batch: 100 avg loss -2.230863 avg loss no lamb -2.230863 time 2019-02-26 10:50:52.853206
Model ind 685 epoch 385 head B batch: 200 avg loss -2.194476 avg loss no lamb -2.194476 time 2019-02-26 10:52:05.161523
Model ind 685 epoch 385 head B batch: 300 avg loss -2.238358 avg loss no lamb -2.238358 time 2019-02-26 10:53:16.887537
Model ind 685 epoch 385 head B batch: 400 avg loss -2.204716 avg loss no lamb -2.204716 time 2019-02-26 10:54:31.106354
Model ind 685 epoch 385 head A batch: 0 avg loss -2.227061 avg loss no lamb -2.227061 time 2019-02-26 10:55:49.016648
Model ind 685 epoch 385 head A batch: 100 avg loss -2.154875 avg loss no lamb -2.154875 time 2019-02-26 10:57:06.802600
Model ind 685 epoch 385 head A batch: 200 avg loss -2.177339 avg loss no lamb -2.177339 time 2019-02-26 10:58:26.243080
Model ind 685 epoch 385 head A batch: 300 avg loss -2.250133 avg loss no lamb -2.250133 time 2019-02-26 10:59:38.376202
Model ind 685 epoch 385 head A batch: 400 avg loss -2.213130 avg loss no lamb -2.213130 time 2019-02-26 11:00:54.697830
Pre: time 2019-02-26 11:02:32.845271: 
 	std: 0.006483615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921143, 0.9790286, 0.9790143]
	train_accs: [0.9924, 0.9790286, 0.9921143, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924

Starting e_i: 386
Model ind 685 epoch 386 head B batch: 0 avg loss -2.232319 avg loss no lamb -2.232319 time 2019-02-26 11:02:35.007394
Model ind 685 epoch 386 head B batch: 100 avg loss -2.218899 avg loss no lamb -2.218899 time 2019-02-26 11:03:52.836752
Model ind 685 epoch 386 head B batch: 200 avg loss -2.218779 avg loss no lamb -2.218779 time 2019-02-26 11:05:08.827993
Model ind 685 epoch 386 head B batch: 300 avg loss -2.203330 avg loss no lamb -2.203330 time 2019-02-26 11:06:22.301594
Model ind 685 epoch 386 head B batch: 400 avg loss -2.192489 avg loss no lamb -2.192489 time 2019-02-26 11:07:41.686026
Model ind 685 epoch 386 head B batch: 0 avg loss -2.233862 avg loss no lamb -2.233862 time 2019-02-26 11:09:00.058017
Model ind 685 epoch 386 head B batch: 100 avg loss -2.233729 avg loss no lamb -2.233729 time 2019-02-26 11:10:18.243700
Model ind 685 epoch 386 head B batch: 200 avg loss -2.210580 avg loss no lamb -2.210580 time 2019-02-26 11:11:34.104296
Model ind 685 epoch 386 head B batch: 300 avg loss -2.203539 avg loss no lamb -2.203539 time 2019-02-26 11:12:52.402962
Model ind 685 epoch 386 head B batch: 400 avg loss -2.190741 avg loss no lamb -2.190741 time 2019-02-26 11:14:05.084556
Model ind 685 epoch 386 head A batch: 0 avg loss -2.226514 avg loss no lamb -2.226514 time 2019-02-26 11:15:22.529534
Model ind 685 epoch 386 head A batch: 100 avg loss -2.237225 avg loss no lamb -2.237225 time 2019-02-26 11:16:39.766728
Model ind 685 epoch 386 head A batch: 200 avg loss -2.197223 avg loss no lamb -2.197223 time 2019-02-26 11:17:57.393959
Model ind 685 epoch 386 head A batch: 300 avg loss -2.198210 avg loss no lamb -2.198210 time 2019-02-26 11:19:12.942548
Model ind 685 epoch 386 head A batch: 400 avg loss -2.166466 avg loss no lamb -2.166466 time 2019-02-26 11:20:29.645479
Pre: time 2019-02-26 11:22:07.173901: 
 	std: 0.0064882697
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.99212855, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.99212855, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.9924143

Starting e_i: 387
Model ind 685 epoch 387 head B batch: 0 avg loss -2.246281 avg loss no lamb -2.246281 time 2019-02-26 11:22:09.566005
Model ind 685 epoch 387 head B batch: 100 avg loss -2.230084 avg loss no lamb -2.230084 time 2019-02-26 11:23:28.325247
Model ind 685 epoch 387 head B batch: 200 avg loss -2.208559 avg loss no lamb -2.208559 time 2019-02-26 11:24:43.521013
Model ind 685 epoch 387 head B batch: 300 avg loss -2.238296 avg loss no lamb -2.238296 time 2019-02-26 11:25:58.818603
Model ind 685 epoch 387 head B batch: 400 avg loss -2.161180 avg loss no lamb -2.161180 time 2019-02-26 11:27:14.915669
Model ind 685 epoch 387 head B batch: 0 avg loss -2.232251 avg loss no lamb -2.232251 time 2019-02-26 11:28:30.393957
Model ind 685 epoch 387 head B batch: 100 avg loss -2.190093 avg loss no lamb -2.190093 time 2019-02-26 11:29:49.040678
Model ind 685 epoch 387 head B batch: 200 avg loss -2.201597 avg loss no lamb -2.201597 time 2019-02-26 11:31:10.136429
Model ind 685 epoch 387 head B batch: 300 avg loss -2.221237 avg loss no lamb -2.221237 time 2019-02-26 11:32:26.598881
Model ind 685 epoch 387 head B batch: 400 avg loss -2.166582 avg loss no lamb -2.166582 time 2019-02-26 11:33:39.141002
Model ind 685 epoch 387 head A batch: 0 avg loss -2.206289 avg loss no lamb -2.206289 time 2019-02-26 11:34:58.689661
Model ind 685 epoch 387 head A batch: 100 avg loss -2.209083 avg loss no lamb -2.209083 time 2019-02-26 11:36:16.878240
Model ind 685 epoch 387 head A batch: 200 avg loss -2.213293 avg loss no lamb -2.213293 time 2019-02-26 11:37:36.734187
Model ind 685 epoch 387 head A batch: 300 avg loss -2.269818 avg loss no lamb -2.269818 time 2019-02-26 11:38:53.008784
Model ind 685 epoch 387 head A batch: 400 avg loss -2.188983 avg loss no lamb -2.188983 time 2019-02-26 11:40:10.854671
Pre: time 2019-02-26 11:41:48.217930: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 388
Model ind 685 epoch 388 head B batch: 0 avg loss -2.217904 avg loss no lamb -2.217904 time 2019-02-26 11:41:50.414735
Model ind 685 epoch 388 head B batch: 100 avg loss -2.152547 avg loss no lamb -2.152547 time 2019-02-26 11:43:08.422595
Model ind 685 epoch 388 head B batch: 200 avg loss -2.220434 avg loss no lamb -2.220434 time 2019-02-26 11:44:25.952386
Model ind 685 epoch 388 head B batch: 300 avg loss -2.246974 avg loss no lamb -2.246974 time 2019-02-26 11:45:47.122496
Model ind 685 epoch 388 head B batch: 400 avg loss -2.177348 avg loss no lamb -2.177348 time 2019-02-26 11:47:05.244870
Model ind 685 epoch 388 head B batch: 0 avg loss -2.209697 avg loss no lamb -2.209697 time 2019-02-26 11:48:21.950828
Model ind 685 epoch 388 head B batch: 100 avg loss -2.210216 avg loss no lamb -2.210216 time 2019-02-26 11:49:40.004435
Model ind 685 epoch 388 head B batch: 200 avg loss -2.212815 avg loss no lamb -2.212815 time 2019-02-26 11:50:59.192379
Model ind 685 epoch 388 head B batch: 300 avg loss -2.245754 avg loss no lamb -2.245754 time 2019-02-26 11:52:20.098415
Model ind 685 epoch 388 head B batch: 400 avg loss -2.177604 avg loss no lamb -2.177604 time 2019-02-26 11:53:37.625456
Model ind 685 epoch 388 head A batch: 0 avg loss -2.221393 avg loss no lamb -2.221393 time 2019-02-26 11:54:51.603177
Model ind 685 epoch 388 head A batch: 100 avg loss -2.241569 avg loss no lamb -2.241569 time 2019-02-26 11:56:05.460980
Model ind 685 epoch 388 head A batch: 200 avg loss -2.241309 avg loss no lamb -2.241309 time 2019-02-26 11:57:22.404881
Model ind 685 epoch 388 head A batch: 300 avg loss -2.188672 avg loss no lamb -2.188672 time 2019-02-26 11:58:40.752756
Model ind 685 epoch 388 head A batch: 400 avg loss -2.207827 avg loss no lamb -2.207827 time 2019-02-26 12:00:02.985854
Pre: time 2019-02-26 12:01:41.656055: 
 	std: 0.0064742696
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9923857, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843143
	best: 0.9923857

Starting e_i: 389
Model ind 685 epoch 389 head B batch: 0 avg loss -2.244520 avg loss no lamb -2.244520 time 2019-02-26 12:01:43.948356
Model ind 685 epoch 389 head B batch: 100 avg loss -2.213368 avg loss no lamb -2.213368 time 2019-02-26 12:03:02.094504
Model ind 685 epoch 389 head B batch: 200 avg loss -2.190393 avg loss no lamb -2.190393 time 2019-02-26 12:04:22.755701
Model ind 685 epoch 389 head B batch: 300 avg loss -2.192448 avg loss no lamb -2.192448 time 2019-02-26 12:05:43.420329
Model ind 685 epoch 389 head B batch: 400 avg loss -2.189087 avg loss no lamb -2.189087 time 2019-02-26 12:07:01.362999
Model ind 685 epoch 389 head B batch: 0 avg loss -2.198107 avg loss no lamb -2.198107 time 2019-02-26 12:08:21.215161
Model ind 685 epoch 389 head B batch: 100 avg loss -2.191986 avg loss no lamb -2.191986 time 2019-02-26 12:09:41.893951
Model ind 685 epoch 389 head B batch: 200 avg loss -2.223616 avg loss no lamb -2.223616 time 2019-02-26 12:11:01.853544
Model ind 685 epoch 389 head B batch: 300 avg loss -2.238705 avg loss no lamb -2.238705 time 2019-02-26 12:12:21.861278
Model ind 685 epoch 389 head B batch: 400 avg loss -2.198225 avg loss no lamb -2.198225 time 2019-02-26 12:13:37.278487
Model ind 685 epoch 389 head A batch: 0 avg loss -2.237883 avg loss no lamb -2.237883 time 2019-02-26 12:14:53.716656
Model ind 685 epoch 389 head A batch: 100 avg loss -2.197156 avg loss no lamb -2.197156 time 2019-02-26 12:16:05.287641
Model ind 685 epoch 389 head A batch: 200 avg loss -2.210287 avg loss no lamb -2.210287 time 2019-02-26 12:17:18.472465
Model ind 685 epoch 389 head A batch: 300 avg loss -2.232209 avg loss no lamb -2.232209 time 2019-02-26 12:18:37.292500
Model ind 685 epoch 389 head A batch: 400 avg loss -2.227776 avg loss no lamb -2.227776 time 2019-02-26 12:19:55.346787
Pre: time 2019-02-26 12:21:29.482444: 
 	std: 0.006484998
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.99207145, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.99207145, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9924

Starting e_i: 390
Model ind 685 epoch 390 head B batch: 0 avg loss -2.192449 avg loss no lamb -2.192449 time 2019-02-26 12:21:31.912729
Model ind 685 epoch 390 head B batch: 100 avg loss -2.236589 avg loss no lamb -2.236589 time 2019-02-26 12:22:48.455057
Model ind 685 epoch 390 head B batch: 200 avg loss -2.224890 avg loss no lamb -2.224890 time 2019-02-26 12:24:04.194320
Model ind 685 epoch 390 head B batch: 300 avg loss -2.226753 avg loss no lamb -2.226753 time 2019-02-26 12:25:23.439000
Model ind 685 epoch 390 head B batch: 400 avg loss -2.172973 avg loss no lamb -2.172973 time 2019-02-26 12:26:45.516879
Model ind 685 epoch 390 head B batch: 0 avg loss -2.205404 avg loss no lamb -2.205404 time 2019-02-26 12:28:03.954928
Model ind 685 epoch 390 head B batch: 100 avg loss -2.237145 avg loss no lamb -2.237145 time 2019-02-26 12:29:21.574255
Model ind 685 epoch 390 head B batch: 200 avg loss -2.184585 avg loss no lamb -2.184585 time 2019-02-26 12:30:39.355937
Model ind 685 epoch 390 head B batch: 300 avg loss -2.232648 avg loss no lamb -2.232648 time 2019-02-26 12:31:56.704335
Model ind 685 epoch 390 head B batch: 400 avg loss -2.221719 avg loss no lamb -2.221719 time 2019-02-26 12:33:15.914996
Model ind 685 epoch 390 head A batch: 0 avg loss -2.224758 avg loss no lamb -2.224758 time 2019-02-26 12:34:33.180317
Model ind 685 epoch 390 head A batch: 100 avg loss -2.201499 avg loss no lamb -2.201499 time 2019-02-26 12:35:48.891321
Model ind 685 epoch 390 head A batch: 200 avg loss -2.217793 avg loss no lamb -2.217793 time 2019-02-26 12:37:08.284702
Model ind 685 epoch 390 head A batch: 300 avg loss -2.228751 avg loss no lamb -2.228751 time 2019-02-26 12:38:28.039234
Model ind 685 epoch 390 head A batch: 400 avg loss -2.215833 avg loss no lamb -2.215833 time 2019-02-26 12:39:45.004102
Pre: time 2019-02-26 12:41:20.711370: 
 	std: 0.006471135
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99245715, 0.97907144, 0.9921, 0.97907144, 0.97907144]
	train_accs: [0.99245715, 0.97907144, 0.9921, 0.97907144, 0.97907144]
	best_train_sub_head: 0
	worst: 0.97907144
	avg: 0.9843543
	best: 0.99245715

Starting e_i: 391
Model ind 685 epoch 391 head B batch: 0 avg loss -2.224522 avg loss no lamb -2.224522 time 2019-02-26 12:41:22.879344
Model ind 685 epoch 391 head B batch: 100 avg loss -2.202867 avg loss no lamb -2.202867 time 2019-02-26 12:42:41.361549
Model ind 685 epoch 391 head B batch: 200 avg loss -2.196278 avg loss no lamb -2.196278 time 2019-02-26 12:44:01.238988
Model ind 685 epoch 391 head B batch: 300 avg loss -2.206287 avg loss no lamb -2.206287 time 2019-02-26 12:45:19.755179
Model ind 685 epoch 391 head B batch: 400 avg loss -2.179975 avg loss no lamb -2.179975 time 2019-02-26 12:46:37.990939
Model ind 685 epoch 391 head B batch: 0 avg loss -2.214957 avg loss no lamb -2.214957 time 2019-02-26 12:47:56.814266
Model ind 685 epoch 391 head B batch: 100 avg loss -2.213281 avg loss no lamb -2.213281 time 2019-02-26 12:49:14.568508
Model ind 685 epoch 391 head B batch: 200 avg loss -2.207902 avg loss no lamb -2.207902 time 2019-02-26 12:50:33.601845
Model ind 685 epoch 391 head B batch: 300 avg loss -2.229401 avg loss no lamb -2.229401 time 2019-02-26 12:51:53.398954
Model ind 685 epoch 391 head B batch: 400 avg loss -2.159421 avg loss no lamb -2.159421 time 2019-02-26 12:53:12.918173
Model ind 685 epoch 391 head A batch: 0 avg loss -2.197766 avg loss no lamb -2.197766 time 2019-02-26 12:54:32.655523
Model ind 685 epoch 391 head A batch: 100 avg loss -2.196190 avg loss no lamb -2.196190 time 2019-02-26 12:55:50.996574
Model ind 685 epoch 391 head A batch: 200 avg loss -2.200341 avg loss no lamb -2.200341 time 2019-02-26 12:57:09.293101
Model ind 685 epoch 391 head A batch: 300 avg loss -2.242884 avg loss no lamb -2.242884 time 2019-02-26 12:58:29.803991
Model ind 685 epoch 391 head A batch: 400 avg loss -2.185422 avg loss no lamb -2.185422 time 2019-02-26 12:59:40.026528
Pre: time 2019-02-26 13:01:17.600852: 
 	std: 0.00648105
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789714, 0.9920857, 0.9789714, 0.9789714]
	train_accs: [0.9923143, 0.9789714, 0.9920857, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842628
	best: 0.9923143

Starting e_i: 392
Model ind 685 epoch 392 head B batch: 0 avg loss -2.227884 avg loss no lamb -2.227884 time 2019-02-26 13:01:19.802432
Model ind 685 epoch 392 head B batch: 100 avg loss -2.232832 avg loss no lamb -2.232832 time 2019-02-26 13:02:38.196460
Model ind 685 epoch 392 head B batch: 200 avg loss -2.185397 avg loss no lamb -2.185397 time 2019-02-26 13:03:56.331645
Model ind 685 epoch 392 head B batch: 300 avg loss -2.248240 avg loss no lamb -2.248240 time 2019-02-26 13:05:15.937628
Model ind 685 epoch 392 head B batch: 400 avg loss -2.218117 avg loss no lamb -2.218117 time 2019-02-26 13:06:34.730796
Model ind 685 epoch 392 head B batch: 0 avg loss -2.217284 avg loss no lamb -2.217284 time 2019-02-26 13:07:53.996947
Model ind 685 epoch 392 head B batch: 100 avg loss -2.193387 avg loss no lamb -2.193387 time 2019-02-26 13:09:12.746118
Model ind 685 epoch 392 head B batch: 200 avg loss -2.218849 avg loss no lamb -2.218849 time 2019-02-26 13:10:28.919244
Model ind 685 epoch 392 head B batch: 300 avg loss -2.237495 avg loss no lamb -2.237495 time 2019-02-26 13:11:44.296829
Model ind 685 epoch 392 head B batch: 400 avg loss -2.173780 avg loss no lamb -2.173780 time 2019-02-26 13:13:00.664405
Model ind 685 epoch 392 head A batch: 0 avg loss -2.210255 avg loss no lamb -2.210255 time 2019-02-26 13:14:16.497145
Model ind 685 epoch 392 head A batch: 100 avg loss -2.220720 avg loss no lamb -2.220720 time 2019-02-26 13:15:36.184355
Model ind 685 epoch 392 head A batch: 200 avg loss -2.184620 avg loss no lamb -2.184620 time 2019-02-26 13:16:58.593279
Model ind 685 epoch 392 head A batch: 300 avg loss -2.224357 avg loss no lamb -2.224357 time 2019-02-26 13:18:21.326090
Model ind 685 epoch 392 head A batch: 400 avg loss -2.211413 avg loss no lamb -2.211413 time 2019-02-26 13:19:39.584716
Pre: time 2019-02-26 13:21:13.147557: 
 	std: 0.006477853
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434573
	best: 0.9924286

Starting e_i: 393
Model ind 685 epoch 393 head B batch: 0 avg loss -2.216381 avg loss no lamb -2.216381 time 2019-02-26 13:21:15.482612
Model ind 685 epoch 393 head B batch: 100 avg loss -2.227548 avg loss no lamb -2.227548 time 2019-02-26 13:22:36.909556
Model ind 685 epoch 393 head B batch: 200 avg loss -2.223467 avg loss no lamb -2.223467 time 2019-02-26 13:23:57.175095
Model ind 685 epoch 393 head B batch: 300 avg loss -2.187888 avg loss no lamb -2.187888 time 2019-02-26 13:25:15.077770
Model ind 685 epoch 393 head B batch: 400 avg loss -2.202306 avg loss no lamb -2.202306 time 2019-02-26 13:26:32.649639
Model ind 685 epoch 393 head B batch: 0 avg loss -2.211081 avg loss no lamb -2.211081 time 2019-02-26 13:27:49.632654
Model ind 685 epoch 393 head B batch: 100 avg loss -2.159874 avg loss no lamb -2.159874 time 2019-02-26 13:29:08.071233
Model ind 685 epoch 393 head B batch: 200 avg loss -2.223055 avg loss no lamb -2.223055 time 2019-02-26 13:30:26.922260
Model ind 685 epoch 393 head B batch: 300 avg loss -2.218993 avg loss no lamb -2.218993 time 2019-02-26 13:31:44.491932
Model ind 685 epoch 393 head B batch: 400 avg loss -2.181094 avg loss no lamb -2.181094 time 2019-02-26 13:33:03.264416
Model ind 685 epoch 393 head A batch: 0 avg loss -2.209375 avg loss no lamb -2.209375 time 2019-02-26 13:34:24.197265
Model ind 685 epoch 393 head A batch: 100 avg loss -2.209660 avg loss no lamb -2.209660 time 2019-02-26 13:35:40.805317
Model ind 685 epoch 393 head A batch: 200 avg loss -2.239594 avg loss no lamb -2.239594 time 2019-02-26 13:37:00.848454
Model ind 685 epoch 393 head A batch: 300 avg loss -2.202342 avg loss no lamb -2.202342 time 2019-02-26 13:38:21.059787
Model ind 685 epoch 393 head A batch: 400 avg loss -2.226182 avg loss no lamb -2.226182 time 2019-02-26 13:39:39.325332
Pre: time 2019-02-26 13:41:16.122909: 
 	std: 0.0064814202
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99207145, 0.9789857, 0.979]
	train_accs: [0.9923857, 0.9790143, 0.99207145, 0.9789857, 0.979]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.98429143
	best: 0.9923857

Starting e_i: 394
Model ind 685 epoch 394 head B batch: 0 avg loss -2.250248 avg loss no lamb -2.250248 time 2019-02-26 13:41:18.702876
Model ind 685 epoch 394 head B batch: 100 avg loss -2.174215 avg loss no lamb -2.174215 time 2019-02-26 13:42:28.003285
Model ind 685 epoch 394 head B batch: 200 avg loss -2.221919 avg loss no lamb -2.221919 time 2019-02-26 13:43:47.411687
Model ind 685 epoch 394 head B batch: 300 avg loss -2.227266 avg loss no lamb -2.227266 time 2019-02-26 13:45:06.523482
Model ind 685 epoch 394 head B batch: 400 avg loss -2.158199 avg loss no lamb -2.158199 time 2019-02-26 13:46:25.916713
Model ind 685 epoch 394 head B batch: 0 avg loss -2.234112 avg loss no lamb -2.234112 time 2019-02-26 13:47:44.727892
Model ind 685 epoch 394 head B batch: 100 avg loss -2.192944 avg loss no lamb -2.192944 time 2019-02-26 13:49:04.604663
Model ind 685 epoch 394 head B batch: 200 avg loss -2.226206 avg loss no lamb -2.226206 time 2019-02-26 13:50:25.066118
Model ind 685 epoch 394 head B batch: 300 avg loss -2.248826 avg loss no lamb -2.248826 time 2019-02-26 13:51:45.608121
Model ind 685 epoch 394 head B batch: 400 avg loss -2.203748 avg loss no lamb -2.203748 time 2019-02-26 13:53:05.571084
Model ind 685 epoch 394 head A batch: 0 avg loss -2.233546 avg loss no lamb -2.233546 time 2019-02-26 13:54:25.403642
Model ind 685 epoch 394 head A batch: 100 avg loss -2.189779 avg loss no lamb -2.189779 time 2019-02-26 13:55:43.501845
Model ind 685 epoch 394 head A batch: 200 avg loss -2.178945 avg loss no lamb -2.178945 time 2019-02-26 13:57:03.074082
Model ind 685 epoch 394 head A batch: 300 avg loss -2.212157 avg loss no lamb -2.212157 time 2019-02-26 13:58:21.981319
Model ind 685 epoch 394 head A batch: 400 avg loss -2.188138 avg loss no lamb -2.188138 time 2019-02-26 13:59:41.181170
Pre: time 2019-02-26 14:01:18.248909: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.9920857, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843029
	best: 0.9923857

Starting e_i: 395
Model ind 685 epoch 395 head B batch: 0 avg loss -2.220729 avg loss no lamb -2.220729 time 2019-02-26 14:01:20.571669
Model ind 685 epoch 395 head B batch: 100 avg loss -2.218102 avg loss no lamb -2.218102 time 2019-02-26 14:02:37.564117
Model ind 685 epoch 395 head B batch: 200 avg loss -2.231581 avg loss no lamb -2.231581 time 2019-02-26 14:03:54.262534
Model ind 685 epoch 395 head B batch: 300 avg loss -2.237235 avg loss no lamb -2.237235 time 2019-02-26 14:05:11.941456
Model ind 685 epoch 395 head B batch: 400 avg loss -2.183246 avg loss no lamb -2.183246 time 2019-02-26 14:06:29.579651
Model ind 685 epoch 395 head B batch: 0 avg loss -2.216877 avg loss no lamb -2.216877 time 2019-02-26 14:07:49.444076
Model ind 685 epoch 395 head B batch: 100 avg loss -2.224319 avg loss no lamb -2.224319 time 2019-02-26 14:09:08.166212
Model ind 685 epoch 395 head B batch: 200 avg loss -2.203919 avg loss no lamb -2.203919 time 2019-02-26 14:10:26.198584
Model ind 685 epoch 395 head B batch: 300 avg loss -2.209981 avg loss no lamb -2.209981 time 2019-02-26 14:11:44.040757
Model ind 685 epoch 395 head B batch: 400 avg loss -2.198813 avg loss no lamb -2.198813 time 2019-02-26 14:13:02.442099
Model ind 685 epoch 395 head A batch: 0 avg loss -2.210807 avg loss no lamb -2.210807 time 2019-02-26 14:14:20.606035
Model ind 685 epoch 395 head A batch: 100 avg loss -2.226960 avg loss no lamb -2.226960 time 2019-02-26 14:15:37.849927
Model ind 685 epoch 395 head A batch: 200 avg loss -2.200820 avg loss no lamb -2.200820 time 2019-02-26 14:16:55.347776
Model ind 685 epoch 395 head A batch: 300 avg loss -2.220469 avg loss no lamb -2.220469 time 2019-02-26 14:18:12.130210
Model ind 685 epoch 395 head A batch: 400 avg loss -2.181281 avg loss no lamb -2.181281 time 2019-02-26 14:19:31.008064
Pre: time 2019-02-26 14:21:07.942337: 
 	std: 0.006470984
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923143, 0.9789429, 0.99198574, 0.9789429, 0.9789429]
	train_accs: [0.9923143, 0.9789429, 0.99198574, 0.9789429, 0.9789429]
	best_train_sub_head: 0
	worst: 0.9789429
	avg: 0.98422575
	best: 0.9923143

Starting e_i: 396
Model ind 685 epoch 396 head B batch: 0 avg loss -2.213929 avg loss no lamb -2.213929 time 2019-02-26 14:21:10.076696
Model ind 685 epoch 396 head B batch: 100 avg loss -2.205788 avg loss no lamb -2.205788 time 2019-02-26 14:22:30.136807
Model ind 685 epoch 396 head B batch: 200 avg loss -2.186689 avg loss no lamb -2.186689 time 2019-02-26 14:23:47.922861
Model ind 685 epoch 396 head B batch: 300 avg loss -2.245064 avg loss no lamb -2.245064 time 2019-02-26 14:25:00.726411
Model ind 685 epoch 396 head B batch: 400 avg loss -2.227849 avg loss no lamb -2.227849 time 2019-02-26 14:26:20.301008
Model ind 685 epoch 396 head B batch: 0 avg loss -2.224078 avg loss no lamb -2.224078 time 2019-02-26 14:27:38.291418
Model ind 685 epoch 396 head B batch: 100 avg loss -2.183717 avg loss no lamb -2.183717 time 2019-02-26 14:28:55.816131
Model ind 685 epoch 396 head B batch: 200 avg loss -2.165169 avg loss no lamb -2.165169 time 2019-02-26 14:30:13.936252
Model ind 685 epoch 396 head B batch: 300 avg loss -2.226930 avg loss no lamb -2.226930 time 2019-02-26 14:31:32.933973
Model ind 685 epoch 396 head B batch: 400 avg loss -2.197706 avg loss no lamb -2.197706 time 2019-02-26 14:32:53.773881
Model ind 685 epoch 396 head A batch: 0 avg loss -2.230988 avg loss no lamb -2.230988 time 2019-02-26 14:34:13.411479
Model ind 685 epoch 396 head A batch: 100 avg loss -2.195086 avg loss no lamb -2.195086 time 2019-02-26 14:35:28.585854
Model ind 685 epoch 396 head A batch: 200 avg loss -2.222745 avg loss no lamb -2.222745 time 2019-02-26 14:36:48.566288
Model ind 685 epoch 396 head A batch: 300 avg loss -2.219466 avg loss no lamb -2.219466 time 2019-02-26 14:38:04.798254
Model ind 685 epoch 396 head A batch: 400 avg loss -2.206666 avg loss no lamb -2.206666 time 2019-02-26 14:39:21.361954
Pre: time 2019-02-26 14:40:53.648751: 
 	std: 0.006483752
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.9789857]
	train_accs: [0.9923857, 0.979, 0.99207145, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842886
	best: 0.9923857

Starting e_i: 397
Model ind 685 epoch 397 head B batch: 0 avg loss -2.217351 avg loss no lamb -2.217351 time 2019-02-26 14:40:56.284405
Model ind 685 epoch 397 head B batch: 100 avg loss -2.163357 avg loss no lamb -2.163357 time 2019-02-26 14:42:13.587391
Model ind 685 epoch 397 head B batch: 200 avg loss -2.210359 avg loss no lamb -2.210359 time 2019-02-26 14:43:31.244112
Model ind 685 epoch 397 head B batch: 300 avg loss -2.203644 avg loss no lamb -2.203644 time 2019-02-26 14:44:49.219316
Model ind 685 epoch 397 head B batch: 400 avg loss -2.188888 avg loss no lamb -2.188888 time 2019-02-26 14:46:07.509916
Model ind 685 epoch 397 head B batch: 0 avg loss -2.221041 avg loss no lamb -2.221041 time 2019-02-26 14:47:24.301483
Model ind 685 epoch 397 head B batch: 100 avg loss -2.217001 avg loss no lamb -2.217001 time 2019-02-26 14:48:41.106649
Model ind 685 epoch 397 head B batch: 200 avg loss -2.222015 avg loss no lamb -2.222015 time 2019-02-26 14:49:58.323736
Model ind 685 epoch 397 head B batch: 300 avg loss -2.232816 avg loss no lamb -2.232816 time 2019-02-26 14:51:18.005937
Model ind 685 epoch 397 head B batch: 400 avg loss -2.172661 avg loss no lamb -2.172661 time 2019-02-26 14:52:36.184606
Model ind 685 epoch 397 head A batch: 0 avg loss -2.210056 avg loss no lamb -2.210056 time 2019-02-26 14:53:53.828978
Model ind 685 epoch 397 head A batch: 100 avg loss -2.153702 avg loss no lamb -2.153702 time 2019-02-26 14:55:13.526055
Model ind 685 epoch 397 head A batch: 200 avg loss -2.213681 avg loss no lamb -2.213681 time 2019-02-26 14:56:31.641974
Model ind 685 epoch 397 head A batch: 300 avg loss -2.250628 avg loss no lamb -2.250628 time 2019-02-26 14:57:51.140818
Model ind 685 epoch 397 head A batch: 400 avg loss -2.199636 avg loss no lamb -2.199636 time 2019-02-26 14:59:09.427425
Pre: time 2019-02-26 15:00:43.206059: 
 	std: 0.0064709987
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434
	best: 0.9924286

Starting e_i: 398
Model ind 685 epoch 398 head B batch: 0 avg loss -2.219235 avg loss no lamb -2.219235 time 2019-02-26 15:00:45.459645
Model ind 685 epoch 398 head B batch: 100 avg loss -2.219206 avg loss no lamb -2.219206 time 2019-02-26 15:02:04.264329
Model ind 685 epoch 398 head B batch: 200 avg loss -2.245245 avg loss no lamb -2.245245 time 2019-02-26 15:03:22.711927
Model ind 685 epoch 398 head B batch: 300 avg loss -2.198475 avg loss no lamb -2.198475 time 2019-02-26 15:04:41.939722
Model ind 685 epoch 398 head B batch: 400 avg loss -2.213106 avg loss no lamb -2.213106 time 2019-02-26 15:06:03.220596
Model ind 685 epoch 398 head B batch: 0 avg loss -2.230561 avg loss no lamb -2.230561 time 2019-02-26 15:07:19.590872
Model ind 685 epoch 398 head B batch: 100 avg loss -2.173587 avg loss no lamb -2.173587 time 2019-02-26 15:08:35.973407
Model ind 685 epoch 398 head B batch: 200 avg loss -2.213444 avg loss no lamb -2.213444 time 2019-02-26 15:09:55.014895
Model ind 685 epoch 398 head B batch: 300 avg loss -2.185158 avg loss no lamb -2.185158 time 2019-02-26 15:11:13.612456
Model ind 685 epoch 398 head B batch: 400 avg loss -2.216580 avg loss no lamb -2.216580 time 2019-02-26 15:12:31.584136
Model ind 685 epoch 398 head A batch: 0 avg loss -2.219258 avg loss no lamb -2.219258 time 2019-02-26 15:13:51.643292
Model ind 685 epoch 398 head A batch: 100 avg loss -2.186028 avg loss no lamb -2.186028 time 2019-02-26 15:15:10.608210
Model ind 685 epoch 398 head A batch: 200 avg loss -2.234909 avg loss no lamb -2.234909 time 2019-02-26 15:16:27.697059
Model ind 685 epoch 398 head A batch: 300 avg loss -2.195712 avg loss no lamb -2.195712 time 2019-02-26 15:17:45.552968
Model ind 685 epoch 398 head A batch: 400 avg loss -2.189789 avg loss no lamb -2.189789 time 2019-02-26 15:19:01.961974
Pre: time 2019-02-26 15:20:34.924900: 
 	std: 0.006481277
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.99207145, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.99207145, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842771
	best: 0.99235713

Starting e_i: 399
Model ind 685 epoch 399 head B batch: 0 avg loss -2.220151 avg loss no lamb -2.220151 time 2019-02-26 15:20:37.118256
Model ind 685 epoch 399 head B batch: 100 avg loss -2.201838 avg loss no lamb -2.201838 time 2019-02-26 15:21:54.782299
Model ind 685 epoch 399 head B batch: 200 avg loss -2.195878 avg loss no lamb -2.195878 time 2019-02-26 15:23:12.355654
Model ind 685 epoch 399 head B batch: 300 avg loss -2.230984 avg loss no lamb -2.230984 time 2019-02-26 15:24:29.477029
Model ind 685 epoch 399 head B batch: 400 avg loss -2.165480 avg loss no lamb -2.165480 time 2019-02-26 15:25:47.558760
Model ind 685 epoch 399 head B batch: 0 avg loss -2.198583 avg loss no lamb -2.198583 time 2019-02-26 15:27:05.576418
Model ind 685 epoch 399 head B batch: 100 avg loss -2.212643 avg loss no lamb -2.212643 time 2019-02-26 15:28:22.804000
Model ind 685 epoch 399 head B batch: 200 avg loss -2.185637 avg loss no lamb -2.185637 time 2019-02-26 15:29:39.630393
Model ind 685 epoch 399 head B batch: 300 avg loss -2.224587 avg loss no lamb -2.224587 time 2019-02-26 15:30:54.960470
Model ind 685 epoch 399 head B batch: 400 avg loss -2.228065 avg loss no lamb -2.228065 time 2019-02-26 15:32:12.930154
Model ind 685 epoch 399 head A batch: 0 avg loss -2.222045 avg loss no lamb -2.222045 time 2019-02-26 15:33:32.252786
Model ind 685 epoch 399 head A batch: 100 avg loss -2.193305 avg loss no lamb -2.193305 time 2019-02-26 15:34:51.124244
Model ind 685 epoch 399 head A batch: 200 avg loss -2.197584 avg loss no lamb -2.197584 time 2019-02-26 15:36:08.647312
Model ind 685 epoch 399 head A batch: 300 avg loss -2.217395 avg loss no lamb -2.217395 time 2019-02-26 15:37:27.522055
Model ind 685 epoch 399 head A batch: 400 avg loss -2.186204 avg loss no lamb -2.186204 time 2019-02-26 15:38:44.449769
Pre: time 2019-02-26 15:40:19.040027: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.97905713, 0.99214286, 0.97905713, 0.97905713]
	train_accs: [0.99244285, 0.97905713, 0.99214286, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98435146
	best: 0.99244285

Starting e_i: 400
Model ind 685 epoch 400 head B batch: 0 avg loss -2.198252 avg loss no lamb -2.198252 time 2019-02-26 15:40:21.707799
Model ind 685 epoch 400 head B batch: 100 avg loss -2.255936 avg loss no lamb -2.255936 time 2019-02-26 15:41:40.927869
Model ind 685 epoch 400 head B batch: 200 avg loss -2.240475 avg loss no lamb -2.240475 time 2019-02-26 15:42:59.365577
Model ind 685 epoch 400 head B batch: 300 avg loss -2.214480 avg loss no lamb -2.214480 time 2019-02-26 15:44:19.782773
Model ind 685 epoch 400 head B batch: 400 avg loss -2.210912 avg loss no lamb -2.210912 time 2019-02-26 15:45:37.184209
Model ind 685 epoch 400 head B batch: 0 avg loss -2.244961 avg loss no lamb -2.244961 time 2019-02-26 15:46:56.505741
Model ind 685 epoch 400 head B batch: 100 avg loss -2.201076 avg loss no lamb -2.201076 time 2019-02-26 15:48:13.868844
Model ind 685 epoch 400 head B batch: 200 avg loss -2.202074 avg loss no lamb -2.202074 time 2019-02-26 15:49:31.072992
Model ind 685 epoch 400 head B batch: 300 avg loss -2.198219 avg loss no lamb -2.198219 time 2019-02-26 15:50:47.433700
Model ind 685 epoch 400 head B batch: 400 avg loss -2.201591 avg loss no lamb -2.201591 time 2019-02-26 15:52:01.992987
Model ind 685 epoch 400 head A batch: 0 avg loss -2.232997 avg loss no lamb -2.232997 time 2019-02-26 15:53:20.063427
Model ind 685 epoch 400 head A batch: 100 avg loss -2.214145 avg loss no lamb -2.214145 time 2019-02-26 15:54:38.996019
Model ind 685 epoch 400 head A batch: 200 avg loss -2.205468 avg loss no lamb -2.205468 time 2019-02-26 15:55:58.253363
Model ind 685 epoch 400 head A batch: 300 avg loss -2.218188 avg loss no lamb -2.218188 time 2019-02-26 15:57:17.552393
Model ind 685 epoch 400 head A batch: 400 avg loss -2.161744 avg loss no lamb -2.161744 time 2019-02-26 15:58:35.773361
Pre: time 2019-02-26 16:00:11.016829: 
 	std: 0.0064709987
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.9921, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434
	best: 0.9924286

Starting e_i: 401
Model ind 685 epoch 401 head B batch: 0 avg loss -2.217379 avg loss no lamb -2.217379 time 2019-02-26 16:00:14.740899
Model ind 685 epoch 401 head B batch: 100 avg loss -2.205017 avg loss no lamb -2.205017 time 2019-02-26 16:01:31.618978
Model ind 685 epoch 401 head B batch: 200 avg loss -2.219177 avg loss no lamb -2.219177 time 2019-02-26 16:02:49.948210
Model ind 685 epoch 401 head B batch: 300 avg loss -2.232481 avg loss no lamb -2.232481 time 2019-02-26 16:04:07.798074
Model ind 685 epoch 401 head B batch: 400 avg loss -2.191023 avg loss no lamb -2.191023 time 2019-02-26 16:05:26.913749
Model ind 685 epoch 401 head B batch: 0 avg loss -2.209211 avg loss no lamb -2.209211 time 2019-02-26 16:06:44.935839
Model ind 685 epoch 401 head B batch: 100 avg loss -2.214399 avg loss no lamb -2.214399 time 2019-02-26 16:08:02.054502
Model ind 685 epoch 401 head B batch: 200 avg loss -2.181154 avg loss no lamb -2.181154 time 2019-02-26 16:09:20.013148
Model ind 685 epoch 401 head B batch: 300 avg loss -2.237697 avg loss no lamb -2.237697 time 2019-02-26 16:10:39.986813
Model ind 685 epoch 401 head B batch: 400 avg loss -2.200712 avg loss no lamb -2.200712 time 2019-02-26 16:12:02.620909
Model ind 685 epoch 401 head A batch: 0 avg loss -2.240831 avg loss no lamb -2.240831 time 2019-02-26 16:13:19.126421
Model ind 685 epoch 401 head A batch: 100 avg loss -2.243632 avg loss no lamb -2.243632 time 2019-02-26 16:14:36.366787
Model ind 685 epoch 401 head A batch: 200 avg loss -2.207226 avg loss no lamb -2.207226 time 2019-02-26 16:15:54.962785
Model ind 685 epoch 401 head A batch: 300 avg loss -2.219417 avg loss no lamb -2.219417 time 2019-02-26 16:17:14.677429
Model ind 685 epoch 401 head A batch: 400 avg loss -2.231899 avg loss no lamb -2.231899 time 2019-02-26 16:18:32.441180
Pre: time 2019-02-26 16:20:03.974787: 
 	std: 0.00648499
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9920857, 0.9790143, 0.979]
	train_accs: [0.9924143, 0.9790286, 0.9920857, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 402
Model ind 685 epoch 402 head B batch: 0 avg loss -2.238863 avg loss no lamb -2.238863 time 2019-02-26 16:20:06.286694
Model ind 685 epoch 402 head B batch: 100 avg loss -2.186110 avg loss no lamb -2.186110 time 2019-02-26 16:21:25.535436
Model ind 685 epoch 402 head B batch: 200 avg loss -2.214558 avg loss no lamb -2.214558 time 2019-02-26 16:22:45.097806
Model ind 685 epoch 402 head B batch: 300 avg loss -2.205266 avg loss no lamb -2.205266 time 2019-02-26 16:24:03.762230
Model ind 685 epoch 402 head B batch: 400 avg loss -2.206446 avg loss no lamb -2.206446 time 2019-02-26 16:25:22.379757
Model ind 685 epoch 402 head B batch: 0 avg loss -2.219576 avg loss no lamb -2.219576 time 2019-02-26 16:26:41.659200
Model ind 685 epoch 402 head B batch: 100 avg loss -2.220787 avg loss no lamb -2.220787 time 2019-02-26 16:28:01.520215
Model ind 685 epoch 402 head B batch: 200 avg loss -2.229471 avg loss no lamb -2.229471 time 2019-02-26 16:29:19.470578
Model ind 685 epoch 402 head B batch: 300 avg loss -2.219675 avg loss no lamb -2.219675 time 2019-02-26 16:30:36.198016
Model ind 685 epoch 402 head B batch: 400 avg loss -2.191053 avg loss no lamb -2.191053 time 2019-02-26 16:31:53.681367
Model ind 685 epoch 402 head A batch: 0 avg loss -2.233502 avg loss no lamb -2.233502 time 2019-02-26 16:33:12.171427
Model ind 685 epoch 402 head A batch: 100 avg loss -2.212020 avg loss no lamb -2.212020 time 2019-02-26 16:34:26.714675
Model ind 685 epoch 402 head A batch: 200 avg loss -2.226705 avg loss no lamb -2.226705 time 2019-02-26 16:35:40.000205
Model ind 685 epoch 402 head A batch: 300 avg loss -2.209518 avg loss no lamb -2.209518 time 2019-02-26 16:36:59.905661
Model ind 685 epoch 402 head A batch: 400 avg loss -2.207519 avg loss no lamb -2.207519 time 2019-02-26 16:38:20.079645
Pre: time 2019-02-26 16:39:54.336553: 
 	std: 0.0064778673
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843314
	best: 0.9924143

Starting e_i: 403
Model ind 685 epoch 403 head B batch: 0 avg loss -2.218992 avg loss no lamb -2.218992 time 2019-02-26 16:39:56.446530
Model ind 685 epoch 403 head B batch: 100 avg loss -2.239783 avg loss no lamb -2.239783 time 2019-02-26 16:41:13.119048
Model ind 685 epoch 403 head B batch: 200 avg loss -2.199487 avg loss no lamb -2.199487 time 2019-02-26 16:42:30.323194
Model ind 685 epoch 403 head B batch: 300 avg loss -2.218611 avg loss no lamb -2.218611 time 2019-02-26 16:43:46.634892
Model ind 685 epoch 403 head B batch: 400 avg loss -2.175978 avg loss no lamb -2.175978 time 2019-02-26 16:45:02.395064
Model ind 685 epoch 403 head B batch: 0 avg loss -2.209258 avg loss no lamb -2.209258 time 2019-02-26 16:46:19.172723
Model ind 685 epoch 403 head B batch: 100 avg loss -2.210463 avg loss no lamb -2.210463 time 2019-02-26 16:47:38.043132
Model ind 685 epoch 403 head B batch: 200 avg loss -2.209501 avg loss no lamb -2.209501 time 2019-02-26 16:48:55.333851
Model ind 685 epoch 403 head B batch: 300 avg loss -2.253338 avg loss no lamb -2.253338 time 2019-02-26 16:50:12.887824
Model ind 685 epoch 403 head B batch: 400 avg loss -2.202872 avg loss no lamb -2.202872 time 2019-02-26 16:51:31.230909
Model ind 685 epoch 403 head A batch: 0 avg loss -2.233778 avg loss no lamb -2.233778 time 2019-02-26 16:52:51.776248
Model ind 685 epoch 403 head A batch: 100 avg loss -2.219802 avg loss no lamb -2.219802 time 2019-02-26 16:54:12.275187
Model ind 685 epoch 403 head A batch: 200 avg loss -2.209587 avg loss no lamb -2.209587 time 2019-02-26 16:55:32.679945
Model ind 685 epoch 403 head A batch: 300 avg loss -2.229681 avg loss no lamb -2.229681 time 2019-02-26 16:56:52.956553
Model ind 685 epoch 403 head A batch: 400 avg loss -2.232455 avg loss no lamb -2.232455 time 2019-02-26 16:58:14.908528
Pre: time 2019-02-26 16:59:54.967586: 
 	std: 0.0064849835
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924286

Starting e_i: 404
Model ind 685 epoch 404 head B batch: 0 avg loss -2.219146 avg loss no lamb -2.219146 time 2019-02-26 16:59:57.207841
Model ind 685 epoch 404 head B batch: 100 avg loss -2.256306 avg loss no lamb -2.256306 time 2019-02-26 17:01:22.849881
Model ind 685 epoch 404 head B batch: 200 avg loss -2.179347 avg loss no lamb -2.179347 time 2019-02-26 17:02:46.581248
Model ind 685 epoch 404 head B batch: 300 avg loss -2.228391 avg loss no lamb -2.228391 time 2019-02-26 17:04:09.091583
Model ind 685 epoch 404 head B batch: 400 avg loss -2.194865 avg loss no lamb -2.194865 time 2019-02-26 17:05:29.760371
Model ind 685 epoch 404 head B batch: 0 avg loss -2.208521 avg loss no lamb -2.208521 time 2019-02-26 17:06:50.959670
Model ind 685 epoch 404 head B batch: 100 avg loss -2.233692 avg loss no lamb -2.233692 time 2019-02-26 17:08:12.081804
Model ind 685 epoch 404 head B batch: 200 avg loss -2.236151 avg loss no lamb -2.236151 time 2019-02-26 17:09:32.159774
Model ind 685 epoch 404 head B batch: 300 avg loss -2.252198 avg loss no lamb -2.252198 time 2019-02-26 17:10:54.386478
Model ind 685 epoch 404 head B batch: 400 avg loss -2.207270 avg loss no lamb -2.207270 time 2019-02-26 17:12:16.739209
Model ind 685 epoch 404 head A batch: 0 avg loss -2.226649 avg loss no lamb -2.226649 time 2019-02-26 17:13:37.996502
Model ind 685 epoch 404 head A batch: 100 avg loss -2.231601 avg loss no lamb -2.231601 time 2019-02-26 17:14:55.744514
Model ind 685 epoch 404 head A batch: 200 avg loss -2.224004 avg loss no lamb -2.224004 time 2019-02-26 17:16:14.955801
Model ind 685 epoch 404 head A batch: 300 avg loss -2.233421 avg loss no lamb -2.233421 time 2019-02-26 17:17:27.973423
Model ind 685 epoch 404 head A batch: 400 avg loss -2.205234 avg loss no lamb -2.205234 time 2019-02-26 17:18:47.778601
Pre: time 2019-02-26 17:20:22.984898: 
 	std: 0.0064814384
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.9921143, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.9843343
	best: 0.9924286

Starting e_i: 405
Model ind 685 epoch 405 head B batch: 0 avg loss -2.232293 avg loss no lamb -2.232293 time 2019-02-26 17:20:25.309100
Model ind 685 epoch 405 head B batch: 100 avg loss -2.213285 avg loss no lamb -2.213285 time 2019-02-26 17:21:45.634453
Model ind 685 epoch 405 head B batch: 200 avg loss -2.165446 avg loss no lamb -2.165446 time 2019-02-26 17:23:10.151555
Model ind 685 epoch 405 head B batch: 300 avg loss -2.228671 avg loss no lamb -2.228671 time 2019-02-26 17:24:28.166538
Model ind 685 epoch 405 head B batch: 400 avg loss -2.215268 avg loss no lamb -2.215268 time 2019-02-26 17:25:44.529710
Model ind 685 epoch 405 head B batch: 0 avg loss -2.245342 avg loss no lamb -2.245342 time 2019-02-26 17:27:04.528919
Model ind 685 epoch 405 head B batch: 100 avg loss -2.180691 avg loss no lamb -2.180691 time 2019-02-26 17:28:20.778322
Model ind 685 epoch 405 head B batch: 200 avg loss -2.216066 avg loss no lamb -2.216066 time 2019-02-26 17:29:37.152461
Model ind 685 epoch 405 head B batch: 300 avg loss -2.205468 avg loss no lamb -2.205468 time 2019-02-26 17:30:56.542003
Model ind 685 epoch 405 head B batch: 400 avg loss -2.199193 avg loss no lamb -2.199193 time 2019-02-26 17:32:11.735852
Model ind 685 epoch 405 head A batch: 0 avg loss -2.231399 avg loss no lamb -2.231399 time 2019-02-26 17:33:30.003446
Model ind 685 epoch 405 head A batch: 100 avg loss -2.220309 avg loss no lamb -2.220309 time 2019-02-26 17:34:49.530145
Model ind 685 epoch 405 head A batch: 200 avg loss -2.211843 avg loss no lamb -2.211843 time 2019-02-26 17:36:05.555557
Model ind 685 epoch 405 head A batch: 300 avg loss -2.216720 avg loss no lamb -2.216720 time 2019-02-26 17:37:22.550046
Model ind 685 epoch 405 head A batch: 400 avg loss -2.222331 avg loss no lamb -2.222331 time 2019-02-26 17:38:40.506620
Pre: time 2019-02-26 17:40:12.890263: 
 	std: 0.006474561
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	train_accs: [0.99235713, 0.9789714, 0.9920143, 0.9789714, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842571
	best: 0.99235713

Starting e_i: 406
Model ind 685 epoch 406 head B batch: 0 avg loss -2.194331 avg loss no lamb -2.194331 time 2019-02-26 17:40:15.064988
Model ind 685 epoch 406 head B batch: 100 avg loss -2.213232 avg loss no lamb -2.213232 time 2019-02-26 17:41:36.449891
Model ind 685 epoch 406 head B batch: 200 avg loss -2.231270 avg loss no lamb -2.231270 time 2019-02-26 17:42:51.991943
Model ind 685 epoch 406 head B batch: 300 avg loss -2.224663 avg loss no lamb -2.224663 time 2019-02-26 17:44:07.920406
Model ind 685 epoch 406 head B batch: 400 avg loss -2.225475 avg loss no lamb -2.225475 time 2019-02-26 17:45:27.132365
Model ind 685 epoch 406 head B batch: 0 avg loss -2.199833 avg loss no lamb -2.199833 time 2019-02-26 17:46:44.598267
Model ind 685 epoch 406 head B batch: 100 avg loss -2.229004 avg loss no lamb -2.229004 time 2019-02-26 17:48:01.847290
Model ind 685 epoch 406 head B batch: 200 avg loss -2.217007 avg loss no lamb -2.217007 time 2019-02-26 17:49:21.119914
Model ind 685 epoch 406 head B batch: 300 avg loss -2.250266 avg loss no lamb -2.250266 time 2019-02-26 17:50:47.330236
Model ind 685 epoch 406 head B batch: 400 avg loss -2.213570 avg loss no lamb -2.213570 time 2019-02-26 17:52:05.575326
Model ind 685 epoch 406 head A batch: 0 avg loss -2.220012 avg loss no lamb -2.220012 time 2019-02-26 17:53:24.727062
Model ind 685 epoch 406 head A batch: 100 avg loss -2.195745 avg loss no lamb -2.195745 time 2019-02-26 17:54:41.598741
Model ind 685 epoch 406 head A batch: 200 avg loss -2.227343 avg loss no lamb -2.227343 time 2019-02-26 17:55:56.347464
Model ind 685 epoch 406 head A batch: 300 avg loss -2.238602 avg loss no lamb -2.238602 time 2019-02-26 17:57:12.683196
Model ind 685 epoch 406 head A batch: 400 avg loss -2.192778 avg loss no lamb -2.192778 time 2019-02-26 17:58:22.693292
Pre: time 2019-02-26 17:59:52.017965: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.9924

Starting e_i: 407
Model ind 685 epoch 407 head B batch: 0 avg loss -2.216567 avg loss no lamb -2.216567 time 2019-02-26 17:59:54.217454
Model ind 685 epoch 407 head B batch: 100 avg loss -2.220282 avg loss no lamb -2.220282 time 2019-02-26 18:01:10.475795
Model ind 685 epoch 407 head B batch: 200 avg loss -2.236281 avg loss no lamb -2.236281 time 2019-02-26 18:02:24.598460
Model ind 685 epoch 407 head B batch: 300 avg loss -2.213031 avg loss no lamb -2.213031 time 2019-02-26 18:03:46.458021
Model ind 685 epoch 407 head B batch: 400 avg loss -2.202199 avg loss no lamb -2.202199 time 2019-02-26 18:05:07.836321
Model ind 685 epoch 407 head B batch: 0 avg loss -2.199398 avg loss no lamb -2.199398 time 2019-02-26 18:06:29.370388
Model ind 685 epoch 407 head B batch: 100 avg loss -2.187973 avg loss no lamb -2.187973 time 2019-02-26 18:07:51.796979
Model ind 685 epoch 407 head B batch: 200 avg loss -2.191243 avg loss no lamb -2.191243 time 2019-02-26 18:09:13.883773
Model ind 685 epoch 407 head B batch: 300 avg loss -2.228256 avg loss no lamb -2.228256 time 2019-02-26 18:10:32.917900
Model ind 685 epoch 407 head B batch: 400 avg loss -2.199113 avg loss no lamb -2.199113 time 2019-02-26 18:11:52.518041
Model ind 685 epoch 407 head A batch: 0 avg loss -2.212104 avg loss no lamb -2.212104 time 2019-02-26 18:13:14.239843
Model ind 685 epoch 407 head A batch: 100 avg loss -2.188530 avg loss no lamb -2.188530 time 2019-02-26 18:14:35.061862
Model ind 685 epoch 407 head A batch: 200 avg loss -2.196709 avg loss no lamb -2.196709 time 2019-02-26 18:15:53.168221
Model ind 685 epoch 407 head A batch: 300 avg loss -2.240951 avg loss no lamb -2.240951 time 2019-02-26 18:17:15.099420
Model ind 685 epoch 407 head A batch: 400 avg loss -2.193380 avg loss no lamb -2.193380 time 2019-02-26 18:18:33.766991
Pre: time 2019-02-26 18:20:10.315637: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 408
Model ind 685 epoch 408 head B batch: 0 avg loss -2.187563 avg loss no lamb -2.187563 time 2019-02-26 18:20:12.389447
Model ind 685 epoch 408 head B batch: 100 avg loss -2.191211 avg loss no lamb -2.191211 time 2019-02-26 18:21:30.325527
Model ind 685 epoch 408 head B batch: 200 avg loss -2.185454 avg loss no lamb -2.185454 time 2019-02-26 18:22:45.996747
Model ind 685 epoch 408 head B batch: 300 avg loss -2.194407 avg loss no lamb -2.194407 time 2019-02-26 18:24:04.081322
Model ind 685 epoch 408 head B batch: 400 avg loss -2.185122 avg loss no lamb -2.185122 time 2019-02-26 18:25:18.542673
Model ind 685 epoch 408 head B batch: 0 avg loss -2.200684 avg loss no lamb -2.200684 time 2019-02-26 18:26:36.941338
Model ind 685 epoch 408 head B batch: 100 avg loss -2.195653 avg loss no lamb -2.195653 time 2019-02-26 18:27:54.769964
Model ind 685 epoch 408 head B batch: 200 avg loss -2.227569 avg loss no lamb -2.227569 time 2019-02-26 18:29:11.417857
Model ind 685 epoch 408 head B batch: 300 avg loss -2.222642 avg loss no lamb -2.222642 time 2019-02-26 18:30:30.049482
Model ind 685 epoch 408 head B batch: 400 avg loss -2.212995 avg loss no lamb -2.212995 time 2019-02-26 18:31:50.178049
Model ind 685 epoch 408 head A batch: 0 avg loss -2.177689 avg loss no lamb -2.177689 time 2019-02-26 18:33:06.746579
Model ind 685 epoch 408 head A batch: 100 avg loss -2.205392 avg loss no lamb -2.205392 time 2019-02-26 18:34:24.001379
Model ind 685 epoch 408 head A batch: 200 avg loss -2.230434 avg loss no lamb -2.230434 time 2019-02-26 18:35:43.199918
Model ind 685 epoch 408 head A batch: 300 avg loss -2.228448 avg loss no lamb -2.228448 time 2019-02-26 18:36:59.162029
Model ind 685 epoch 408 head A batch: 400 avg loss -2.178774 avg loss no lamb -2.178774 time 2019-02-26 18:38:14.177192
Pre: time 2019-02-26 18:39:47.850408: 
 	std: 0.006491991
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924286

Starting e_i: 409
Model ind 685 epoch 409 head B batch: 0 avg loss -2.225189 avg loss no lamb -2.225189 time 2019-02-26 18:39:50.235797
Model ind 685 epoch 409 head B batch: 100 avg loss -2.195644 avg loss no lamb -2.195644 time 2019-02-26 18:40:52.958862
Model ind 685 epoch 409 head B batch: 200 avg loss -2.243467 avg loss no lamb -2.243467 time 2019-02-26 18:42:12.539906
Model ind 685 epoch 409 head B batch: 300 avg loss -2.237931 avg loss no lamb -2.237931 time 2019-02-26 18:43:32.358737
Model ind 685 epoch 409 head B batch: 400 avg loss -2.224584 avg loss no lamb -2.224584 time 2019-02-26 18:44:48.324703
Model ind 685 epoch 409 head B batch: 0 avg loss -2.228435 avg loss no lamb -2.228435 time 2019-02-26 18:46:05.746833
Model ind 685 epoch 409 head B batch: 100 avg loss -2.205844 avg loss no lamb -2.205844 time 2019-02-26 18:47:24.002887
Model ind 685 epoch 409 head B batch: 200 avg loss -2.202881 avg loss no lamb -2.202881 time 2019-02-26 18:48:42.089612
Model ind 685 epoch 409 head B batch: 300 avg loss -2.240322 avg loss no lamb -2.240322 time 2019-02-26 18:50:00.595864
Model ind 685 epoch 409 head B batch: 400 avg loss -2.205119 avg loss no lamb -2.205119 time 2019-02-26 18:51:20.772529
Model ind 685 epoch 409 head A batch: 0 avg loss -2.218870 avg loss no lamb -2.218870 time 2019-02-26 18:52:37.625017
Model ind 685 epoch 409 head A batch: 100 avg loss -2.203340 avg loss no lamb -2.203340 time 2019-02-26 18:53:55.320069
Model ind 685 epoch 409 head A batch: 200 avg loss -2.209290 avg loss no lamb -2.209290 time 2019-02-26 18:55:11.920917
Model ind 685 epoch 409 head A batch: 300 avg loss -2.246974 avg loss no lamb -2.246974 time 2019-02-26 18:56:28.197576
Model ind 685 epoch 409 head A batch: 400 avg loss -2.209810 avg loss no lamb -2.209810 time 2019-02-26 18:57:43.901518
Pre: time 2019-02-26 18:59:12.745630: 
 	std: 0.006500097
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.979, 0.9921143, 0.9790143, 0.979]
	train_accs: [0.9924286, 0.979, 0.9921143, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98431146
	best: 0.9924286

Starting e_i: 410
Model ind 685 epoch 410 head B batch: 0 avg loss -2.243263 avg loss no lamb -2.243263 time 2019-02-26 18:59:15.077119
Model ind 685 epoch 410 head B batch: 100 avg loss -2.186459 avg loss no lamb -2.186459 time 2019-02-26 19:00:31.370844
Model ind 685 epoch 410 head B batch: 200 avg loss -2.167768 avg loss no lamb -2.167768 time 2019-02-26 19:01:49.335237
Model ind 685 epoch 410 head B batch: 300 avg loss -2.215116 avg loss no lamb -2.215116 time 2019-02-26 19:03:04.615670
Model ind 685 epoch 410 head B batch: 400 avg loss -2.201011 avg loss no lamb -2.201011 time 2019-02-26 19:04:17.159595
Model ind 685 epoch 410 head B batch: 0 avg loss -2.242755 avg loss no lamb -2.242755 time 2019-02-26 19:05:36.978719
Model ind 685 epoch 410 head B batch: 100 avg loss -2.227517 avg loss no lamb -2.227517 time 2019-02-26 19:06:55.509063
Model ind 685 epoch 410 head B batch: 200 avg loss -2.214301 avg loss no lamb -2.214301 time 2019-02-26 19:08:12.831487
Model ind 685 epoch 410 head B batch: 300 avg loss -2.258092 avg loss no lamb -2.258092 time 2019-02-26 19:09:30.747064
Model ind 685 epoch 410 head B batch: 400 avg loss -2.192139 avg loss no lamb -2.192139 time 2019-02-26 19:10:51.392238
Model ind 685 epoch 410 head A batch: 0 avg loss -2.179607 avg loss no lamb -2.179607 time 2019-02-26 19:12:06.711976
Model ind 685 epoch 410 head A batch: 100 avg loss -2.208924 avg loss no lamb -2.208924 time 2019-02-26 19:13:25.666070
Model ind 685 epoch 410 head A batch: 200 avg loss -2.193683 avg loss no lamb -2.193683 time 2019-02-26 19:14:41.975393
Model ind 685 epoch 410 head A batch: 300 avg loss -2.251352 avg loss no lamb -2.251352 time 2019-02-26 19:15:58.917208
Model ind 685 epoch 410 head A batch: 400 avg loss -2.187223 avg loss no lamb -2.187223 time 2019-02-26 19:17:16.965912
Pre: time 2019-02-26 19:18:49.742576: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 411
Model ind 685 epoch 411 head B batch: 0 avg loss -2.223382 avg loss no lamb -2.223382 time 2019-02-26 19:18:51.838213
Model ind 685 epoch 411 head B batch: 100 avg loss -2.234525 avg loss no lamb -2.234525 time 2019-02-26 19:20:05.853950
Model ind 685 epoch 411 head B batch: 200 avg loss -2.169706 avg loss no lamb -2.169706 time 2019-02-26 19:21:23.582441
Model ind 685 epoch 411 head B batch: 300 avg loss -2.214047 avg loss no lamb -2.214047 time 2019-02-26 19:22:40.296773
Model ind 685 epoch 411 head B batch: 400 avg loss -2.211265 avg loss no lamb -2.211265 time 2019-02-26 19:23:50.644283
Model ind 685 epoch 411 head B batch: 0 avg loss -2.211680 avg loss no lamb -2.211680 time 2019-02-26 19:25:09.270636
Model ind 685 epoch 411 head B batch: 100 avg loss -2.203309 avg loss no lamb -2.203309 time 2019-02-26 19:26:26.265371
Model ind 685 epoch 411 head B batch: 200 avg loss -2.221598 avg loss no lamb -2.221598 time 2019-02-26 19:27:43.303917
Model ind 685 epoch 411 head B batch: 300 avg loss -2.188843 avg loss no lamb -2.188843 time 2019-02-26 19:29:00.957429
Model ind 685 epoch 411 head B batch: 400 avg loss -2.174816 avg loss no lamb -2.174816 time 2019-02-26 19:30:16.233464
Model ind 685 epoch 411 head A batch: 0 avg loss -2.210904 avg loss no lamb -2.210904 time 2019-02-26 19:31:35.510148
Model ind 685 epoch 411 head A batch: 100 avg loss -2.213515 avg loss no lamb -2.213515 time 2019-02-26 19:32:54.896843
Model ind 685 epoch 411 head A batch: 200 avg loss -2.220964 avg loss no lamb -2.220964 time 2019-02-26 19:34:13.217713
Model ind 685 epoch 411 head A batch: 300 avg loss -2.201787 avg loss no lamb -2.201787 time 2019-02-26 19:35:25.637750
Model ind 685 epoch 411 head A batch: 400 avg loss -2.172516 avg loss no lamb -2.172516 time 2019-02-26 19:36:40.086748
Pre: time 2019-02-26 19:38:10.650873: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 412
Model ind 685 epoch 412 head B batch: 0 avg loss -2.220213 avg loss no lamb -2.220213 time 2019-02-26 19:38:12.514457
Model ind 685 epoch 412 head B batch: 100 avg loss -2.187273 avg loss no lamb -2.187273 time 2019-02-26 19:39:27.887129
Model ind 685 epoch 412 head B batch: 200 avg loss -2.186441 avg loss no lamb -2.186441 time 2019-02-26 19:40:43.320034
Model ind 685 epoch 412 head B batch: 300 avg loss -2.226045 avg loss no lamb -2.226045 time 2019-02-26 19:41:56.455683
Model ind 685 epoch 412 head B batch: 400 avg loss -2.180832 avg loss no lamb -2.180832 time 2019-02-26 19:43:18.251195
Model ind 685 epoch 412 head B batch: 0 avg loss -2.234296 avg loss no lamb -2.234296 time 2019-02-26 19:44:36.296598
Model ind 685 epoch 412 head B batch: 100 avg loss -2.186745 avg loss no lamb -2.186745 time 2019-02-26 19:45:55.284360
Model ind 685 epoch 412 head B batch: 200 avg loss -2.183753 avg loss no lamb -2.183753 time 2019-02-26 19:47:15.181236
Model ind 685 epoch 412 head B batch: 300 avg loss -2.243381 avg loss no lamb -2.243381 time 2019-02-26 19:48:32.482267
Model ind 685 epoch 412 head B batch: 400 avg loss -2.192768 avg loss no lamb -2.192768 time 2019-02-26 19:49:52.962757
Model ind 685 epoch 412 head A batch: 0 avg loss -2.212874 avg loss no lamb -2.212874 time 2019-02-26 19:51:08.262814
Model ind 685 epoch 412 head A batch: 100 avg loss -2.248926 avg loss no lamb -2.248926 time 2019-02-26 19:52:27.257040
Model ind 685 epoch 412 head A batch: 200 avg loss -2.192759 avg loss no lamb -2.192759 time 2019-02-26 19:53:41.146752
Model ind 685 epoch 412 head A batch: 300 avg loss -2.252647 avg loss no lamb -2.252647 time 2019-02-26 19:54:54.590064
Model ind 685 epoch 412 head A batch: 400 avg loss -2.165427 avg loss no lamb -2.165427 time 2019-02-26 19:56:12.526846
Pre: time 2019-02-26 19:57:48.135021: 
 	std: 0.006483615
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921143, 0.9790143, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921143, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924

Starting e_i: 413
Model ind 685 epoch 413 head B batch: 0 avg loss -2.194597 avg loss no lamb -2.194597 time 2019-02-26 19:57:50.191768
Model ind 685 epoch 413 head B batch: 100 avg loss -2.201661 avg loss no lamb -2.201661 time 2019-02-26 19:59:05.928205
Model ind 685 epoch 413 head B batch: 200 avg loss -2.247638 avg loss no lamb -2.247638 time 2019-02-26 20:00:22.699523
Model ind 685 epoch 413 head B batch: 300 avg loss -2.228980 avg loss no lamb -2.228980 time 2019-02-26 20:01:35.229106
Model ind 685 epoch 413 head B batch: 400 avg loss -2.150573 avg loss no lamb -2.150573 time 2019-02-26 20:02:53.056857
Model ind 685 epoch 413 head B batch: 0 avg loss -2.220269 avg loss no lamb -2.220269 time 2019-02-26 20:04:12.947706
Model ind 685 epoch 413 head B batch: 100 avg loss -2.229633 avg loss no lamb -2.229633 time 2019-02-26 20:05:27.123393
Model ind 685 epoch 413 head B batch: 200 avg loss -2.214159 avg loss no lamb -2.214159 time 2019-02-26 20:06:45.025504
Model ind 685 epoch 413 head B batch: 300 avg loss -2.228940 avg loss no lamb -2.228940 time 2019-02-26 20:08:02.357554
Model ind 685 epoch 413 head B batch: 400 avg loss -2.160254 avg loss no lamb -2.160254 time 2019-02-26 20:09:20.238620
Model ind 685 epoch 413 head A batch: 0 avg loss -2.222405 avg loss no lamb -2.222405 time 2019-02-26 20:10:37.797573
Model ind 685 epoch 413 head A batch: 100 avg loss -2.199260 avg loss no lamb -2.199260 time 2019-02-26 20:11:54.027789
Model ind 685 epoch 413 head A batch: 200 avg loss -2.151808 avg loss no lamb -2.151808 time 2019-02-26 20:13:10.280733
Model ind 685 epoch 413 head A batch: 300 avg loss -2.213100 avg loss no lamb -2.213100 time 2019-02-26 20:14:28.807972
Model ind 685 epoch 413 head A batch: 400 avg loss -2.190479 avg loss no lamb -2.190479 time 2019-02-26 20:15:49.195732
Pre: time 2019-02-26 20:17:24.962858: 
 	std: 0.0064825113
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790286, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790286, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924

Starting e_i: 414
Model ind 685 epoch 414 head B batch: 0 avg loss -2.210562 avg loss no lamb -2.210562 time 2019-02-26 20:17:27.160561
Model ind 685 epoch 414 head B batch: 100 avg loss -2.217220 avg loss no lamb -2.217220 time 2019-02-26 20:18:46.658147
Model ind 685 epoch 414 head B batch: 200 avg loss -2.222751 avg loss no lamb -2.222751 time 2019-02-26 20:20:05.246044
Model ind 685 epoch 414 head B batch: 300 avg loss -2.250268 avg loss no lamb -2.250268 time 2019-02-26 20:21:25.454848
Model ind 685 epoch 414 head B batch: 400 avg loss -2.201033 avg loss no lamb -2.201033 time 2019-02-26 20:22:44.703795
Model ind 685 epoch 414 head B batch: 0 avg loss -2.248201 avg loss no lamb -2.248201 time 2019-02-26 20:24:00.526207
Model ind 685 epoch 414 head B batch: 100 avg loss -2.216053 avg loss no lamb -2.216053 time 2019-02-26 20:25:19.891015
Model ind 685 epoch 414 head B batch: 200 avg loss -2.226315 avg loss no lamb -2.226315 time 2019-02-26 20:26:39.460957
Model ind 685 epoch 414 head B batch: 300 avg loss -2.208854 avg loss no lamb -2.208854 time 2019-02-26 20:27:56.949181
Model ind 685 epoch 414 head B batch: 400 avg loss -2.210482 avg loss no lamb -2.210482 time 2019-02-26 20:29:10.648649
Model ind 685 epoch 414 head A batch: 0 avg loss -2.219966 avg loss no lamb -2.219966 time 2019-02-26 20:30:33.697626
Model ind 685 epoch 414 head A batch: 100 avg loss -2.201097 avg loss no lamb -2.201097 time 2019-02-26 20:31:53.850308
Model ind 685 epoch 414 head A batch: 200 avg loss -2.203254 avg loss no lamb -2.203254 time 2019-02-26 20:33:10.272042
Model ind 685 epoch 414 head A batch: 300 avg loss -2.224808 avg loss no lamb -2.224808 time 2019-02-26 20:34:26.969565
Model ind 685 epoch 414 head A batch: 400 avg loss -2.224546 avg loss no lamb -2.224546 time 2019-02-26 20:35:47.263540
Pre: time 2019-02-26 20:37:22.725458: 
 	std: 0.006495292
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921143, 0.979, 0.979]
	train_accs: [0.9924, 0.979, 0.9921143, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9924

Starting e_i: 415
Model ind 685 epoch 415 head B batch: 0 avg loss -2.222236 avg loss no lamb -2.222236 time 2019-02-26 20:37:24.859703
Model ind 685 epoch 415 head B batch: 100 avg loss -2.198427 avg loss no lamb -2.198427 time 2019-02-26 20:38:44.737195
Model ind 685 epoch 415 head B batch: 200 avg loss -2.192186 avg loss no lamb -2.192186 time 2019-02-26 20:40:00.956683
Model ind 685 epoch 415 head B batch: 300 avg loss -2.260722 avg loss no lamb -2.260722 time 2019-02-26 20:41:22.033174
Model ind 685 epoch 415 head B batch: 400 avg loss -2.197717 avg loss no lamb -2.197717 time 2019-02-26 20:42:36.342381
Model ind 685 epoch 415 head B batch: 0 avg loss -2.237189 avg loss no lamb -2.237189 time 2019-02-26 20:43:53.318123
Model ind 685 epoch 415 head B batch: 100 avg loss -2.207926 avg loss no lamb -2.207926 time 2019-02-26 20:45:11.511193
Model ind 685 epoch 415 head B batch: 200 avg loss -2.197528 avg loss no lamb -2.197528 time 2019-02-26 20:46:28.533723
Model ind 685 epoch 415 head B batch: 300 avg loss -2.243271 avg loss no lamb -2.243271 time 2019-02-26 20:47:32.233583
Model ind 685 epoch 415 head B batch: 400 avg loss -2.228643 avg loss no lamb -2.228643 time 2019-02-26 20:48:43.118022
Model ind 685 epoch 415 head A batch: 0 avg loss -2.225322 avg loss no lamb -2.225322 time 2019-02-26 20:50:03.660523
Model ind 685 epoch 415 head A batch: 100 avg loss -2.223693 avg loss no lamb -2.223693 time 2019-02-26 20:51:22.048005
Model ind 685 epoch 415 head A batch: 200 avg loss -2.185383 avg loss no lamb -2.185383 time 2019-02-26 20:52:43.444161
Model ind 685 epoch 415 head A batch: 300 avg loss -2.201872 avg loss no lamb -2.201872 time 2019-02-26 20:54:02.227820
Model ind 685 epoch 415 head A batch: 400 avg loss -2.190198 avg loss no lamb -2.190198 time 2019-02-26 20:55:19.624765
Pre: time 2019-02-26 20:56:49.077947: 
 	std: 0.0064930897
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9921, 0.979, 0.9790143]
	train_accs: [0.9924143, 0.979, 0.9921, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9924143

Starting e_i: 416
Model ind 685 epoch 416 head B batch: 0 avg loss -2.240418 avg loss no lamb -2.240418 time 2019-02-26 20:56:51.048005
Model ind 685 epoch 416 head B batch: 100 avg loss -2.223347 avg loss no lamb -2.223347 time 2019-02-26 20:58:10.598550
Model ind 685 epoch 416 head B batch: 200 avg loss -2.233555 avg loss no lamb -2.233555 time 2019-02-26 20:59:28.710837
Model ind 685 epoch 416 head B batch: 300 avg loss -2.173389 avg loss no lamb -2.173389 time 2019-02-26 21:00:47.859753
Model ind 685 epoch 416 head B batch: 400 avg loss -2.215911 avg loss no lamb -2.215911 time 2019-02-26 21:02:01.566141
Model ind 685 epoch 416 head B batch: 0 avg loss -2.234132 avg loss no lamb -2.234132 time 2019-02-26 21:03:20.095835
Model ind 685 epoch 416 head B batch: 100 avg loss -2.233225 avg loss no lamb -2.233225 time 2019-02-26 21:04:40.037423
Model ind 685 epoch 416 head B batch: 200 avg loss -2.180424 avg loss no lamb -2.180424 time 2019-02-26 21:06:00.806712
Model ind 685 epoch 416 head B batch: 300 avg loss -2.251585 avg loss no lamb -2.251585 time 2019-02-26 21:07:24.128611
Model ind 685 epoch 416 head B batch: 400 avg loss -2.210138 avg loss no lamb -2.210138 time 2019-02-26 21:08:44.821748
Model ind 685 epoch 416 head A batch: 0 avg loss -2.179302 avg loss no lamb -2.179302 time 2019-02-26 21:10:05.482636
Model ind 685 epoch 416 head A batch: 100 avg loss -2.210143 avg loss no lamb -2.210143 time 2019-02-26 21:11:25.387853
Model ind 685 epoch 416 head A batch: 200 avg loss -2.193041 avg loss no lamb -2.193041 time 2019-02-26 21:12:43.946830
Model ind 685 epoch 416 head A batch: 300 avg loss -2.268216 avg loss no lamb -2.268216 time 2019-02-26 21:13:58.943044
Model ind 685 epoch 416 head A batch: 400 avg loss -2.199126 avg loss no lamb -2.199126 time 2019-02-26 21:15:19.827328
Pre: time 2019-02-26 21:16:56.814316: 
 	std: 0.006487183
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9789857, 0.99207145, 0.9789857, 0.9789714]
	train_accs: [0.99237144, 0.9789857, 0.99207145, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842771
	best: 0.99237144

Starting e_i: 417
Model ind 685 epoch 417 head B batch: 0 avg loss -2.194345 avg loss no lamb -2.194345 time 2019-02-26 21:16:59.891143
Model ind 685 epoch 417 head B batch: 100 avg loss -2.221003 avg loss no lamb -2.221003 time 2019-02-26 21:18:21.801870
Model ind 685 epoch 417 head B batch: 200 avg loss -2.220787 avg loss no lamb -2.220787 time 2019-02-26 21:19:42.113451
Model ind 685 epoch 417 head B batch: 300 avg loss -2.276088 avg loss no lamb -2.276088 time 2019-02-26 21:21:01.226588
Model ind 685 epoch 417 head B batch: 400 avg loss -2.200219 avg loss no lamb -2.200219 time 2019-02-26 21:22:15.470044
Model ind 685 epoch 417 head B batch: 0 avg loss -2.232397 avg loss no lamb -2.232397 time 2019-02-26 21:23:34.218867
Model ind 685 epoch 417 head B batch: 100 avg loss -2.204349 avg loss no lamb -2.204349 time 2019-02-26 21:24:49.149749
Model ind 685 epoch 417 head B batch: 200 avg loss -2.177028 avg loss no lamb -2.177028 time 2019-02-26 21:26:06.572944
Model ind 685 epoch 417 head B batch: 300 avg loss -2.262087 avg loss no lamb -2.262087 time 2019-02-26 21:27:25.458981
Model ind 685 epoch 417 head B batch: 400 avg loss -2.230500 avg loss no lamb -2.230500 time 2019-02-26 21:28:45.033579
Model ind 685 epoch 417 head A batch: 0 avg loss -2.218462 avg loss no lamb -2.218462 time 2019-02-26 21:29:48.697402
Model ind 685 epoch 417 head A batch: 100 avg loss -2.228702 avg loss no lamb -2.228702 time 2019-02-26 21:31:10.159691
Model ind 685 epoch 417 head A batch: 200 avg loss -2.212846 avg loss no lamb -2.212846 time 2019-02-26 21:32:28.257133
Model ind 685 epoch 417 head A batch: 300 avg loss -2.201000 avg loss no lamb -2.201000 time 2019-02-26 21:33:43.919440
Model ind 685 epoch 417 head A batch: 400 avg loss -2.217404 avg loss no lamb -2.217404 time 2019-02-26 21:34:59.377073
Pre: time 2019-02-26 21:36:33.655891: 
 	std: 0.006495424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924286

Starting e_i: 418
Model ind 685 epoch 418 head B batch: 0 avg loss -2.218404 avg loss no lamb -2.218404 time 2019-02-26 21:36:35.611286
Model ind 685 epoch 418 head B batch: 100 avg loss -2.193345 avg loss no lamb -2.193345 time 2019-02-26 21:37:52.810888
Model ind 685 epoch 418 head B batch: 200 avg loss -2.223114 avg loss no lamb -2.223114 time 2019-02-26 21:39:10.930634
Model ind 685 epoch 418 head B batch: 300 avg loss -2.216601 avg loss no lamb -2.216601 time 2019-02-26 21:40:31.378679
Model ind 685 epoch 418 head B batch: 400 avg loss -2.194748 avg loss no lamb -2.194748 time 2019-02-26 21:41:46.295494
Model ind 685 epoch 418 head B batch: 0 avg loss -2.223666 avg loss no lamb -2.223666 time 2019-02-26 21:43:04.321970
Model ind 685 epoch 418 head B batch: 100 avg loss -2.233824 avg loss no lamb -2.233824 time 2019-02-26 21:44:24.429958
Model ind 685 epoch 418 head B batch: 200 avg loss -2.197469 avg loss no lamb -2.197469 time 2019-02-26 21:45:45.582844
Model ind 685 epoch 418 head B batch: 300 avg loss -2.225217 avg loss no lamb -2.225217 time 2019-02-26 21:47:06.858827
Model ind 685 epoch 418 head B batch: 400 avg loss -2.187079 avg loss no lamb -2.187079 time 2019-02-26 21:48:24.200438
Model ind 685 epoch 418 head A batch: 0 avg loss -2.241381 avg loss no lamb -2.241381 time 2019-02-26 21:49:38.308592
Model ind 685 epoch 418 head A batch: 100 avg loss -2.182914 avg loss no lamb -2.182914 time 2019-02-26 21:50:56.646181
Model ind 685 epoch 418 head A batch: 200 avg loss -2.255838 avg loss no lamb -2.255838 time 2019-02-26 21:52:16.755737
Model ind 685 epoch 418 head A batch: 300 avg loss -2.181250 avg loss no lamb -2.181250 time 2019-02-26 21:53:37.010981
Model ind 685 epoch 418 head A batch: 400 avg loss -2.210410 avg loss no lamb -2.210410 time 2019-02-26 21:55:00.488940
Pre: time 2019-02-26 21:56:34.871259: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 419
Model ind 685 epoch 419 head B batch: 0 avg loss -2.224835 avg loss no lamb -2.224835 time 2019-02-26 21:56:37.051021
Model ind 685 epoch 419 head B batch: 100 avg loss -2.233792 avg loss no lamb -2.233792 time 2019-02-26 21:57:57.512253
Model ind 685 epoch 419 head B batch: 200 avg loss -2.208864 avg loss no lamb -2.208864 time 2019-02-26 21:59:19.919560
Model ind 685 epoch 419 head B batch: 300 avg loss -2.212170 avg loss no lamb -2.212170 time 2019-02-26 22:00:39.883459
Model ind 685 epoch 419 head B batch: 400 avg loss -2.230085 avg loss no lamb -2.230085 time 2019-02-26 22:01:59.005563
Model ind 685 epoch 419 head B batch: 0 avg loss -2.215207 avg loss no lamb -2.215207 time 2019-02-26 22:03:13.812001
Model ind 685 epoch 419 head B batch: 100 avg loss -2.202116 avg loss no lamb -2.202116 time 2019-02-26 22:04:30.141650
Model ind 685 epoch 419 head B batch: 200 avg loss -2.206895 avg loss no lamb -2.206895 time 2019-02-26 22:05:51.221416
Model ind 685 epoch 419 head B batch: 300 avg loss -2.210834 avg loss no lamb -2.210834 time 2019-02-26 22:07:14.454833
Model ind 685 epoch 419 head B batch: 400 avg loss -2.206418 avg loss no lamb -2.206418 time 2019-02-26 22:08:35.229259
Model ind 685 epoch 419 head A batch: 0 avg loss -2.230315 avg loss no lamb -2.230315 time 2019-02-26 22:09:55.653372
Model ind 685 epoch 419 head A batch: 100 avg loss -2.205000 avg loss no lamb -2.205000 time 2019-02-26 22:11:09.990272
Model ind 685 epoch 419 head A batch: 200 avg loss -2.223825 avg loss no lamb -2.223825 time 2019-02-26 22:12:26.797473
Model ind 685 epoch 419 head A batch: 300 avg loss -2.189567 avg loss no lamb -2.189567 time 2019-02-26 22:13:45.638401
Model ind 685 epoch 419 head A batch: 400 avg loss -2.199552 avg loss no lamb -2.199552 time 2019-02-26 22:15:09.641774
Pre: time 2019-02-26 22:16:44.164228: 
 	std: 0.0064583863
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923428, 0.9789714, 0.99197143, 0.9789857, 0.9789714]
	train_accs: [0.9923428, 0.9789714, 0.99197143, 0.9789857, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842485
	best: 0.9923428

Starting e_i: 420
Model ind 685 epoch 420 head B batch: 0 avg loss -2.223387 avg loss no lamb -2.223387 time 2019-02-26 22:16:46.452518
Model ind 685 epoch 420 head B batch: 100 avg loss -2.203508 avg loss no lamb -2.203508 time 2019-02-26 22:18:02.494045
Model ind 685 epoch 420 head B batch: 200 avg loss -2.206265 avg loss no lamb -2.206265 time 2019-02-26 22:19:21.806442
Model ind 685 epoch 420 head B batch: 300 avg loss -2.236268 avg loss no lamb -2.236268 time 2019-02-26 22:20:42.462668
Model ind 685 epoch 420 head B batch: 400 avg loss -2.212275 avg loss no lamb -2.212275 time 2019-02-26 22:22:04.767471
Model ind 685 epoch 420 head B batch: 0 avg loss -2.210136 avg loss no lamb -2.210136 time 2019-02-26 22:23:23.217208
Model ind 685 epoch 420 head B batch: 100 avg loss -2.214001 avg loss no lamb -2.214001 time 2019-02-26 22:24:40.538228
Model ind 685 epoch 420 head B batch: 200 avg loss -2.189645 avg loss no lamb -2.189645 time 2019-02-26 22:25:57.968191
Model ind 685 epoch 420 head B batch: 300 avg loss -2.221756 avg loss no lamb -2.221756 time 2019-02-26 22:27:12.623285
Model ind 685 epoch 420 head B batch: 400 avg loss -2.172372 avg loss no lamb -2.172372 time 2019-02-26 22:28:28.508278
Model ind 685 epoch 420 head A batch: 0 avg loss -2.211584 avg loss no lamb -2.211584 time 2019-02-26 22:29:46.259557
Model ind 685 epoch 420 head A batch: 100 avg loss -2.221014 avg loss no lamb -2.221014 time 2019-02-26 22:31:03.741919
Model ind 685 epoch 420 head A batch: 200 avg loss -2.228478 avg loss no lamb -2.228478 time 2019-02-26 22:32:18.117949
Model ind 685 epoch 420 head A batch: 300 avg loss -2.214701 avg loss no lamb -2.214701 time 2019-02-26 22:33:35.417421
Model ind 685 epoch 420 head A batch: 400 avg loss -2.184954 avg loss no lamb -2.184954 time 2019-02-26 22:34:56.209513
Pre: time 2019-02-26 22:36:31.121402: 
 	std: 0.006470984
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.9790143, 0.99205714, 0.9790143, 0.9790143]
	train_accs: [0.9923857, 0.9790143, 0.99205714, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98429716
	best: 0.9923857

Starting e_i: 421
Model ind 685 epoch 421 head B batch: 0 avg loss -2.218474 avg loss no lamb -2.218474 time 2019-02-26 22:36:34.493537
Model ind 685 epoch 421 head B batch: 100 avg loss -2.237056 avg loss no lamb -2.237056 time 2019-02-26 22:37:55.308731
Model ind 685 epoch 421 head B batch: 200 avg loss -2.230520 avg loss no lamb -2.230520 time 2019-02-26 22:39:15.387071
Model ind 685 epoch 421 head B batch: 300 avg loss -2.255892 avg loss no lamb -2.255892 time 2019-02-26 22:40:35.485283
Model ind 685 epoch 421 head B batch: 400 avg loss -2.185664 avg loss no lamb -2.185664 time 2019-02-26 22:41:56.251820
Model ind 685 epoch 421 head B batch: 0 avg loss -2.246274 avg loss no lamb -2.246274 time 2019-02-26 22:43:17.919771
Model ind 685 epoch 421 head B batch: 100 avg loss -2.208235 avg loss no lamb -2.208235 time 2019-02-26 22:44:36.995696
Model ind 685 epoch 421 head B batch: 200 avg loss -2.194431 avg loss no lamb -2.194431 time 2019-02-26 22:45:54.586275
Model ind 685 epoch 421 head B batch: 300 avg loss -2.214145 avg loss no lamb -2.214145 time 2019-02-26 22:47:13.749809
Model ind 685 epoch 421 head B batch: 400 avg loss -2.197232 avg loss no lamb -2.197232 time 2019-02-26 22:48:35.452349
Model ind 685 epoch 421 head A batch: 0 avg loss -2.248304 avg loss no lamb -2.248304 time 2019-02-26 22:49:53.678238
Model ind 685 epoch 421 head A batch: 100 avg loss -2.198092 avg loss no lamb -2.198092 time 2019-02-26 22:51:14.261606
Model ind 685 epoch 421 head A batch: 200 avg loss -2.232400 avg loss no lamb -2.232400 time 2019-02-26 22:52:32.648891
Model ind 685 epoch 421 head A batch: 300 avg loss -2.206999 avg loss no lamb -2.206999 time 2019-02-26 22:53:48.109241
Model ind 685 epoch 421 head A batch: 400 avg loss -2.156575 avg loss no lamb -2.156575 time 2019-02-26 22:55:09.003092
Pre: time 2019-02-26 22:56:45.024458: 
 	std: 0.006481277
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924

Starting e_i: 422
Model ind 685 epoch 422 head B batch: 0 avg loss -2.190493 avg loss no lamb -2.190493 time 2019-02-26 22:56:47.204765
Model ind 685 epoch 422 head B batch: 100 avg loss -2.195000 avg loss no lamb -2.195000 time 2019-02-26 22:58:10.256759
Model ind 685 epoch 422 head B batch: 200 avg loss -2.228445 avg loss no lamb -2.228445 time 2019-02-26 22:59:30.249922
Model ind 685 epoch 422 head B batch: 300 avg loss -2.254482 avg loss no lamb -2.254482 time 2019-02-26 23:00:43.910218
Model ind 685 epoch 422 head B batch: 400 avg loss -2.208405 avg loss no lamb -2.208405 time 2019-02-26 23:01:59.024758
Model ind 685 epoch 422 head B batch: 0 avg loss -2.238247 avg loss no lamb -2.238247 time 2019-02-26 23:03:15.187435
Model ind 685 epoch 422 head B batch: 100 avg loss -2.185999 avg loss no lamb -2.185999 time 2019-02-26 23:04:29.081130
Model ind 685 epoch 422 head B batch: 200 avg loss -2.220377 avg loss no lamb -2.220377 time 2019-02-26 23:05:46.870698
Model ind 685 epoch 422 head B batch: 300 avg loss -2.257903 avg loss no lamb -2.257903 time 2019-02-26 23:07:03.512722
Model ind 685 epoch 422 head B batch: 400 avg loss -2.185253 avg loss no lamb -2.185253 time 2019-02-26 23:08:17.929091
Model ind 685 epoch 422 head A batch: 0 avg loss -2.225974 avg loss no lamb -2.225974 time 2019-02-26 23:09:30.953312
Model ind 685 epoch 422 head A batch: 100 avg loss -2.188625 avg loss no lamb -2.188625 time 2019-02-26 23:10:49.058425
Model ind 685 epoch 422 head A batch: 200 avg loss -2.240849 avg loss no lamb -2.240849 time 2019-02-26 23:12:13.595310
Model ind 685 epoch 422 head A batch: 300 avg loss -2.216008 avg loss no lamb -2.216008 time 2019-02-26 23:13:36.022880
Model ind 685 epoch 422 head A batch: 400 avg loss -2.224865 avg loss no lamb -2.224865 time 2019-02-26 23:14:59.216163
Pre: time 2019-02-26 23:16:35.579018: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.979, 0.99205714, 0.979, 0.979]
	train_accs: [0.99237144, 0.979, 0.99205714, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842857
	best: 0.99237144

Starting e_i: 423
Model ind 685 epoch 423 head B batch: 0 avg loss -2.228414 avg loss no lamb -2.228414 time 2019-02-26 23:16:37.804580
Model ind 685 epoch 423 head B batch: 100 avg loss -2.231912 avg loss no lamb -2.231912 time 2019-02-26 23:17:59.136477
Model ind 685 epoch 423 head B batch: 200 avg loss -2.200581 avg loss no lamb -2.200581 time 2019-02-26 23:19:20.448093
Model ind 685 epoch 423 head B batch: 300 avg loss -2.208844 avg loss no lamb -2.208844 time 2019-02-26 23:20:39.572320
Model ind 685 epoch 423 head B batch: 400 avg loss -2.202913 avg loss no lamb -2.202913 time 2019-02-26 23:22:00.439869
Model ind 685 epoch 423 head B batch: 0 avg loss -2.194823 avg loss no lamb -2.194823 time 2019-02-26 23:23:20.402901
Model ind 685 epoch 423 head B batch: 100 avg loss -2.231492 avg loss no lamb -2.231492 time 2019-02-26 23:24:42.296709
Model ind 685 epoch 423 head B batch: 200 avg loss -2.219897 avg loss no lamb -2.219897 time 2019-02-26 23:26:01.220957
Model ind 685 epoch 423 head B batch: 300 avg loss -2.194233 avg loss no lamb -2.194233 time 2019-02-26 23:27:20.725013
Model ind 685 epoch 423 head B batch: 400 avg loss -2.211075 avg loss no lamb -2.211075 time 2019-02-26 23:28:41.197132
Model ind 685 epoch 423 head A batch: 0 avg loss -2.202759 avg loss no lamb -2.202759 time 2019-02-26 23:30:01.602052
Model ind 685 epoch 423 head A batch: 100 avg loss -2.210119 avg loss no lamb -2.210119 time 2019-02-26 23:31:21.563755
Model ind 685 epoch 423 head A batch: 200 avg loss -2.205060 avg loss no lamb -2.205060 time 2019-02-26 23:32:42.685313
Model ind 685 epoch 423 head A batch: 300 avg loss -2.235867 avg loss no lamb -2.235867 time 2019-02-26 23:34:03.256553
Model ind 685 epoch 423 head A batch: 400 avg loss -2.210356 avg loss no lamb -2.210356 time 2019-02-26 23:35:14.161536
Pre: time 2019-02-26 23:36:51.391233: 
 	std: 0.0064953943
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	train_accs: [0.9924, 0.9789857, 0.9920857, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842886
	best: 0.9924

Starting e_i: 424
Model ind 685 epoch 424 head B batch: 0 avg loss -2.203812 avg loss no lamb -2.203812 time 2019-02-26 23:36:53.691136
Model ind 685 epoch 424 head B batch: 100 avg loss -2.166990 avg loss no lamb -2.166990 time 2019-02-26 23:38:12.086366
Model ind 685 epoch 424 head B batch: 200 avg loss -2.201245 avg loss no lamb -2.201245 time 2019-02-26 23:39:29.939193
Model ind 685 epoch 424 head B batch: 300 avg loss -2.251714 avg loss no lamb -2.251714 time 2019-02-26 23:40:46.115687
Model ind 685 epoch 424 head B batch: 400 avg loss -2.192764 avg loss no lamb -2.192764 time 2019-02-26 23:42:05.193137
Model ind 685 epoch 424 head B batch: 0 avg loss -2.211188 avg loss no lamb -2.211188 time 2019-02-26 23:43:25.826384
Model ind 685 epoch 424 head B batch: 100 avg loss -2.201339 avg loss no lamb -2.201339 time 2019-02-26 23:44:46.111024
Model ind 685 epoch 424 head B batch: 200 avg loss -2.190221 avg loss no lamb -2.190221 time 2019-02-26 23:46:03.993563
Model ind 685 epoch 424 head B batch: 300 avg loss -2.228336 avg loss no lamb -2.228336 time 2019-02-26 23:47:20.122898
Model ind 685 epoch 424 head B batch: 400 avg loss -2.214206 avg loss no lamb -2.214206 time 2019-02-26 23:48:35.196360
Model ind 685 epoch 424 head A batch: 0 avg loss -2.220775 avg loss no lamb -2.220775 time 2019-02-26 23:49:53.704648
Model ind 685 epoch 424 head A batch: 100 avg loss -2.217856 avg loss no lamb -2.217856 time 2019-02-26 23:51:12.845339
Model ind 685 epoch 424 head A batch: 200 avg loss -2.239685 avg loss no lamb -2.239685 time 2019-02-26 23:52:29.779778
Model ind 685 epoch 424 head A batch: 300 avg loss -2.212781 avg loss no lamb -2.212781 time 2019-02-26 23:53:48.422544
Model ind 685 epoch 424 head A batch: 400 avg loss -2.215586 avg loss no lamb -2.215586 time 2019-02-26 23:55:08.281466
Pre: time 2019-02-26 23:56:44.714967: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843229
	best: 0.9924143

Starting e_i: 425
Model ind 685 epoch 425 head B batch: 0 avg loss -2.220650 avg loss no lamb -2.220650 time 2019-02-26 23:56:47.274292
Model ind 685 epoch 425 head B batch: 100 avg loss -2.157855 avg loss no lamb -2.157855 time 2019-02-26 23:58:04.907878
Model ind 685 epoch 425 head B batch: 200 avg loss -2.211548 avg loss no lamb -2.211548 time 2019-02-26 23:59:23.506408
Model ind 685 epoch 425 head B batch: 300 avg loss -2.245663 avg loss no lamb -2.245663 time 2019-02-27 00:00:41.240430
Model ind 685 epoch 425 head B batch: 400 avg loss -2.221837 avg loss no lamb -2.221837 time 2019-02-27 00:01:59.314825
Model ind 685 epoch 425 head B batch: 0 avg loss -2.226801 avg loss no lamb -2.226801 time 2019-02-27 00:03:13.671706
Model ind 685 epoch 425 head B batch: 100 avg loss -2.251351 avg loss no lamb -2.251351 time 2019-02-27 00:04:33.907009
Model ind 685 epoch 425 head B batch: 200 avg loss -2.221205 avg loss no lamb -2.221205 time 2019-02-27 00:05:51.382197
Model ind 685 epoch 425 head B batch: 300 avg loss -2.243791 avg loss no lamb -2.243791 time 2019-02-27 00:07:10.998778
Model ind 685 epoch 425 head B batch: 400 avg loss -2.189730 avg loss no lamb -2.189730 time 2019-02-27 00:08:28.692585
Model ind 685 epoch 425 head A batch: 0 avg loss -2.210122 avg loss no lamb -2.210122 time 2019-02-27 00:09:46.837958
Model ind 685 epoch 425 head A batch: 100 avg loss -2.212891 avg loss no lamb -2.212891 time 2019-02-27 00:11:07.341961
Model ind 685 epoch 425 head A batch: 200 avg loss -2.208653 avg loss no lamb -2.208653 time 2019-02-27 00:12:26.908649
Model ind 685 epoch 425 head A batch: 300 avg loss -2.208563 avg loss no lamb -2.208563 time 2019-02-27 00:13:46.164327
Model ind 685 epoch 425 head A batch: 400 avg loss -2.185752 avg loss no lamb -2.185752 time 2019-02-27 00:15:05.697066
Pre: time 2019-02-27 00:16:44.733943: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 426
Model ind 685 epoch 426 head B batch: 0 avg loss -2.213960 avg loss no lamb -2.213960 time 2019-02-27 00:16:47.209158
Model ind 685 epoch 426 head B batch: 100 avg loss -2.198798 avg loss no lamb -2.198798 time 2019-02-27 00:17:58.906208
Model ind 685 epoch 426 head B batch: 200 avg loss -2.246454 avg loss no lamb -2.246454 time 2019-02-27 00:19:19.097787
Model ind 685 epoch 426 head B batch: 300 avg loss -2.182736 avg loss no lamb -2.182736 time 2019-02-27 00:20:39.145595
Model ind 685 epoch 426 head B batch: 400 avg loss -2.198231 avg loss no lamb -2.198231 time 2019-02-27 00:22:03.640372
Model ind 685 epoch 426 head B batch: 0 avg loss -2.205773 avg loss no lamb -2.205773 time 2019-02-27 00:23:27.077152
Model ind 685 epoch 426 head B batch: 100 avg loss -2.211843 avg loss no lamb -2.211843 time 2019-02-27 00:24:51.434898
Model ind 685 epoch 426 head B batch: 200 avg loss -2.172587 avg loss no lamb -2.172587 time 2019-02-27 00:26:12.204408
Model ind 685 epoch 426 head B batch: 300 avg loss -2.243943 avg loss no lamb -2.243943 time 2019-02-27 00:27:34.638456
Model ind 685 epoch 426 head B batch: 400 avg loss -2.199217 avg loss no lamb -2.199217 time 2019-02-27 00:28:56.109619
Model ind 685 epoch 426 head A batch: 0 avg loss -2.220803 avg loss no lamb -2.220803 time 2019-02-27 00:30:17.803265
Model ind 685 epoch 426 head A batch: 100 avg loss -2.247424 avg loss no lamb -2.247424 time 2019-02-27 00:31:40.505411
Model ind 685 epoch 426 head A batch: 200 avg loss -2.200632 avg loss no lamb -2.200632 time 2019-02-27 00:33:03.460878
Model ind 685 epoch 426 head A batch: 300 avg loss -2.210063 avg loss no lamb -2.210063 time 2019-02-27 00:34:26.475726
Model ind 685 epoch 426 head A batch: 400 avg loss -2.149144 avg loss no lamb -2.149144 time 2019-02-27 00:35:50.269292
Pre: time 2019-02-27 00:37:31.321095: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	train_accs: [0.9923857, 0.979, 0.9920857, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842943
	best: 0.9923857

Starting e_i: 427
Model ind 685 epoch 427 head B batch: 0 avg loss -2.241119 avg loss no lamb -2.241119 time 2019-02-27 00:37:33.460261
Model ind 685 epoch 427 head B batch: 100 avg loss -2.197053 avg loss no lamb -2.197053 time 2019-02-27 00:38:51.395881
Model ind 685 epoch 427 head B batch: 200 avg loss -2.220577 avg loss no lamb -2.220577 time 2019-02-27 00:40:11.065046
Model ind 685 epoch 427 head B batch: 300 avg loss -2.232296 avg loss no lamb -2.232296 time 2019-02-27 00:41:32.469830
Model ind 685 epoch 427 head B batch: 400 avg loss -2.191839 avg loss no lamb -2.191839 time 2019-02-27 00:42:54.712216
Model ind 685 epoch 427 head B batch: 0 avg loss -2.218443 avg loss no lamb -2.218443 time 2019-02-27 00:44:16.687869
Model ind 685 epoch 427 head B batch: 100 avg loss -2.206980 avg loss no lamb -2.206980 time 2019-02-27 00:45:37.217896
Model ind 685 epoch 427 head B batch: 200 avg loss -2.229763 avg loss no lamb -2.229763 time 2019-02-27 00:46:59.763194
Model ind 685 epoch 427 head B batch: 300 avg loss -2.219615 avg loss no lamb -2.219615 time 2019-02-27 00:48:17.669222
Model ind 685 epoch 427 head B batch: 400 avg loss -2.216620 avg loss no lamb -2.216620 time 2019-02-27 00:49:36.740554
Model ind 685 epoch 427 head A batch: 0 avg loss -2.251036 avg loss no lamb -2.251036 time 2019-02-27 00:50:55.898211
Model ind 685 epoch 427 head A batch: 100 avg loss -2.187509 avg loss no lamb -2.187509 time 2019-02-27 00:52:15.795144
Model ind 685 epoch 427 head A batch: 200 avg loss -2.229045 avg loss no lamb -2.229045 time 2019-02-27 00:53:36.433340
Model ind 685 epoch 427 head A batch: 300 avg loss -2.213273 avg loss no lamb -2.213273 time 2019-02-27 00:54:56.789325
Model ind 685 epoch 427 head A batch: 400 avg loss -2.204749 avg loss no lamb -2.204749 time 2019-02-27 00:56:15.816774
Pre: time 2019-02-27 00:57:49.315458: 
 	std: 0.0064765024
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9790143, 0.9921, 0.979, 0.9790143]
	train_accs: [0.99235713, 0.9790143, 0.9921, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98429716
	best: 0.99235713

Starting e_i: 428
Model ind 685 epoch 428 head B batch: 0 avg loss -2.226109 avg loss no lamb -2.226109 time 2019-02-27 00:57:51.396454
Model ind 685 epoch 428 head B batch: 100 avg loss -2.230371 avg loss no lamb -2.230371 time 2019-02-27 00:59:09.130810
Model ind 685 epoch 428 head B batch: 200 avg loss -2.207197 avg loss no lamb -2.207197 time 2019-02-27 01:00:25.827339
Model ind 685 epoch 428 head B batch: 300 avg loss -2.247192 avg loss no lamb -2.247192 time 2019-02-27 01:01:41.470420
Model ind 685 epoch 428 head B batch: 400 avg loss -2.164987 avg loss no lamb -2.164987 time 2019-02-27 01:02:57.905574
Model ind 685 epoch 428 head B batch: 0 avg loss -2.229859 avg loss no lamb -2.229859 time 2019-02-27 01:04:17.572318
Model ind 685 epoch 428 head B batch: 100 avg loss -2.223061 avg loss no lamb -2.223061 time 2019-02-27 01:05:35.984059
Model ind 685 epoch 428 head B batch: 200 avg loss -2.211658 avg loss no lamb -2.211658 time 2019-02-27 01:06:54.859585
Model ind 685 epoch 428 head B batch: 300 avg loss -2.211793 avg loss no lamb -2.211793 time 2019-02-27 01:08:12.767771
Model ind 685 epoch 428 head B batch: 400 avg loss -2.190463 avg loss no lamb -2.190463 time 2019-02-27 01:09:31.952299
Model ind 685 epoch 428 head A batch: 0 avg loss -2.230246 avg loss no lamb -2.230246 time 2019-02-27 01:10:51.223494
Model ind 685 epoch 428 head A batch: 100 avg loss -2.180587 avg loss no lamb -2.180587 time 2019-02-27 01:12:10.317045
Model ind 685 epoch 428 head A batch: 200 avg loss -2.227768 avg loss no lamb -2.227768 time 2019-02-27 01:13:29.170600
Model ind 685 epoch 428 head A batch: 300 avg loss -2.229400 avg loss no lamb -2.229400 time 2019-02-27 01:14:48.345299
Model ind 685 epoch 428 head A batch: 400 avg loss -2.182209 avg loss no lamb -2.182209 time 2019-02-27 01:16:06.169771
Pre: time 2019-02-27 01:17:42.013981: 
 	std: 0.006489514
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843314
	best: 0.9924286

Starting e_i: 429
Model ind 685 epoch 429 head B batch: 0 avg loss -2.221006 avg loss no lamb -2.221006 time 2019-02-27 01:17:44.193492
Model ind 685 epoch 429 head B batch: 100 avg loss -2.224646 avg loss no lamb -2.224646 time 2019-02-27 01:19:03.569075
Model ind 685 epoch 429 head B batch: 200 avg loss -2.242174 avg loss no lamb -2.242174 time 2019-02-27 01:20:22.873901
Model ind 685 epoch 429 head B batch: 300 avg loss -2.193141 avg loss no lamb -2.193141 time 2019-02-27 01:21:41.977311
Model ind 685 epoch 429 head B batch: 400 avg loss -2.207091 avg loss no lamb -2.207091 time 2019-02-27 01:23:03.430343
Model ind 685 epoch 429 head B batch: 0 avg loss -2.235170 avg loss no lamb -2.235170 time 2019-02-27 01:24:21.639849
Model ind 685 epoch 429 head B batch: 100 avg loss -2.193660 avg loss no lamb -2.193660 time 2019-02-27 01:25:38.853307
Model ind 685 epoch 429 head B batch: 200 avg loss -2.214264 avg loss no lamb -2.214264 time 2019-02-27 01:26:55.272703
Model ind 685 epoch 429 head B batch: 300 avg loss -2.232141 avg loss no lamb -2.232141 time 2019-02-27 01:28:13.486656
Model ind 685 epoch 429 head B batch: 400 avg loss -2.183619 avg loss no lamb -2.183619 time 2019-02-27 01:29:32.742625
Model ind 685 epoch 429 head A batch: 0 avg loss -2.220052 avg loss no lamb -2.220052 time 2019-02-27 01:30:50.079130
Model ind 685 epoch 429 head A batch: 100 avg loss -2.242859 avg loss no lamb -2.242859 time 2019-02-27 01:32:08.978177
Model ind 685 epoch 429 head A batch: 200 avg loss -2.232877 avg loss no lamb -2.232877 time 2019-02-27 01:33:29.341418
Model ind 685 epoch 429 head A batch: 300 avg loss -2.223184 avg loss no lamb -2.223184 time 2019-02-27 01:34:49.589797
Model ind 685 epoch 429 head A batch: 400 avg loss -2.169589 avg loss no lamb -2.169589 time 2019-02-27 01:36:07.793863
Pre: time 2019-02-27 01:37:46.113355: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.9924286

Starting e_i: 430
Model ind 685 epoch 430 head B batch: 0 avg loss -2.214174 avg loss no lamb -2.214174 time 2019-02-27 01:37:48.264244
Model ind 685 epoch 430 head B batch: 100 avg loss -2.185714 avg loss no lamb -2.185714 time 2019-02-27 01:39:07.569941
Model ind 685 epoch 430 head B batch: 200 avg loss -2.258709 avg loss no lamb -2.258709 time 2019-02-27 01:40:25.176381
Model ind 685 epoch 430 head B batch: 300 avg loss -2.223524 avg loss no lamb -2.223524 time 2019-02-27 01:41:44.257586
Model ind 685 epoch 430 head B batch: 400 avg loss -2.194250 avg loss no lamb -2.194250 time 2019-02-27 01:43:00.984188
Model ind 685 epoch 430 head B batch: 0 avg loss -2.208039 avg loss no lamb -2.208039 time 2019-02-27 01:44:13.478057
Model ind 685 epoch 430 head B batch: 100 avg loss -2.236154 avg loss no lamb -2.236154 time 2019-02-27 01:45:33.185065
Model ind 685 epoch 430 head B batch: 200 avg loss -2.214192 avg loss no lamb -2.214192 time 2019-02-27 01:46:50.188204
Model ind 685 epoch 430 head B batch: 300 avg loss -2.228086 avg loss no lamb -2.228086 time 2019-02-27 01:48:09.578151
Model ind 685 epoch 430 head B batch: 400 avg loss -2.175885 avg loss no lamb -2.175885 time 2019-02-27 01:49:27.999233
Model ind 685 epoch 430 head A batch: 0 avg loss -2.194333 avg loss no lamb -2.194333 time 2019-02-27 01:50:45.843176
Model ind 685 epoch 430 head A batch: 100 avg loss -2.201334 avg loss no lamb -2.201334 time 2019-02-27 01:52:04.647044
Model ind 685 epoch 430 head A batch: 200 avg loss -2.244271 avg loss no lamb -2.244271 time 2019-02-27 01:53:23.814248
Model ind 685 epoch 430 head A batch: 300 avg loss -2.204365 avg loss no lamb -2.204365 time 2019-02-27 01:54:42.082559
Model ind 685 epoch 430 head A batch: 400 avg loss -2.213425 avg loss no lamb -2.213425 time 2019-02-27 01:56:02.242764
Pre: time 2019-02-27 01:57:37.410844: 
 	std: 0.00647674
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9920857, 0.9790143, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9920857, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924

Starting e_i: 431
Model ind 685 epoch 431 head B batch: 0 avg loss -2.237864 avg loss no lamb -2.237864 time 2019-02-27 01:57:39.502079
Model ind 685 epoch 431 head B batch: 100 avg loss -2.188815 avg loss no lamb -2.188815 time 2019-02-27 01:58:58.295475
Model ind 685 epoch 431 head B batch: 200 avg loss -2.238625 avg loss no lamb -2.238625 time 2019-02-27 02:00:14.392632
Model ind 685 epoch 431 head B batch: 300 avg loss -2.223460 avg loss no lamb -2.223460 time 2019-02-27 02:01:33.863113
Model ind 685 epoch 431 head B batch: 400 avg loss -2.182880 avg loss no lamb -2.182880 time 2019-02-27 02:02:54.035576
Model ind 685 epoch 431 head B batch: 0 avg loss -2.212609 avg loss no lamb -2.212609 time 2019-02-27 02:04:12.826219
Model ind 685 epoch 431 head B batch: 100 avg loss -2.215581 avg loss no lamb -2.215581 time 2019-02-27 02:05:32.080936
Model ind 685 epoch 431 head B batch: 200 avg loss -2.210595 avg loss no lamb -2.210595 time 2019-02-27 02:06:49.587467
Model ind 685 epoch 431 head B batch: 300 avg loss -2.206434 avg loss no lamb -2.206434 time 2019-02-27 02:08:09.590108
Model ind 685 epoch 431 head B batch: 400 avg loss -2.230325 avg loss no lamb -2.230325 time 2019-02-27 02:09:29.085124
Model ind 685 epoch 431 head A batch: 0 avg loss -2.235971 avg loss no lamb -2.235971 time 2019-02-27 02:10:47.126617
Model ind 685 epoch 431 head A batch: 100 avg loss -2.207402 avg loss no lamb -2.207402 time 2019-02-27 02:12:06.660580
Model ind 685 epoch 431 head A batch: 200 avg loss -2.189476 avg loss no lamb -2.189476 time 2019-02-27 02:13:25.387363
Model ind 685 epoch 431 head A batch: 300 avg loss -2.247040 avg loss no lamb -2.247040 time 2019-02-27 02:14:45.532349
Model ind 685 epoch 431 head A batch: 400 avg loss -2.185093 avg loss no lamb -2.185093 time 2019-02-27 02:16:05.944780
Pre: time 2019-02-27 02:17:42.361465: 
 	std: 0.0064943284
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.979, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.979, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98431146
	best: 0.9924286

Starting e_i: 432
Model ind 685 epoch 432 head B batch: 0 avg loss -2.240461 avg loss no lamb -2.240461 time 2019-02-27 02:17:44.897014
Model ind 685 epoch 432 head B batch: 100 avg loss -2.220467 avg loss no lamb -2.220467 time 2019-02-27 02:19:05.115079
Model ind 685 epoch 432 head B batch: 200 avg loss -2.219355 avg loss no lamb -2.219355 time 2019-02-27 02:20:25.952103
Model ind 685 epoch 432 head B batch: 300 avg loss -2.203327 avg loss no lamb -2.203327 time 2019-02-27 02:21:46.908559
Model ind 685 epoch 432 head B batch: 400 avg loss -2.232347 avg loss no lamb -2.232347 time 2019-02-27 02:23:09.540843
Model ind 685 epoch 432 head B batch: 0 avg loss -2.233730 avg loss no lamb -2.233730 time 2019-02-27 02:24:31.966081
Model ind 685 epoch 432 head B batch: 100 avg loss -2.200783 avg loss no lamb -2.200783 time 2019-02-27 02:25:46.878410
Model ind 685 epoch 432 head B batch: 200 avg loss -2.228132 avg loss no lamb -2.228132 time 2019-02-27 02:27:03.762862
Model ind 685 epoch 432 head B batch: 300 avg loss -2.254602 avg loss no lamb -2.254602 time 2019-02-27 02:28:22.753597
Model ind 685 epoch 432 head B batch: 400 avg loss -2.200382 avg loss no lamb -2.200382 time 2019-02-27 02:29:41.076891
Model ind 685 epoch 432 head A batch: 0 avg loss -2.223668 avg loss no lamb -2.223668 time 2019-02-27 02:31:01.878650
Model ind 685 epoch 432 head A batch: 100 avg loss -2.186298 avg loss no lamb -2.186298 time 2019-02-27 02:32:22.896694
Model ind 685 epoch 432 head A batch: 200 avg loss -2.198838 avg loss no lamb -2.198838 time 2019-02-27 02:33:41.837565
Model ind 685 epoch 432 head A batch: 300 avg loss -2.249692 avg loss no lamb -2.249692 time 2019-02-27 02:35:01.414512
Model ind 685 epoch 432 head A batch: 400 avg loss -2.203466 avg loss no lamb -2.203466 time 2019-02-27 02:36:20.523522
Pre: time 2019-02-27 02:37:56.962293: 
 	std: 0.0064919973
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921, 0.9790286, 0.979]
	train_accs: [0.9924286, 0.9790143, 0.9921, 0.9790286, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843143
	best: 0.9924286

Starting e_i: 433
Model ind 685 epoch 433 head B batch: 0 avg loss -2.225374 avg loss no lamb -2.225374 time 2019-02-27 02:37:59.161595
Model ind 685 epoch 433 head B batch: 100 avg loss -2.233572 avg loss no lamb -2.233572 time 2019-02-27 02:39:16.158548
Model ind 685 epoch 433 head B batch: 200 avg loss -2.216422 avg loss no lamb -2.216422 time 2019-02-27 02:40:32.920125
Model ind 685 epoch 433 head B batch: 300 avg loss -2.231875 avg loss no lamb -2.231875 time 2019-02-27 02:41:52.395446
Model ind 685 epoch 433 head B batch: 400 avg loss -2.197904 avg loss no lamb -2.197904 time 2019-02-27 02:43:10.528277
Model ind 685 epoch 433 head B batch: 0 avg loss -2.223841 avg loss no lamb -2.223841 time 2019-02-27 02:44:29.640668
Model ind 685 epoch 433 head B batch: 100 avg loss -2.196438 avg loss no lamb -2.196438 time 2019-02-27 02:45:48.414301
Model ind 685 epoch 433 head B batch: 200 avg loss -2.191971 avg loss no lamb -2.191971 time 2019-02-27 02:47:06.598717
Model ind 685 epoch 433 head B batch: 300 avg loss -2.227796 avg loss no lamb -2.227796 time 2019-02-27 02:48:25.514310
Model ind 685 epoch 433 head B batch: 400 avg loss -2.215477 avg loss no lamb -2.215477 time 2019-02-27 02:49:43.841532
Model ind 685 epoch 433 head A batch: 0 avg loss -2.245491 avg loss no lamb -2.245491 time 2019-02-27 02:51:01.203437
Model ind 685 epoch 433 head A batch: 100 avg loss -2.230087 avg loss no lamb -2.230087 time 2019-02-27 02:52:21.476280
Model ind 685 epoch 433 head A batch: 200 avg loss -2.248779 avg loss no lamb -2.248779 time 2019-02-27 02:53:39.429776
Model ind 685 epoch 433 head A batch: 300 avg loss -2.218575 avg loss no lamb -2.218575 time 2019-02-27 02:54:57.287657
Model ind 685 epoch 433 head A batch: 400 avg loss -2.234518 avg loss no lamb -2.234518 time 2019-02-27 02:56:17.738123
Pre: time 2019-02-27 02:57:54.529111: 
 	std: 0.006474431
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	train_accs: [0.9924143, 0.9790428, 0.9921, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924143

Starting e_i: 434
Model ind 685 epoch 434 head B batch: 0 avg loss -2.222054 avg loss no lamb -2.222054 time 2019-02-27 02:57:56.790141
Model ind 685 epoch 434 head B batch: 100 avg loss -2.174576 avg loss no lamb -2.174576 time 2019-02-27 02:59:14.254335
Model ind 685 epoch 434 head B batch: 200 avg loss -2.225742 avg loss no lamb -2.225742 time 2019-02-27 03:00:33.525447
Model ind 685 epoch 434 head B batch: 300 avg loss -2.233820 avg loss no lamb -2.233820 time 2019-02-27 03:01:52.750202
Model ind 685 epoch 434 head B batch: 400 avg loss -2.221708 avg loss no lamb -2.221708 time 2019-02-27 03:03:11.882762
Model ind 685 epoch 434 head B batch: 0 avg loss -2.191368 avg loss no lamb -2.191368 time 2019-02-27 03:04:31.741397
Model ind 685 epoch 434 head B batch: 100 avg loss -2.201486 avg loss no lamb -2.201486 time 2019-02-27 03:05:51.817974
Model ind 685 epoch 434 head B batch: 200 avg loss -2.234526 avg loss no lamb -2.234526 time 2019-02-27 03:07:11.695720
Model ind 685 epoch 434 head B batch: 300 avg loss -2.248541 avg loss no lamb -2.248541 time 2019-02-27 03:08:26.590182
Model ind 685 epoch 434 head B batch: 400 avg loss -2.223419 avg loss no lamb -2.223419 time 2019-02-27 03:09:42.361736
Model ind 685 epoch 434 head A batch: 0 avg loss -2.205512 avg loss no lamb -2.205512 time 2019-02-27 03:10:59.580096
Model ind 685 epoch 434 head A batch: 100 avg loss -2.226070 avg loss no lamb -2.226070 time 2019-02-27 03:12:17.515976
Model ind 685 epoch 434 head A batch: 200 avg loss -2.251917 avg loss no lamb -2.251917 time 2019-02-27 03:13:35.918543
Model ind 685 epoch 434 head A batch: 300 avg loss -2.218226 avg loss no lamb -2.218226 time 2019-02-27 03:14:54.131459
Model ind 685 epoch 434 head A batch: 400 avg loss -2.196429 avg loss no lamb -2.196429 time 2019-02-27 03:16:12.946635
Pre: time 2019-02-27 03:17:48.897622: 
 	std: 0.0064978916
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9922857, 0.97884285, 0.9919429, 0.97885716, 0.97885716]
	train_accs: [0.9922857, 0.97884285, 0.9919429, 0.97885716, 0.97885716]
	best_train_sub_head: 0
	worst: 0.97884285
	avg: 0.9841571
	best: 0.9922857

Starting e_i: 435
Model ind 685 epoch 435 head B batch: 0 avg loss -2.217153 avg loss no lamb -2.217153 time 2019-02-27 03:17:51.094511
Model ind 685 epoch 435 head B batch: 100 avg loss -2.209415 avg loss no lamb -2.209415 time 2019-02-27 03:19:10.408808
Model ind 685 epoch 435 head B batch: 200 avg loss -2.211177 avg loss no lamb -2.211177 time 2019-02-27 03:20:27.838219
Model ind 685 epoch 435 head B batch: 300 avg loss -2.233541 avg loss no lamb -2.233541 time 2019-02-27 03:21:46.620999
Model ind 685 epoch 435 head B batch: 400 avg loss -2.207436 avg loss no lamb -2.207436 time 2019-02-27 03:23:05.651673
Model ind 685 epoch 435 head B batch: 0 avg loss -2.203503 avg loss no lamb -2.203503 time 2019-02-27 03:24:25.770187
Model ind 685 epoch 435 head B batch: 100 avg loss -2.189738 avg loss no lamb -2.189738 time 2019-02-27 03:25:45.163408
Model ind 685 epoch 435 head B batch: 200 avg loss -2.201182 avg loss no lamb -2.201182 time 2019-02-27 03:27:05.958600
Model ind 685 epoch 435 head B batch: 300 avg loss -2.265833 avg loss no lamb -2.265833 time 2019-02-27 03:28:25.344550
Model ind 685 epoch 435 head B batch: 400 avg loss -2.190058 avg loss no lamb -2.190058 time 2019-02-27 03:29:45.918444
Model ind 685 epoch 435 head A batch: 0 avg loss -2.178069 avg loss no lamb -2.178069 time 2019-02-27 03:31:05.060947
Model ind 685 epoch 435 head A batch: 100 avg loss -2.197537 avg loss no lamb -2.197537 time 2019-02-27 03:32:24.741066
Model ind 685 epoch 435 head A batch: 200 avg loss -2.193148 avg loss no lamb -2.193148 time 2019-02-27 03:33:41.765946
Model ind 685 epoch 435 head A batch: 300 avg loss -2.217707 avg loss no lamb -2.217707 time 2019-02-27 03:34:59.768928
Model ind 685 epoch 435 head A batch: 400 avg loss -2.225736 avg loss no lamb -2.225736 time 2019-02-27 03:36:18.415725
Pre: time 2019-02-27 03:37:57.241487: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 436
Model ind 685 epoch 436 head B batch: 0 avg loss -2.229638 avg loss no lamb -2.229638 time 2019-02-27 03:37:59.518733
Model ind 685 epoch 436 head B batch: 100 avg loss -2.189390 avg loss no lamb -2.189390 time 2019-02-27 03:39:18.010561
Model ind 685 epoch 436 head B batch: 200 avg loss -2.203655 avg loss no lamb -2.203655 time 2019-02-27 03:40:38.619398
Model ind 685 epoch 436 head B batch: 300 avg loss -2.248765 avg loss no lamb -2.248765 time 2019-02-27 03:41:57.337286
Model ind 685 epoch 436 head B batch: 400 avg loss -2.219277 avg loss no lamb -2.219277 time 2019-02-27 03:43:16.636755
Model ind 685 epoch 436 head B batch: 0 avg loss -2.229920 avg loss no lamb -2.229920 time 2019-02-27 03:44:37.762819
Model ind 685 epoch 436 head B batch: 100 avg loss -2.227639 avg loss no lamb -2.227639 time 2019-02-27 03:45:57.542379
Model ind 685 epoch 436 head B batch: 200 avg loss -2.179439 avg loss no lamb -2.179439 time 2019-02-27 03:47:18.083910
Model ind 685 epoch 436 head B batch: 300 avg loss -2.187405 avg loss no lamb -2.187405 time 2019-02-27 03:48:38.770254
Model ind 685 epoch 436 head B batch: 400 avg loss -2.209016 avg loss no lamb -2.209016 time 2019-02-27 03:49:59.725264
Model ind 685 epoch 436 head A batch: 0 avg loss -2.218411 avg loss no lamb -2.218411 time 2019-02-27 03:51:11.218203
Model ind 685 epoch 436 head A batch: 100 avg loss -2.207034 avg loss no lamb -2.207034 time 2019-02-27 03:52:24.391380
Model ind 685 epoch 436 head A batch: 200 avg loss -2.215866 avg loss no lamb -2.215866 time 2019-02-27 03:53:42.918923
Model ind 685 epoch 436 head A batch: 300 avg loss -2.216964 avg loss no lamb -2.216964 time 2019-02-27 03:55:02.025332
Model ind 685 epoch 436 head A batch: 400 avg loss -2.219793 avg loss no lamb -2.219793 time 2019-02-27 03:56:21.045161
Pre: time 2019-02-27 03:57:58.425729: 
 	std: 0.0064801755
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790143, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790143, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924

Starting e_i: 437
Model ind 685 epoch 437 head B batch: 0 avg loss -2.210662 avg loss no lamb -2.210662 time 2019-02-27 03:58:01.034086
Model ind 685 epoch 437 head B batch: 100 avg loss -2.221103 avg loss no lamb -2.221103 time 2019-02-27 03:59:21.416557
Model ind 685 epoch 437 head B batch: 200 avg loss -2.226767 avg loss no lamb -2.226767 time 2019-02-27 04:00:39.189869
Model ind 685 epoch 437 head B batch: 300 avg loss -2.243810 avg loss no lamb -2.243810 time 2019-02-27 04:01:59.516118
Model ind 685 epoch 437 head B batch: 400 avg loss -2.195428 avg loss no lamb -2.195428 time 2019-02-27 04:03:17.178262
Model ind 685 epoch 437 head B batch: 0 avg loss -2.222919 avg loss no lamb -2.222919 time 2019-02-27 04:04:39.369307
Model ind 685 epoch 437 head B batch: 100 avg loss -2.198422 avg loss no lamb -2.198422 time 2019-02-27 04:05:57.921869
Model ind 685 epoch 437 head B batch: 200 avg loss -2.200425 avg loss no lamb -2.200425 time 2019-02-27 04:07:16.872717
Model ind 685 epoch 437 head B batch: 300 avg loss -2.245880 avg loss no lamb -2.245880 time 2019-02-27 04:08:38.270709
Model ind 685 epoch 437 head B batch: 400 avg loss -2.232375 avg loss no lamb -2.232375 time 2019-02-27 04:09:59.779131
Model ind 685 epoch 437 head A batch: 0 avg loss -2.239906 avg loss no lamb -2.239906 time 2019-02-27 04:11:20.145844
Model ind 685 epoch 437 head A batch: 100 avg loss -2.191318 avg loss no lamb -2.191318 time 2019-02-27 04:12:39.712272
Model ind 685 epoch 437 head A batch: 200 avg loss -2.226212 avg loss no lamb -2.226212 time 2019-02-27 04:14:00.761147
Model ind 685 epoch 437 head A batch: 300 avg loss -2.230363 avg loss no lamb -2.230363 time 2019-02-27 04:15:20.294177
Model ind 685 epoch 437 head A batch: 400 avg loss -2.218230 avg loss no lamb -2.218230 time 2019-02-27 04:16:39.894838
Pre: time 2019-02-27 04:18:15.239938: 
 	std: 0.006496657
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9789714, 0.9920857, 0.9790143, 0.9789857]
	train_accs: [0.9924143, 0.9789714, 0.9920857, 0.9790143, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842943
	best: 0.9924143

Starting e_i: 438
Model ind 685 epoch 438 head B batch: 0 avg loss -2.243112 avg loss no lamb -2.243112 time 2019-02-27 04:18:17.430601
Model ind 685 epoch 438 head B batch: 100 avg loss -2.194437 avg loss no lamb -2.194437 time 2019-02-27 04:19:34.795160
Model ind 685 epoch 438 head B batch: 200 avg loss -2.245386 avg loss no lamb -2.245386 time 2019-02-27 04:20:52.616046
Model ind 685 epoch 438 head B batch: 300 avg loss -2.176397 avg loss no lamb -2.176397 time 2019-02-27 04:22:12.558471
Model ind 685 epoch 438 head B batch: 400 avg loss -2.192682 avg loss no lamb -2.192682 time 2019-02-27 04:23:31.795973
Model ind 685 epoch 438 head B batch: 0 avg loss -2.229767 avg loss no lamb -2.229767 time 2019-02-27 04:24:51.387921
Model ind 685 epoch 438 head B batch: 100 avg loss -2.210124 avg loss no lamb -2.210124 time 2019-02-27 04:26:09.855437
Model ind 685 epoch 438 head B batch: 200 avg loss -2.221390 avg loss no lamb -2.221390 time 2019-02-27 04:27:27.529594
Model ind 685 epoch 438 head B batch: 300 avg loss -2.234912 avg loss no lamb -2.234912 time 2019-02-27 04:28:44.265421
Model ind 685 epoch 438 head B batch: 400 avg loss -2.197259 avg loss no lamb -2.197259 time 2019-02-27 04:30:02.928717
Model ind 685 epoch 438 head A batch: 0 avg loss -2.180701 avg loss no lamb -2.180701 time 2019-02-27 04:31:20.444004
Model ind 685 epoch 438 head A batch: 100 avg loss -2.246486 avg loss no lamb -2.246486 time 2019-02-27 04:32:37.525217
Model ind 685 epoch 438 head A batch: 200 avg loss -2.209824 avg loss no lamb -2.209824 time 2019-02-27 04:33:52.929877
Model ind 685 epoch 438 head A batch: 300 avg loss -2.242121 avg loss no lamb -2.242121 time 2019-02-27 04:35:10.219286
Model ind 685 epoch 438 head A batch: 400 avg loss -2.225541 avg loss no lamb -2.225541 time 2019-02-27 04:36:29.913359
Pre: time 2019-02-27 04:38:05.046009: 
 	std: 0.0065047718
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.979, 0.9921143, 0.9790143, 0.9789714]
	train_accs: [0.9924286, 0.979, 0.9921143, 0.9790143, 0.9789714]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.98430574
	best: 0.9924286

Starting e_i: 439
Model ind 685 epoch 439 head B batch: 0 avg loss -2.207087 avg loss no lamb -2.207087 time 2019-02-27 04:38:07.190563
Model ind 685 epoch 439 head B batch: 100 avg loss -2.228933 avg loss no lamb -2.228933 time 2019-02-27 04:39:27.005525
Model ind 685 epoch 439 head B batch: 200 avg loss -2.227292 avg loss no lamb -2.227292 time 2019-02-27 04:40:44.454020
Model ind 685 epoch 439 head B batch: 300 avg loss -2.208086 avg loss no lamb -2.208086 time 2019-02-27 04:42:02.066164
Model ind 685 epoch 439 head B batch: 400 avg loss -2.223459 avg loss no lamb -2.223459 time 2019-02-27 04:43:20.482146
Model ind 685 epoch 439 head B batch: 0 avg loss -2.217861 avg loss no lamb -2.217861 time 2019-02-27 04:44:39.331909
Model ind 685 epoch 439 head B batch: 100 avg loss -2.214312 avg loss no lamb -2.214312 time 2019-02-27 04:45:58.377253
Model ind 685 epoch 439 head B batch: 200 avg loss -2.232969 avg loss no lamb -2.232969 time 2019-02-27 04:47:18.227991
Model ind 685 epoch 439 head B batch: 300 avg loss -2.221573 avg loss no lamb -2.221573 time 2019-02-27 04:48:35.921615
Model ind 685 epoch 439 head B batch: 400 avg loss -2.205174 avg loss no lamb -2.205174 time 2019-02-27 04:49:56.067912
Model ind 685 epoch 439 head A batch: 0 avg loss -2.242486 avg loss no lamb -2.242486 time 2019-02-27 04:51:17.994802
Model ind 685 epoch 439 head A batch: 100 avg loss -2.165838 avg loss no lamb -2.165838 time 2019-02-27 04:52:37.870686
Model ind 685 epoch 439 head A batch: 200 avg loss -2.210298 avg loss no lamb -2.210298 time 2019-02-27 04:53:55.794848
Model ind 685 epoch 439 head A batch: 300 avg loss -2.250401 avg loss no lamb -2.250401 time 2019-02-27 04:55:15.084582
Model ind 685 epoch 439 head A batch: 400 avg loss -2.208641 avg loss no lamb -2.208641 time 2019-02-27 04:56:36.113787
Pre: time 2019-02-27 04:58:13.410701: 
 	std: 0.006507923
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9923, 0.9789, 0.99205714, 0.9788857, 0.9789]
	train_accs: [0.9923, 0.9789, 0.99205714, 0.9788857, 0.9789]
	best_train_sub_head: 0
	worst: 0.9788857
	avg: 0.9842086
	best: 0.9923

Starting e_i: 440
Model ind 685 epoch 440 head B batch: 0 avg loss -2.225472 avg loss no lamb -2.225472 time 2019-02-27 04:58:15.534861
Model ind 685 epoch 440 head B batch: 100 avg loss -2.190510 avg loss no lamb -2.190510 time 2019-02-27 04:59:36.210329
Model ind 685 epoch 440 head B batch: 200 avg loss -2.218938 avg loss no lamb -2.218938 time 2019-02-27 05:00:55.719734
Model ind 685 epoch 440 head B batch: 300 avg loss -2.240379 avg loss no lamb -2.240379 time 2019-02-27 05:02:13.538168
Model ind 685 epoch 440 head B batch: 400 avg loss -2.203178 avg loss no lamb -2.203178 time 2019-02-27 05:03:30.901194
Model ind 685 epoch 440 head B batch: 0 avg loss -2.217670 avg loss no lamb -2.217670 time 2019-02-27 05:04:50.277919
Model ind 685 epoch 440 head B batch: 100 avg loss -2.208703 avg loss no lamb -2.208703 time 2019-02-27 05:06:07.967019
Model ind 685 epoch 440 head B batch: 200 avg loss -2.222421 avg loss no lamb -2.222421 time 2019-02-27 05:07:28.075988
Model ind 685 epoch 440 head B batch: 300 avg loss -2.242876 avg loss no lamb -2.242876 time 2019-02-27 05:08:48.558020
Model ind 685 epoch 440 head B batch: 400 avg loss -2.231544 avg loss no lamb -2.231544 time 2019-02-27 05:10:09.544479
Model ind 685 epoch 440 head A batch: 0 avg loss -2.205206 avg loss no lamb -2.205206 time 2019-02-27 05:11:28.249565
Model ind 685 epoch 440 head A batch: 100 avg loss -2.202330 avg loss no lamb -2.202330 time 2019-02-27 05:12:47.692770
Model ind 685 epoch 440 head A batch: 200 avg loss -2.200828 avg loss no lamb -2.200828 time 2019-02-27 05:14:07.522485
Model ind 685 epoch 440 head A batch: 300 avg loss -2.251198 avg loss no lamb -2.251198 time 2019-02-27 05:15:28.016787
Model ind 685 epoch 440 head A batch: 400 avg loss -2.180886 avg loss no lamb -2.180886 time 2019-02-27 05:16:37.854590
Pre: time 2019-02-27 05:18:12.876599: 
 	std: 0.006477838
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843172
	best: 0.9924

Starting e_i: 441
Model ind 685 epoch 441 head B batch: 0 avg loss -2.231728 avg loss no lamb -2.231728 time 2019-02-27 05:18:16.103789
Model ind 685 epoch 441 head B batch: 100 avg loss -2.207247 avg loss no lamb -2.207247 time 2019-02-27 05:19:33.921163
Model ind 685 epoch 441 head B batch: 200 avg loss -2.210898 avg loss no lamb -2.210898 time 2019-02-27 05:20:53.580499
Model ind 685 epoch 441 head B batch: 300 avg loss -2.243699 avg loss no lamb -2.243699 time 2019-02-27 05:22:12.940186
Model ind 685 epoch 441 head B batch: 400 avg loss -2.228734 avg loss no lamb -2.228734 time 2019-02-27 05:23:30.896189
Model ind 685 epoch 441 head B batch: 0 avg loss -2.227190 avg loss no lamb -2.227190 time 2019-02-27 05:24:48.807077
Model ind 685 epoch 441 head B batch: 100 avg loss -2.215361 avg loss no lamb -2.215361 time 2019-02-27 05:26:08.713405
Model ind 685 epoch 441 head B batch: 200 avg loss -2.176100 avg loss no lamb -2.176100 time 2019-02-27 05:27:27.017782
Model ind 685 epoch 441 head B batch: 300 avg loss -2.247067 avg loss no lamb -2.247067 time 2019-02-27 05:28:45.344965
Model ind 685 epoch 441 head B batch: 400 avg loss -2.206077 avg loss no lamb -2.206077 time 2019-02-27 05:30:03.521659
Model ind 685 epoch 441 head A batch: 0 avg loss -2.195986 avg loss no lamb -2.195986 time 2019-02-27 05:31:21.846751
Model ind 685 epoch 441 head A batch: 100 avg loss -2.200894 avg loss no lamb -2.200894 time 2019-02-27 05:32:41.152540
Model ind 685 epoch 441 head A batch: 200 avg loss -2.190138 avg loss no lamb -2.190138 time 2019-02-27 05:33:59.822606
Model ind 685 epoch 441 head A batch: 300 avg loss -2.253068 avg loss no lamb -2.253068 time 2019-02-27 05:35:18.934045
Model ind 685 epoch 441 head A batch: 400 avg loss -2.191964 avg loss no lamb -2.191964 time 2019-02-27 05:36:39.748631
Pre: time 2019-02-27 05:38:18.228380: 
 	std: 0.0065036523
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9789857, 0.9921, 0.979, 0.9789857]
	train_accs: [0.9924286, 0.9789857, 0.9921, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9843
	best: 0.9924286

Starting e_i: 442
Model ind 685 epoch 442 head B batch: 0 avg loss -2.231766 avg loss no lamb -2.231766 time 2019-02-27 05:38:20.436527
Model ind 685 epoch 442 head B batch: 100 avg loss -2.204098 avg loss no lamb -2.204098 time 2019-02-27 05:39:41.463042
Model ind 685 epoch 442 head B batch: 200 avg loss -2.216404 avg loss no lamb -2.216404 time 2019-02-27 05:41:00.304227
Model ind 685 epoch 442 head B batch: 300 avg loss -2.251842 avg loss no lamb -2.251842 time 2019-02-27 05:42:19.430385
Model ind 685 epoch 442 head B batch: 400 avg loss -2.204849 avg loss no lamb -2.204849 time 2019-02-27 05:43:40.091670
Model ind 685 epoch 442 head B batch: 0 avg loss -2.248145 avg loss no lamb -2.248145 time 2019-02-27 05:45:00.192195
Model ind 685 epoch 442 head B batch: 100 avg loss -2.226554 avg loss no lamb -2.226554 time 2019-02-27 05:46:20.064864
Model ind 685 epoch 442 head B batch: 200 avg loss -2.215478 avg loss no lamb -2.215478 time 2019-02-27 05:47:39.471659
Model ind 685 epoch 442 head B batch: 300 avg loss -2.242959 avg loss no lamb -2.242959 time 2019-02-27 05:48:58.333703
Model ind 685 epoch 442 head B batch: 400 avg loss -2.171547 avg loss no lamb -2.171547 time 2019-02-27 05:50:18.202928
Model ind 685 epoch 442 head A batch: 0 avg loss -2.213958 avg loss no lamb -2.213958 time 2019-02-27 05:51:38.096139
Model ind 685 epoch 442 head A batch: 100 avg loss -2.188232 avg loss no lamb -2.188232 time 2019-02-27 05:52:55.975857
Model ind 685 epoch 442 head A batch: 200 avg loss -2.228840 avg loss no lamb -2.228840 time 2019-02-27 05:54:14.771486
Model ind 685 epoch 442 head A batch: 300 avg loss -2.226420 avg loss no lamb -2.226420 time 2019-02-27 05:55:35.216831
Model ind 685 epoch 442 head A batch: 400 avg loss -2.185486 avg loss no lamb -2.185486 time 2019-02-27 05:56:51.105054
Pre: time 2019-02-27 05:58:25.921657: 
 	std: 0.006487183
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.979, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.979, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.98430574
	best: 0.9924

Starting e_i: 443
Model ind 685 epoch 443 head B batch: 0 avg loss -2.231983 avg loss no lamb -2.231983 time 2019-02-27 05:58:28.197325
Model ind 685 epoch 443 head B batch: 100 avg loss -2.177646 avg loss no lamb -2.177646 time 2019-02-27 05:59:38.951804
Model ind 685 epoch 443 head B batch: 200 avg loss -2.248423 avg loss no lamb -2.248423 time 2019-02-27 06:00:58.065568
Model ind 685 epoch 443 head B batch: 300 avg loss -2.233211 avg loss no lamb -2.233211 time 2019-02-27 06:02:17.406021
Model ind 685 epoch 443 head B batch: 400 avg loss -2.180069 avg loss no lamb -2.180069 time 2019-02-27 06:03:37.277846
Model ind 685 epoch 443 head B batch: 0 avg loss -2.221619 avg loss no lamb -2.221619 time 2019-02-27 06:04:56.202366
Model ind 685 epoch 443 head B batch: 100 avg loss -2.224747 avg loss no lamb -2.224747 time 2019-02-27 06:06:14.864213
Model ind 685 epoch 443 head B batch: 200 avg loss -2.203935 avg loss no lamb -2.203935 time 2019-02-27 06:07:33.157964
Model ind 685 epoch 443 head B batch: 300 avg loss -2.229110 avg loss no lamb -2.229110 time 2019-02-27 06:08:51.383979
Model ind 685 epoch 443 head B batch: 400 avg loss -2.197375 avg loss no lamb -2.197375 time 2019-02-27 06:10:06.501845
Model ind 685 epoch 443 head A batch: 0 avg loss -2.226293 avg loss no lamb -2.226293 time 2019-02-27 06:11:27.941623
Model ind 685 epoch 443 head A batch: 100 avg loss -2.221133 avg loss no lamb -2.221133 time 2019-02-27 06:12:48.354193
Model ind 685 epoch 443 head A batch: 200 avg loss -2.211868 avg loss no lamb -2.211868 time 2019-02-27 06:14:09.103200
Model ind 685 epoch 443 head A batch: 300 avg loss -2.224186 avg loss no lamb -2.224186 time 2019-02-27 06:15:28.987368
Model ind 685 epoch 443 head A batch: 400 avg loss -2.184875 avg loss no lamb -2.184875 time 2019-02-27 06:16:49.397462
Pre: time 2019-02-27 06:18:30.238512: 
 	std: 0.0064720977
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99237144, 0.9790143, 0.99205714, 0.979, 0.979]
	train_accs: [0.99237144, 0.9790143, 0.99205714, 0.979, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9842886
	best: 0.99237144

Starting e_i: 444
Model ind 685 epoch 444 head B batch: 0 avg loss -2.215384 avg loss no lamb -2.215384 time 2019-02-27 06:18:32.708527
Model ind 685 epoch 444 head B batch: 100 avg loss -2.188058 avg loss no lamb -2.188058 time 2019-02-27 06:19:54.849383
Model ind 685 epoch 444 head B batch: 200 avg loss -2.204964 avg loss no lamb -2.204964 time 2019-02-27 06:21:13.203850
Model ind 685 epoch 444 head B batch: 300 avg loss -2.227796 avg loss no lamb -2.227796 time 2019-02-27 06:22:30.611866
Model ind 685 epoch 444 head B batch: 400 avg loss -2.197060 avg loss no lamb -2.197060 time 2019-02-27 06:23:45.803255
Model ind 685 epoch 444 head B batch: 0 avg loss -2.246726 avg loss no lamb -2.246726 time 2019-02-27 06:24:59.765590
Model ind 685 epoch 444 head B batch: 100 avg loss -2.207810 avg loss no lamb -2.207810 time 2019-02-27 06:26:15.222946
Model ind 685 epoch 444 head B batch: 200 avg loss -2.214919 avg loss no lamb -2.214919 time 2019-02-27 06:27:29.785070
Model ind 685 epoch 444 head B batch: 300 avg loss -2.246426 avg loss no lamb -2.246426 time 2019-02-27 06:28:44.581755
Model ind 685 epoch 444 head B batch: 400 avg loss -2.204081 avg loss no lamb -2.204081 time 2019-02-27 06:29:59.725549
Model ind 685 epoch 444 head A batch: 0 avg loss -2.224516 avg loss no lamb -2.224516 time 2019-02-27 06:31:14.301267
Model ind 685 epoch 444 head A batch: 100 avg loss -2.209279 avg loss no lamb -2.209279 time 2019-02-27 06:32:30.940047
Model ind 685 epoch 444 head A batch: 200 avg loss -2.213428 avg loss no lamb -2.213428 time 2019-02-27 06:33:47.496410
Model ind 685 epoch 444 head A batch: 300 avg loss -2.226295 avg loss no lamb -2.226295 time 2019-02-27 06:35:02.634881
Model ind 685 epoch 444 head A batch: 400 avg loss -2.215494 avg loss no lamb -2.215494 time 2019-02-27 06:36:17.635014
Pre: time 2019-02-27 06:37:51.535995: 
 	std: 0.0064918525
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843143
	best: 0.9924143

Starting e_i: 445
Model ind 685 epoch 445 head B batch: 0 avg loss -2.231936 avg loss no lamb -2.231936 time 2019-02-27 06:37:53.780728
Model ind 685 epoch 445 head B batch: 100 avg loss -2.209539 avg loss no lamb -2.209539 time 2019-02-27 06:39:06.964919
Model ind 685 epoch 445 head B batch: 200 avg loss -2.183493 avg loss no lamb -2.183493 time 2019-02-27 06:40:22.832979
Model ind 685 epoch 445 head B batch: 300 avg loss -2.221676 avg loss no lamb -2.221676 time 2019-02-27 06:41:37.634912
Model ind 685 epoch 445 head B batch: 400 avg loss -2.206596 avg loss no lamb -2.206596 time 2019-02-27 06:42:41.006211
Model ind 685 epoch 445 head B batch: 0 avg loss -2.239265 avg loss no lamb -2.239265 time 2019-02-27 06:43:58.033089
Model ind 685 epoch 445 head B batch: 100 avg loss -2.215605 avg loss no lamb -2.215605 time 2019-02-27 06:45:12.692678
Model ind 685 epoch 445 head B batch: 200 avg loss -2.235390 avg loss no lamb -2.235390 time 2019-02-27 06:46:26.891920
Model ind 685 epoch 445 head B batch: 300 avg loss -2.244847 avg loss no lamb -2.244847 time 2019-02-27 06:47:42.937202
Model ind 685 epoch 445 head B batch: 400 avg loss -2.181435 avg loss no lamb -2.181435 time 2019-02-27 06:48:57.027255
Model ind 685 epoch 445 head A batch: 0 avg loss -2.211217 avg loss no lamb -2.211217 time 2019-02-27 06:50:11.263939
Model ind 685 epoch 445 head A batch: 100 avg loss -2.224036 avg loss no lamb -2.224036 time 2019-02-27 06:51:26.705323
Model ind 685 epoch 445 head A batch: 200 avg loss -2.206754 avg loss no lamb -2.206754 time 2019-02-27 06:52:40.579098
Model ind 685 epoch 445 head A batch: 300 avg loss -2.238616 avg loss no lamb -2.238616 time 2019-02-27 06:53:56.885101
Model ind 685 epoch 445 head A batch: 400 avg loss -2.169075 avg loss no lamb -2.169075 time 2019-02-27 06:55:09.373892
Pre: time 2019-02-27 06:56:42.016828: 
 	std: 0.006477853
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	train_accs: [0.9924286, 0.97905713, 0.99212855, 0.97905713, 0.97905713]
	best_train_sub_head: 0
	worst: 0.97905713
	avg: 0.98434573
	best: 0.9924286

Starting e_i: 446
Model ind 685 epoch 446 head B batch: 0 avg loss -2.209307 avg loss no lamb -2.209307 time 2019-02-27 06:56:44.282179
Model ind 685 epoch 446 head B batch: 100 avg loss -2.185616 avg loss no lamb -2.185616 time 2019-02-27 06:57:58.611378
Model ind 685 epoch 446 head B batch: 200 avg loss -2.238639 avg loss no lamb -2.238639 time 2019-02-27 06:59:17.144358
Model ind 685 epoch 446 head B batch: 300 avg loss -2.251836 avg loss no lamb -2.251836 time 2019-02-27 07:00:32.002277
Model ind 685 epoch 446 head B batch: 400 avg loss -2.167590 avg loss no lamb -2.167590 time 2019-02-27 07:01:47.422716
Model ind 685 epoch 446 head B batch: 0 avg loss -2.224524 avg loss no lamb -2.224524 time 2019-02-27 07:03:01.238123
Model ind 685 epoch 446 head B batch: 100 avg loss -2.232115 avg loss no lamb -2.232115 time 2019-02-27 07:04:14.988959
Model ind 685 epoch 446 head B batch: 200 avg loss -2.206847 avg loss no lamb -2.206847 time 2019-02-27 07:05:31.709300
Model ind 685 epoch 446 head B batch: 300 avg loss -2.242661 avg loss no lamb -2.242661 time 2019-02-27 07:06:48.525287
Model ind 685 epoch 446 head B batch: 400 avg loss -2.169794 avg loss no lamb -2.169794 time 2019-02-27 07:08:04.181644
Model ind 685 epoch 446 head A batch: 0 avg loss -2.232464 avg loss no lamb -2.232464 time 2019-02-27 07:09:20.966294
Model ind 685 epoch 446 head A batch: 100 avg loss -2.203644 avg loss no lamb -2.203644 time 2019-02-27 07:10:37.080055
Model ind 685 epoch 446 head A batch: 200 avg loss -2.198012 avg loss no lamb -2.198012 time 2019-02-27 07:11:54.268474
Model ind 685 epoch 446 head A batch: 300 avg loss -2.235350 avg loss no lamb -2.235350 time 2019-02-27 07:13:10.059659
Model ind 685 epoch 446 head A batch: 400 avg loss -2.191432 avg loss no lamb -2.191432 time 2019-02-27 07:14:26.322442
Pre: time 2019-02-27 07:16:00.613255: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 447
Model ind 685 epoch 447 head B batch: 0 avg loss -2.198506 avg loss no lamb -2.198506 time 2019-02-27 07:16:02.779393
Model ind 685 epoch 447 head B batch: 100 avg loss -2.210308 avg loss no lamb -2.210308 time 2019-02-27 07:17:18.976291
Model ind 685 epoch 447 head B batch: 200 avg loss -2.241648 avg loss no lamb -2.241648 time 2019-02-27 07:18:33.902825
Model ind 685 epoch 447 head B batch: 300 avg loss -2.242202 avg loss no lamb -2.242202 time 2019-02-27 07:19:48.393750
Model ind 685 epoch 447 head B batch: 400 avg loss -2.222852 avg loss no lamb -2.222852 time 2019-02-27 07:21:02.726295
Model ind 685 epoch 447 head B batch: 0 avg loss -2.201084 avg loss no lamb -2.201084 time 2019-02-27 07:22:16.984184
Model ind 685 epoch 447 head B batch: 100 avg loss -2.182386 avg loss no lamb -2.182386 time 2019-02-27 07:23:31.781990
Model ind 685 epoch 447 head B batch: 200 avg loss -2.213189 avg loss no lamb -2.213189 time 2019-02-27 07:24:47.002090
Model ind 685 epoch 447 head B batch: 300 avg loss -2.225747 avg loss no lamb -2.225747 time 2019-02-27 07:25:54.462442
Model ind 685 epoch 447 head B batch: 400 avg loss -2.165774 avg loss no lamb -2.165774 time 2019-02-27 07:27:08.419675
Model ind 685 epoch 447 head A batch: 0 avg loss -2.203337 avg loss no lamb -2.203337 time 2019-02-27 07:28:24.743064
Model ind 685 epoch 447 head A batch: 100 avg loss -2.197921 avg loss no lamb -2.197921 time 2019-02-27 07:29:41.474297
Model ind 685 epoch 447 head A batch: 200 avg loss -2.178179 avg loss no lamb -2.178179 time 2019-02-27 07:31:00.651605
Model ind 685 epoch 447 head A batch: 300 avg loss -2.242476 avg loss no lamb -2.242476 time 2019-02-27 07:32:20.675078
Model ind 685 epoch 447 head A batch: 400 avg loss -2.198132 avg loss no lamb -2.198132 time 2019-02-27 07:33:38.612187
Pre: time 2019-02-27 07:35:08.969515: 
 	std: 0.006487188
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.99212855, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843343
	best: 0.9924286

Starting e_i: 448
Model ind 685 epoch 448 head B batch: 0 avg loss -2.226586 avg loss no lamb -2.226586 time 2019-02-27 07:35:11.197139
Model ind 685 epoch 448 head B batch: 100 avg loss -2.220452 avg loss no lamb -2.220452 time 2019-02-27 07:36:25.352640
Model ind 685 epoch 448 head B batch: 200 avg loss -2.212453 avg loss no lamb -2.212453 time 2019-02-27 07:37:38.787941
Model ind 685 epoch 448 head B batch: 300 avg loss -2.241095 avg loss no lamb -2.241095 time 2019-02-27 07:38:56.590041
Model ind 685 epoch 448 head B batch: 400 avg loss -2.198231 avg loss no lamb -2.198231 time 2019-02-27 07:40:13.427841
Model ind 685 epoch 448 head B batch: 0 avg loss -2.227843 avg loss no lamb -2.227843 time 2019-02-27 07:41:27.193042
Model ind 685 epoch 448 head B batch: 100 avg loss -2.246801 avg loss no lamb -2.246801 time 2019-02-27 07:42:40.743754
Model ind 685 epoch 448 head B batch: 200 avg loss -2.213924 avg loss no lamb -2.213924 time 2019-02-27 07:43:52.608337
Model ind 685 epoch 448 head B batch: 300 avg loss -2.205899 avg loss no lamb -2.205899 time 2019-02-27 07:45:05.898892
Model ind 685 epoch 448 head B batch: 400 avg loss -2.198797 avg loss no lamb -2.198797 time 2019-02-27 07:46:20.810280
Model ind 685 epoch 448 head A batch: 0 avg loss -2.223368 avg loss no lamb -2.223368 time 2019-02-27 07:47:35.978882
Model ind 685 epoch 448 head A batch: 100 avg loss -2.186340 avg loss no lamb -2.186340 time 2019-02-27 07:48:50.554838
Model ind 685 epoch 448 head A batch: 200 avg loss -2.205880 avg loss no lamb -2.205880 time 2019-02-27 07:50:06.893808
Model ind 685 epoch 448 head A batch: 300 avg loss -2.240938 avg loss no lamb -2.240938 time 2019-02-27 07:51:21.763655
Model ind 685 epoch 448 head A batch: 400 avg loss -2.200988 avg loss no lamb -2.200988 time 2019-02-27 07:52:36.653716
Pre: time 2019-02-27 07:54:07.710667: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	train_accs: [0.9924286, 0.9790286, 0.9921143, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432577
	best: 0.9924286

Starting e_i: 449
Model ind 685 epoch 449 head B batch: 0 avg loss -2.227327 avg loss no lamb -2.227327 time 2019-02-27 07:54:09.714453
Model ind 685 epoch 449 head B batch: 100 avg loss -2.202380 avg loss no lamb -2.202380 time 2019-02-27 07:55:22.766592
Model ind 685 epoch 449 head B batch: 200 avg loss -2.227898 avg loss no lamb -2.227898 time 2019-02-27 07:56:37.003546
Model ind 685 epoch 449 head B batch: 300 avg loss -2.209120 avg loss no lamb -2.209120 time 2019-02-27 07:57:52.000777
Model ind 685 epoch 449 head B batch: 400 avg loss -2.212146 avg loss no lamb -2.212146 time 2019-02-27 07:59:05.955797
Model ind 685 epoch 449 head B batch: 0 avg loss -2.235547 avg loss no lamb -2.235547 time 2019-02-27 08:00:20.954542
Model ind 685 epoch 449 head B batch: 100 avg loss -2.251332 avg loss no lamb -2.251332 time 2019-02-27 08:01:36.993637
Model ind 685 epoch 449 head B batch: 200 avg loss -2.203368 avg loss no lamb -2.203368 time 2019-02-27 08:02:53.891262
Model ind 685 epoch 449 head B batch: 300 avg loss -2.220219 avg loss no lamb -2.220219 time 2019-02-27 08:04:10.466366
Model ind 685 epoch 449 head B batch: 400 avg loss -2.166487 avg loss no lamb -2.166487 time 2019-02-27 08:05:24.503802
Model ind 685 epoch 449 head A batch: 0 avg loss -2.178979 avg loss no lamb -2.178979 time 2019-02-27 08:06:41.267192
Model ind 685 epoch 449 head A batch: 100 avg loss -2.247150 avg loss no lamb -2.247150 time 2019-02-27 08:07:54.669874
Model ind 685 epoch 449 head A batch: 200 avg loss -2.199187 avg loss no lamb -2.199187 time 2019-02-27 08:09:00.969507
Model ind 685 epoch 449 head A batch: 300 avg loss -2.200439 avg loss no lamb -2.200439 time 2019-02-27 08:10:14.122832
Model ind 685 epoch 449 head A batch: 400 avg loss -2.214415 avg loss no lamb -2.214415 time 2019-02-27 08:11:33.010971
Pre: time 2019-02-27 08:13:03.010625: 
 	std: 0.0064745764
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9920857, 0.9790428, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.9920857, 0.9790428, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790428
	avg: 0.98432857
	best: 0.9924286

Starting e_i: 450
Model ind 685 epoch 450 head B batch: 0 avg loss -2.242949 avg loss no lamb -2.242949 time 2019-02-27 08:13:05.060557
Model ind 685 epoch 450 head B batch: 100 avg loss -2.228924 avg loss no lamb -2.228924 time 2019-02-27 08:14:19.277965
Model ind 685 epoch 450 head B batch: 200 avg loss -2.202294 avg loss no lamb -2.202294 time 2019-02-27 08:15:33.165882
Model ind 685 epoch 450 head B batch: 300 avg loss -2.226717 avg loss no lamb -2.226717 time 2019-02-27 08:16:46.246173
Model ind 685 epoch 450 head B batch: 400 avg loss -2.189667 avg loss no lamb -2.189667 time 2019-02-27 08:18:00.623722
Model ind 685 epoch 450 head B batch: 0 avg loss -2.191418 avg loss no lamb -2.191418 time 2019-02-27 08:19:14.643169
Model ind 685 epoch 450 head B batch: 100 avg loss -2.200963 avg loss no lamb -2.200963 time 2019-02-27 08:20:29.400129
Model ind 685 epoch 450 head B batch: 200 avg loss -2.221792 avg loss no lamb -2.221792 time 2019-02-27 08:21:46.296952
Model ind 685 epoch 450 head B batch: 300 avg loss -2.207495 avg loss no lamb -2.207495 time 2019-02-27 08:23:01.506157
Model ind 685 epoch 450 head B batch: 400 avg loss -2.199207 avg loss no lamb -2.199207 time 2019-02-27 08:24:16.333136
Model ind 685 epoch 450 head A batch: 0 avg loss -2.211771 avg loss no lamb -2.211771 time 2019-02-27 08:25:31.947627
Model ind 685 epoch 450 head A batch: 100 avg loss -2.217102 avg loss no lamb -2.217102 time 2019-02-27 08:26:47.630902
Model ind 685 epoch 450 head A batch: 200 avg loss -2.183891 avg loss no lamb -2.183891 time 2019-02-27 08:28:04.305224
Model ind 685 epoch 450 head A batch: 300 avg loss -2.183435 avg loss no lamb -2.183435 time 2019-02-27 08:29:20.530302
Model ind 685 epoch 450 head A batch: 400 avg loss -2.176641 avg loss no lamb -2.176641 time 2019-02-27 08:30:35.625539
Pre: time 2019-02-27 08:32:10.933228: 
 	std: 0.006481409
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	train_accs: [0.9924143, 0.9790286, 0.9921, 0.9790286, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.98432004
	best: 0.9924143

Starting e_i: 451
Model ind 685 epoch 451 head B batch: 0 avg loss -2.233774 avg loss no lamb -2.233774 time 2019-02-27 08:32:13.236411
Model ind 685 epoch 451 head B batch: 100 avg loss -2.205216 avg loss no lamb -2.205216 time 2019-02-27 08:33:28.743956
Model ind 685 epoch 451 head B batch: 200 avg loss -2.204743 avg loss no lamb -2.204743 time 2019-02-27 08:34:44.814643
Model ind 685 epoch 451 head B batch: 300 avg loss -2.242984 avg loss no lamb -2.242984 time 2019-02-27 08:36:01.664665
Model ind 685 epoch 451 head B batch: 400 avg loss -2.205851 avg loss no lamb -2.205851 time 2019-02-27 08:37:19.358225
Model ind 685 epoch 451 head B batch: 0 avg loss -2.209823 avg loss no lamb -2.209823 time 2019-02-27 08:38:33.786444
Model ind 685 epoch 451 head B batch: 100 avg loss -2.171337 avg loss no lamb -2.171337 time 2019-02-27 08:39:49.750536
Model ind 685 epoch 451 head B batch: 200 avg loss -2.222467 avg loss no lamb -2.222467 time 2019-02-27 08:41:05.291905
Model ind 685 epoch 451 head B batch: 300 avg loss -2.219875 avg loss no lamb -2.219875 time 2019-02-27 08:42:21.258344
Model ind 685 epoch 451 head B batch: 400 avg loss -2.171091 avg loss no lamb -2.171091 time 2019-02-27 08:43:37.281172
Model ind 685 epoch 451 head A batch: 0 avg loss -2.256244 avg loss no lamb -2.256244 time 2019-02-27 08:44:53.768300
Model ind 685 epoch 451 head A batch: 100 avg loss -2.208677 avg loss no lamb -2.208677 time 2019-02-27 08:46:06.110947
Model ind 685 epoch 451 head A batch: 200 avg loss -2.218380 avg loss no lamb -2.218380 time 2019-02-27 08:47:22.634775
Model ind 685 epoch 451 head A batch: 300 avg loss -2.237193 avg loss no lamb -2.237193 time 2019-02-27 08:48:41.024487
Model ind 685 epoch 451 head A batch: 400 avg loss -2.227341 avg loss no lamb -2.227341 time 2019-02-27 08:49:56.633931
Pre: time 2019-02-27 08:51:31.915529: 
 	std: 0.006480334
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790428, 0.9921, 0.9790286, 0.9790428]
	train_accs: [0.9924286, 0.9790428, 0.9921, 0.9790286, 0.9790428]
	best_train_sub_head: 0
	worst: 0.9790286
	avg: 0.9843286
	best: 0.9924286

Starting e_i: 452
Model ind 685 epoch 452 head B batch: 0 avg loss -2.236656 avg loss no lamb -2.236656 time 2019-02-27 08:51:34.162732
Model ind 685 epoch 452 head B batch: 100 avg loss -2.226461 avg loss no lamb -2.226461 time 2019-02-27 08:52:37.516612
Model ind 685 epoch 452 head B batch: 200 avg loss -2.200759 avg loss no lamb -2.200759 time 2019-02-27 08:53:52.380566
Model ind 685 epoch 452 head B batch: 300 avg loss -2.220324 avg loss no lamb -2.220324 time 2019-02-27 08:55:11.403058
Model ind 685 epoch 452 head B batch: 400 avg loss -2.159231 avg loss no lamb -2.159231 time 2019-02-27 08:56:26.691343
Model ind 685 epoch 452 head B batch: 0 avg loss -2.219316 avg loss no lamb -2.219316 time 2019-02-27 08:57:43.539319
Model ind 685 epoch 452 head B batch: 100 avg loss -2.255361 avg loss no lamb -2.255361 time 2019-02-27 08:59:00.321701
Model ind 685 epoch 452 head B batch: 200 avg loss -2.206598 avg loss no lamb -2.206598 time 2019-02-27 09:00:17.616822
Model ind 685 epoch 452 head B batch: 300 avg loss -2.234368 avg loss no lamb -2.234368 time 2019-02-27 09:01:36.688707
Model ind 685 epoch 452 head B batch: 400 avg loss -2.181747 avg loss no lamb -2.181747 time 2019-02-27 09:02:54.975775
Model ind 685 epoch 452 head A batch: 0 avg loss -2.186105 avg loss no lamb -2.186105 time 2019-02-27 09:04:14.850333
Model ind 685 epoch 452 head A batch: 100 avg loss -2.181598 avg loss no lamb -2.181598 time 2019-02-27 09:05:33.724823
Model ind 685 epoch 452 head A batch: 200 avg loss -2.171910 avg loss no lamb -2.171910 time 2019-02-27 09:06:50.137026
Model ind 685 epoch 452 head A batch: 300 avg loss -2.213520 avg loss no lamb -2.213520 time 2019-02-27 09:08:08.615273
Model ind 685 epoch 452 head A batch: 400 avg loss -2.212351 avg loss no lamb -2.212351 time 2019-02-27 09:09:27.275140
Pre: time 2019-02-27 09:11:00.935844: 
 	std: 0.0064907544
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.979]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.979]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843086
	best: 0.9924143

Starting e_i: 453
Model ind 685 epoch 453 head B batch: 0 avg loss -2.238062 avg loss no lamb -2.238062 time 2019-02-27 09:11:03.101445
Model ind 685 epoch 453 head B batch: 100 avg loss -2.204211 avg loss no lamb -2.204211 time 2019-02-27 09:12:19.866707
Model ind 685 epoch 453 head B batch: 200 avg loss -2.214266 avg loss no lamb -2.214266 time 2019-02-27 09:13:36.520732
Model ind 685 epoch 453 head B batch: 300 avg loss -2.247611 avg loss no lamb -2.247611 time 2019-02-27 09:14:55.163393
Model ind 685 epoch 453 head B batch: 400 avg loss -2.211074 avg loss no lamb -2.211074 time 2019-02-27 09:16:11.181528
Model ind 685 epoch 453 head B batch: 0 avg loss -2.219386 avg loss no lamb -2.219386 time 2019-02-27 09:17:27.286879
Model ind 685 epoch 453 head B batch: 100 avg loss -2.207153 avg loss no lamb -2.207153 time 2019-02-27 09:18:45.458090
Model ind 685 epoch 453 head B batch: 200 avg loss -2.210150 avg loss no lamb -2.210150 time 2019-02-27 09:20:02.268283
Model ind 685 epoch 453 head B batch: 300 avg loss -2.226268 avg loss no lamb -2.226268 time 2019-02-27 09:21:19.458221
Model ind 685 epoch 453 head B batch: 400 avg loss -2.195537 avg loss no lamb -2.195537 time 2019-02-27 09:22:35.782176
Model ind 685 epoch 453 head A batch: 0 avg loss -2.217564 avg loss no lamb -2.217564 time 2019-02-27 09:23:52.837053
Model ind 685 epoch 453 head A batch: 100 avg loss -2.193764 avg loss no lamb -2.193764 time 2019-02-27 09:25:11.430493
Model ind 685 epoch 453 head A batch: 200 avg loss -2.212693 avg loss no lamb -2.212693 time 2019-02-27 09:26:31.628341
Model ind 685 epoch 453 head A batch: 300 avg loss -2.234437 avg loss no lamb -2.234437 time 2019-02-27 09:27:50.731824
Model ind 685 epoch 453 head A batch: 400 avg loss -2.219763 avg loss no lamb -2.219763 time 2019-02-27 09:29:09.824183
Pre: time 2019-02-27 09:30:45.899750: 
 	std: 0.0064896573
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.979, 0.9920857, 0.979, 0.9790143]
	train_accs: [0.9924143, 0.979, 0.9920857, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.979
	avg: 0.9843029
	best: 0.9924143

Starting e_i: 454
Model ind 685 epoch 454 head B batch: 0 avg loss -2.197832 avg loss no lamb -2.197832 time 2019-02-27 09:30:48.109089
Model ind 685 epoch 454 head B batch: 100 avg loss -2.198898 avg loss no lamb -2.198898 time 2019-02-27 09:32:04.667496
Model ind 685 epoch 454 head B batch: 200 avg loss -2.229027 avg loss no lamb -2.229027 time 2019-02-27 09:33:21.663161
Model ind 685 epoch 454 head B batch: 300 avg loss -2.219801 avg loss no lamb -2.219801 time 2019-02-27 09:34:39.820291
Model ind 685 epoch 454 head B batch: 400 avg loss -2.218026 avg loss no lamb -2.218026 time 2019-02-27 09:35:48.625793
Model ind 685 epoch 454 head B batch: 0 avg loss -2.231989 avg loss no lamb -2.231989 time 2019-02-27 09:37:07.730809
Model ind 685 epoch 454 head B batch: 100 avg loss -2.226824 avg loss no lamb -2.226824 time 2019-02-27 09:38:26.083459
Model ind 685 epoch 454 head B batch: 200 avg loss -2.210799 avg loss no lamb -2.210799 time 2019-02-27 09:39:41.778343
Model ind 685 epoch 454 head B batch: 300 avg loss -2.255530 avg loss no lamb -2.255530 time 2019-02-27 09:40:59.834142
Model ind 685 epoch 454 head B batch: 400 avg loss -2.191584 avg loss no lamb -2.191584 time 2019-02-27 09:42:18.856958
Model ind 685 epoch 454 head A batch: 0 avg loss -2.211002 avg loss no lamb -2.211002 time 2019-02-27 09:43:37.518058
Model ind 685 epoch 454 head A batch: 100 avg loss -2.204834 avg loss no lamb -2.204834 time 2019-02-27 09:44:56.470281
Model ind 685 epoch 454 head A batch: 200 avg loss -2.202601 avg loss no lamb -2.202601 time 2019-02-27 09:46:14.312843
Model ind 685 epoch 454 head A batch: 300 avg loss -2.222622 avg loss no lamb -2.222622 time 2019-02-27 09:47:32.293494
Model ind 685 epoch 454 head A batch: 400 avg loss -2.191506 avg loss no lamb -2.191506 time 2019-02-27 09:48:51.492419
Pre: time 2019-02-27 09:50:27.465260: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 455
Model ind 685 epoch 455 head B batch: 0 avg loss -2.263348 avg loss no lamb -2.263348 time 2019-02-27 09:50:29.961763
Model ind 685 epoch 455 head B batch: 100 avg loss -2.220892 avg loss no lamb -2.220892 time 2019-02-27 09:51:52.565349
Model ind 685 epoch 455 head B batch: 200 avg loss -2.221656 avg loss no lamb -2.221656 time 2019-02-27 09:53:15.881678
Model ind 685 epoch 455 head B batch: 300 avg loss -2.238900 avg loss no lamb -2.238900 time 2019-02-27 09:54:38.673283
Model ind 685 epoch 455 head B batch: 400 avg loss -2.212461 avg loss no lamb -2.212461 time 2019-02-27 09:55:59.161452
Model ind 685 epoch 455 head B batch: 0 avg loss -2.234328 avg loss no lamb -2.234328 time 2019-02-27 09:57:20.305004
Model ind 685 epoch 455 head B batch: 100 avg loss -2.184873 avg loss no lamb -2.184873 time 2019-02-27 09:58:42.744810
Model ind 685 epoch 455 head B batch: 200 avg loss -2.208789 avg loss no lamb -2.208789 time 2019-02-27 10:00:04.333011
Model ind 685 epoch 455 head B batch: 300 avg loss -2.222916 avg loss no lamb -2.222916 time 2019-02-27 10:01:26.292588
Model ind 685 epoch 455 head B batch: 400 avg loss -2.189632 avg loss no lamb -2.189632 time 2019-02-27 10:02:46.041495
Model ind 685 epoch 455 head A batch: 0 avg loss -2.225829 avg loss no lamb -2.225829 time 2019-02-27 10:04:05.969291
Model ind 685 epoch 455 head A batch: 100 avg loss -2.229164 avg loss no lamb -2.229164 time 2019-02-27 10:05:26.041489
Model ind 685 epoch 455 head A batch: 200 avg loss -2.223361 avg loss no lamb -2.223361 time 2019-02-27 10:06:46.652834
Model ind 685 epoch 455 head A batch: 300 avg loss -2.211543 avg loss no lamb -2.211543 time 2019-02-27 10:08:06.153620
Model ind 685 epoch 455 head A batch: 400 avg loss -2.217544 avg loss no lamb -2.217544 time 2019-02-27 10:09:26.435428
Pre: time 2019-02-27 10:11:02.411462: 
 	std: 0.0064884163
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924143, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98431146
	best: 0.9924143

Starting e_i: 456
Model ind 685 epoch 456 head B batch: 0 avg loss -2.245847 avg loss no lamb -2.245847 time 2019-02-27 10:11:04.676562
Model ind 685 epoch 456 head B batch: 100 avg loss -2.225219 avg loss no lamb -2.225219 time 2019-02-27 10:12:23.942731
Model ind 685 epoch 456 head B batch: 200 avg loss -2.194319 avg loss no lamb -2.194319 time 2019-02-27 10:13:43.512342
Model ind 685 epoch 456 head B batch: 300 avg loss -2.241423 avg loss no lamb -2.241423 time 2019-02-27 10:15:03.361844
Model ind 685 epoch 456 head B batch: 400 avg loss -2.197067 avg loss no lamb -2.197067 time 2019-02-27 10:16:22.837844
Model ind 685 epoch 456 head B batch: 0 avg loss -2.228334 avg loss no lamb -2.228334 time 2019-02-27 10:17:37.340830
Model ind 685 epoch 456 head B batch: 100 avg loss -2.220125 avg loss no lamb -2.220125 time 2019-02-27 10:18:52.841485
Model ind 685 epoch 456 head B batch: 200 avg loss -2.188216 avg loss no lamb -2.188216 time 2019-02-27 10:20:11.598233
Model ind 685 epoch 456 head B batch: 300 avg loss -2.212476 avg loss no lamb -2.212476 time 2019-02-27 10:21:33.222869
Model ind 685 epoch 456 head B batch: 400 avg loss -2.198739 avg loss no lamb -2.198739 time 2019-02-27 10:22:52.551779
Model ind 685 epoch 456 head A batch: 0 avg loss -2.242432 avg loss no lamb -2.242432 time 2019-02-27 10:24:12.296661
Model ind 685 epoch 456 head A batch: 100 avg loss -2.206103 avg loss no lamb -2.206103 time 2019-02-27 10:25:31.996556
Model ind 685 epoch 456 head A batch: 200 avg loss -2.230880 avg loss no lamb -2.230880 time 2019-02-27 10:26:51.335331
Model ind 685 epoch 456 head A batch: 300 avg loss -2.230826 avg loss no lamb -2.230826 time 2019-02-27 10:28:11.166913
Model ind 685 epoch 456 head A batch: 400 avg loss -2.212133 avg loss no lamb -2.212133 time 2019-02-27 10:29:31.078283
Pre: time 2019-02-27 10:31:07.781190: 
 	std: 0.0064915987
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.9921143, 0.9789857, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842857
	best: 0.99235713

Starting e_i: 457
Model ind 685 epoch 457 head B batch: 0 avg loss -2.240103 avg loss no lamb -2.240103 time 2019-02-27 10:31:10.089367
Model ind 685 epoch 457 head B batch: 100 avg loss -2.233064 avg loss no lamb -2.233064 time 2019-02-27 10:32:30.478823
Model ind 685 epoch 457 head B batch: 200 avg loss -2.218860 avg loss no lamb -2.218860 time 2019-02-27 10:33:47.692075
Model ind 685 epoch 457 head B batch: 300 avg loss -2.175840 avg loss no lamb -2.175840 time 2019-02-27 10:35:06.211356
Model ind 685 epoch 457 head B batch: 400 avg loss -2.196109 avg loss no lamb -2.196109 time 2019-02-27 10:36:27.636625
Model ind 685 epoch 457 head B batch: 0 avg loss -2.221031 avg loss no lamb -2.221031 time 2019-02-27 10:37:48.867153
Model ind 685 epoch 457 head B batch: 100 avg loss -2.195294 avg loss no lamb -2.195294 time 2019-02-27 10:39:07.290409
Model ind 685 epoch 457 head B batch: 200 avg loss -2.208499 avg loss no lamb -2.208499 time 2019-02-27 10:40:26.104977
Model ind 685 epoch 457 head B batch: 300 avg loss -2.229936 avg loss no lamb -2.229936 time 2019-02-27 10:41:44.778687
Model ind 685 epoch 457 head B batch: 400 avg loss -2.217878 avg loss no lamb -2.217878 time 2019-02-27 10:43:04.889769
Model ind 685 epoch 457 head A batch: 0 avg loss -2.230538 avg loss no lamb -2.230538 time 2019-02-27 10:44:25.077478
Model ind 685 epoch 457 head A batch: 100 avg loss -2.207160 avg loss no lamb -2.207160 time 2019-02-27 10:45:43.467653
Model ind 685 epoch 457 head A batch: 200 avg loss -2.198343 avg loss no lamb -2.198343 time 2019-02-27 10:47:02.728206
Model ind 685 epoch 457 head A batch: 300 avg loss -2.249281 avg loss no lamb -2.249281 time 2019-02-27 10:48:21.487442
Model ind 685 epoch 457 head A batch: 400 avg loss -2.193072 avg loss no lamb -2.193072 time 2019-02-27 10:49:40.292619
Pre: time 2019-02-27 10:51:15.723349: 
 	std: 0.006495424
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	train_accs: [0.9924286, 0.9790143, 0.9921143, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843172
	best: 0.9924286

Starting e_i: 458
Model ind 685 epoch 458 head B batch: 0 avg loss -2.240065 avg loss no lamb -2.240065 time 2019-02-27 10:51:17.987965
Model ind 685 epoch 458 head B batch: 100 avg loss -2.224323 avg loss no lamb -2.224323 time 2019-02-27 10:52:36.520044
Model ind 685 epoch 458 head B batch: 200 avg loss -2.241233 avg loss no lamb -2.241233 time 2019-02-27 10:53:54.866683
Model ind 685 epoch 458 head B batch: 300 avg loss -2.223201 avg loss no lamb -2.223201 time 2019-02-27 10:55:13.534492
Model ind 685 epoch 458 head B batch: 400 avg loss -2.204118 avg loss no lamb -2.204118 time 2019-02-27 10:56:31.673064
Model ind 685 epoch 458 head B batch: 0 avg loss -2.245292 avg loss no lamb -2.245292 time 2019-02-27 10:57:48.961395
Model ind 685 epoch 458 head B batch: 100 avg loss -2.203223 avg loss no lamb -2.203223 time 2019-02-27 10:59:07.010349
Model ind 685 epoch 458 head B batch: 200 avg loss -2.181716 avg loss no lamb -2.181716 time 2019-02-27 11:00:20.294248
Model ind 685 epoch 458 head B batch: 300 avg loss -2.238934 avg loss no lamb -2.238934 time 2019-02-27 11:01:32.027593
Model ind 685 epoch 458 head B batch: 400 avg loss -2.182930 avg loss no lamb -2.182930 time 2019-02-27 11:02:49.056490
Model ind 685 epoch 458 head A batch: 0 avg loss -2.223124 avg loss no lamb -2.223124 time 2019-02-27 11:04:04.083940
Model ind 685 epoch 458 head A batch: 100 avg loss -2.218569 avg loss no lamb -2.218569 time 2019-02-27 11:05:22.042875
Model ind 685 epoch 458 head A batch: 200 avg loss -2.200118 avg loss no lamb -2.200118 time 2019-02-27 11:06:40.930358
Model ind 685 epoch 458 head A batch: 300 avg loss -2.230566 avg loss no lamb -2.230566 time 2019-02-27 11:07:58.278017
Model ind 685 epoch 458 head A batch: 400 avg loss -2.195470 avg loss no lamb -2.195470 time 2019-02-27 11:09:15.885600
Pre: time 2019-02-27 11:10:52.150914: 
 	std: 0.006481727
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99244285, 0.9790143, 0.99207145, 0.9790428, 0.9790286]
	train_accs: [0.99244285, 0.9790143, 0.99207145, 0.9790428, 0.9790286]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.98432
	best: 0.99244285

Starting e_i: 459
Model ind 685 epoch 459 head B batch: 0 avg loss -2.200840 avg loss no lamb -2.200840 time 2019-02-27 11:10:54.421276
Model ind 685 epoch 459 head B batch: 100 avg loss -2.199575 avg loss no lamb -2.199575 time 2019-02-27 11:12:14.238717
Model ind 685 epoch 459 head B batch: 200 avg loss -2.202790 avg loss no lamb -2.202790 time 2019-02-27 11:13:32.849537
Model ind 685 epoch 459 head B batch: 300 avg loss -2.245124 avg loss no lamb -2.245124 time 2019-02-27 11:14:50.331597
Model ind 685 epoch 459 head B batch: 400 avg loss -2.193428 avg loss no lamb -2.193428 time 2019-02-27 11:16:08.632719
Model ind 685 epoch 459 head B batch: 0 avg loss -2.234929 avg loss no lamb -2.234929 time 2019-02-27 11:17:27.181633
Model ind 685 epoch 459 head B batch: 100 avg loss -2.229341 avg loss no lamb -2.229341 time 2019-02-27 11:18:45.492784
Model ind 685 epoch 459 head B batch: 200 avg loss -2.231165 avg loss no lamb -2.231165 time 2019-02-27 11:20:03.707927
Model ind 685 epoch 459 head B batch: 300 avg loss -2.238459 avg loss no lamb -2.238459 time 2019-02-27 11:21:22.511577
Model ind 685 epoch 459 head B batch: 400 avg loss -2.198200 avg loss no lamb -2.198200 time 2019-02-27 11:22:40.615924
Model ind 685 epoch 459 head A batch: 0 avg loss -2.203404 avg loss no lamb -2.203404 time 2019-02-27 11:24:01.094541
Model ind 685 epoch 459 head A batch: 100 avg loss -2.230911 avg loss no lamb -2.230911 time 2019-02-27 11:25:19.685797
Model ind 685 epoch 459 head A batch: 200 avg loss -2.271538 avg loss no lamb -2.271538 time 2019-02-27 11:26:39.286929
Model ind 685 epoch 459 head A batch: 300 avg loss -2.235252 avg loss no lamb -2.235252 time 2019-02-27 11:27:57.635024
Model ind 685 epoch 459 head A batch: 400 avg loss -2.185471 avg loss no lamb -2.185471 time 2019-02-27 11:29:16.756467
Pre: time 2019-02-27 11:30:52.014775: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 460
Model ind 685 epoch 460 head B batch: 0 avg loss -2.233722 avg loss no lamb -2.233722 time 2019-02-27 11:30:54.102594
Model ind 685 epoch 460 head B batch: 100 avg loss -2.219771 avg loss no lamb -2.219771 time 2019-02-27 11:32:10.630439
Model ind 685 epoch 460 head B batch: 200 avg loss -2.231193 avg loss no lamb -2.231193 time 2019-02-27 11:33:28.097239
Model ind 685 epoch 460 head B batch: 300 avg loss -2.211070 avg loss no lamb -2.211070 time 2019-02-27 11:34:49.286461
Model ind 685 epoch 460 head B batch: 400 avg loss -2.219908 avg loss no lamb -2.219908 time 2019-02-27 11:36:09.124180
Model ind 685 epoch 460 head B batch: 0 avg loss -2.242054 avg loss no lamb -2.242054 time 2019-02-27 11:37:25.892623
Model ind 685 epoch 460 head B batch: 100 avg loss -2.225702 avg loss no lamb -2.225702 time 2019-02-27 11:38:43.337079
Model ind 685 epoch 460 head B batch: 200 avg loss -2.228538 avg loss no lamb -2.228538 time 2019-02-27 11:40:01.586811
Model ind 685 epoch 460 head B batch: 300 avg loss -2.253645 avg loss no lamb -2.253645 time 2019-02-27 11:41:21.375219
Model ind 685 epoch 460 head B batch: 400 avg loss -2.178601 avg loss no lamb -2.178601 time 2019-02-27 11:42:40.059890
Model ind 685 epoch 460 head A batch: 0 avg loss -2.198079 avg loss no lamb -2.198079 time 2019-02-27 11:43:40.681365
Model ind 685 epoch 460 head A batch: 100 avg loss -2.200906 avg loss no lamb -2.200906 time 2019-02-27 11:44:57.182692
Model ind 685 epoch 460 head A batch: 200 avg loss -2.176324 avg loss no lamb -2.176324 time 2019-02-27 11:46:10.210278
Model ind 685 epoch 460 head A batch: 300 avg loss -2.218609 avg loss no lamb -2.218609 time 2019-02-27 11:47:26.489942
Model ind 685 epoch 460 head A batch: 400 avg loss -2.208498 avg loss no lamb -2.208498 time 2019-02-27 11:48:40.714255
Pre: time 2019-02-27 11:50:10.203976: 
 	std: 0.0064848447
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	train_accs: [0.9924, 0.9790143, 0.9921, 0.9790143, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9790143
	avg: 0.9843086
	best: 0.9924

Starting e_i: 461
Model ind 685 epoch 461 head B batch: 0 avg loss -2.206566 avg loss no lamb -2.206566 time 2019-02-27 11:50:13.609288
Model ind 685 epoch 461 head B batch: 100 avg loss -2.220120 avg loss no lamb -2.220120 time 2019-02-27 11:51:31.919755
Model ind 685 epoch 461 head B batch: 200 avg loss -2.207218 avg loss no lamb -2.207218 time 2019-02-27 11:52:47.106791
Model ind 685 epoch 461 head B batch: 300 avg loss -2.245349 avg loss no lamb -2.245349 time 2019-02-27 11:54:06.955906
Model ind 685 epoch 461 head B batch: 400 avg loss -2.186938 avg loss no lamb -2.186938 time 2019-02-27 11:55:26.162887
Model ind 685 epoch 461 head B batch: 0 avg loss -2.183955 avg loss no lamb -2.183955 time 2019-02-27 11:56:41.970597
Model ind 685 epoch 461 head B batch: 100 avg loss -2.248804 avg loss no lamb -2.248804 time 2019-02-27 11:58:00.559123
Model ind 685 epoch 461 head B batch: 200 avg loss -2.223749 avg loss no lamb -2.223749 time 2019-02-27 11:59:17.838463
Model ind 685 epoch 461 head B batch: 300 avg loss -2.262500 avg loss no lamb -2.262500 time 2019-02-27 12:00:33.030025
Model ind 685 epoch 461 head B batch: 400 avg loss -2.168110 avg loss no lamb -2.168110 time 2019-02-27 12:01:50.088098
Model ind 685 epoch 461 head A batch: 0 avg loss -2.223891 avg loss no lamb -2.223891 time 2019-02-27 12:03:04.946459
Model ind 685 epoch 461 head A batch: 100 avg loss -2.244397 avg loss no lamb -2.244397 time 2019-02-27 12:04:18.001684
Model ind 685 epoch 461 head A batch: 200 avg loss -2.202691 avg loss no lamb -2.202691 time 2019-02-27 12:05:34.607094
Model ind 685 epoch 461 head A batch: 300 avg loss -2.228412 avg loss no lamb -2.228412 time 2019-02-27 12:06:48.903766
Model ind 685 epoch 461 head A batch: 400 avg loss -2.177110 avg loss no lamb -2.177110 time 2019-02-27 12:08:03.345401
Pre: time 2019-02-27 12:09:37.944729: 
 	std: 0.006483909
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.9924, 0.9789714, 0.99205714, 0.979, 0.9790143]
	train_accs: [0.9924, 0.9789714, 0.99205714, 0.979, 0.9790143]
	best_train_sub_head: 0
	worst: 0.9789714
	avg: 0.9842886
	best: 0.9924

Starting e_i: 462
Model ind 685 epoch 462 head B batch: 0 avg loss -2.231063 avg loss no lamb -2.231063 time 2019-02-27 12:09:40.003614
Model ind 685 epoch 462 head B batch: 100 avg loss -2.203116 avg loss no lamb -2.203116 time 2019-02-27 12:10:57.074785
Model ind 685 epoch 462 head B batch: 200 avg loss -2.207002 avg loss no lamb -2.207002 time 2019-02-27 12:12:13.882207
Model ind 685 epoch 462 head B batch: 300 avg loss -2.240656 avg loss no lamb -2.240656 time 2019-02-27 12:13:33.486499
Model ind 685 epoch 462 head B batch: 400 avg loss -2.229534 avg loss no lamb -2.229534 time 2019-02-27 12:14:54.288273
Model ind 685 epoch 462 head B batch: 0 avg loss -2.238544 avg loss no lamb -2.238544 time 2019-02-27 12:16:09.201453
Model ind 685 epoch 462 head B batch: 100 avg loss -2.229368 avg loss no lamb -2.229368 time 2019-02-27 12:17:20.712475
Model ind 685 epoch 462 head B batch: 200 avg loss -2.231111 avg loss no lamb -2.231111 time 2019-02-27 12:18:38.577126
Model ind 685 epoch 462 head B batch: 300 avg loss -2.216864 avg loss no lamb -2.216864 time 2019-02-27 12:19:54.262013
Model ind 685 epoch 462 head B batch: 400 avg loss -2.204049 avg loss no lamb -2.204049 time 2019-02-27 12:21:13.874660
Model ind 685 epoch 462 head A batch: 0 avg loss -2.228906 avg loss no lamb -2.228906 time 2019-02-27 12:22:33.439400
Model ind 685 epoch 462 head A batch: 100 avg loss -2.216005 avg loss no lamb -2.216005 time 2019-02-27 12:23:50.648170
Model ind 685 epoch 462 head A batch: 200 avg loss -2.224643 avg loss no lamb -2.224643 time 2019-02-27 12:25:11.231717
Model ind 685 epoch 462 head A batch: 300 avg loss -2.224259 avg loss no lamb -2.224259 time 2019-02-27 12:26:22.198058
Model ind 685 epoch 462 head A batch: 400 avg loss -2.201895 avg loss no lamb -2.201895 time 2019-02-27 12:27:38.829815
Pre: time 2019-02-27 12:29:14.857186: 
 	std: 0.0064755133
	best_train_sub_head_match: [(0, 9), (1, 3), (2, 1), (3, 4), (4, 7), (5, 8), (6, 5), (7, 6), (8, 0), (9, 2)]
	test_accs: [0.99235713, 0.9789857, 0.99205714, 0.979, 0.9789857]
	train_accs: [0.99235713, 0.9789857, 0.99205714, 0.979, 0.9789857]
	best_train_sub_head: 0
	worst: 0.9789857
	avg: 0.9842771
	best: 0.99235713

Starting e_i: 463
Model ind 685 epoch 463 head B batch: 0 avg loss -2.256561 avg loss no lamb -2.256561 time 2019-02-27 12:29:17.051412
Model ind 685 epoch 463 head B batch: 100 avg loss -2.205677 avg loss no lamb -2.205677 time 2019-02-27 12:30:35.714410
Model ind 685 epoch 463 head B batch: 200 avg loss -2.214517 avg loss no lamb -2.214517 time 2019-02-27 12:31:53.800750
Model ind 685 epoch 463 head B batch: 300 avg loss -2.192060 avg loss no lamb -2.192060 time 2019-02-27 12:33:15.207505
Model ind 685 epoch 463 head B batch: 400 avg loss -2.216071 avg loss no lamb -2.216071 time 2019-02-27 12:34:36.111257
Model ind 685 epoch 463 head B batch: 0 avg loss -2.235356 avg loss no lamb -2.235356 time 2019-02-27 12:35:55.454301
