Debug: False
Loading restarting config from: /scratch/shared/slow/xuji/iid_private/640/config.pickle
(_sobel_multioutput_make_transforms) config.include_rgb: False
not using cutout
not demeaning data
not per image demeaning data
Making datasets with <class 'torchvision.datasets.cifar.CIFAR10'> and None
Creating auxiliary dataloader ind 0 out of 3 time 2019-02-22 11:06:57.345564
Creating auxiliary dataloader ind 1 out of 3 time 2019-02-22 11:06:58.518301
Creating auxiliary dataloader ind 2 out of 3 time 2019-02-22 11:06:59.681840
Length of datasets vector 4
Number of batches per epoch: 273
Creating auxiliary dataloader ind 0 out of 3 time 2019-02-22 11:07:02.001639
Creating auxiliary dataloader ind 1 out of 3 time 2019-02-22 11:07:03.147850
Creating auxiliary dataloader ind 2 out of 3 time 2019-02-22 11:07:04.307839
Length of datasets vector 4
Number of batches per epoch: 273
avg_pool_sz 3
semisup: False
starting from epoch 471
Starting e_i: 471
Model ind 640 epoch 471 head A head_i_epoch 0 batch 0: avg loss -2.984811 avg loss no lamb -2.984811 time 2019-02-22 11:07:14.503008
Model ind 640 epoch 471 head A head_i_epoch 0 batch 1: avg loss -3.013364 avg loss no lamb -3.013364 time 2019-02-22 11:07:15.621131
Model ind 640 epoch 471 head A head_i_epoch 0 batch 2: avg loss -3.017820 avg loss no lamb -3.017820 time 2019-02-22 11:07:16.728111
Model ind 640 epoch 471 head A head_i_epoch 0 batch 3: avg loss -2.991946 avg loss no lamb -2.991946 time 2019-02-22 11:07:17.799561
Model ind 640 epoch 471 head A head_i_epoch 0 batch 4: avg loss -3.068928 avg loss no lamb -3.068928 time 2019-02-22 11:07:18.925140
Model ind 640 epoch 471 head A head_i_epoch 0 batch 5: avg loss -2.986180 avg loss no lamb -2.986180 time 2019-02-22 11:07:19.982339
Model ind 640 epoch 471 head A head_i_epoch 0 batch 6: avg loss -2.983600 avg loss no lamb -2.983600 time 2019-02-22 11:07:21.069747
Model ind 640 epoch 471 head A head_i_epoch 0 batch 7: avg loss -2.979846 avg loss no lamb -2.979846 time 2019-02-22 11:07:22.161000
Model ind 640 epoch 471 head A head_i_epoch 0 batch 8: avg loss -2.965488 avg loss no lamb -2.965488 time 2019-02-22 11:07:23.262028
Model ind 640 epoch 471 head A head_i_epoch 0 batch 9: avg loss -2.982079 avg loss no lamb -2.982079 time 2019-02-22 11:07:24.383916
Model ind 640 epoch 471 head A head_i_epoch 0 batch 100: avg loss -3.006114 avg loss no lamb -3.006114 time 2019-02-22 11:09:04.510696
Model ind 640 epoch 471 head A head_i_epoch 0 batch 200: avg loss -3.047638 avg loss no lamb -3.047638 time 2019-02-22 11:10:51.609612
last batch sz 160
Model ind 640 epoch 471 head B head_i_epoch 0 batch 0: avg loss -1.738204 avg loss no lamb -1.738204 time 2019-02-22 11:12:12.026486
Model ind 640 epoch 471 head B head_i_epoch 0 batch 1: avg loss -1.700227 avg loss no lamb -1.700227 time 2019-02-22 11:12:13.152134
Model ind 640 epoch 471 head B head_i_epoch 0 batch 2: avg loss -1.612627 avg loss no lamb -1.612627 time 2019-02-22 11:12:14.322847
Model ind 640 epoch 471 head B head_i_epoch 0 batch 3: avg loss -1.594656 avg loss no lamb -1.594656 time 2019-02-22 11:12:15.480775
Model ind 640 epoch 471 head B head_i_epoch 0 batch 4: avg loss -1.738428 avg loss no lamb -1.738428 time 2019-02-22 11:12:16.559120
Model ind 640 epoch 471 head B head_i_epoch 0 batch 5: avg loss -1.649500 avg loss no lamb -1.649500 time 2019-02-22 11:12:17.663432
Model ind 640 epoch 471 head B head_i_epoch 0 batch 6: avg loss -1.614079 avg loss no lamb -1.614079 time 2019-02-22 11:12:18.798636
Model ind 640 epoch 471 head B head_i_epoch 0 batch 7: avg loss -1.608327 avg loss no lamb -1.608327 time 2019-02-22 11:12:19.923572
Model ind 640 epoch 471 head B head_i_epoch 0 batch 8: avg loss -1.638626 avg loss no lamb -1.638626 time 2019-02-22 11:12:21.065916
Model ind 640 epoch 471 head B head_i_epoch 0 batch 9: avg loss -1.628845 avg loss no lamb -1.628845 time 2019-02-22 11:12:22.219359
Model ind 640 epoch 471 head B head_i_epoch 0 batch 100: avg loss -1.658064 avg loss no lamb -1.658064 time 2019-02-22 11:14:03.147089
Model ind 640 epoch 471 head B head_i_epoch 0 batch 200: avg loss -1.724163 avg loss no lamb -1.724163 time 2019-02-22 11:15:53.957917
last batch sz 160
Model ind 640 epoch 471 head B head_i_epoch 1 batch 0: avg loss -1.700383 avg loss no lamb -1.700383 time 2019-02-22 11:17:14.982553
Model ind 640 epoch 471 head B head_i_epoch 1 batch 1: avg loss -1.682128 avg loss no lamb -1.682128 time 2019-02-22 11:17:16.096164
Model ind 640 epoch 471 head B head_i_epoch 1 batch 2: avg loss -1.609219 avg loss no lamb -1.609219 time 2019-02-22 11:17:17.221918
Model ind 640 epoch 471 head B head_i_epoch 1 batch 3: avg loss -1.643366 avg loss no lamb -1.643366 time 2019-02-22 11:17:18.368758
Model ind 640 epoch 471 head B head_i_epoch 1 batch 4: avg loss -1.785264 avg loss no lamb -1.785264 time 2019-02-22 11:17:19.518305
Model ind 640 epoch 471 head B head_i_epoch 1 batch 5: avg loss -1.737481 avg loss no lamb -1.737481 time 2019-02-22 11:17:20.696704
Model ind 640 epoch 471 head B head_i_epoch 1 batch 6: avg loss -1.592280 avg loss no lamb -1.592280 time 2019-02-22 11:17:21.771898
Model ind 640 epoch 471 head B head_i_epoch 1 batch 7: avg loss -1.629326 avg loss no lamb -1.629326 time 2019-02-22 11:17:22.847238
Model ind 640 epoch 471 head B head_i_epoch 1 batch 8: avg loss -1.657600 avg loss no lamb -1.657600 time 2019-02-22 11:17:23.925026
Model ind 640 epoch 471 head B head_i_epoch 1 batch 9: avg loss -1.566548 avg loss no lamb -1.566548 time 2019-02-22 11:17:25.049439
Model ind 640 epoch 471 head B head_i_epoch 1 batch 100: avg loss -1.633541 avg loss no lamb -1.633541 time 2019-02-22 11:19:06.522831
Model ind 640 epoch 471 head B head_i_epoch 1 batch 200: avg loss -1.777569 avg loss no lamb -1.777569 time 2019-02-22 11:20:57.691756
last batch sz 160
Pre: time 2019-02-22 11:22:39.715337: 
 	std: 0.051606413
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50165, 0.6063333, 0.60658336, 0.6065, 0.50061667]
	train_accs: [0.50165, 0.6063333, 0.60658336, 0.6065, 0.50061667]
	best_train_sub_head: 2
	worst: 0.50061667
	avg: 0.56433666
	best: 0.60658336

Starting e_i: 472
Model ind 640 epoch 472 head A head_i_epoch 0 batch 0: avg loss -3.018611 avg loss no lamb -3.018611 time 2019-02-22 11:22:42.024844
Model ind 640 epoch 472 head A head_i_epoch 0 batch 100: avg loss -2.955565 avg loss no lamb -2.955565 time 2019-02-22 11:24:33.312048
Model ind 640 epoch 472 head A head_i_epoch 0 batch 200: avg loss -3.111318 avg loss no lamb -3.111318 time 2019-02-22 11:26:24.527621
last batch sz 160
Model ind 640 epoch 472 head B head_i_epoch 0 batch 0: avg loss -1.687631 avg loss no lamb -1.687631 time 2019-02-22 11:27:45.646703
Model ind 640 epoch 472 head B head_i_epoch 0 batch 100: avg loss -1.536485 avg loss no lamb -1.536485 time 2019-02-22 11:29:37.677487
Model ind 640 epoch 472 head B head_i_epoch 0 batch 200: avg loss -1.851693 avg loss no lamb -1.851693 time 2019-02-22 11:31:31.035312
last batch sz 160
Model ind 640 epoch 472 head B head_i_epoch 1 batch 0: avg loss -1.743899 avg loss no lamb -1.743899 time 2019-02-22 11:32:50.963186
Model ind 640 epoch 472 head B head_i_epoch 1 batch 100: avg loss -1.566243 avg loss no lamb -1.566243 time 2019-02-22 11:34:42.615679
Model ind 640 epoch 472 head B head_i_epoch 1 batch 200: avg loss -1.920730 avg loss no lamb -1.920730 time 2019-02-22 11:36:33.371109
last batch sz 160
Pre: time 2019-02-22 11:38:15.476113: 
 	std: 0.053258877
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49973333, 0.60735, 0.6081333, 0.60751665, 0.49818334]
	train_accs: [0.49973333, 0.60735, 0.6081333, 0.60751665, 0.49818334]
	best_train_sub_head: 2
	worst: 0.49818334
	avg: 0.56418335
	best: 0.6081333

Starting e_i: 473
Model ind 640 epoch 473 head A head_i_epoch 0 batch 0: avg loss -3.070181 avg loss no lamb -3.070181 time 2019-02-22 11:38:17.722834
Model ind 640 epoch 473 head A head_i_epoch 0 batch 100: avg loss -2.954457 avg loss no lamb -2.954457 time 2019-02-22 11:40:08.765699
Model ind 640 epoch 473 head A head_i_epoch 0 batch 200: avg loss -3.068291 avg loss no lamb -3.068291 time 2019-02-22 11:42:02.936546
last batch sz 160
Model ind 640 epoch 473 head B head_i_epoch 0 batch 0: avg loss -1.719101 avg loss no lamb -1.719101 time 2019-02-22 11:43:23.432049
Model ind 640 epoch 473 head B head_i_epoch 0 batch 100: avg loss -1.588123 avg loss no lamb -1.588123 time 2019-02-22 11:45:15.392553
Model ind 640 epoch 473 head B head_i_epoch 0 batch 200: avg loss -1.855710 avg loss no lamb -1.855710 time 2019-02-22 11:47:07.160791
last batch sz 160
Model ind 640 epoch 473 head B head_i_epoch 1 batch 0: avg loss -1.638794 avg loss no lamb -1.638794 time 2019-02-22 11:48:27.946664
Model ind 640 epoch 473 head B head_i_epoch 1 batch 100: avg loss -1.607810 avg loss no lamb -1.607810 time 2019-02-22 11:50:19.331635
Model ind 640 epoch 473 head B head_i_epoch 1 batch 200: avg loss -1.719177 avg loss no lamb -1.719177 time 2019-02-22 11:52:10.950450
last batch sz 160
Pre: time 2019-02-22 11:53:53.137785: 
 	std: 0.0521215
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50093335, 0.6067167, 0.607, 0.6064, 0.4997]
	train_accs: [0.50093335, 0.6067167, 0.607, 0.6064, 0.4997]
	best_train_sub_head: 2
	worst: 0.4997
	avg: 0.56415004
	best: 0.607

Starting e_i: 474
Model ind 640 epoch 474 head A head_i_epoch 0 batch 0: avg loss -3.017133 avg loss no lamb -3.017133 time 2019-02-22 11:53:55.354709
Model ind 640 epoch 474 head A head_i_epoch 0 batch 100: avg loss -2.946287 avg loss no lamb -2.946287 time 2019-02-22 11:55:46.501090
Model ind 640 epoch 474 head A head_i_epoch 0 batch 200: avg loss -2.997276 avg loss no lamb -2.997276 time 2019-02-22 11:57:38.151377
last batch sz 160
Model ind 640 epoch 474 head B head_i_epoch 0 batch 0: avg loss -1.734026 avg loss no lamb -1.734026 time 2019-02-22 11:58:59.219772
Model ind 640 epoch 474 head B head_i_epoch 0 batch 100: avg loss -1.536482 avg loss no lamb -1.536482 time 2019-02-22 12:00:50.556890
Model ind 640 epoch 474 head B head_i_epoch 0 batch 200: avg loss -1.826692 avg loss no lamb -1.826692 time 2019-02-22 12:02:42.245071
last batch sz 160
Model ind 640 epoch 474 head B head_i_epoch 1 batch 0: avg loss -1.734152 avg loss no lamb -1.734152 time 2019-02-22 12:04:03.572711
Model ind 640 epoch 474 head B head_i_epoch 1 batch 100: avg loss -1.588071 avg loss no lamb -1.588071 time 2019-02-22 12:05:55.770826
Model ind 640 epoch 474 head B head_i_epoch 1 batch 200: avg loss -1.713194 avg loss no lamb -1.713194 time 2019-02-22 12:07:47.281443
last batch sz 160
Pre: time 2019-02-22 12:09:29.333697: 
 	std: 0.052723132
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50226665, 0.60891664, 0.60905, 0.6084667, 0.50013334]
	train_accs: [0.50226665, 0.60891664, 0.60905, 0.6084667, 0.50013334]
	best_train_sub_head: 2
	worst: 0.50013334
	avg: 0.56576663
	best: 0.60905

Starting e_i: 475
Model ind 640 epoch 475 head A head_i_epoch 0 batch 0: avg loss -3.029457 avg loss no lamb -3.029457 time 2019-02-22 12:09:31.395829
Model ind 640 epoch 475 head A head_i_epoch 0 batch 100: avg loss -2.986142 avg loss no lamb -2.986142 time 2019-02-22 12:11:21.264877
Model ind 640 epoch 475 head A head_i_epoch 0 batch 200: avg loss -3.043933 avg loss no lamb -3.043933 time 2019-02-22 12:13:12.717385
last batch sz 160
Model ind 640 epoch 475 head B head_i_epoch 0 batch 0: avg loss -1.710427 avg loss no lamb -1.710427 time 2019-02-22 12:14:34.156324
Model ind 640 epoch 475 head B head_i_epoch 0 batch 100: avg loss -1.568376 avg loss no lamb -1.568376 time 2019-02-22 12:16:26.549074
Model ind 640 epoch 475 head B head_i_epoch 0 batch 200: avg loss -1.683705 avg loss no lamb -1.683705 time 2019-02-22 12:18:17.485716
last batch sz 160
Model ind 640 epoch 475 head B head_i_epoch 1 batch 0: avg loss -1.739163 avg loss no lamb -1.739163 time 2019-02-22 12:19:39.747123
Model ind 640 epoch 475 head B head_i_epoch 1 batch 100: avg loss -1.632922 avg loss no lamb -1.632922 time 2019-02-22 12:21:31.517491
Model ind 640 epoch 475 head B head_i_epoch 1 batch 200: avg loss -1.765022 avg loss no lamb -1.765022 time 2019-02-22 12:23:23.434314
last batch sz 160
Pre: time 2019-02-22 12:25:06.499807: 
 	std: 0.051968895
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5046333, 0.6099833, 0.6099667, 0.61038333, 0.50343335]
	train_accs: [0.5046333, 0.6099833, 0.6099667, 0.61038333, 0.50343335]
	best_train_sub_head: 3
	worst: 0.50343335
	avg: 0.56768
	best: 0.61038333

Starting e_i: 476
Model ind 640 epoch 476 head A head_i_epoch 0 batch 0: avg loss -2.995830 avg loss no lamb -2.995830 time 2019-02-22 12:25:08.783098
Model ind 640 epoch 476 head A head_i_epoch 0 batch 100: avg loss -2.937343 avg loss no lamb -2.937343 time 2019-02-22 12:27:00.547177
Model ind 640 epoch 476 head A head_i_epoch 0 batch 200: avg loss -3.044032 avg loss no lamb -3.044032 time 2019-02-22 12:28:52.305388
last batch sz 160
Model ind 640 epoch 476 head B head_i_epoch 0 batch 0: avg loss -1.688117 avg loss no lamb -1.688117 time 2019-02-22 12:30:13.117272
Model ind 640 epoch 476 head B head_i_epoch 0 batch 100: avg loss -1.574947 avg loss no lamb -1.574947 time 2019-02-22 12:32:05.051992
Model ind 640 epoch 476 head B head_i_epoch 0 batch 200: avg loss -1.756783 avg loss no lamb -1.756783 time 2019-02-22 12:33:56.729340
last batch sz 160
Model ind 640 epoch 476 head B head_i_epoch 1 batch 0: avg loss -1.667164 avg loss no lamb -1.667164 time 2019-02-22 12:35:18.257084
Model ind 640 epoch 476 head B head_i_epoch 1 batch 100: avg loss -1.615783 avg loss no lamb -1.615783 time 2019-02-22 12:37:10.046074
Model ind 640 epoch 476 head B head_i_epoch 1 batch 200: avg loss -1.794607 avg loss no lamb -1.794607 time 2019-02-22 12:39:01.939907
last batch sz 160
Pre: time 2019-02-22 12:40:43.257466: 
 	std: 0.052456055
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.4993, 0.6056333, 0.6059833, 0.60565, 0.49806666]
	train_accs: [0.4993, 0.6056333, 0.6059833, 0.60565, 0.49806666]
	best_train_sub_head: 2
	worst: 0.49806666
	avg: 0.56292665
	best: 0.6059833

Starting e_i: 477
Model ind 640 epoch 477 head A head_i_epoch 0 batch 0: avg loss -2.994646 avg loss no lamb -2.994646 time 2019-02-22 12:40:45.562516
Model ind 640 epoch 477 head A head_i_epoch 0 batch 100: avg loss -2.939443 avg loss no lamb -2.939443 time 2019-02-22 12:42:39.331858
Model ind 640 epoch 477 head A head_i_epoch 0 batch 200: avg loss -3.084453 avg loss no lamb -3.084453 time 2019-02-22 12:44:30.379054
last batch sz 160
Model ind 640 epoch 477 head B head_i_epoch 0 batch 0: avg loss -1.655062 avg loss no lamb -1.655062 time 2019-02-22 12:45:50.450159
Model ind 640 epoch 477 head B head_i_epoch 0 batch 100: avg loss -1.638600 avg loss no lamb -1.638600 time 2019-02-22 12:47:42.526746
Model ind 640 epoch 477 head B head_i_epoch 0 batch 200: avg loss -1.838926 avg loss no lamb -1.838926 time 2019-02-22 12:49:36.529789
last batch sz 160
Model ind 640 epoch 477 head B head_i_epoch 1 batch 0: avg loss -1.730255 avg loss no lamb -1.730255 time 2019-02-22 12:50:57.653567
Model ind 640 epoch 477 head B head_i_epoch 1 batch 100: avg loss -1.662771 avg loss no lamb -1.662771 time 2019-02-22 12:52:49.217948
Model ind 640 epoch 477 head B head_i_epoch 1 batch 200: avg loss -1.784824 avg loss no lamb -1.784824 time 2019-02-22 12:54:41.557493
last batch sz 160
Pre: time 2019-02-22 12:56:23.136314: 
 	std: 0.051764775
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5024667, 0.60796666, 0.60833335, 0.60786664, 0.50231665]
	train_accs: [0.5024667, 0.60796666, 0.60833335, 0.60786664, 0.50231665]
	best_train_sub_head: 2
	worst: 0.50231665
	avg: 0.56579006
	best: 0.60833335

Starting e_i: 478
Model ind 640 epoch 478 head A head_i_epoch 0 batch 0: avg loss -3.014338 avg loss no lamb -3.014338 time 2019-02-22 12:56:25.301800
Model ind 640 epoch 478 head A head_i_epoch 0 batch 100: avg loss -2.905753 avg loss no lamb -2.905753 time 2019-02-22 12:58:16.518321
Model ind 640 epoch 478 head A head_i_epoch 0 batch 200: avg loss -3.042375 avg loss no lamb -3.042375 time 2019-02-22 13:00:08.059443
last batch sz 160
Model ind 640 epoch 478 head B head_i_epoch 0 batch 0: avg loss -1.613045 avg loss no lamb -1.613045 time 2019-02-22 13:01:29.241695
Model ind 640 epoch 478 head B head_i_epoch 0 batch 100: avg loss -1.592651 avg loss no lamb -1.592651 time 2019-02-22 13:03:20.140063
Model ind 640 epoch 478 head B head_i_epoch 0 batch 200: avg loss -1.737251 avg loss no lamb -1.737251 time 2019-02-22 13:05:10.148410
last batch sz 160
Model ind 640 epoch 478 head B head_i_epoch 1 batch 0: avg loss -1.744276 avg loss no lamb -1.744276 time 2019-02-22 13:06:30.957052
Model ind 640 epoch 478 head B head_i_epoch 1 batch 100: avg loss -1.675349 avg loss no lamb -1.675349 time 2019-02-22 13:08:23.227598
Model ind 640 epoch 478 head B head_i_epoch 1 batch 200: avg loss -1.746926 avg loss no lamb -1.746926 time 2019-02-22 13:10:15.138703
last batch sz 160
Pre: time 2019-02-22 13:11:56.736859: 
 	std: 0.051724598
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50413334, 0.61015, 0.60926664, 0.6094, 0.5039167]
	train_accs: [0.50413334, 0.61015, 0.60926664, 0.6094, 0.5039167]
	best_train_sub_head: 1
	worst: 0.5039167
	avg: 0.56737334
	best: 0.61015

Starting e_i: 479
Model ind 640 epoch 479 head A head_i_epoch 0 batch 0: avg loss -3.037788 avg loss no lamb -3.037788 time 2019-02-22 13:11:58.889886
Model ind 640 epoch 479 head A head_i_epoch 0 batch 100: avg loss -2.987558 avg loss no lamb -2.987558 time 2019-02-22 13:13:50.797874
Model ind 640 epoch 479 head A head_i_epoch 0 batch 200: avg loss -3.116090 avg loss no lamb -3.116090 time 2019-02-22 13:15:42.465227
last batch sz 160
Model ind 640 epoch 479 head B head_i_epoch 0 batch 0: avg loss -1.735768 avg loss no lamb -1.735768 time 2019-02-22 13:17:03.124625
Model ind 640 epoch 479 head B head_i_epoch 0 batch 100: avg loss -1.668618 avg loss no lamb -1.668618 time 2019-02-22 13:18:54.392318
Model ind 640 epoch 479 head B head_i_epoch 0 batch 200: avg loss -1.776060 avg loss no lamb -1.776060 time 2019-02-22 13:20:46.315587
last batch sz 160
Model ind 640 epoch 479 head B head_i_epoch 1 batch 0: avg loss -1.666323 avg loss no lamb -1.666323 time 2019-02-22 13:22:07.598384
Model ind 640 epoch 479 head B head_i_epoch 1 batch 100: avg loss -1.634131 avg loss no lamb -1.634131 time 2019-02-22 13:23:59.035831
Model ind 640 epoch 479 head B head_i_epoch 1 batch 200: avg loss -1.754634 avg loss no lamb -1.754634 time 2019-02-22 13:25:51.978834
last batch sz 160
Pre: time 2019-02-22 13:27:33.679604: 
 	std: 0.051831853
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5032667, 0.6091, 0.60961664, 0.6088333, 0.5035]
	train_accs: [0.5032667, 0.6091, 0.60961664, 0.6088333, 0.5035]
	best_train_sub_head: 2
	worst: 0.5032667
	avg: 0.56686336
	best: 0.60961664

Starting e_i: 480
Model ind 640 epoch 480 head A head_i_epoch 0 batch 0: avg loss -2.975340 avg loss no lamb -2.975340 time 2019-02-22 13:27:35.877789
Model ind 640 epoch 480 head A head_i_epoch 0 batch 100: avg loss -2.912206 avg loss no lamb -2.912206 time 2019-02-22 13:29:27.489030
Model ind 640 epoch 480 head A head_i_epoch 0 batch 200: avg loss -3.016691 avg loss no lamb -3.016691 time 2019-02-22 13:31:18.433139
last batch sz 160
Model ind 640 epoch 480 head B head_i_epoch 0 batch 0: avg loss -1.753661 avg loss no lamb -1.753661 time 2019-02-22 13:32:39.733459
Model ind 640 epoch 480 head B head_i_epoch 0 batch 100: avg loss -1.666549 avg loss no lamb -1.666549 time 2019-02-22 13:34:31.087883
Model ind 640 epoch 480 head B head_i_epoch 0 batch 200: avg loss -1.758426 avg loss no lamb -1.758426 time 2019-02-22 13:36:21.962327
last batch sz 160
Model ind 640 epoch 480 head B head_i_epoch 1 batch 0: avg loss -1.716445 avg loss no lamb -1.716445 time 2019-02-22 13:37:43.514104
Model ind 640 epoch 480 head B head_i_epoch 1 batch 100: avg loss -1.594351 avg loss no lamb -1.594351 time 2019-02-22 13:39:34.731593
Model ind 640 epoch 480 head B head_i_epoch 1 batch 200: avg loss -1.814480 avg loss no lamb -1.814480 time 2019-02-22 13:41:26.770672
last batch sz 160
Pre: time 2019-02-22 13:43:08.481204: 
 	std: 0.051264096
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50335, 0.60868335, 0.6088167, 0.6081167, 0.50445]
	train_accs: [0.50335, 0.60868335, 0.6088167, 0.6081167, 0.50445]
	best_train_sub_head: 2
	worst: 0.50335
	avg: 0.56668335
	best: 0.6088167

Starting e_i: 481
Model ind 640 epoch 481 head A head_i_epoch 0 batch 0: avg loss -3.026253 avg loss no lamb -3.026253 time 2019-02-22 13:43:13.925839
Model ind 640 epoch 481 head A head_i_epoch 0 batch 100: avg loss -2.892987 avg loss no lamb -2.892987 time 2019-02-22 13:45:08.103508
Model ind 640 epoch 481 head A head_i_epoch 0 batch 200: avg loss -3.023826 avg loss no lamb -3.023826 time 2019-02-22 13:46:58.623926
last batch sz 160
Model ind 640 epoch 481 head B head_i_epoch 0 batch 0: avg loss -1.719446 avg loss no lamb -1.719446 time 2019-02-22 13:48:20.676021
Model ind 640 epoch 481 head B head_i_epoch 0 batch 100: avg loss -1.615356 avg loss no lamb -1.615356 time 2019-02-22 13:50:12.006101
Model ind 640 epoch 481 head B head_i_epoch 0 batch 200: avg loss -1.748483 avg loss no lamb -1.748483 time 2019-02-22 13:52:03.408994
last batch sz 160
Model ind 640 epoch 481 head B head_i_epoch 1 batch 0: avg loss -1.683278 avg loss no lamb -1.683278 time 2019-02-22 13:53:24.525498
Model ind 640 epoch 481 head B head_i_epoch 1 batch 100: avg loss -1.618650 avg loss no lamb -1.618650 time 2019-02-22 13:55:16.218746
Model ind 640 epoch 481 head B head_i_epoch 1 batch 200: avg loss -1.733413 avg loss no lamb -1.733413 time 2019-02-22 13:57:08.038603
last batch sz 160
Pre: time 2019-02-22 13:58:50.321068: 
 	std: 0.052629396
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49875, 0.6067, 0.6066667, 0.60641664, 0.49958333]
	train_accs: [0.49875, 0.6067, 0.6066667, 0.60641664, 0.49958333]
	best_train_sub_head: 1
	worst: 0.49875
	avg: 0.5636233
	best: 0.6067

Starting e_i: 482
Model ind 640 epoch 482 head A head_i_epoch 0 batch 0: avg loss -3.002603 avg loss no lamb -3.002603 time 2019-02-22 13:58:52.444733
Model ind 640 epoch 482 head A head_i_epoch 0 batch 100: avg loss -2.909136 avg loss no lamb -2.909136 time 2019-02-22 14:00:43.210754
Model ind 640 epoch 482 head A head_i_epoch 0 batch 200: avg loss -3.059580 avg loss no lamb -3.059580 time 2019-02-22 14:02:34.020929
last batch sz 160
Model ind 640 epoch 482 head B head_i_epoch 0 batch 0: avg loss -1.725043 avg loss no lamb -1.725043 time 2019-02-22 14:03:55.515262
Model ind 640 epoch 482 head B head_i_epoch 0 batch 100: avg loss -1.648045 avg loss no lamb -1.648045 time 2019-02-22 14:05:47.290393
Model ind 640 epoch 482 head B head_i_epoch 0 batch 200: avg loss -1.765289 avg loss no lamb -1.765289 time 2019-02-22 14:07:40.743872
last batch sz 160
Model ind 640 epoch 482 head B head_i_epoch 1 batch 0: avg loss -1.728566 avg loss no lamb -1.728566 time 2019-02-22 14:09:00.907392
Model ind 640 epoch 482 head B head_i_epoch 1 batch 100: avg loss -1.616525 avg loss no lamb -1.616525 time 2019-02-22 14:10:52.738252
Model ind 640 epoch 482 head B head_i_epoch 1 batch 200: avg loss -1.804003 avg loss no lamb -1.804003 time 2019-02-22 14:12:43.960937
last batch sz 160
Pre: time 2019-02-22 14:14:25.575059: 
 	std: 0.05374491
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.49225, 0.60165, 0.6019667, 0.60155, 0.49178332]
	train_accs: [0.49225, 0.60165, 0.6019667, 0.60155, 0.49178332]
	best_train_sub_head: 2
	worst: 0.49178332
	avg: 0.55784
	best: 0.6019667

Starting e_i: 483
Model ind 640 epoch 483 head A head_i_epoch 0 batch 0: avg loss -2.995579 avg loss no lamb -2.995579 time 2019-02-22 14:14:27.769728
Model ind 640 epoch 483 head A head_i_epoch 0 batch 100: avg loss -2.927871 avg loss no lamb -2.927871 time 2019-02-22 14:16:19.101163
Model ind 640 epoch 483 head A head_i_epoch 0 batch 200: avg loss -3.043323 avg loss no lamb -3.043323 time 2019-02-22 14:18:11.125757
last batch sz 160
Model ind 640 epoch 483 head B head_i_epoch 0 batch 0: avg loss -1.717055 avg loss no lamb -1.717055 time 2019-02-22 14:19:32.503470
Model ind 640 epoch 483 head B head_i_epoch 0 batch 100: avg loss -1.656449 avg loss no lamb -1.656449 time 2019-02-22 14:21:24.451507
Model ind 640 epoch 483 head B head_i_epoch 0 batch 200: avg loss -1.752920 avg loss no lamb -1.752920 time 2019-02-22 14:23:15.861063
last batch sz 160
Model ind 640 epoch 483 head B head_i_epoch 1 batch 0: avg loss -1.673359 avg loss no lamb -1.673359 time 2019-02-22 14:24:36.654164
Model ind 640 epoch 483 head B head_i_epoch 1 batch 100: avg loss -1.612206 avg loss no lamb -1.612206 time 2019-02-22 14:26:28.214441
Model ind 640 epoch 483 head B head_i_epoch 1 batch 200: avg loss -1.777867 avg loss no lamb -1.777867 time 2019-02-22 14:28:20.060387
last batch sz 160
Pre: time 2019-02-22 14:30:02.006131: 
 	std: 0.05107672
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5058, 0.6106833, 0.61036664, 0.61, 0.50638336]
	train_accs: [0.5058, 0.6106833, 0.61036664, 0.61, 0.50638336]
	best_train_sub_head: 1
	worst: 0.5058
	avg: 0.5686467
	best: 0.6106833

Starting e_i: 484
Model ind 640 epoch 484 head A head_i_epoch 0 batch 0: avg loss -3.015840 avg loss no lamb -3.015840 time 2019-02-22 14:30:04.163598
Model ind 640 epoch 484 head A head_i_epoch 0 batch 100: avg loss -2.970409 avg loss no lamb -2.970409 time 2019-02-22 14:31:53.722912
Model ind 640 epoch 484 head A head_i_epoch 0 batch 200: avg loss -3.099020 avg loss no lamb -3.099020 time 2019-02-22 14:33:46.797647
last batch sz 160
Model ind 640 epoch 484 head B head_i_epoch 0 batch 0: avg loss -1.730789 avg loss no lamb -1.730789 time 2019-02-22 14:35:07.730717
Model ind 640 epoch 484 head B head_i_epoch 0 batch 100: avg loss -1.629635 avg loss no lamb -1.629635 time 2019-02-22 14:37:00.023397
Model ind 640 epoch 484 head B head_i_epoch 0 batch 200: avg loss -1.768540 avg loss no lamb -1.768540 time 2019-02-22 14:38:51.606838
last batch sz 160
Model ind 640 epoch 484 head B head_i_epoch 1 batch 0: avg loss -1.691337 avg loss no lamb -1.691337 time 2019-02-22 14:40:12.470095
Model ind 640 epoch 484 head B head_i_epoch 1 batch 100: avg loss -1.606161 avg loss no lamb -1.606161 time 2019-02-22 14:42:04.001390
Model ind 640 epoch 484 head B head_i_epoch 1 batch 200: avg loss -1.751221 avg loss no lamb -1.751221 time 2019-02-22 14:43:55.453755
last batch sz 160
Pre: time 2019-02-22 14:45:39.977766: 
 	std: 0.0517446
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50168335, 0.6073167, 0.60685, 0.6063167, 0.5007333]
	train_accs: [0.50168335, 0.6073167, 0.60685, 0.6063167, 0.5007333]
	best_train_sub_head: 1
	worst: 0.5007333
	avg: 0.56458
	best: 0.6073167

Starting e_i: 485
Model ind 640 epoch 485 head A head_i_epoch 0 batch 0: avg loss -3.009290 avg loss no lamb -3.009290 time 2019-02-22 14:45:42.180812
Model ind 640 epoch 485 head A head_i_epoch 0 batch 100: avg loss -2.955355 avg loss no lamb -2.955355 time 2019-02-22 14:47:33.937487
Model ind 640 epoch 485 head A head_i_epoch 0 batch 200: avg loss -3.028527 avg loss no lamb -3.028527 time 2019-02-22 14:49:26.073539
last batch sz 160
Model ind 640 epoch 485 head B head_i_epoch 0 batch 0: avg loss -1.687298 avg loss no lamb -1.687298 time 2019-02-22 14:50:47.925609
Model ind 640 epoch 485 head B head_i_epoch 0 batch 100: avg loss -1.674334 avg loss no lamb -1.674334 time 2019-02-22 14:52:40.243258
Model ind 640 epoch 485 head B head_i_epoch 0 batch 200: avg loss -1.809018 avg loss no lamb -1.809018 time 2019-02-22 14:54:31.396182
last batch sz 160
Model ind 640 epoch 485 head B head_i_epoch 1 batch 0: avg loss -1.710290 avg loss no lamb -1.710290 time 2019-02-22 14:55:53.297384
Model ind 640 epoch 485 head B head_i_epoch 1 batch 100: avg loss -1.606973 avg loss no lamb -1.606973 time 2019-02-22 14:57:44.200011
Model ind 640 epoch 485 head B head_i_epoch 1 batch 200: avg loss -1.848683 avg loss no lamb -1.848683 time 2019-02-22 14:59:36.006497
last batch sz 160
Pre: time 2019-02-22 15:01:18.076120: 
 	std: 0.052763958
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5046667, 0.6124167, 0.6125, 0.61181664, 0.50441664]
	train_accs: [0.5046667, 0.6124167, 0.6125, 0.61181664, 0.50441664]
	best_train_sub_head: 2
	worst: 0.50441664
	avg: 0.5691633
	best: 0.6125

Starting e_i: 486
Model ind 640 epoch 486 head A head_i_epoch 0 batch 0: avg loss -3.047343 avg loss no lamb -3.047343 time 2019-02-22 15:01:24.516329
Model ind 640 epoch 486 head A head_i_epoch 0 batch 100: avg loss -2.926289 avg loss no lamb -2.926289 time 2019-02-22 15:03:15.552887
Model ind 640 epoch 486 head A head_i_epoch 0 batch 200: avg loss -3.094833 avg loss no lamb -3.094833 time 2019-02-22 15:05:07.139772
last batch sz 160
Model ind 640 epoch 486 head B head_i_epoch 0 batch 0: avg loss -1.751386 avg loss no lamb -1.751386 time 2019-02-22 15:06:28.290251
Model ind 640 epoch 486 head B head_i_epoch 0 batch 100: avg loss -1.614497 avg loss no lamb -1.614497 time 2019-02-22 15:08:19.712564
Model ind 640 epoch 486 head B head_i_epoch 0 batch 200: avg loss -1.783446 avg loss no lamb -1.783446 time 2019-02-22 15:10:11.047724
last batch sz 160
Model ind 640 epoch 486 head B head_i_epoch 1 batch 0: avg loss -1.760667 avg loss no lamb -1.760667 time 2019-02-22 15:11:31.943581
Model ind 640 epoch 486 head B head_i_epoch 1 batch 100: avg loss -1.615373 avg loss no lamb -1.615373 time 2019-02-22 15:13:23.696449
Model ind 640 epoch 486 head B head_i_epoch 1 batch 200: avg loss -1.746299 avg loss no lamb -1.746299 time 2019-02-22 15:15:14.593139
last batch sz 160
Pre: time 2019-02-22 15:16:56.435174: 
 	std: 0.051785402
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5020667, 0.60788333, 0.6078, 0.60728335, 0.5018333]
	train_accs: [0.5020667, 0.60788333, 0.6078, 0.60728335, 0.5018333]
	best_train_sub_head: 1
	worst: 0.5018333
	avg: 0.5653733
	best: 0.60788333

Starting e_i: 487
Model ind 640 epoch 487 head A head_i_epoch 0 batch 0: avg loss -2.975342 avg loss no lamb -2.975342 time 2019-02-22 15:16:58.934064
Model ind 640 epoch 487 head A head_i_epoch 0 batch 100: avg loss -2.959750 avg loss no lamb -2.959750 time 2019-02-22 15:18:51.376251
Model ind 640 epoch 487 head A head_i_epoch 0 batch 200: avg loss -3.110432 avg loss no lamb -3.110432 time 2019-02-22 15:20:42.751574
last batch sz 160
Model ind 640 epoch 487 head B head_i_epoch 0 batch 0: avg loss -1.793069 avg loss no lamb -1.793069 time 2019-02-22 15:22:04.268298
Model ind 640 epoch 487 head B head_i_epoch 0 batch 100: avg loss -1.556746 avg loss no lamb -1.556746 time 2019-02-22 15:23:56.827283
Model ind 640 epoch 487 head B head_i_epoch 0 batch 200: avg loss -1.789699 avg loss no lamb -1.789699 time 2019-02-22 15:25:49.268893
last batch sz 160
Model ind 640 epoch 487 head B head_i_epoch 1 batch 0: avg loss -1.692712 avg loss no lamb -1.692712 time 2019-02-22 15:27:09.990348
Model ind 640 epoch 487 head B head_i_epoch 1 batch 100: avg loss -1.613521 avg loss no lamb -1.613521 time 2019-02-22 15:29:01.789906
Model ind 640 epoch 487 head B head_i_epoch 1 batch 200: avg loss -1.776736 avg loss no lamb -1.776736 time 2019-02-22 15:30:53.221376
last batch sz 160
Pre: time 2019-02-22 15:32:35.401215: 
 	std: 0.05180242
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50425, 0.61015, 0.60968333, 0.6092333, 0.50365]
	train_accs: [0.50425, 0.61015, 0.60968333, 0.6092333, 0.50365]
	best_train_sub_head: 1
	worst: 0.50365
	avg: 0.5673933
	best: 0.61015

Starting e_i: 488
Model ind 640 epoch 488 head A head_i_epoch 0 batch 0: avg loss -3.052508 avg loss no lamb -3.052508 time 2019-02-22 15:32:37.734969
Model ind 640 epoch 488 head A head_i_epoch 0 batch 100: avg loss -2.988529 avg loss no lamb -2.988529 time 2019-02-22 15:34:29.361867
Model ind 640 epoch 488 head A head_i_epoch 0 batch 200: avg loss -3.077302 avg loss no lamb -3.077302 time 2019-02-22 15:36:21.234062
last batch sz 160
Model ind 640 epoch 488 head B head_i_epoch 0 batch 0: avg loss -1.798405 avg loss no lamb -1.798405 time 2019-02-22 15:37:42.082340
Model ind 640 epoch 488 head B head_i_epoch 0 batch 100: avg loss -1.668329 avg loss no lamb -1.668329 time 2019-02-22 15:39:33.209263
Model ind 640 epoch 488 head B head_i_epoch 0 batch 200: avg loss -1.819091 avg loss no lamb -1.819091 time 2019-02-22 15:41:24.241515
last batch sz 160
Model ind 640 epoch 488 head B head_i_epoch 1 batch 0: avg loss -1.694059 avg loss no lamb -1.694059 time 2019-02-22 15:42:45.364364
Model ind 640 epoch 488 head B head_i_epoch 1 batch 100: avg loss -1.667451 avg loss no lamb -1.667451 time 2019-02-22 15:44:37.699509
Model ind 640 epoch 488 head B head_i_epoch 1 batch 200: avg loss -1.744896 avg loss no lamb -1.744896 time 2019-02-22 15:46:30.371659
last batch sz 160
Pre: time 2019-02-22 15:48:13.562331: 
 	std: 0.05124724
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50835, 0.61231667, 0.61291665, 0.61233336, 0.5074833]
	train_accs: [0.50835, 0.61231667, 0.61291665, 0.61233336, 0.5074833]
	best_train_sub_head: 2
	worst: 0.5074833
	avg: 0.57068
	best: 0.61291665

Starting e_i: 489
Model ind 640 epoch 489 head A head_i_epoch 0 batch 0: avg loss -2.988125 avg loss no lamb -2.988125 time 2019-02-22 15:48:20.136665
Model ind 640 epoch 489 head A head_i_epoch 0 batch 100: avg loss -2.973421 avg loss no lamb -2.973421 time 2019-02-22 15:50:11.165819
Model ind 640 epoch 489 head A head_i_epoch 0 batch 200: avg loss -3.124895 avg loss no lamb -3.124895 time 2019-02-22 15:52:02.977857
last batch sz 160
Model ind 640 epoch 489 head B head_i_epoch 0 batch 0: avg loss -1.766766 avg loss no lamb -1.766766 time 2019-02-22 15:53:24.150043
Model ind 640 epoch 489 head B head_i_epoch 0 batch 100: avg loss -1.595427 avg loss no lamb -1.595427 time 2019-02-22 15:55:15.786164
Model ind 640 epoch 489 head B head_i_epoch 0 batch 200: avg loss -1.757956 avg loss no lamb -1.757956 time 2019-02-22 15:57:07.911011
last batch sz 160
Model ind 640 epoch 489 head B head_i_epoch 1 batch 0: avg loss -1.686161 avg loss no lamb -1.686161 time 2019-02-22 15:58:29.099448
Model ind 640 epoch 489 head B head_i_epoch 1 batch 100: avg loss -1.591070 avg loss no lamb -1.591070 time 2019-02-22 16:00:20.509712
Model ind 640 epoch 489 head B head_i_epoch 1 batch 200: avg loss -1.767201 avg loss no lamb -1.767201 time 2019-02-22 16:02:13.228444
last batch sz 160
Pre: time 2019-02-22 16:03:54.960012: 
 	std: 0.051286094
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.504, 0.60815, 0.60858333, 0.60885, 0.5036833]
	train_accs: [0.504, 0.60815, 0.60858333, 0.60885, 0.5036833]
	best_train_sub_head: 3
	worst: 0.5036833
	avg: 0.5666533
	best: 0.60885

Starting e_i: 490
Model ind 640 epoch 490 head A head_i_epoch 0 batch 0: avg loss -3.006135 avg loss no lamb -3.006135 time 2019-02-22 16:03:57.282369
Model ind 640 epoch 490 head A head_i_epoch 0 batch 100: avg loss -2.853609 avg loss no lamb -2.853609 time 2019-02-22 16:05:48.769957
Model ind 640 epoch 490 head A head_i_epoch 0 batch 200: avg loss -3.033232 avg loss no lamb -3.033232 time 2019-02-22 16:07:41.063286
last batch sz 160
Model ind 640 epoch 490 head B head_i_epoch 0 batch 0: avg loss -1.793261 avg loss no lamb -1.793261 time 2019-02-22 16:09:02.254135
Model ind 640 epoch 490 head B head_i_epoch 0 batch 100: avg loss -1.627136 avg loss no lamb -1.627136 time 2019-02-22 16:10:54.297150
Model ind 640 epoch 490 head B head_i_epoch 0 batch 200: avg loss -1.791914 avg loss no lamb -1.791914 time 2019-02-22 16:12:46.243048
last batch sz 160
Model ind 640 epoch 490 head B head_i_epoch 1 batch 0: avg loss -1.666788 avg loss no lamb -1.666788 time 2019-02-22 16:14:07.188112
Model ind 640 epoch 490 head B head_i_epoch 1 batch 100: avg loss -1.628201 avg loss no lamb -1.628201 time 2019-02-22 16:15:59.204125
Model ind 640 epoch 490 head B head_i_epoch 1 batch 200: avg loss -1.837652 avg loss no lamb -1.837652 time 2019-02-22 16:17:51.244591
last batch sz 160
Pre: time 2019-02-22 16:19:33.426737: 
 	std: 0.05221486
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5046667, 0.6106667, 0.6106667, 0.6106833, 0.5035167]
	train_accs: [0.5046667, 0.6106667, 0.6106667, 0.6106833, 0.5035167]
	best_train_sub_head: 3
	worst: 0.5035167
	avg: 0.56804
	best: 0.6106833

Starting e_i: 491
Model ind 640 epoch 491 head A head_i_epoch 0 batch 0: avg loss -2.982167 avg loss no lamb -2.982167 time 2019-02-22 16:19:38.808870
Model ind 640 epoch 491 head A head_i_epoch 0 batch 100: avg loss -2.914765 avg loss no lamb -2.914765 time 2019-02-22 16:21:29.831007
Model ind 640 epoch 491 head A head_i_epoch 0 batch 200: avg loss -3.073840 avg loss no lamb -3.073840 time 2019-02-22 16:23:20.512588
last batch sz 160
Model ind 640 epoch 491 head B head_i_epoch 0 batch 0: avg loss -1.712215 avg loss no lamb -1.712215 time 2019-02-22 16:24:43.549324
Model ind 640 epoch 491 head B head_i_epoch 0 batch 100: avg loss -1.643386 avg loss no lamb -1.643386 time 2019-02-22 16:26:35.565500
Model ind 640 epoch 491 head B head_i_epoch 0 batch 200: avg loss -1.706236 avg loss no lamb -1.706236 time 2019-02-22 16:28:27.452829
last batch sz 160
Model ind 640 epoch 491 head B head_i_epoch 1 batch 0: avg loss -1.763136 avg loss no lamb -1.763136 time 2019-02-22 16:29:48.883877
Model ind 640 epoch 491 head B head_i_epoch 1 batch 100: avg loss -1.591241 avg loss no lamb -1.591241 time 2019-02-22 16:31:40.863304
Model ind 640 epoch 491 head B head_i_epoch 1 batch 200: avg loss -1.772333 avg loss no lamb -1.772333 time 2019-02-22 16:33:32.730982
last batch sz 160
Pre: time 2019-02-22 16:35:15.128095: 
 	std: 0.051917
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5010833, 0.60693336, 0.60705, 0.60691667, 0.5009]
	train_accs: [0.5010833, 0.60693336, 0.60705, 0.60691667, 0.5009]
	best_train_sub_head: 2
	worst: 0.5009
	avg: 0.5645767
	best: 0.60705

Starting e_i: 492
Model ind 640 epoch 492 head A head_i_epoch 0 batch 0: avg loss -2.989038 avg loss no lamb -2.989038 time 2019-02-22 16:35:17.344066
Model ind 640 epoch 492 head A head_i_epoch 0 batch 100: avg loss -3.010521 avg loss no lamb -3.010521 time 2019-02-22 16:37:09.304853
Model ind 640 epoch 492 head A head_i_epoch 0 batch 200: avg loss -2.993020 avg loss no lamb -2.993020 time 2019-02-22 16:39:00.965919
last batch sz 160
Model ind 640 epoch 492 head B head_i_epoch 0 batch 0: avg loss -1.655089 avg loss no lamb -1.655089 time 2019-02-22 16:40:22.196437
Model ind 640 epoch 492 head B head_i_epoch 0 batch 100: avg loss -1.632306 avg loss no lamb -1.632306 time 2019-02-22 16:42:15.394894
Model ind 640 epoch 492 head B head_i_epoch 0 batch 200: avg loss -1.779797 avg loss no lamb -1.779797 time 2019-02-22 16:44:08.094345
last batch sz 160
Model ind 640 epoch 492 head B head_i_epoch 1 batch 0: avg loss -1.747381 avg loss no lamb -1.747381 time 2019-02-22 16:45:27.980248
Model ind 640 epoch 492 head B head_i_epoch 1 batch 100: avg loss -1.610015 avg loss no lamb -1.610015 time 2019-02-22 16:47:19.745505
Model ind 640 epoch 492 head B head_i_epoch 1 batch 200: avg loss -1.771044 avg loss no lamb -1.771044 time 2019-02-22 16:49:14.048915
last batch sz 160
Pre: time 2019-02-22 16:50:56.354021: 
 	std: 0.052846637
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50061667, 0.6084667, 0.60833335, 0.6081167, 0.50025]
	train_accs: [0.50061667, 0.6084667, 0.60833335, 0.6081167, 0.50025]
	best_train_sub_head: 1
	worst: 0.50025
	avg: 0.56515664
	best: 0.6084667

Starting e_i: 493
Model ind 640 epoch 493 head A head_i_epoch 0 batch 0: avg loss -3.035186 avg loss no lamb -3.035186 time 2019-02-22 16:50:58.706694
Model ind 640 epoch 493 head A head_i_epoch 0 batch 100: avg loss -2.917031 avg loss no lamb -2.917031 time 2019-02-22 16:52:50.328000
Model ind 640 epoch 493 head A head_i_epoch 0 batch 200: avg loss -3.063782 avg loss no lamb -3.063782 time 2019-02-22 16:54:42.455950
last batch sz 160
Model ind 640 epoch 493 head B head_i_epoch 0 batch 0: avg loss -1.654848 avg loss no lamb -1.654848 time 2019-02-22 16:56:04.056527
Model ind 640 epoch 493 head B head_i_epoch 0 batch 100: avg loss -1.649367 avg loss no lamb -1.649367 time 2019-02-22 16:57:55.476542
Model ind 640 epoch 493 head B head_i_epoch 0 batch 200: avg loss -1.765005 avg loss no lamb -1.765005 time 2019-02-22 16:59:46.765087
last batch sz 160
Model ind 640 epoch 493 head B head_i_epoch 1 batch 0: avg loss -1.705465 avg loss no lamb -1.705465 time 2019-02-22 17:01:07.985891
Model ind 640 epoch 493 head B head_i_epoch 1 batch 100: avg loss -1.644590 avg loss no lamb -1.644590 time 2019-02-22 17:02:59.546023
Model ind 640 epoch 493 head B head_i_epoch 1 batch 200: avg loss -1.801211 avg loss no lamb -1.801211 time 2019-02-22 17:04:50.977846
last batch sz 160
Pre: time 2019-02-22 17:06:33.287815: 
 	std: 0.054721177
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49685, 0.6091833, 0.60895, 0.60898334, 0.49783334]
	train_accs: [0.49685, 0.6091833, 0.60895, 0.60898334, 0.49783334]
	best_train_sub_head: 1
	worst: 0.49685
	avg: 0.56436
	best: 0.6091833

Starting e_i: 494
Model ind 640 epoch 494 head A head_i_epoch 0 batch 0: avg loss -3.013963 avg loss no lamb -3.013963 time 2019-02-22 17:06:35.572642
Model ind 640 epoch 494 head A head_i_epoch 0 batch 100: avg loss -2.991287 avg loss no lamb -2.991287 time 2019-02-22 17:08:26.915778
Model ind 640 epoch 494 head A head_i_epoch 0 batch 200: avg loss -3.078270 avg loss no lamb -3.078270 time 2019-02-22 17:10:19.129709
last batch sz 160
Model ind 640 epoch 494 head B head_i_epoch 0 batch 0: avg loss -1.715284 avg loss no lamb -1.715284 time 2019-02-22 17:11:40.304856
Model ind 640 epoch 494 head B head_i_epoch 0 batch 100: avg loss -1.722472 avg loss no lamb -1.722472 time 2019-02-22 17:13:32.337054
Model ind 640 epoch 494 head B head_i_epoch 0 batch 200: avg loss -1.735984 avg loss no lamb -1.735984 time 2019-02-22 17:15:23.923250
last batch sz 160
Model ind 640 epoch 494 head B head_i_epoch 1 batch 0: avg loss -1.723068 avg loss no lamb -1.723068 time 2019-02-22 17:16:44.364002
Model ind 640 epoch 494 head B head_i_epoch 1 batch 100: avg loss -1.543958 avg loss no lamb -1.543958 time 2019-02-22 17:18:34.762759
Model ind 640 epoch 494 head B head_i_epoch 1 batch 200: avg loss -1.674311 avg loss no lamb -1.674311 time 2019-02-22 17:20:25.020312
last batch sz 160
Pre: time 2019-02-22 17:22:06.608301: 
 	std: 0.05347205
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49963334, 0.60866666, 0.6081, 0.60825, 0.49875]
	train_accs: [0.49963334, 0.60866666, 0.6081, 0.60825, 0.49875]
	best_train_sub_head: 1
	worst: 0.49875
	avg: 0.56468
	best: 0.60866666

Starting e_i: 495
Model ind 640 epoch 495 head A head_i_epoch 0 batch 0: avg loss -3.041211 avg loss no lamb -3.041211 time 2019-02-22 17:22:08.920538
Model ind 640 epoch 495 head A head_i_epoch 0 batch 100: avg loss -3.044470 avg loss no lamb -3.044470 time 2019-02-22 17:24:00.507627
Model ind 640 epoch 495 head A head_i_epoch 0 batch 200: avg loss -3.016456 avg loss no lamb -3.016456 time 2019-02-22 17:25:51.905807
last batch sz 160
Model ind 640 epoch 495 head B head_i_epoch 0 batch 0: avg loss -1.673097 avg loss no lamb -1.673097 time 2019-02-22 17:27:12.921316
Model ind 640 epoch 495 head B head_i_epoch 0 batch 100: avg loss -1.608576 avg loss no lamb -1.608576 time 2019-02-22 17:29:05.035770
Model ind 640 epoch 495 head B head_i_epoch 0 batch 200: avg loss -1.740779 avg loss no lamb -1.740779 time 2019-02-22 17:30:56.583154
last batch sz 160
Model ind 640 epoch 495 head B head_i_epoch 1 batch 0: avg loss -1.749524 avg loss no lamb -1.749524 time 2019-02-22 17:32:17.405496
Model ind 640 epoch 495 head B head_i_epoch 1 batch 100: avg loss -1.638981 avg loss no lamb -1.638981 time 2019-02-22 17:34:09.308360
Model ind 640 epoch 495 head B head_i_epoch 1 batch 200: avg loss -1.795423 avg loss no lamb -1.795423 time 2019-02-22 17:36:01.229269
last batch sz 160
Pre: time 2019-02-22 17:37:43.234941: 
 	std: 0.05412705
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49746665, 0.6081667, 0.6081, 0.60786664, 0.49765]
	train_accs: [0.49746665, 0.6081667, 0.6081, 0.60786664, 0.49765]
	best_train_sub_head: 1
	worst: 0.49746665
	avg: 0.56385
	best: 0.6081667

Starting e_i: 496
Model ind 640 epoch 496 head A head_i_epoch 0 batch 0: avg loss -3.072191 avg loss no lamb -3.072191 time 2019-02-22 17:37:45.524063
Model ind 640 epoch 496 head A head_i_epoch 0 batch 100: avg loss -2.925836 avg loss no lamb -2.925836 time 2019-02-22 17:39:37.652944
Model ind 640 epoch 496 head A head_i_epoch 0 batch 200: avg loss -3.032892 avg loss no lamb -3.032892 time 2019-02-22 17:41:28.960836
last batch sz 160
Model ind 640 epoch 496 head B head_i_epoch 0 batch 0: avg loss -1.647534 avg loss no lamb -1.647534 time 2019-02-22 17:42:49.493236
Model ind 640 epoch 496 head B head_i_epoch 0 batch 100: avg loss -1.528318 avg loss no lamb -1.528318 time 2019-02-22 17:44:42.329851
Model ind 640 epoch 496 head B head_i_epoch 0 batch 200: avg loss -1.769167 avg loss no lamb -1.769167 time 2019-02-22 17:46:34.567313
last batch sz 160
Model ind 640 epoch 496 head B head_i_epoch 1 batch 0: avg loss -1.668563 avg loss no lamb -1.668563 time 2019-02-22 17:47:55.062776
Model ind 640 epoch 496 head B head_i_epoch 1 batch 100: avg loss -1.671738 avg loss no lamb -1.671738 time 2019-02-22 17:49:48.728220
Model ind 640 epoch 496 head B head_i_epoch 1 batch 200: avg loss -1.750933 avg loss no lamb -1.750933 time 2019-02-22 17:51:40.213855
last batch sz 160
Pre: time 2019-02-22 17:53:22.799237: 
 	std: 0.053067293
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49581668, 0.60335, 0.6034833, 0.6035, 0.49443334]
	train_accs: [0.49581668, 0.60335, 0.6034833, 0.6035, 0.49443334]
	best_train_sub_head: 3
	worst: 0.49443334
	avg: 0.56011665
	best: 0.6035

Starting e_i: 497
Model ind 640 epoch 497 head A head_i_epoch 0 batch 0: avg loss -3.027394 avg loss no lamb -3.027394 time 2019-02-22 17:53:25.533592
Model ind 640 epoch 497 head A head_i_epoch 0 batch 100: avg loss -2.925233 avg loss no lamb -2.925233 time 2019-02-22 17:55:17.417510
Model ind 640 epoch 497 head A head_i_epoch 0 batch 200: avg loss -3.071803 avg loss no lamb -3.071803 time 2019-02-22 17:57:08.588449
last batch sz 160
Model ind 640 epoch 497 head B head_i_epoch 0 batch 0: avg loss -1.726278 avg loss no lamb -1.726278 time 2019-02-22 17:58:30.039310
Model ind 640 epoch 497 head B head_i_epoch 0 batch 100: avg loss -1.649094 avg loss no lamb -1.649094 time 2019-02-22 18:00:23.548283
Model ind 640 epoch 497 head B head_i_epoch 0 batch 200: avg loss -1.777177 avg loss no lamb -1.777177 time 2019-02-22 18:02:15.040097
last batch sz 160
Model ind 640 epoch 497 head B head_i_epoch 1 batch 0: avg loss -1.764674 avg loss no lamb -1.764674 time 2019-02-22 18:03:35.324115
Model ind 640 epoch 497 head B head_i_epoch 1 batch 100: avg loss -1.649677 avg loss no lamb -1.649677 time 2019-02-22 18:05:25.944096
Model ind 640 epoch 497 head B head_i_epoch 1 batch 200: avg loss -1.802648 avg loss no lamb -1.802648 time 2019-02-22 18:07:17.675828
last batch sz 160
Pre: time 2019-02-22 18:09:00.668053: 
 	std: 0.053909652
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5006667, 0.61005, 0.61016667, 0.60985, 0.4993]
	train_accs: [0.5006667, 0.61005, 0.61016667, 0.60985, 0.4993]
	best_train_sub_head: 2
	worst: 0.4993
	avg: 0.56600666
	best: 0.61016667

Starting e_i: 498
Model ind 640 epoch 498 head A head_i_epoch 0 batch 0: avg loss -3.013397 avg loss no lamb -3.013397 time 2019-02-22 18:09:02.876544
Model ind 640 epoch 498 head A head_i_epoch 0 batch 100: avg loss -2.953384 avg loss no lamb -2.953384 time 2019-02-22 18:10:53.728996
Model ind 640 epoch 498 head A head_i_epoch 0 batch 200: avg loss -3.063092 avg loss no lamb -3.063092 time 2019-02-22 18:12:45.925355
last batch sz 160
Model ind 640 epoch 498 head B head_i_epoch 0 batch 0: avg loss -1.769798 avg loss no lamb -1.769798 time 2019-02-22 18:14:06.685657
Model ind 640 epoch 498 head B head_i_epoch 0 batch 100: avg loss -1.563801 avg loss no lamb -1.563801 time 2019-02-22 18:15:57.959394
Model ind 640 epoch 498 head B head_i_epoch 0 batch 200: avg loss -1.761267 avg loss no lamb -1.761267 time 2019-02-22 18:17:48.685636
last batch sz 160
Model ind 640 epoch 498 head B head_i_epoch 1 batch 0: avg loss -1.794672 avg loss no lamb -1.794672 time 2019-02-22 18:19:09.886024
Model ind 640 epoch 498 head B head_i_epoch 1 batch 100: avg loss -1.578694 avg loss no lamb -1.578694 time 2019-02-22 18:21:02.123034
Model ind 640 epoch 498 head B head_i_epoch 1 batch 200: avg loss -1.805851 avg loss no lamb -1.805851 time 2019-02-22 18:22:53.802708
last batch sz 160
Pre: time 2019-02-22 18:24:35.827083: 
 	std: 0.051975995
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50265, 0.60865, 0.6081833, 0.6088, 0.50225]
	train_accs: [0.50265, 0.60865, 0.6081833, 0.6088, 0.50225]
	best_train_sub_head: 3
	worst: 0.50225
	avg: 0.5661067
	best: 0.6088

Starting e_i: 499
Model ind 640 epoch 499 head A head_i_epoch 0 batch 0: avg loss -3.016212 avg loss no lamb -3.016212 time 2019-02-22 18:24:37.947179
Model ind 640 epoch 499 head A head_i_epoch 0 batch 100: avg loss -2.937518 avg loss no lamb -2.937518 time 2019-02-22 18:26:29.799090
Model ind 640 epoch 499 head A head_i_epoch 0 batch 200: avg loss -3.103367 avg loss no lamb -3.103367 time 2019-02-22 18:28:22.051851
last batch sz 160
Model ind 640 epoch 499 head B head_i_epoch 0 batch 0: avg loss -1.738479 avg loss no lamb -1.738479 time 2019-02-22 18:29:43.234833
Model ind 640 epoch 499 head B head_i_epoch 0 batch 100: avg loss -1.633003 avg loss no lamb -1.633003 time 2019-02-22 18:31:35.464315
Model ind 640 epoch 499 head B head_i_epoch 0 batch 200: avg loss -1.848995 avg loss no lamb -1.848995 time 2019-02-22 18:33:26.939740
last batch sz 160
Model ind 640 epoch 499 head B head_i_epoch 1 batch 0: avg loss -1.626504 avg loss no lamb -1.626504 time 2019-02-22 18:34:48.254918
Model ind 640 epoch 499 head B head_i_epoch 1 batch 100: avg loss -1.560377 avg loss no lamb -1.560377 time 2019-02-22 18:36:39.030742
Model ind 640 epoch 499 head B head_i_epoch 1 batch 200: avg loss -1.746078 avg loss no lamb -1.746078 time 2019-02-22 18:38:31.072880
last batch sz 160
Pre: time 2019-02-22 18:40:13.247629: 
 	std: 0.051331017
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5054833, 0.60973334, 0.6093, 0.60976666, 0.50416666]
	train_accs: [0.5054833, 0.60973334, 0.6093, 0.60976666, 0.50416666]
	best_train_sub_head: 3
	worst: 0.50416666
	avg: 0.56769
	best: 0.60976666

Starting e_i: 500
Model ind 640 epoch 500 head A head_i_epoch 0 batch 0: avg loss -3.079715 avg loss no lamb -3.079715 time 2019-02-22 18:40:15.393161
Model ind 640 epoch 500 head A head_i_epoch 0 batch 100: avg loss -2.890376 avg loss no lamb -2.890376 time 2019-02-22 18:42:07.473723
Model ind 640 epoch 500 head A head_i_epoch 0 batch 200: avg loss -3.007441 avg loss no lamb -3.007441 time 2019-02-22 18:43:59.571590
last batch sz 160
Model ind 640 epoch 500 head B head_i_epoch 0 batch 0: avg loss -1.774770 avg loss no lamb -1.774770 time 2019-02-22 18:45:20.947037
Model ind 640 epoch 500 head B head_i_epoch 0 batch 100: avg loss -1.724450 avg loss no lamb -1.724450 time 2019-02-22 18:47:12.870414
Model ind 640 epoch 500 head B head_i_epoch 0 batch 200: avg loss -1.751994 avg loss no lamb -1.751994 time 2019-02-22 18:49:03.759027
last batch sz 160
Model ind 640 epoch 500 head B head_i_epoch 1 batch 0: avg loss -1.669104 avg loss no lamb -1.669104 time 2019-02-22 18:50:25.316019
Model ind 640 epoch 500 head B head_i_epoch 1 batch 100: avg loss -1.643402 avg loss no lamb -1.643402 time 2019-02-22 18:52:18.892047
Model ind 640 epoch 500 head B head_i_epoch 1 batch 200: avg loss -1.811785 avg loss no lamb -1.811785 time 2019-02-22 18:54:10.596613
last batch sz 160
Pre: time 2019-02-22 18:55:53.588161: 
 	std: 0.0532335
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5036833, 0.61163336, 0.61106664, 0.6116667, 0.50191665]
	train_accs: [0.5036833, 0.61163336, 0.61106664, 0.6116667, 0.50191665]
	best_train_sub_head: 3
	worst: 0.50191665
	avg: 0.5679933
	best: 0.6116667

Starting e_i: 501
Model ind 640 epoch 501 head A head_i_epoch 0 batch 0: avg loss -3.029125 avg loss no lamb -3.029125 time 2019-02-22 18:55:59.581459
Model ind 640 epoch 501 head A head_i_epoch 0 batch 100: avg loss -2.890955 avg loss no lamb -2.890955 time 2019-02-22 18:57:51.420346
Model ind 640 epoch 501 head A head_i_epoch 0 batch 200: avg loss -3.094098 avg loss no lamb -3.094098 time 2019-02-22 18:59:41.919849
last batch sz 160
Model ind 640 epoch 501 head B head_i_epoch 0 batch 0: avg loss -1.749799 avg loss no lamb -1.749799 time 2019-02-22 19:01:04.551908
Model ind 640 epoch 501 head B head_i_epoch 0 batch 100: avg loss -1.643813 avg loss no lamb -1.643813 time 2019-02-22 19:02:55.963289
Model ind 640 epoch 501 head B head_i_epoch 0 batch 200: avg loss -1.757551 avg loss no lamb -1.757551 time 2019-02-22 19:04:47.780100
last batch sz 160
Model ind 640 epoch 501 head B head_i_epoch 1 batch 0: avg loss -1.725908 avg loss no lamb -1.725908 time 2019-02-22 19:06:08.945927
Model ind 640 epoch 501 head B head_i_epoch 1 batch 100: avg loss -1.576587 avg loss no lamb -1.576587 time 2019-02-22 19:08:01.101047
Model ind 640 epoch 501 head B head_i_epoch 1 batch 200: avg loss -1.766015 avg loss no lamb -1.766015 time 2019-02-22 19:09:52.197294
last batch sz 160
Pre: time 2019-02-22 19:11:34.445289: 
 	std: 0.05111797
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50545, 0.60895, 0.6085, 0.60903335, 0.5035333]
	train_accs: [0.50545, 0.60895, 0.6085, 0.60903335, 0.5035333]
	best_train_sub_head: 3
	worst: 0.5035333
	avg: 0.5670934
	best: 0.60903335

Starting e_i: 502
Model ind 640 epoch 502 head A head_i_epoch 0 batch 0: avg loss -2.950775 avg loss no lamb -2.950775 time 2019-02-22 19:11:36.661738
Model ind 640 epoch 502 head A head_i_epoch 0 batch 100: avg loss -2.927583 avg loss no lamb -2.927583 time 2019-02-22 19:13:28.607801
Model ind 640 epoch 502 head A head_i_epoch 0 batch 200: avg loss -3.133079 avg loss no lamb -3.133079 time 2019-02-22 19:15:20.696591
last batch sz 160
Model ind 640 epoch 502 head B head_i_epoch 0 batch 0: avg loss -1.774309 avg loss no lamb -1.774309 time 2019-02-22 19:16:42.156963
Model ind 640 epoch 502 head B head_i_epoch 0 batch 100: avg loss -1.773750 avg loss no lamb -1.773750 time 2019-02-22 19:18:35.944404
Model ind 640 epoch 502 head B head_i_epoch 0 batch 200: avg loss -1.740630 avg loss no lamb -1.740630 time 2019-02-22 19:20:27.860247
last batch sz 160
Model ind 640 epoch 502 head B head_i_epoch 1 batch 0: avg loss -1.745706 avg loss no lamb -1.745706 time 2019-02-22 19:21:47.575937
Model ind 640 epoch 502 head B head_i_epoch 1 batch 100: avg loss -1.714393 avg loss no lamb -1.714393 time 2019-02-22 19:23:40.012678
Model ind 640 epoch 502 head B head_i_epoch 1 batch 200: avg loss -1.780087 avg loss no lamb -1.780087 time 2019-02-22 19:25:31.967613
last batch sz 160
Pre: time 2019-02-22 19:27:14.545266: 
 	std: 0.05214058
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50301665, 0.6080667, 0.6079, 0.60823333, 0.50028336]
	train_accs: [0.50301665, 0.6080667, 0.6079, 0.60823333, 0.50028336]
	best_train_sub_head: 3
	worst: 0.50028336
	avg: 0.56549996
	best: 0.60823333

Starting e_i: 503
Model ind 640 epoch 503 head A head_i_epoch 0 batch 0: avg loss -3.062596 avg loss no lamb -3.062596 time 2019-02-22 19:27:16.660392
Model ind 640 epoch 503 head A head_i_epoch 0 batch 100: avg loss -2.986846 avg loss no lamb -2.986846 time 2019-02-22 19:29:08.105456
Model ind 640 epoch 503 head A head_i_epoch 0 batch 200: avg loss -3.028507 avg loss no lamb -3.028507 time 2019-02-22 19:30:59.944346
last batch sz 160
Model ind 640 epoch 503 head B head_i_epoch 0 batch 0: avg loss -1.742564 avg loss no lamb -1.742564 time 2019-02-22 19:32:21.369852
Model ind 640 epoch 503 head B head_i_epoch 0 batch 100: avg loss -1.692668 avg loss no lamb -1.692668 time 2019-02-22 19:34:13.175309
Model ind 640 epoch 503 head B head_i_epoch 0 batch 200: avg loss -1.731660 avg loss no lamb -1.731660 time 2019-02-22 19:36:04.337990
last batch sz 160
Model ind 640 epoch 503 head B head_i_epoch 1 batch 0: avg loss -1.764055 avg loss no lamb -1.764055 time 2019-02-22 19:37:25.333343
Model ind 640 epoch 503 head B head_i_epoch 1 batch 100: avg loss -1.575217 avg loss no lamb -1.575217 time 2019-02-22 19:39:17.109419
Model ind 640 epoch 503 head B head_i_epoch 1 batch 200: avg loss -1.703090 avg loss no lamb -1.703090 time 2019-02-22 19:41:09.274203
last batch sz 160
Pre: time 2019-02-22 19:42:51.804357: 
 	std: 0.05355485
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49876666, 0.60705, 0.6066833, 0.60711664, 0.49651667]
	train_accs: [0.49876666, 0.60705, 0.6066833, 0.60711664, 0.49651667]
	best_train_sub_head: 3
	worst: 0.49651667
	avg: 0.56322664
	best: 0.60711664

Starting e_i: 504
Model ind 640 epoch 504 head A head_i_epoch 0 batch 0: avg loss -3.055795 avg loss no lamb -3.055795 time 2019-02-22 19:42:54.147913
Model ind 640 epoch 504 head A head_i_epoch 0 batch 100: avg loss -2.924797 avg loss no lamb -2.924797 time 2019-02-22 19:45:46.965753
Model ind 640 epoch 504 head A head_i_epoch 0 batch 200: avg loss -3.016490 avg loss no lamb -3.016490 time 2019-02-22 19:49:04.228626
last batch sz 160
Model ind 640 epoch 504 head B head_i_epoch 0 batch 0: avg loss -1.691778 avg loss no lamb -1.691778 time 2019-02-22 19:51:48.445460
Model ind 640 epoch 504 head B head_i_epoch 0 batch 100: avg loss -1.670935 avg loss no lamb -1.670935 time 2019-02-22 19:55:08.756308
Model ind 640 epoch 504 head B head_i_epoch 0 batch 200: avg loss -1.788973 avg loss no lamb -1.788973 time 2019-02-22 19:58:25.502156
last batch sz 160
Model ind 640 epoch 504 head B head_i_epoch 1 batch 0: avg loss -1.583493 avg loss no lamb -1.583493 time 2019-02-22 20:01:09.686594
Model ind 640 epoch 504 head B head_i_epoch 1 batch 100: avg loss -1.657913 avg loss no lamb -1.657913 time 2019-02-22 20:04:17.710080
Model ind 640 epoch 504 head B head_i_epoch 1 batch 200: avg loss -1.751318 avg loss no lamb -1.751318 time 2019-02-22 20:07:33.624164
last batch sz 160
Pre: time 2019-02-22 20:10:49.195657: 
 	std: 0.05352107
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49878332, 0.6077167, 0.60725, 0.60775, 0.49786666]
	train_accs: [0.49878332, 0.6077167, 0.60725, 0.60775, 0.49786666]
	best_train_sub_head: 3
	worst: 0.49786666
	avg: 0.56387335
	best: 0.60775

Starting e_i: 505
Model ind 640 epoch 505 head A head_i_epoch 0 batch 0: avg loss -3.031946 avg loss no lamb -3.031946 time 2019-02-22 20:10:51.586826
Model ind 640 epoch 505 head A head_i_epoch 0 batch 100: avg loss -3.053017 avg loss no lamb -3.053017 time 2019-02-22 20:14:10.816479
Model ind 640 epoch 505 head A head_i_epoch 0 batch 200: avg loss -3.092091 avg loss no lamb -3.092091 time 2019-02-22 20:17:26.786897
last batch sz 160
Model ind 640 epoch 505 head B head_i_epoch 0 batch 0: avg loss -1.654400 avg loss no lamb -1.654400 time 2019-02-22 20:20:04.910343
Model ind 640 epoch 505 head B head_i_epoch 0 batch 100: avg loss -1.590557 avg loss no lamb -1.590557 time 2019-02-22 20:23:23.860439
Model ind 640 epoch 505 head B head_i_epoch 0 batch 200: avg loss -1.808687 avg loss no lamb -1.808687 time 2019-02-22 20:26:41.924525
last batch sz 160
Model ind 640 epoch 505 head B head_i_epoch 1 batch 0: avg loss -1.638230 avg loss no lamb -1.638230 time 2019-02-22 20:29:16.347597
Model ind 640 epoch 505 head B head_i_epoch 1 batch 100: avg loss -1.534140 avg loss no lamb -1.534140 time 2019-02-22 20:32:40.910420
Model ind 640 epoch 505 head B head_i_epoch 1 batch 200: avg loss -1.785213 avg loss no lamb -1.785213 time 2019-02-22 20:35:59.746102
last batch sz 160
Pre: time 2019-02-22 20:38:57.375622: 
 	std: 0.052085984
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50261664, 0.60788333, 0.6077833, 0.60791665, 0.50048333]
	train_accs: [0.50261664, 0.60788333, 0.6077833, 0.60791665, 0.50048333]
	best_train_sub_head: 3
	worst: 0.50048333
	avg: 0.56533664
	best: 0.60791665

Starting e_i: 506
Model ind 640 epoch 506 head A head_i_epoch 0 batch 0: avg loss -3.006393 avg loss no lamb -3.006393 time 2019-02-22 20:39:01.137477
Model ind 640 epoch 506 head A head_i_epoch 0 batch 100: avg loss -2.921552 avg loss no lamb -2.921552 time 2019-02-22 20:42:45.701228
Model ind 640 epoch 506 head A head_i_epoch 0 batch 200: avg loss -3.103484 avg loss no lamb -3.103484 time 2019-02-22 20:46:05.586937
last batch sz 160
Model ind 640 epoch 506 head B head_i_epoch 0 batch 0: avg loss -1.684499 avg loss no lamb -1.684499 time 2019-02-22 20:48:20.542287
Model ind 640 epoch 506 head B head_i_epoch 0 batch 100: avg loss -1.582273 avg loss no lamb -1.582273 time 2019-02-22 20:51:58.716024
Model ind 640 epoch 506 head B head_i_epoch 0 batch 200: avg loss -1.784578 avg loss no lamb -1.784578 time 2019-02-22 20:55:23.503543
last batch sz 160
Model ind 640 epoch 506 head B head_i_epoch 1 batch 0: avg loss -1.762082 avg loss no lamb -1.762082 time 2019-02-22 20:57:40.807218
Model ind 640 epoch 506 head B head_i_epoch 1 batch 100: avg loss -1.594131 avg loss no lamb -1.594131 time 2019-02-22 21:01:13.260248
Model ind 640 epoch 506 head B head_i_epoch 1 batch 200: avg loss -1.859846 avg loss no lamb -1.859846 time 2019-02-22 21:04:41.446329
last batch sz 160
Pre: time 2019-02-22 21:07:32.791884: 
 	std: 0.052932613
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50011665, 0.6077667, 0.6077833, 0.60786664, 0.4994]
	train_accs: [0.50011665, 0.6077667, 0.6077833, 0.60786664, 0.4994]
	best_train_sub_head: 3
	worst: 0.4994
	avg: 0.56458664
	best: 0.60786664

Starting e_i: 507
Model ind 640 epoch 507 head A head_i_epoch 0 batch 0: avg loss -2.983986 avg loss no lamb -2.983986 time 2019-02-22 21:07:35.767400
Model ind 640 epoch 507 head A head_i_epoch 0 batch 100: avg loss -2.864295 avg loss no lamb -2.864295 time 2019-02-22 21:10:50.758030
Model ind 640 epoch 507 head A head_i_epoch 0 batch 200: avg loss -3.195614 avg loss no lamb -3.195614 time 2019-02-22 21:14:32.778509
last batch sz 160
Model ind 640 epoch 507 head B head_i_epoch 0 batch 0: avg loss -1.700271 avg loss no lamb -1.700271 time 2019-02-22 21:16:47.579839
Model ind 640 epoch 507 head B head_i_epoch 0 batch 100: avg loss -1.697528 avg loss no lamb -1.697528 time 2019-02-22 21:20:02.307207
Model ind 640 epoch 507 head B head_i_epoch 0 batch 200: avg loss -1.850659 avg loss no lamb -1.850659 time 2019-02-22 21:23:44.892510
last batch sz 160
Model ind 640 epoch 507 head B head_i_epoch 1 batch 0: avg loss -1.672269 avg loss no lamb -1.672269 time 2019-02-22 21:26:00.321537
Model ind 640 epoch 507 head B head_i_epoch 1 batch 100: avg loss -1.650252 avg loss no lamb -1.650252 time 2019-02-22 21:29:15.143289
Model ind 640 epoch 507 head B head_i_epoch 1 batch 200: avg loss -1.717720 avg loss no lamb -1.717720 time 2019-02-22 21:32:57.325131
last batch sz 160
Pre: time 2019-02-22 21:35:55.163579: 
 	std: 0.0513427
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50268334, 0.60693336, 0.60658336, 0.6065, 0.5010667]
	train_accs: [0.50268334, 0.60693336, 0.60658336, 0.6065, 0.5010667]
	best_train_sub_head: 1
	worst: 0.5010667
	avg: 0.56475335
	best: 0.60693336

Starting e_i: 508
Model ind 640 epoch 508 head A head_i_epoch 0 batch 0: avg loss -2.979927 avg loss no lamb -2.979927 time 2019-02-22 21:35:58.135372
Model ind 640 epoch 508 head A head_i_epoch 0 batch 100: avg loss -2.946257 avg loss no lamb -2.946257 time 2019-02-22 21:39:14.370381
Model ind 640 epoch 508 head A head_i_epoch 0 batch 200: avg loss -3.043407 avg loss no lamb -3.043407 time 2019-02-22 21:42:33.402526
last batch sz 160
Model ind 640 epoch 508 head B head_i_epoch 0 batch 0: avg loss -1.649016 avg loss no lamb -1.649016 time 2019-02-22 21:45:12.548685
Model ind 640 epoch 508 head B head_i_epoch 0 batch 100: avg loss -1.603995 avg loss no lamb -1.603995 time 2019-02-22 21:48:29.250100
Model ind 640 epoch 508 head B head_i_epoch 0 batch 200: avg loss -1.779920 avg loss no lamb -1.779920 time 2019-02-22 21:51:46.133480
last batch sz 160
Model ind 640 epoch 508 head B head_i_epoch 1 batch 0: avg loss -1.707469 avg loss no lamb -1.707469 time 2019-02-22 21:54:31.620175
Model ind 640 epoch 508 head B head_i_epoch 1 batch 100: avg loss -1.621044 avg loss no lamb -1.621044 time 2019-02-22 21:57:44.899150
Model ind 640 epoch 508 head B head_i_epoch 1 batch 200: avg loss -1.764039 avg loss no lamb -1.764039 time 2019-02-22 22:01:02.565059
last batch sz 160
Pre: time 2019-02-22 22:04:23.790106: 
 	std: 0.053790033
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49988332, 0.60866666, 0.60856664, 0.6084333, 0.49765]
	train_accs: [0.49988332, 0.60866666, 0.60856664, 0.6084333, 0.49765]
	best_train_sub_head: 1
	worst: 0.49765
	avg: 0.5646399
	best: 0.60866666

Starting e_i: 509
Model ind 640 epoch 509 head A head_i_epoch 0 batch 0: avg loss -3.048978 avg loss no lamb -3.048978 time 2019-02-22 22:04:26.611849
Model ind 640 epoch 509 head A head_i_epoch 0 batch 100: avg loss -2.986965 avg loss no lamb -2.986965 time 2019-02-22 22:07:42.500705
Model ind 640 epoch 509 head A head_i_epoch 0 batch 200: avg loss -3.094508 avg loss no lamb -3.094508 time 2019-02-22 22:10:59.586991
last batch sz 160
Model ind 640 epoch 509 head B head_i_epoch 0 batch 0: avg loss -1.667842 avg loss no lamb -1.667842 time 2019-02-22 22:13:44.122165
Model ind 640 epoch 509 head B head_i_epoch 0 batch 100: avg loss -1.689577 avg loss no lamb -1.689577 time 2019-02-22 22:16:58.288745
Model ind 640 epoch 509 head B head_i_epoch 0 batch 200: avg loss -1.759418 avg loss no lamb -1.759418 time 2019-02-22 22:20:13.358410
last batch sz 160
Model ind 640 epoch 509 head B head_i_epoch 1 batch 0: avg loss -1.682204 avg loss no lamb -1.682204 time 2019-02-22 22:22:55.160356
Model ind 640 epoch 509 head B head_i_epoch 1 batch 100: avg loss -1.674659 avg loss no lamb -1.674659 time 2019-02-22 22:26:17.690758
Model ind 640 epoch 509 head B head_i_epoch 1 batch 200: avg loss -1.750257 avg loss no lamb -1.750257 time 2019-02-22 22:29:32.702343
last batch sz 160
Pre: time 2019-02-22 22:32:33.434989: 
 	std: 0.051167935
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50635, 0.6103333, 0.6098833, 0.60955, 0.5046167]
	train_accs: [0.50635, 0.6103333, 0.6098833, 0.60955, 0.5046167]
	best_train_sub_head: 1
	worst: 0.5046167
	avg: 0.56814665
	best: 0.6103333

Starting e_i: 510
Model ind 640 epoch 510 head A head_i_epoch 0 batch 0: avg loss -2.967295 avg loss no lamb -2.967295 time 2019-02-22 22:32:37.300984
Model ind 640 epoch 510 head A head_i_epoch 0 batch 100: avg loss -2.939173 avg loss no lamb -2.939173 time 2019-02-22 22:36:23.624648
Model ind 640 epoch 510 head A head_i_epoch 0 batch 200: avg loss -3.035390 avg loss no lamb -3.035390 time 2019-02-22 22:39:40.247644
last batch sz 160
Model ind 640 epoch 510 head B head_i_epoch 0 batch 0: avg loss -1.610328 avg loss no lamb -1.610328 time 2019-02-22 22:41:54.651550
Model ind 640 epoch 510 head B head_i_epoch 0 batch 100: avg loss -1.641407 avg loss no lamb -1.641407 time 2019-02-22 22:45:38.391940
Model ind 640 epoch 510 head B head_i_epoch 0 batch 200: avg loss -1.789269 avg loss no lamb -1.789269 time 2019-02-22 22:49:00.370653
last batch sz 160
Model ind 640 epoch 510 head B head_i_epoch 1 batch 0: avg loss -1.654731 avg loss no lamb -1.654731 time 2019-02-22 22:51:13.455848
Model ind 640 epoch 510 head B head_i_epoch 1 batch 100: avg loss -1.607723 avg loss no lamb -1.607723 time 2019-02-22 22:54:50.701596
Model ind 640 epoch 510 head B head_i_epoch 1 batch 200: avg loss -1.813816 avg loss no lamb -1.813816 time 2019-02-22 22:58:15.396891
last batch sz 160
Pre: time 2019-02-22 23:01:08.511091: 
 	std: 0.05156914
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50405, 0.60935, 0.6091167, 0.60875, 0.5035667]
	train_accs: [0.50405, 0.60935, 0.6091167, 0.60875, 0.5035667]
	best_train_sub_head: 1
	worst: 0.5035667
	avg: 0.56696665
	best: 0.60935

Starting e_i: 511
Model ind 640 epoch 511 head A head_i_epoch 0 batch 0: avg loss -3.008371 avg loss no lamb -3.008371 time 2019-02-22 23:01:15.742198
Model ind 640 epoch 511 head A head_i_epoch 0 batch 100: avg loss -3.011770 avg loss no lamb -3.011770 time 2019-02-22 23:04:33.194797
Model ind 640 epoch 511 head A head_i_epoch 0 batch 200: avg loss -3.165383 avg loss no lamb -3.165383 time 2019-02-22 23:08:14.239451
last batch sz 160
Model ind 640 epoch 511 head B head_i_epoch 0 batch 0: avg loss -1.670303 avg loss no lamb -1.670303 time 2019-02-22 23:10:33.016453
Model ind 640 epoch 511 head B head_i_epoch 0 batch 100: avg loss -1.616742 avg loss no lamb -1.616742 time 2019-02-22 23:13:54.890427
Model ind 640 epoch 511 head B head_i_epoch 0 batch 200: avg loss -1.793458 avg loss no lamb -1.793458 time 2019-02-22 23:17:26.913398
last batch sz 160
Model ind 640 epoch 511 head B head_i_epoch 1 batch 0: avg loss -1.659720 avg loss no lamb -1.659720 time 2019-02-22 23:19:57.163091
Model ind 640 epoch 511 head B head_i_epoch 1 batch 100: avg loss -1.639872 avg loss no lamb -1.639872 time 2019-02-22 23:23:10.951470
Model ind 640 epoch 511 head B head_i_epoch 1 batch 200: avg loss -1.751050 avg loss no lamb -1.751050 time 2019-02-22 23:26:40.704176
last batch sz 160
Pre: time 2019-02-22 23:29:50.143837: 
 	std: 0.05295415
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50455, 0.61225, 0.61233336, 0.6116833, 0.50345]
	train_accs: [0.50455, 0.61225, 0.61233336, 0.6116833, 0.50345]
	best_train_sub_head: 2
	worst: 0.50345
	avg: 0.5688533
	best: 0.61233336

Starting e_i: 512
Model ind 640 epoch 512 head A head_i_epoch 0 batch 0: avg loss -3.052253 avg loss no lamb -3.052253 time 2019-02-22 23:29:53.098238
Model ind 640 epoch 512 head A head_i_epoch 0 batch 100: avg loss -3.000024 avg loss no lamb -3.000024 time 2019-02-22 23:33:10.749107
Model ind 640 epoch 512 head A head_i_epoch 0 batch 200: avg loss -3.048075 avg loss no lamb -3.048075 time 2019-02-22 23:36:24.486575
last batch sz 160
Model ind 640 epoch 512 head B head_i_epoch 0 batch 0: avg loss -1.720375 avg loss no lamb -1.720375 time 2019-02-22 23:39:08.615468
Model ind 640 epoch 512 head B head_i_epoch 0 batch 100: avg loss -1.652148 avg loss no lamb -1.652148 time 2019-02-22 23:42:25.641503
Model ind 640 epoch 512 head B head_i_epoch 0 batch 200: avg loss -1.797897 avg loss no lamb -1.797897 time 2019-02-22 23:45:40.274798
last batch sz 160
Model ind 640 epoch 512 head B head_i_epoch 1 batch 0: avg loss -1.627420 avg loss no lamb -1.627420 time 2019-02-22 23:48:23.712121
Model ind 640 epoch 512 head B head_i_epoch 1 batch 100: avg loss -1.613951 avg loss no lamb -1.613951 time 2019-02-22 23:51:42.311686
Model ind 640 epoch 512 head B head_i_epoch 1 batch 200: avg loss -1.741970 avg loss no lamb -1.741970 time 2019-02-22 23:55:01.213884
last batch sz 160
Pre: time 2019-02-22 23:58:17.659119: 
 	std: 0.05258279
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50375, 0.6110333, 0.6106, 0.61086667, 0.50325]
	train_accs: [0.50375, 0.6110333, 0.6106, 0.61086667, 0.50325]
	best_train_sub_head: 1
	worst: 0.50325
	avg: 0.5679
	best: 0.6110333

Starting e_i: 513
Model ind 640 epoch 513 head A head_i_epoch 0 batch 0: avg loss -3.044628 avg loss no lamb -3.044628 time 2019-02-22 23:58:20.098203
Model ind 640 epoch 513 head A head_i_epoch 0 batch 100: avg loss -2.954377 avg loss no lamb -2.954377 time 2019-02-23 00:01:42.889635
Model ind 640 epoch 513 head A head_i_epoch 0 batch 200: avg loss -3.067689 avg loss no lamb -3.067689 time 2019-02-23 00:05:00.022763
last batch sz 160
Model ind 640 epoch 513 head B head_i_epoch 0 batch 0: avg loss -1.779413 avg loss no lamb -1.779413 time 2019-02-23 00:07:32.384767
Model ind 640 epoch 513 head B head_i_epoch 0 batch 100: avg loss -1.575632 avg loss no lamb -1.575632 time 2019-02-23 00:11:00.225859
Model ind 640 epoch 513 head B head_i_epoch 0 batch 200: avg loss -1.811083 avg loss no lamb -1.811083 time 2019-02-23 00:14:23.233037
last batch sz 160
Model ind 640 epoch 513 head B head_i_epoch 1 batch 0: avg loss -1.729503 avg loss no lamb -1.729503 time 2019-02-23 00:16:49.347471
Model ind 640 epoch 513 head B head_i_epoch 1 batch 100: avg loss -1.670817 avg loss no lamb -1.670817 time 2019-02-23 00:20:24.746646
Model ind 640 epoch 513 head B head_i_epoch 1 batch 200: avg loss -1.769040 avg loss no lamb -1.769040 time 2019-02-23 00:23:39.491114
last batch sz 160
Pre: time 2019-02-23 00:26:36.760494: 
 	std: 0.053137165
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5021667, 0.6102, 0.61041665, 0.60995, 0.50128335]
	train_accs: [0.5021667, 0.6102, 0.61041665, 0.60995, 0.50128335]
	best_train_sub_head: 2
	worst: 0.50128335
	avg: 0.56680334
	best: 0.61041665

Starting e_i: 514
Model ind 640 epoch 514 head A head_i_epoch 0 batch 0: avg loss -3.071910 avg loss no lamb -3.071910 time 2019-02-23 00:26:39.691055
Model ind 640 epoch 514 head A head_i_epoch 0 batch 100: avg loss -2.973738 avg loss no lamb -2.973738 time 2019-02-23 00:30:14.696626
Model ind 640 epoch 514 head A head_i_epoch 0 batch 200: avg loss -3.008715 avg loss no lamb -3.008715 time 2019-02-23 00:33:42.294496
last batch sz 160
Model ind 640 epoch 514 head B head_i_epoch 0 batch 0: avg loss -1.723385 avg loss no lamb -1.723385 time 2019-02-23 00:36:02.315741
Model ind 640 epoch 514 head B head_i_epoch 0 batch 100: avg loss -1.626786 avg loss no lamb -1.626786 time 2019-02-23 00:39:32.803646
Model ind 640 epoch 514 head B head_i_epoch 0 batch 200: avg loss -1.765068 avg loss no lamb -1.765068 time 2019-02-23 00:43:04.626621
last batch sz 160
Model ind 640 epoch 514 head B head_i_epoch 1 batch 0: avg loss -1.763016 avg loss no lamb -1.763016 time 2019-02-23 00:45:19.558660
Model ind 640 epoch 514 head B head_i_epoch 1 batch 100: avg loss -1.602137 avg loss no lamb -1.602137 time 2019-02-23 00:48:45.296608
Model ind 640 epoch 514 head B head_i_epoch 1 batch 200: avg loss -1.767332 avg loss no lamb -1.767332 time 2019-02-23 00:52:22.480477
last batch sz 160
Pre: time 2019-02-23 00:55:14.241822: 
 	std: 0.052605
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50175, 0.60835, 0.60865, 0.6082, 0.5003]
	train_accs: [0.50175, 0.60835, 0.60865, 0.6082, 0.5003]
	best_train_sub_head: 2
	worst: 0.5003
	avg: 0.56545
	best: 0.60865

Starting e_i: 515
Model ind 640 epoch 515 head A head_i_epoch 0 batch 0: avg loss -3.128361 avg loss no lamb -3.128361 time 2019-02-23 00:55:17.419997
Model ind 640 epoch 515 head A head_i_epoch 0 batch 100: avg loss -2.925640 avg loss no lamb -2.925640 time 2019-02-23 00:58:30.038526
Model ind 640 epoch 515 head A head_i_epoch 0 batch 200: avg loss -3.052084 avg loss no lamb -3.052084 time 2019-02-23 01:02:04.500703
last batch sz 160
Model ind 640 epoch 515 head B head_i_epoch 0 batch 0: avg loss -1.686837 avg loss no lamb -1.686837 time 2019-02-23 01:04:31.677251
Model ind 640 epoch 515 head B head_i_epoch 0 batch 100: avg loss -1.615242 avg loss no lamb -1.615242 time 2019-02-23 01:07:50.516635
Model ind 640 epoch 515 head B head_i_epoch 0 batch 200: avg loss -1.831516 avg loss no lamb -1.831516 time 2019-02-23 01:11:22.049949
last batch sz 160
Model ind 640 epoch 515 head B head_i_epoch 1 batch 0: avg loss -1.675025 avg loss no lamb -1.675025 time 2019-02-23 01:13:51.397269
Model ind 640 epoch 515 head B head_i_epoch 1 batch 100: avg loss -1.670670 avg loss no lamb -1.670670 time 2019-02-23 01:17:05.822858
Model ind 640 epoch 515 head B head_i_epoch 1 batch 200: avg loss -1.786867 avg loss no lamb -1.786867 time 2019-02-23 01:20:33.713804
last batch sz 160
Pre: time 2019-02-23 01:23:42.154359: 
 	std: 0.05175278
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.4995, 0.60506666, 0.60501665, 0.6045833, 0.499]
	train_accs: [0.4995, 0.60506666, 0.60501665, 0.6045833, 0.499]
	best_train_sub_head: 1
	worst: 0.499
	avg: 0.56263334
	best: 0.60506666

Starting e_i: 516
Model ind 640 epoch 516 head A head_i_epoch 0 batch 0: avg loss -2.987994 avg loss no lamb -2.987994 time 2019-02-23 01:23:45.019629
Model ind 640 epoch 516 head A head_i_epoch 0 batch 100: avg loss -2.928567 avg loss no lamb -2.928567 time 2019-02-23 01:27:07.294378
Model ind 640 epoch 516 head A head_i_epoch 0 batch 200: avg loss -3.043400 avg loss no lamb -3.043400 time 2019-02-23 01:30:26.698912
last batch sz 160
Model ind 640 epoch 516 head B head_i_epoch 0 batch 0: avg loss -1.684788 avg loss no lamb -1.684788 time 2019-02-23 01:33:10.964178
Model ind 640 epoch 516 head B head_i_epoch 0 batch 100: avg loss -1.698535 avg loss no lamb -1.698535 time 2019-02-23 01:36:27.062058
Model ind 640 epoch 516 head B head_i_epoch 0 batch 200: avg loss -1.730983 avg loss no lamb -1.730983 time 2019-02-23 01:39:42.056795
last batch sz 160
Model ind 640 epoch 516 head B head_i_epoch 1 batch 0: avg loss -1.742303 avg loss no lamb -1.742303 time 2019-02-23 01:42:27.126165
Model ind 640 epoch 516 head B head_i_epoch 1 batch 100: avg loss -1.630451 avg loss no lamb -1.630451 time 2019-02-23 01:45:46.627094
Model ind 640 epoch 516 head B head_i_epoch 1 batch 200: avg loss -1.730073 avg loss no lamb -1.730073 time 2019-02-23 01:49:05.878435
last batch sz 160
Pre: time 2019-02-23 01:52:16.986517: 
 	std: 0.05319751
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5028667, 0.6116167, 0.6114333, 0.61151665, 0.503]
	train_accs: [0.5028667, 0.6116167, 0.6114333, 0.61151665, 0.503]
	best_train_sub_head: 1
	worst: 0.5028667
	avg: 0.5680867
	best: 0.6116167

Starting e_i: 517
Model ind 640 epoch 517 head A head_i_epoch 0 batch 0: avg loss -3.093088 avg loss no lamb -3.093088 time 2019-02-23 01:52:19.479328
Model ind 640 epoch 517 head A head_i_epoch 0 batch 100: avg loss -2.993117 avg loss no lamb -2.993117 time 2019-02-23 01:55:54.149866
Model ind 640 epoch 517 head A head_i_epoch 0 batch 200: avg loss -3.091528 avg loss no lamb -3.091528 time 2019-02-23 01:59:09.251972
last batch sz 160
Model ind 640 epoch 517 head B head_i_epoch 0 batch 0: avg loss -1.675788 avg loss no lamb -1.675788 time 2019-02-23 02:01:31.265272
Model ind 640 epoch 517 head B head_i_epoch 0 batch 100: avg loss -1.535438 avg loss no lamb -1.535438 time 2019-02-23 02:05:11.457577
Model ind 640 epoch 517 head B head_i_epoch 0 batch 200: avg loss -1.732901 avg loss no lamb -1.732901 time 2019-02-23 02:08:30.713471
last batch sz 160
Model ind 640 epoch 517 head B head_i_epoch 1 batch 0: avg loss -1.719623 avg loss no lamb -1.719623 time 2019-02-23 02:10:47.783752
Model ind 640 epoch 517 head B head_i_epoch 1 batch 100: avg loss -1.576599 avg loss no lamb -1.576599 time 2019-02-23 02:14:30.813056
Model ind 640 epoch 517 head B head_i_epoch 1 batch 200: avg loss -1.808098 avg loss no lamb -1.808098 time 2019-02-23 02:17:51.869163
last batch sz 160
Pre: time 2019-02-23 02:20:44.877592: 
 	std: 0.052669518
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50388336, 0.6114333, 0.6113667, 0.61123335, 0.50378335]
	train_accs: [0.50388336, 0.6114333, 0.6113667, 0.61123335, 0.50378335]
	best_train_sub_head: 1
	worst: 0.50378335
	avg: 0.56834
	best: 0.6114333

Starting e_i: 518
Model ind 640 epoch 518 head A head_i_epoch 0 batch 0: avg loss -3.107780 avg loss no lamb -3.107780 time 2019-02-23 02:20:47.858667
Model ind 640 epoch 518 head A head_i_epoch 0 batch 100: avg loss -3.026431 avg loss no lamb -3.026431 time 2019-02-23 02:24:12.710704
Model ind 640 epoch 518 head A head_i_epoch 0 batch 200: avg loss -3.119355 avg loss no lamb -3.119355 time 2019-02-23 02:27:53.604513
last batch sz 160
Model ind 640 epoch 518 head B head_i_epoch 0 batch 0: avg loss -1.661236 avg loss no lamb -1.661236 time 2019-02-23 02:30:09.986907
Model ind 640 epoch 518 head B head_i_epoch 0 batch 100: avg loss -1.613049 avg loss no lamb -1.613049 time 2019-02-23 02:33:28.755826
Model ind 640 epoch 518 head B head_i_epoch 0 batch 200: avg loss -1.731974 avg loss no lamb -1.731974 time 2019-02-23 02:37:14.501965
last batch sz 160
Model ind 640 epoch 518 head B head_i_epoch 1 batch 0: avg loss -1.682893 avg loss no lamb -1.682893 time 2019-02-23 02:39:31.772404
Model ind 640 epoch 518 head B head_i_epoch 1 batch 100: avg loss -1.680598 avg loss no lamb -1.680598 time 2019-02-23 02:42:47.655994
Model ind 640 epoch 518 head B head_i_epoch 1 batch 200: avg loss -1.719447 avg loss no lamb -1.719447 time 2019-02-23 02:46:25.950519
last batch sz 160
Pre: time 2019-02-23 02:49:24.141940: 
 	std: 0.051649153
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5042667, 0.6095, 0.60931665, 0.6094667, 0.50373334]
	train_accs: [0.5042667, 0.6095, 0.60931665, 0.6094667, 0.50373334]
	best_train_sub_head: 1
	worst: 0.50373334
	avg: 0.5672567
	best: 0.6095

Starting e_i: 519
Model ind 640 epoch 519 head A head_i_epoch 0 batch 0: avg loss -3.086417 avg loss no lamb -3.086417 time 2019-02-23 02:49:26.957257
Model ind 640 epoch 519 head A head_i_epoch 0 batch 100: avg loss -2.999396 avg loss no lamb -2.999396 time 2019-02-23 02:52:39.769477
Model ind 640 epoch 519 head A head_i_epoch 0 batch 200: avg loss -3.100998 avg loss no lamb -3.100998 time 2019-02-23 02:56:03.782405
last batch sz 160
Model ind 640 epoch 519 head B head_i_epoch 0 batch 0: avg loss -1.687224 avg loss no lamb -1.687224 time 2019-02-23 02:58:43.002729
Model ind 640 epoch 519 head B head_i_epoch 0 batch 100: avg loss -1.660342 avg loss no lamb -1.660342 time 2019-02-23 03:01:57.662757
Model ind 640 epoch 519 head B head_i_epoch 0 batch 200: avg loss -1.779100 avg loss no lamb -1.779100 time 2019-02-23 03:05:16.395447
last batch sz 160
Model ind 640 epoch 519 head B head_i_epoch 1 batch 0: avg loss -1.634773 avg loss no lamb -1.634773 time 2019-02-23 03:07:59.643586
Model ind 640 epoch 519 head B head_i_epoch 1 batch 100: avg loss -1.652268 avg loss no lamb -1.652268 time 2019-02-23 03:11:19.427245
Model ind 640 epoch 519 head B head_i_epoch 1 batch 200: avg loss -1.735761 avg loss no lamb -1.735761 time 2019-02-23 03:14:36.654655
last batch sz 160
Pre: time 2019-02-23 03:17:56.861458: 
 	std: 0.051662542
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50505, 0.6105667, 0.61053336, 0.61071664, 0.50525]
	train_accs: [0.50505, 0.6105667, 0.61053336, 0.61071664, 0.50525]
	best_train_sub_head: 3
	worst: 0.50505
	avg: 0.56842333
	best: 0.61071664

Starting e_i: 520
Model ind 640 epoch 520 head A head_i_epoch 0 batch 0: avg loss -3.024571 avg loss no lamb -3.024571 time 2019-02-23 03:17:59.740637
Model ind 640 epoch 520 head A head_i_epoch 0 batch 100: avg loss -2.906775 avg loss no lamb -2.906775 time 2019-02-23 03:21:16.775785
Model ind 640 epoch 520 head A head_i_epoch 0 batch 200: avg loss -3.173203 avg loss no lamb -3.173203 time 2019-02-23 03:24:36.584975
last batch sz 160
Model ind 640 epoch 520 head B head_i_epoch 0 batch 0: avg loss -1.716182 avg loss no lamb -1.716182 time 2019-02-23 03:27:17.811047
Model ind 640 epoch 520 head B head_i_epoch 0 batch 100: avg loss -1.625449 avg loss no lamb -1.625449 time 2019-02-23 03:30:35.817051
Model ind 640 epoch 520 head B head_i_epoch 0 batch 200: avg loss -1.845077 avg loss no lamb -1.845077 time 2019-02-23 03:33:54.349467
last batch sz 160
Model ind 640 epoch 520 head B head_i_epoch 1 batch 0: avg loss -1.708571 avg loss no lamb -1.708571 time 2019-02-23 03:36:28.807241
Model ind 640 epoch 520 head B head_i_epoch 1 batch 100: avg loss -1.585276 avg loss no lamb -1.585276 time 2019-02-23 03:39:53.956023
Model ind 640 epoch 520 head B head_i_epoch 1 batch 200: avg loss -1.792994 avg loss no lamb -1.792994 time 2019-02-23 03:43:15.955204
last batch sz 160
Pre: time 2019-02-23 03:46:16.382187: 
 	std: 0.051788054
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50453335, 0.61045, 0.61, 0.6105833, 0.5047333]
	train_accs: [0.50453335, 0.61045, 0.61, 0.6105833, 0.5047333]
	best_train_sub_head: 3
	worst: 0.50453335
	avg: 0.56806
	best: 0.6105833

Starting e_i: 521
Model ind 640 epoch 521 head A head_i_epoch 0 batch 0: avg loss -3.003487 avg loss no lamb -3.003487 time 2019-02-23 03:46:23.584704
Model ind 640 epoch 521 head A head_i_epoch 0 batch 100: avg loss -3.039010 avg loss no lamb -3.039010 time 2019-02-23 03:49:58.213042
Model ind 640 epoch 521 head A head_i_epoch 0 batch 200: avg loss -3.129813 avg loss no lamb -3.129813 time 2019-02-23 03:53:28.640615
last batch sz 160
Model ind 640 epoch 521 head B head_i_epoch 0 batch 0: avg loss -1.722261 avg loss no lamb -1.722261 time 2019-02-23 03:55:43.449120
Model ind 640 epoch 521 head B head_i_epoch 0 batch 100: avg loss -1.676159 avg loss no lamb -1.676159 time 2019-02-23 03:59:15.027434
Model ind 640 epoch 521 head B head_i_epoch 0 batch 200: avg loss -1.795983 avg loss no lamb -1.795983 time 2019-02-23 04:02:47.301404
last batch sz 160
Model ind 640 epoch 521 head B head_i_epoch 1 batch 0: avg loss -1.678715 avg loss no lamb -1.678715 time 2019-02-23 04:05:03.475581
Model ind 640 epoch 521 head B head_i_epoch 1 batch 100: avg loss -1.597330 avg loss no lamb -1.597330 time 2019-02-23 04:08:26.332414
Model ind 640 epoch 521 head B head_i_epoch 1 batch 200: avg loss -1.674171 avg loss no lamb -1.674171 time 2019-02-23 04:12:02.751370
last batch sz 160
Pre: time 2019-02-23 04:14:55.487578: 
 	std: 0.05132639
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5053833, 0.61006665, 0.6102833, 0.61008334, 0.5053667]
	train_accs: [0.5053833, 0.61006665, 0.6102833, 0.61008334, 0.5053667]
	best_train_sub_head: 2
	worst: 0.5053667
	avg: 0.56823665
	best: 0.6102833

Starting e_i: 522
Model ind 640 epoch 522 head A head_i_epoch 0 batch 0: avg loss -3.094425 avg loss no lamb -3.094425 time 2019-02-23 04:14:58.490286
Model ind 640 epoch 522 head A head_i_epoch 0 batch 100: avg loss -2.903681 avg loss no lamb -2.903681 time 2019-02-23 04:18:17.774994
Model ind 640 epoch 522 head A head_i_epoch 0 batch 200: avg loss -3.064700 avg loss no lamb -3.064700 time 2019-02-23 04:21:50.584095
last batch sz 160
Model ind 640 epoch 522 head B head_i_epoch 0 batch 0: avg loss -1.684890 avg loss no lamb -1.684890 time 2019-02-23 04:24:20.947286
Model ind 640 epoch 522 head B head_i_epoch 0 batch 100: avg loss -1.622088 avg loss no lamb -1.622088 time 2019-02-23 04:27:37.295434
Model ind 640 epoch 522 head B head_i_epoch 0 batch 200: avg loss -1.830785 avg loss no lamb -1.830785 time 2019-02-23 04:31:08.046951
last batch sz 160
Model ind 640 epoch 522 head B head_i_epoch 1 batch 0: avg loss -1.651546 avg loss no lamb -1.651546 time 2019-02-23 04:33:36.348751
Model ind 640 epoch 522 head B head_i_epoch 1 batch 100: avg loss -1.638409 avg loss no lamb -1.638409 time 2019-02-23 04:36:57.131352
Model ind 640 epoch 522 head B head_i_epoch 1 batch 200: avg loss -1.800355 avg loss no lamb -1.800355 time 2019-02-23 04:40:20.863041
last batch sz 160
Pre: time 2019-02-23 04:43:35.765913: 
 	std: 0.052500065
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5039667, 0.61095, 0.61086667, 0.6105, 0.50325]
	train_accs: [0.5039667, 0.61095, 0.61086667, 0.6105, 0.50325]
	best_train_sub_head: 1
	worst: 0.50325
	avg: 0.5679067
	best: 0.61095

Starting e_i: 523
Model ind 640 epoch 523 head A head_i_epoch 0 batch 0: avg loss -3.088130 avg loss no lamb -3.088130 time 2019-02-23 04:43:38.879585
Model ind 640 epoch 523 head A head_i_epoch 0 batch 100: avg loss -2.979489 avg loss no lamb -2.979489 time 2019-02-23 04:46:54.233189
Model ind 640 epoch 523 head A head_i_epoch 0 batch 200: avg loss -3.120693 avg loss no lamb -3.120693 time 2019-02-23 04:50:11.490880
last batch sz 160
Model ind 640 epoch 523 head B head_i_epoch 0 batch 0: avg loss -1.728376 avg loss no lamb -1.728376 time 2019-02-23 04:52:54.534772
Model ind 640 epoch 523 head B head_i_epoch 0 batch 100: avg loss -1.616164 avg loss no lamb -1.616164 time 2019-02-23 04:56:09.610444
Model ind 640 epoch 523 head B head_i_epoch 0 batch 200: avg loss -1.756555 avg loss no lamb -1.756555 time 2019-02-23 04:59:28.049479
last batch sz 160
Model ind 640 epoch 523 head B head_i_epoch 1 batch 0: avg loss -1.728951 avg loss no lamb -1.728951 time 2019-02-23 05:02:12.260924
Model ind 640 epoch 523 head B head_i_epoch 1 batch 100: avg loss -1.634215 avg loss no lamb -1.634215 time 2019-02-23 05:05:30.743142
Model ind 640 epoch 523 head B head_i_epoch 1 batch 200: avg loss -1.796913 avg loss no lamb -1.796913 time 2019-02-23 05:08:45.552986
last batch sz 160
Pre: time 2019-02-23 05:11:58.318646: 
 	std: 0.052862782
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50021666, 0.60791665, 0.6081167, 0.6081333, 0.5000833]
	train_accs: [0.50021666, 0.60791665, 0.6081167, 0.6081333, 0.5000833]
	best_train_sub_head: 3
	worst: 0.5000833
	avg: 0.56489336
	best: 0.6081333

Starting e_i: 524
Model ind 640 epoch 524 head A head_i_epoch 0 batch 0: avg loss -3.025944 avg loss no lamb -3.025944 time 2019-02-23 05:12:00.690605
Model ind 640 epoch 524 head A head_i_epoch 0 batch 100: avg loss -2.963065 avg loss no lamb -2.963065 time 2019-02-23 05:15:35.391043
Model ind 640 epoch 524 head A head_i_epoch 0 batch 200: avg loss -3.076432 avg loss no lamb -3.076432 time 2019-02-23 05:18:52.761643
last batch sz 160
Model ind 640 epoch 524 head B head_i_epoch 0 batch 0: avg loss -1.727730 avg loss no lamb -1.727730 time 2019-02-23 05:21:14.856966
Model ind 640 epoch 524 head B head_i_epoch 0 batch 100: avg loss -1.753231 avg loss no lamb -1.753231 time 2019-02-23 05:24:54.566144
Model ind 640 epoch 524 head B head_i_epoch 0 batch 200: avg loss -1.915579 avg loss no lamb -1.915579 time 2019-02-23 05:28:08.138499
last batch sz 160
Model ind 640 epoch 524 head B head_i_epoch 1 batch 0: avg loss -1.763947 avg loss no lamb -1.763947 time 2019-02-23 05:30:28.904753
Model ind 640 epoch 524 head B head_i_epoch 1 batch 100: avg loss -1.637264 avg loss no lamb -1.637264 time 2019-02-23 05:34:08.843901
Model ind 640 epoch 524 head B head_i_epoch 1 batch 200: avg loss -1.784693 avg loss no lamb -1.784693 time 2019-02-23 05:37:26.653450
last batch sz 160
Pre: time 2019-02-23 05:40:23.235131: 
 	std: 0.05193297
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5057333, 0.6113167, 0.61115, 0.6112, 0.5047]
	train_accs: [0.5057333, 0.6113167, 0.61115, 0.6112, 0.5047]
	best_train_sub_head: 1
	worst: 0.5047
	avg: 0.56881994
	best: 0.6113167

Starting e_i: 525
Model ind 640 epoch 525 head A head_i_epoch 0 batch 0: avg loss -3.040869 avg loss no lamb -3.040869 time 2019-02-23 05:40:26.552466
Model ind 640 epoch 525 head A head_i_epoch 0 batch 100: avg loss -3.039558 avg loss no lamb -3.039558 time 2019-02-23 05:43:57.506643
Model ind 640 epoch 525 head A head_i_epoch 0 batch 200: avg loss -3.044139 avg loss no lamb -3.044139 time 2019-02-23 05:47:33.284984
last batch sz 160
Model ind 640 epoch 525 head B head_i_epoch 0 batch 0: avg loss -1.745634 avg loss no lamb -1.745634 time 2019-02-23 05:49:49.821230
Model ind 640 epoch 525 head B head_i_epoch 0 batch 100: avg loss -1.610411 avg loss no lamb -1.610411 time 2019-02-23 05:53:11.600303
Model ind 640 epoch 525 head B head_i_epoch 0 batch 200: avg loss -1.845449 avg loss no lamb -1.845449 time 2019-02-23 05:56:55.438101
last batch sz 160
Model ind 640 epoch 525 head B head_i_epoch 1 batch 0: avg loss -1.762293 avg loss no lamb -1.762293 time 2019-02-23 05:59:13.410926
Model ind 640 epoch 525 head B head_i_epoch 1 batch 100: avg loss -1.646104 avg loss no lamb -1.646104 time 2019-02-23 06:02:33.233790
Model ind 640 epoch 525 head B head_i_epoch 1 batch 200: avg loss -1.809949 avg loss no lamb -1.809949 time 2019-02-23 06:06:11.783536
last batch sz 160
Pre: time 2019-02-23 06:09:12.747275: 
 	std: 0.05180218
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50325, 0.6084333, 0.6084167, 0.6088167, 0.50238335]
	train_accs: [0.50325, 0.6084333, 0.6084167, 0.6088167, 0.50238335]
	best_train_sub_head: 3
	worst: 0.50238335
	avg: 0.56626004
	best: 0.6088167

Starting e_i: 526
Model ind 640 epoch 526 head A head_i_epoch 0 batch 0: avg loss -2.959142 avg loss no lamb -2.959142 time 2019-02-23 06:09:15.688661
Model ind 640 epoch 526 head A head_i_epoch 0 batch 100: avg loss -3.021516 avg loss no lamb -3.021516 time 2019-02-23 06:12:35.079596
Model ind 640 epoch 526 head A head_i_epoch 0 batch 200: avg loss -3.069342 avg loss no lamb -3.069342 time 2019-02-23 06:15:55.159923
last batch sz 160
Model ind 640 epoch 526 head B head_i_epoch 0 batch 0: avg loss -1.694453 avg loss no lamb -1.694453 time 2019-02-23 06:18:39.628506
Model ind 640 epoch 526 head B head_i_epoch 0 batch 100: avg loss -1.682035 avg loss no lamb -1.682035 time 2019-02-23 06:21:56.638147
Model ind 640 epoch 526 head B head_i_epoch 0 batch 200: avg loss -1.757416 avg loss no lamb -1.757416 time 2019-02-23 06:25:15.913965
last batch sz 160
Model ind 640 epoch 526 head B head_i_epoch 1 batch 0: avg loss -1.817364 avg loss no lamb -1.817364 time 2019-02-23 06:28:00.011756
Model ind 640 epoch 526 head B head_i_epoch 1 batch 100: avg loss -1.577605 avg loss no lamb -1.577605 time 2019-02-23 06:31:14.901639
Model ind 640 epoch 526 head B head_i_epoch 1 batch 200: avg loss -1.809268 avg loss no lamb -1.809268 time 2019-02-23 06:34:35.250180
last batch sz 160
Pre: time 2019-02-23 06:37:49.930407: 
 	std: 0.049754795
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50886667, 0.61015, 0.61018336, 0.6103, 0.50843334]
	train_accs: [0.50886667, 0.61015, 0.61018336, 0.6103, 0.50843334]
	best_train_sub_head: 3
	worst: 0.50843334
	avg: 0.56958663
	best: 0.6103

Starting e_i: 527
Model ind 640 epoch 527 head A head_i_epoch 0 batch 0: avg loss -3.078355 avg loss no lamb -3.078355 time 2019-02-23 06:37:52.236789
Model ind 640 epoch 527 head A head_i_epoch 0 batch 100: avg loss -3.008241 avg loss no lamb -3.008241 time 2019-02-23 06:41:20.146250
Model ind 640 epoch 527 head A head_i_epoch 0 batch 200: avg loss -3.110145 avg loss no lamb -3.110145 time 2019-02-23 06:44:39.733599
last batch sz 160
Model ind 640 epoch 527 head B head_i_epoch 0 batch 0: avg loss -1.823194 avg loss no lamb -1.823194 time 2019-02-23 06:47:08.976011
Model ind 640 epoch 527 head B head_i_epoch 0 batch 100: avg loss -1.643599 avg loss no lamb -1.643599 time 2019-02-23 06:50:43.661324
Model ind 640 epoch 527 head B head_i_epoch 0 batch 200: avg loss -1.728861 avg loss no lamb -1.728861 time 2019-02-23 06:54:03.650089
last batch sz 160
Model ind 640 epoch 527 head B head_i_epoch 1 batch 0: avg loss -1.742879 avg loss no lamb -1.742879 time 2019-02-23 06:56:22.510093
Model ind 640 epoch 527 head B head_i_epoch 1 batch 100: avg loss -1.753517 avg loss no lamb -1.753517 time 2019-02-23 07:00:07.179279
Model ind 640 epoch 527 head B head_i_epoch 1 batch 200: avg loss -1.740087 avg loss no lamb -1.740087 time 2019-02-23 07:03:23.295017
last batch sz 160
Pre: time 2019-02-23 07:06:17.068217: 
 	std: 0.052214134
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50186664, 0.6088167, 0.6087, 0.6089, 0.5025833]
	train_accs: [0.50186664, 0.6088167, 0.6087, 0.6089, 0.5025833]
	best_train_sub_head: 3
	worst: 0.50186664
	avg: 0.5661733
	best: 0.6089

Starting e_i: 528
Model ind 640 epoch 528 head A head_i_epoch 0 batch 0: avg loss -3.068470 avg loss no lamb -3.068470 time 2019-02-23 07:06:20.538036
Model ind 640 epoch 528 head A head_i_epoch 0 batch 100: avg loss -2.976724 avg loss no lamb -2.976724 time 2019-02-23 07:09:47.393916
Model ind 640 epoch 528 head A head_i_epoch 0 batch 200: avg loss -3.049301 avg loss no lamb -3.049301 time 2019-02-23 07:13:21.430667
last batch sz 160
Model ind 640 epoch 528 head B head_i_epoch 0 batch 0: avg loss -1.752180 avg loss no lamb -1.752180 time 2019-02-23 07:15:37.974016
Model ind 640 epoch 528 head B head_i_epoch 0 batch 100: avg loss -1.543933 avg loss no lamb -1.543933 time 2019-02-23 07:19:01.190492
Model ind 640 epoch 528 head B head_i_epoch 0 batch 200: avg loss -1.881633 avg loss no lamb -1.881633 time 2019-02-23 07:22:43.131983
last batch sz 160
Model ind 640 epoch 528 head B head_i_epoch 1 batch 0: avg loss -1.647416 avg loss no lamb -1.647416 time 2019-02-23 07:24:59.650391
Model ind 640 epoch 528 head B head_i_epoch 1 batch 100: avg loss -1.656226 avg loss no lamb -1.656226 time 2019-02-23 07:28:19.794791
Model ind 640 epoch 528 head B head_i_epoch 1 batch 200: avg loss -1.821546 avg loss no lamb -1.821546 time 2019-02-23 07:32:02.445214
last batch sz 160
Pre: time 2019-02-23 07:34:54.547065: 
 	std: 0.051650424
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50495, 0.61035, 0.6105833, 0.6106833, 0.50526667]
	train_accs: [0.50495, 0.61035, 0.6105833, 0.6106833, 0.50526667]
	best_train_sub_head: 3
	worst: 0.50495
	avg: 0.56836665
	best: 0.6106833

Starting e_i: 529
Model ind 640 epoch 529 head A head_i_epoch 0 batch 0: avg loss -3.039486 avg loss no lamb -3.039486 time 2019-02-23 07:34:57.618530
Model ind 640 epoch 529 head A head_i_epoch 0 batch 100: avg loss -3.077607 avg loss no lamb -3.077607 time 2019-02-23 07:38:14.901748
Model ind 640 epoch 529 head A head_i_epoch 0 batch 200: avg loss -3.124141 avg loss no lamb -3.124141 time 2019-02-23 07:41:39.347015
last batch sz 160
Model ind 640 epoch 529 head B head_i_epoch 0 batch 0: avg loss -1.690099 avg loss no lamb -1.690099 time 2019-02-23 07:44:21.416310
Model ind 640 epoch 529 head B head_i_epoch 0 batch 100: avg loss -1.667432 avg loss no lamb -1.667432 time 2019-02-23 07:47:41.570343
Model ind 640 epoch 529 head B head_i_epoch 0 batch 200: avg loss -1.804144 avg loss no lamb -1.804144 time 2019-02-23 07:50:56.034789
last batch sz 160
Model ind 640 epoch 529 head B head_i_epoch 1 batch 0: avg loss -1.675733 avg loss no lamb -1.675733 time 2019-02-23 07:53:40.467670
Model ind 640 epoch 529 head B head_i_epoch 1 batch 100: avg loss -1.668912 avg loss no lamb -1.668912 time 2019-02-23 07:56:58.371872
Model ind 640 epoch 529 head B head_i_epoch 1 batch 200: avg loss -1.833533 avg loss no lamb -1.833533 time 2019-02-23 08:00:14.433919
last batch sz 160
Pre: time 2019-02-23 08:03:36.540083: 
 	std: 0.05136198
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5029167, 0.60755, 0.60746664, 0.6074833, 0.5024]
	train_accs: [0.5029167, 0.60755, 0.60746664, 0.6074833, 0.5024]
	best_train_sub_head: 1
	worst: 0.5024
	avg: 0.5655633
	best: 0.60755

Starting e_i: 530
Model ind 640 epoch 530 head A head_i_epoch 0 batch 0: avg loss -3.036044 avg loss no lamb -3.036044 time 2019-02-23 08:03:39.585751
Model ind 640 epoch 530 head A head_i_epoch 0 batch 100: avg loss -2.990945 avg loss no lamb -2.990945 time 2019-02-23 08:07:01.256922
Model ind 640 epoch 530 head A head_i_epoch 0 batch 200: avg loss -2.986522 avg loss no lamb -2.986522 time 2019-02-23 08:10:19.239155
last batch sz 160
Model ind 640 epoch 530 head B head_i_epoch 0 batch 0: avg loss -1.717379 avg loss no lamb -1.717379 time 2019-02-23 08:12:53.864312
Model ind 640 epoch 530 head B head_i_epoch 0 batch 100: avg loss -1.606972 avg loss no lamb -1.606972 time 2019-02-23 08:16:20.909013
Model ind 640 epoch 530 head B head_i_epoch 0 batch 200: avg loss -1.795635 avg loss no lamb -1.795635 time 2019-02-23 08:19:39.834792
last batch sz 160
Model ind 640 epoch 530 head B head_i_epoch 1 batch 0: avg loss -1.687709 avg loss no lamb -1.687709 time 2019-02-23 08:22:09.733072
Model ind 640 epoch 530 head B head_i_epoch 1 batch 100: avg loss -1.531723 avg loss no lamb -1.531723 time 2019-02-23 08:25:42.664244
Model ind 640 epoch 530 head B head_i_epoch 1 batch 200: avg loss -1.811815 avg loss no lamb -1.811815 time 2019-02-23 08:29:02.005368
last batch sz 160
Pre: time 2019-02-23 08:32:01.583151: 
 	std: 0.05382905
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50163335, 0.61128336, 0.61158335, 0.6116667, 0.50163335]
	train_accs: [0.50163335, 0.61128336, 0.61158335, 0.6116667, 0.50163335]
	best_train_sub_head: 3
	worst: 0.50163335
	avg: 0.5675601
	best: 0.6116667

Starting e_i: 531
Model ind 640 epoch 531 head A head_i_epoch 0 batch 0: avg loss -3.112776 avg loss no lamb -3.112776 time 2019-02-23 08:32:08.890166
Model ind 640 epoch 531 head A head_i_epoch 0 batch 100: avg loss -3.001981 avg loss no lamb -3.001981 time 2019-02-23 08:35:39.171111
Model ind 640 epoch 531 head A head_i_epoch 0 batch 200: avg loss -3.033883 avg loss no lamb -3.033883 time 2019-02-23 08:39:08.115460
last batch sz 160
Model ind 640 epoch 531 head B head_i_epoch 0 batch 0: avg loss -1.695363 avg loss no lamb -1.695363 time 2019-02-23 08:41:27.371769
Model ind 640 epoch 531 head B head_i_epoch 0 batch 100: avg loss -1.697942 avg loss no lamb -1.697942 time 2019-02-23 08:44:53.425256
Model ind 640 epoch 531 head B head_i_epoch 0 batch 200: avg loss -1.801927 avg loss no lamb -1.801927 time 2019-02-23 08:48:32.081407
last batch sz 160
Model ind 640 epoch 531 head B head_i_epoch 1 batch 0: avg loss -1.709549 avg loss no lamb -1.709549 time 2019-02-23 08:50:47.022804
Model ind 640 epoch 531 head B head_i_epoch 1 batch 100: avg loss -1.591030 avg loss no lamb -1.591030 time 2019-02-23 08:54:07.653260
Model ind 640 epoch 531 head B head_i_epoch 1 batch 200: avg loss -1.707274 avg loss no lamb -1.707274 time 2019-02-23 08:57:53.295481
last batch sz 160
Pre: time 2019-02-23 09:00:48.290911: 
 	std: 0.051076684
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5072, 0.61086667, 0.61116666, 0.61111665, 0.50638336]
	train_accs: [0.5072, 0.61086667, 0.61116666, 0.61111665, 0.50638336]
	best_train_sub_head: 2
	worst: 0.50638336
	avg: 0.56934667
	best: 0.61116666

Starting e_i: 532
Model ind 640 epoch 532 head A head_i_epoch 0 batch 0: avg loss -3.016087 avg loss no lamb -3.016087 time 2019-02-23 09:00:51.014631
Model ind 640 epoch 532 head A head_i_epoch 0 batch 100: avg loss -2.999095 avg loss no lamb -2.999095 time 2019-02-23 09:04:07.504911
Model ind 640 epoch 532 head A head_i_epoch 0 batch 200: avg loss -3.081657 avg loss no lamb -3.081657 time 2019-02-23 09:07:32.829473
last batch sz 160
Model ind 640 epoch 532 head B head_i_epoch 0 batch 0: avg loss -1.625442 avg loss no lamb -1.625442 time 2019-02-23 09:10:12.770018
Model ind 640 epoch 532 head B head_i_epoch 0 batch 100: avg loss -1.686985 avg loss no lamb -1.686985 time 2019-02-23 09:13:28.397955
Model ind 640 epoch 532 head B head_i_epoch 0 batch 200: avg loss -1.799255 avg loss no lamb -1.799255 time 2019-02-23 09:16:47.818656
last batch sz 160
Model ind 640 epoch 532 head B head_i_epoch 1 batch 0: avg loss -1.690355 avg loss no lamb -1.690355 time 2019-02-23 09:19:32.720242
Model ind 640 epoch 532 head B head_i_epoch 1 batch 100: avg loss -1.633383 avg loss no lamb -1.633383 time 2019-02-23 09:22:48.753398
Model ind 640 epoch 532 head B head_i_epoch 1 batch 200: avg loss -1.806760 avg loss no lamb -1.806760 time 2019-02-23 09:26:01.380988
last batch sz 160
Pre: time 2019-02-23 09:29:23.450236: 
 	std: 0.0522123
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5036333, 0.6092833, 0.61011666, 0.6102, 0.50295]
	train_accs: [0.5036333, 0.6092833, 0.61011666, 0.6102, 0.50295]
	best_train_sub_head: 3
	worst: 0.50295
	avg: 0.56723666
	best: 0.6102

Starting e_i: 533
Model ind 640 epoch 533 head A head_i_epoch 0 batch 0: avg loss -3.024889 avg loss no lamb -3.024889 time 2019-02-23 09:29:26.357984
Model ind 640 epoch 533 head A head_i_epoch 0 batch 100: avg loss -2.958046 avg loss no lamb -2.958046 time 2019-02-23 09:32:46.766513
Model ind 640 epoch 533 head A head_i_epoch 0 batch 200: avg loss -3.040622 avg loss no lamb -3.040622 time 2019-02-23 09:36:04.798631
last batch sz 160
Model ind 640 epoch 533 head B head_i_epoch 0 batch 0: avg loss -1.749971 avg loss no lamb -1.749971 time 2019-02-23 09:38:48.570520
Model ind 640 epoch 533 head B head_i_epoch 0 batch 100: avg loss -1.652671 avg loss no lamb -1.652671 time 2019-02-23 09:42:10.952564
Model ind 640 epoch 533 head B head_i_epoch 0 batch 200: avg loss -1.809765 avg loss no lamb -1.809765 time 2019-02-23 09:45:28.034929
last batch sz 160
Model ind 640 epoch 533 head B head_i_epoch 1 batch 0: avg loss -1.695663 avg loss no lamb -1.695663 time 2019-02-23 09:48:01.225756
Model ind 640 epoch 533 head B head_i_epoch 1 batch 100: avg loss -1.633904 avg loss no lamb -1.633904 time 2019-02-23 09:51:30.446531
Model ind 640 epoch 533 head B head_i_epoch 1 batch 200: avg loss -1.812457 avg loss no lamb -1.812457 time 2019-02-23 09:54:47.348273
last batch sz 160
Pre: time 2019-02-23 09:57:43.979882: 
 	std: 0.051921595
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5042667, 0.6098, 0.61018336, 0.6105667, 0.50413334]
	train_accs: [0.5042667, 0.6098, 0.61018336, 0.6105667, 0.50413334]
	best_train_sub_head: 3
	worst: 0.50413334
	avg: 0.56779003
	best: 0.6105667

Starting e_i: 534
Model ind 640 epoch 534 head A head_i_epoch 0 batch 0: avg loss -3.085472 avg loss no lamb -3.085472 time 2019-02-23 09:57:48.495138
Model ind 640 epoch 534 head A head_i_epoch 0 batch 100: avg loss -3.037703 avg loss no lamb -3.037703 time 2019-02-23 10:01:31.842704
Model ind 640 epoch 534 head A head_i_epoch 0 batch 200: avg loss -3.071824 avg loss no lamb -3.071824 time 2019-02-23 10:04:53.985387
last batch sz 160
Model ind 640 epoch 534 head B head_i_epoch 0 batch 0: avg loss -1.707332 avg loss no lamb -1.707332 time 2019-02-23 10:07:12.446479
Model ind 640 epoch 534 head B head_i_epoch 0 batch 100: avg loss -1.581405 avg loss no lamb -1.581405 time 2019-02-23 10:10:46.138431
Model ind 640 epoch 534 head B head_i_epoch 0 batch 200: avg loss -1.741736 avg loss no lamb -1.741736 time 2019-02-23 10:14:19.425277
last batch sz 160
Model ind 640 epoch 534 head B head_i_epoch 1 batch 0: avg loss -1.741062 avg loss no lamb -1.741062 time 2019-02-23 10:16:34.800156
Model ind 640 epoch 534 head B head_i_epoch 1 batch 100: avg loss -1.610389 avg loss no lamb -1.610389 time 2019-02-23 10:20:02.096197
Model ind 640 epoch 534 head B head_i_epoch 1 batch 200: avg loss -1.848397 avg loss no lamb -1.848397 time 2019-02-23 10:23:35.852502
last batch sz 160
Pre: time 2019-02-23 10:26:33.394495: 
 	std: 0.052703876
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50273335, 0.6099333, 0.6099667, 0.6102167, 0.5021833]
	train_accs: [0.50273335, 0.6099333, 0.6099667, 0.6102167, 0.5021833]
	best_train_sub_head: 3
	worst: 0.5021833
	avg: 0.5670067
	best: 0.6102167

Starting e_i: 535
Model ind 640 epoch 535 head A head_i_epoch 0 batch 0: avg loss -2.998171 avg loss no lamb -2.998171 time 2019-02-23 10:26:36.437809
Model ind 640 epoch 535 head A head_i_epoch 0 batch 100: avg loss -2.956275 avg loss no lamb -2.956275 time 2019-02-23 10:29:53.874924
Model ind 640 epoch 535 head A head_i_epoch 0 batch 200: avg loss -3.048203 avg loss no lamb -3.048203 time 2019-02-23 10:33:27.503867
last batch sz 160
Model ind 640 epoch 535 head B head_i_epoch 0 batch 0: avg loss -1.708397 avg loss no lamb -1.708397 time 2019-02-23 10:36:00.906436
Model ind 640 epoch 535 head B head_i_epoch 0 batch 100: avg loss -1.659173 avg loss no lamb -1.659173 time 2019-02-23 10:39:19.596270
Model ind 640 epoch 535 head B head_i_epoch 0 batch 200: avg loss -1.857200 avg loss no lamb -1.857200 time 2019-02-23 10:42:40.993671
last batch sz 160
Model ind 640 epoch 535 head B head_i_epoch 1 batch 0: avg loss -1.758015 avg loss no lamb -1.758015 time 2019-02-23 10:45:18.684845
Model ind 640 epoch 535 head B head_i_epoch 1 batch 100: avg loss -1.704709 avg loss no lamb -1.704709 time 2019-02-23 10:48:34.466580
Model ind 640 epoch 535 head B head_i_epoch 1 batch 200: avg loss -1.832458 avg loss no lamb -1.832458 time 2019-02-23 10:51:53.698398
last batch sz 160
Pre: time 2019-02-23 10:53:36.520295: 
 	std: 0.051347286
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50261664, 0.6070667, 0.6071333, 0.60768336, 0.50235]
	train_accs: [0.50261664, 0.6070667, 0.6071333, 0.60768336, 0.50235]
	best_train_sub_head: 3
	worst: 0.50235
	avg: 0.56536996
	best: 0.60768336

Starting e_i: 536
Model ind 640 epoch 536 head A head_i_epoch 0 batch 0: avg loss -3.131782 avg loss no lamb -3.131782 time 2019-02-23 10:53:38.785758
Model ind 640 epoch 536 head A head_i_epoch 0 batch 100: avg loss -2.926602 avg loss no lamb -2.926602 time 2019-02-23 10:55:30.027542
Model ind 640 epoch 536 head A head_i_epoch 0 batch 200: avg loss -3.111163 avg loss no lamb -3.111163 time 2019-02-23 10:57:21.409646
last batch sz 160
Model ind 640 epoch 536 head B head_i_epoch 0 batch 0: avg loss -1.786423 avg loss no lamb -1.786423 time 2019-02-23 10:58:41.994669
Model ind 640 epoch 536 head B head_i_epoch 0 batch 100: avg loss -1.666428 avg loss no lamb -1.666428 time 2019-02-23 11:00:33.119867
Model ind 640 epoch 536 head B head_i_epoch 0 batch 200: avg loss -1.822116 avg loss no lamb -1.822116 time 2019-02-23 11:02:24.334622
last batch sz 160
Model ind 640 epoch 536 head B head_i_epoch 1 batch 0: avg loss -1.691909 avg loss no lamb -1.691909 time 2019-02-23 11:03:45.606923
Model ind 640 epoch 536 head B head_i_epoch 1 batch 100: avg loss -1.673878 avg loss no lamb -1.673878 time 2019-02-23 11:05:37.937808
Model ind 640 epoch 536 head B head_i_epoch 1 batch 200: avg loss -1.735823 avg loss no lamb -1.735823 time 2019-02-23 11:07:29.040647
last batch sz 160
Pre: time 2019-02-23 11:09:11.534649: 
 	std: 0.053794283
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50226665, 0.61215, 0.61198336, 0.6127333, 0.5027]
	train_accs: [0.50226665, 0.61215, 0.61198336, 0.6127333, 0.5027]
	best_train_sub_head: 3
	worst: 0.50226665
	avg: 0.56836665
	best: 0.6127333

Starting e_i: 537
Model ind 640 epoch 537 head A head_i_epoch 0 batch 0: avg loss -3.048832 avg loss no lamb -3.048832 time 2019-02-23 11:09:13.907334
Model ind 640 epoch 537 head A head_i_epoch 0 batch 100: avg loss -2.901137 avg loss no lamb -2.901137 time 2019-02-23 11:11:07.227058
Model ind 640 epoch 537 head A head_i_epoch 0 batch 200: avg loss -3.050326 avg loss no lamb -3.050326 time 2019-02-23 11:13:00.856875
last batch sz 160
Model ind 640 epoch 537 head B head_i_epoch 0 batch 0: avg loss -1.730935 avg loss no lamb -1.730935 time 2019-02-23 11:14:21.021272
Model ind 640 epoch 537 head B head_i_epoch 0 batch 100: avg loss -1.650550 avg loss no lamb -1.650550 time 2019-02-23 11:16:15.020195
Model ind 640 epoch 537 head B head_i_epoch 0 batch 200: avg loss -1.762369 avg loss no lamb -1.762369 time 2019-02-23 11:18:06.883002
last batch sz 160
Model ind 640 epoch 537 head B head_i_epoch 1 batch 0: avg loss -1.746409 avg loss no lamb -1.746409 time 2019-02-23 11:19:27.615340
Model ind 640 epoch 537 head B head_i_epoch 1 batch 100: avg loss -1.612453 avg loss no lamb -1.612453 time 2019-02-23 11:21:18.876946
Model ind 640 epoch 537 head B head_i_epoch 1 batch 200: avg loss -1.786125 avg loss no lamb -1.786125 time 2019-02-23 11:23:09.726127
last batch sz 160
Pre: time 2019-02-23 11:24:51.732650: 
 	std: 0.052240368
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5036167, 0.61076665, 0.6102, 0.61083335, 0.5043167]
	train_accs: [0.5036167, 0.61076665, 0.6102, 0.61083335, 0.5043167]
	best_train_sub_head: 3
	worst: 0.5036167
	avg: 0.56794673
	best: 0.61083335

Starting e_i: 538
Model ind 640 epoch 538 head A head_i_epoch 0 batch 0: avg loss -3.033556 avg loss no lamb -3.033556 time 2019-02-23 11:24:53.928114
Model ind 640 epoch 538 head A head_i_epoch 0 batch 100: avg loss -2.949341 avg loss no lamb -2.949341 time 2019-02-23 11:26:45.911886
Model ind 640 epoch 538 head A head_i_epoch 0 batch 200: avg loss -3.093662 avg loss no lamb -3.093662 time 2019-02-23 11:28:38.139242
last batch sz 160
Model ind 640 epoch 538 head B head_i_epoch 0 batch 0: avg loss -1.777995 avg loss no lamb -1.777995 time 2019-02-23 11:29:59.026591
Model ind 640 epoch 538 head B head_i_epoch 0 batch 100: avg loss -1.591388 avg loss no lamb -1.591388 time 2019-02-23 11:31:49.974415
Model ind 640 epoch 538 head B head_i_epoch 0 batch 200: avg loss -1.761723 avg loss no lamb -1.761723 time 2019-02-23 11:33:41.844972
last batch sz 160
Model ind 640 epoch 538 head B head_i_epoch 1 batch 0: avg loss -1.689607 avg loss no lamb -1.689607 time 2019-02-23 11:35:03.128202
Model ind 640 epoch 538 head B head_i_epoch 1 batch 100: avg loss -1.637964 avg loss no lamb -1.637964 time 2019-02-23 11:36:54.521514
Model ind 640 epoch 538 head B head_i_epoch 1 batch 200: avg loss -1.818148 avg loss no lamb -1.818148 time 2019-02-23 11:38:47.665086
last batch sz 160
Pre: time 2019-02-23 11:40:30.616938: 
 	std: 0.052513435
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50168335, 0.60978335, 0.6088833, 0.6095, 0.50271666]
	train_accs: [0.50168335, 0.60978335, 0.6088833, 0.6095, 0.50271666]
	best_train_sub_head: 1
	worst: 0.50168335
	avg: 0.56651336
	best: 0.60978335

Starting e_i: 539
Model ind 640 epoch 539 head A head_i_epoch 0 batch 0: avg loss -3.027690 avg loss no lamb -3.027690 time 2019-02-23 11:40:32.802763
Model ind 640 epoch 539 head A head_i_epoch 0 batch 100: avg loss -3.017745 avg loss no lamb -3.017745 time 2019-02-23 11:42:24.071274
Model ind 640 epoch 539 head A head_i_epoch 0 batch 200: avg loss -3.045052 avg loss no lamb -3.045052 time 2019-02-23 11:44:16.027338
last batch sz 160
Model ind 640 epoch 539 head B head_i_epoch 0 batch 0: avg loss -1.718784 avg loss no lamb -1.718784 time 2019-02-23 11:45:37.642727
Model ind 640 epoch 539 head B head_i_epoch 0 batch 100: avg loss -1.682726 avg loss no lamb -1.682726 time 2019-02-23 11:47:29.890444
Model ind 640 epoch 539 head B head_i_epoch 0 batch 200: avg loss -1.845528 avg loss no lamb -1.845528 time 2019-02-23 11:49:21.161489
last batch sz 160
Model ind 640 epoch 539 head B head_i_epoch 1 batch 0: avg loss -1.789153 avg loss no lamb -1.789153 time 2019-02-23 11:50:43.031292
Model ind 640 epoch 539 head B head_i_epoch 1 batch 100: avg loss -1.651810 avg loss no lamb -1.651810 time 2019-02-23 11:52:34.964344
Model ind 640 epoch 539 head B head_i_epoch 1 batch 200: avg loss -1.738668 avg loss no lamb -1.738668 time 2019-02-23 11:54:27.245248
last batch sz 160
Pre: time 2019-02-23 11:56:09.710747: 
 	std: 0.053916205
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.49701667, 0.60685, 0.60685, 0.60681665, 0.49655]
	train_accs: [0.49701667, 0.60685, 0.60685, 0.60681665, 0.49655]
	best_train_sub_head: 1
	worst: 0.49655
	avg: 0.56281674
	best: 0.60685

Starting e_i: 540
Model ind 640 epoch 540 head A head_i_epoch 0 batch 0: avg loss -3.053721 avg loss no lamb -3.053721 time 2019-02-23 11:56:11.938744
Model ind 640 epoch 540 head A head_i_epoch 0 batch 100: avg loss -2.971981 avg loss no lamb -2.971981 time 2019-02-23 11:58:03.999839
Model ind 640 epoch 540 head A head_i_epoch 0 batch 200: avg loss -3.098108 avg loss no lamb -3.098108 time 2019-02-23 11:59:56.802510
last batch sz 160
Model ind 640 epoch 540 head B head_i_epoch 0 batch 0: avg loss -1.743877 avg loss no lamb -1.743877 time 2019-02-23 12:01:19.313820
Model ind 640 epoch 540 head B head_i_epoch 0 batch 100: avg loss -1.617295 avg loss no lamb -1.617295 time 2019-02-23 12:03:11.431620
Model ind 640 epoch 540 head B head_i_epoch 0 batch 200: avg loss -1.792728 avg loss no lamb -1.792728 time 2019-02-23 12:05:03.283946
last batch sz 160
Model ind 640 epoch 540 head B head_i_epoch 1 batch 0: avg loss -1.766969 avg loss no lamb -1.766969 time 2019-02-23 12:06:24.052662
Model ind 640 epoch 540 head B head_i_epoch 1 batch 100: avg loss -1.639150 avg loss no lamb -1.639150 time 2019-02-23 12:08:15.710458
Model ind 640 epoch 540 head B head_i_epoch 1 batch 200: avg loss -1.833914 avg loss no lamb -1.833914 time 2019-02-23 12:10:10.180157
last batch sz 160
Pre: time 2019-02-23 12:11:52.360751: 
 	std: 0.05354112
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50233334, 0.61123335, 0.6110333, 0.61145, 0.50156665]
	train_accs: [0.50233334, 0.61123335, 0.6110333, 0.61145, 0.50156665]
	best_train_sub_head: 3
	worst: 0.50156665
	avg: 0.56752336
	best: 0.61145

Starting e_i: 541
Model ind 640 epoch 541 head A head_i_epoch 0 batch 0: avg loss -3.019166 avg loss no lamb -3.019166 time 2019-02-23 12:11:57.655840
Model ind 640 epoch 541 head A head_i_epoch 0 batch 100: avg loss -2.988910 avg loss no lamb -2.988910 time 2019-02-23 12:13:53.165313
Model ind 640 epoch 541 head A head_i_epoch 0 batch 200: avg loss -3.142859 avg loss no lamb -3.142859 time 2019-02-23 12:15:45.211207
last batch sz 160
Model ind 640 epoch 541 head B head_i_epoch 0 batch 0: avg loss -1.785942 avg loss no lamb -1.785942 time 2019-02-23 12:17:06.664206
Model ind 640 epoch 541 head B head_i_epoch 0 batch 100: avg loss -1.676247 avg loss no lamb -1.676247 time 2019-02-23 12:18:58.296936
Model ind 640 epoch 541 head B head_i_epoch 0 batch 200: avg loss -1.764581 avg loss no lamb -1.764581 time 2019-02-23 12:20:49.475878
last batch sz 160
Model ind 640 epoch 541 head B head_i_epoch 1 batch 0: avg loss -1.742102 avg loss no lamb -1.742102 time 2019-02-23 12:22:10.718824
Model ind 640 epoch 541 head B head_i_epoch 1 batch 100: avg loss -1.659792 avg loss no lamb -1.659792 time 2019-02-23 12:23:58.183886
Model ind 640 epoch 541 head B head_i_epoch 1 batch 200: avg loss -1.726470 avg loss no lamb -1.726470 time 2019-02-23 12:25:46.015227
last batch sz 160
Pre: time 2019-02-23 12:27:24.131161: 
 	std: 0.052369263
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50208336, 0.6084833, 0.60863334, 0.6088, 0.5014]
	train_accs: [0.50208336, 0.6084833, 0.60863334, 0.6088, 0.5014]
	best_train_sub_head: 3
	worst: 0.5014
	avg: 0.56587994
	best: 0.6088

Starting e_i: 542
Model ind 640 epoch 542 head A head_i_epoch 0 batch 0: avg loss -3.004427 avg loss no lamb -3.004427 time 2019-02-23 12:27:26.111649
Model ind 640 epoch 542 head A head_i_epoch 0 batch 100: avg loss -3.076630 avg loss no lamb -3.076630 time 2019-02-23 12:29:14.517403
Model ind 640 epoch 542 head A head_i_epoch 0 batch 200: avg loss -3.162605 avg loss no lamb -3.162605 time 2019-02-23 12:31:03.878841
last batch sz 160
Model ind 640 epoch 542 head B head_i_epoch 0 batch 0: avg loss -1.753351 avg loss no lamb -1.753351 time 2019-02-23 12:32:22.914662
Model ind 640 epoch 542 head B head_i_epoch 0 batch 100: avg loss -1.573060 avg loss no lamb -1.573060 time 2019-02-23 12:34:11.736641
Model ind 640 epoch 542 head B head_i_epoch 0 batch 200: avg loss -1.841960 avg loss no lamb -1.841960 time 2019-02-23 12:35:59.799169
last batch sz 160
Model ind 640 epoch 542 head B head_i_epoch 1 batch 0: avg loss -1.714324 avg loss no lamb -1.714324 time 2019-02-23 12:37:19.325558
Model ind 640 epoch 542 head B head_i_epoch 1 batch 100: avg loss -1.628557 avg loss no lamb -1.628557 time 2019-02-23 12:39:09.077902
Model ind 640 epoch 542 head B head_i_epoch 1 batch 200: avg loss -1.812891 avg loss no lamb -1.812891 time 2019-02-23 12:40:58.436353
last batch sz 160
Pre: time 2019-02-23 12:42:38.461385: 
 	std: 0.052767362
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50658333, 0.6139333, 0.6134667, 0.61395, 0.50556666]
	train_accs: [0.50658333, 0.6139333, 0.6134667, 0.61395, 0.50556666]
	best_train_sub_head: 3
	worst: 0.50556666
	avg: 0.5707
	best: 0.61395

Starting e_i: 543
Model ind 640 epoch 543 head A head_i_epoch 0 batch 0: avg loss -3.022104 avg loss no lamb -3.022104 time 2019-02-23 12:42:44.442141
Model ind 640 epoch 543 head A head_i_epoch 0 batch 100: avg loss -3.032106 avg loss no lamb -3.032106 time 2019-02-23 12:44:33.272311
Model ind 640 epoch 543 head A head_i_epoch 0 batch 200: avg loss -3.139691 avg loss no lamb -3.139691 time 2019-02-23 12:46:21.772145
last batch sz 160
Model ind 640 epoch 543 head B head_i_epoch 0 batch 0: avg loss -1.767704 avg loss no lamb -1.767704 time 2019-02-23 12:47:41.646349
Model ind 640 epoch 543 head B head_i_epoch 0 batch 100: avg loss -1.612582 avg loss no lamb -1.612582 time 2019-02-23 12:49:29.976798
Model ind 640 epoch 543 head B head_i_epoch 0 batch 200: avg loss -1.832487 avg loss no lamb -1.832487 time 2019-02-23 12:51:18.555249
last batch sz 160
Model ind 640 epoch 543 head B head_i_epoch 1 batch 0: avg loss -1.644967 avg loss no lamb -1.644967 time 2019-02-23 12:52:37.165804
Model ind 640 epoch 543 head B head_i_epoch 1 batch 100: avg loss -1.663770 avg loss no lamb -1.663770 time 2019-02-23 12:54:25.222365
Model ind 640 epoch 543 head B head_i_epoch 1 batch 200: avg loss -1.770715 avg loss no lamb -1.770715 time 2019-02-23 12:56:13.641484
last batch sz 160
Pre: time 2019-02-23 12:57:52.997695: 
 	std: 0.05240786
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5043333, 0.61071664, 0.61088336, 0.6108, 0.50331664]
	train_accs: [0.5043333, 0.61071664, 0.61088336, 0.6108, 0.50331664]
	best_train_sub_head: 2
	worst: 0.50331664
	avg: 0.56801
	best: 0.61088336

Starting e_i: 544
Model ind 640 epoch 544 head A head_i_epoch 0 batch 0: avg loss -3.043830 avg loss no lamb -3.043830 time 2019-02-23 12:57:55.083349
Model ind 640 epoch 544 head A head_i_epoch 0 batch 100: avg loss -2.970831 avg loss no lamb -2.970831 time 2019-02-23 12:59:44.369787
Model ind 640 epoch 544 head A head_i_epoch 0 batch 200: avg loss -3.119944 avg loss no lamb -3.119944 time 2019-02-23 13:01:33.798194
last batch sz 160
Model ind 640 epoch 544 head B head_i_epoch 0 batch 0: avg loss -1.737480 avg loss no lamb -1.737480 time 2019-02-23 13:02:53.832043
Model ind 640 epoch 544 head B head_i_epoch 0 batch 100: avg loss -1.663025 avg loss no lamb -1.663025 time 2019-02-23 13:04:42.935928
Model ind 640 epoch 544 head B head_i_epoch 0 batch 200: avg loss -1.849975 avg loss no lamb -1.849975 time 2019-02-23 13:06:31.270016
last batch sz 160
Model ind 640 epoch 544 head B head_i_epoch 1 batch 0: avg loss -1.684096 avg loss no lamb -1.684096 time 2019-02-23 13:07:49.983268
Model ind 640 epoch 544 head B head_i_epoch 1 batch 100: avg loss -1.629734 avg loss no lamb -1.629734 time 2019-02-23 13:09:38.594015
Model ind 640 epoch 544 head B head_i_epoch 1 batch 200: avg loss -1.790402 avg loss no lamb -1.790402 time 2019-02-23 13:11:27.236712
last batch sz 160
Pre: time 2019-02-23 13:13:06.463736: 
 	std: 0.05062591
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5075833, 0.61071664, 0.6105, 0.61065, 0.50698334]
	train_accs: [0.5075833, 0.61071664, 0.6105, 0.61065, 0.50698334]
	best_train_sub_head: 1
	worst: 0.50698334
	avg: 0.56928664
	best: 0.61071664

Starting e_i: 545
Model ind 640 epoch 545 head A head_i_epoch 0 batch 0: avg loss -3.087418 avg loss no lamb -3.087418 time 2019-02-23 13:13:08.546763
Model ind 640 epoch 545 head A head_i_epoch 0 batch 100: avg loss -2.941543 avg loss no lamb -2.941543 time 2019-02-23 13:14:59.198878
Model ind 640 epoch 545 head A head_i_epoch 0 batch 200: avg loss -3.131455 avg loss no lamb -3.131455 time 2019-02-23 13:16:46.931453
last batch sz 160
Model ind 640 epoch 545 head B head_i_epoch 0 batch 0: avg loss -1.749311 avg loss no lamb -1.749311 time 2019-02-23 13:18:04.716567
Model ind 640 epoch 545 head B head_i_epoch 0 batch 100: avg loss -1.655223 avg loss no lamb -1.655223 time 2019-02-23 13:19:54.201352
Model ind 640 epoch 545 head B head_i_epoch 0 batch 200: avg loss -1.867990 avg loss no lamb -1.867990 time 2019-02-23 13:21:43.322049
last batch sz 160
Model ind 640 epoch 545 head B head_i_epoch 1 batch 0: avg loss -1.710718 avg loss no lamb -1.710718 time 2019-02-23 13:23:02.578854
Model ind 640 epoch 545 head B head_i_epoch 1 batch 100: avg loss -1.656618 avg loss no lamb -1.656618 time 2019-02-23 13:24:52.014379
Model ind 640 epoch 545 head B head_i_epoch 1 batch 200: avg loss -1.826077 avg loss no lamb -1.826077 time 2019-02-23 13:26:41.319486
last batch sz 160
Pre: time 2019-02-23 13:28:22.351588: 
 	std: 0.0531636
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.49945, 0.60793334, 0.6078333, 0.60821664, 0.4995]
	train_accs: [0.49945, 0.60793334, 0.6078333, 0.60821664, 0.4995]
	best_train_sub_head: 3
	worst: 0.49945
	avg: 0.5645867
	best: 0.60821664

Starting e_i: 546
Model ind 640 epoch 546 head A head_i_epoch 0 batch 0: avg loss -3.053324 avg loss no lamb -3.053324 time 2019-02-23 13:28:24.362069
Model ind 640 epoch 546 head A head_i_epoch 0 batch 100: avg loss -3.017172 avg loss no lamb -3.017172 time 2019-02-23 13:30:13.469843
Model ind 640 epoch 546 head A head_i_epoch 0 batch 200: avg loss -3.058589 avg loss no lamb -3.058589 time 2019-02-23 13:32:03.353594
last batch sz 160
Model ind 640 epoch 546 head B head_i_epoch 0 batch 0: avg loss -1.764082 avg loss no lamb -1.764082 time 2019-02-23 13:33:23.286156
Model ind 640 epoch 546 head B head_i_epoch 0 batch 100: avg loss -1.684230 avg loss no lamb -1.684230 time 2019-02-23 13:35:12.457058
Model ind 640 epoch 546 head B head_i_epoch 0 batch 200: avg loss -1.801628 avg loss no lamb -1.801628 time 2019-02-23 13:37:01.678806
last batch sz 160
Model ind 640 epoch 546 head B head_i_epoch 1 batch 0: avg loss -1.671579 avg loss no lamb -1.671579 time 2019-02-23 13:38:20.661314
Model ind 640 epoch 546 head B head_i_epoch 1 batch 100: avg loss -1.668777 avg loss no lamb -1.668777 time 2019-02-23 13:40:09.643794
Model ind 640 epoch 546 head B head_i_epoch 1 batch 200: avg loss -1.842427 avg loss no lamb -1.842427 time 2019-02-23 13:41:58.569494
last batch sz 160
Pre: time 2019-02-23 13:43:38.145043: 
 	std: 0.052072663
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5046667, 0.61071664, 0.61038333, 0.6108, 0.50401664]
	train_accs: [0.5046667, 0.61071664, 0.61038333, 0.6108, 0.50401664]
	best_train_sub_head: 3
	worst: 0.50401664
	avg: 0.56811666
	best: 0.6108

Starting e_i: 547
Model ind 640 epoch 547 head A head_i_epoch 0 batch 0: avg loss -3.099664 avg loss no lamb -3.099664 time 2019-02-23 13:43:40.266458
Model ind 640 epoch 547 head A head_i_epoch 0 batch 100: avg loss -3.029551 avg loss no lamb -3.029551 time 2019-02-23 13:45:29.447082
Model ind 640 epoch 547 head A head_i_epoch 0 batch 200: avg loss -3.102491 avg loss no lamb -3.102491 time 2019-02-23 13:47:18.766508
last batch sz 160
Model ind 640 epoch 547 head B head_i_epoch 0 batch 0: avg loss -1.658078 avg loss no lamb -1.658078 time 2019-02-23 13:48:38.399721
Model ind 640 epoch 547 head B head_i_epoch 0 batch 100: avg loss -1.617553 avg loss no lamb -1.617553 time 2019-02-23 13:50:27.842687
Model ind 640 epoch 547 head B head_i_epoch 0 batch 200: avg loss -1.725506 avg loss no lamb -1.725506 time 2019-02-23 13:52:17.072467
last batch sz 160
Model ind 640 epoch 547 head B head_i_epoch 1 batch 0: avg loss -1.669902 avg loss no lamb -1.669902 time 2019-02-23 13:53:35.921591
Model ind 640 epoch 547 head B head_i_epoch 1 batch 100: avg loss -1.644625 avg loss no lamb -1.644625 time 2019-02-23 13:55:25.273260
Model ind 640 epoch 547 head B head_i_epoch 1 batch 200: avg loss -1.839207 avg loss no lamb -1.839207 time 2019-02-23 13:57:14.255079
last batch sz 160
Pre: time 2019-02-23 13:58:53.735961: 
 	std: 0.05200005
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5006833, 0.60588336, 0.6059667, 0.60606664, 0.49898332]
	train_accs: [0.5006833, 0.60588336, 0.6059667, 0.60606664, 0.49898332]
	best_train_sub_head: 3
	worst: 0.49898332
	avg: 0.5635167
	best: 0.60606664

Starting e_i: 548
Model ind 640 epoch 548 head A head_i_epoch 0 batch 0: avg loss -3.073577 avg loss no lamb -3.073577 time 2019-02-23 13:58:55.929993
Model ind 640 epoch 548 head A head_i_epoch 0 batch 100: avg loss -2.973134 avg loss no lamb -2.973134 time 2019-02-23 14:00:45.543265
Model ind 640 epoch 548 head A head_i_epoch 0 batch 200: avg loss -3.026732 avg loss no lamb -3.026732 time 2019-02-23 14:02:34.425938
last batch sz 160
Model ind 640 epoch 548 head B head_i_epoch 0 batch 0: avg loss -1.771019 avg loss no lamb -1.771019 time 2019-02-23 14:03:53.795469
Model ind 640 epoch 548 head B head_i_epoch 0 batch 100: avg loss -1.568542 avg loss no lamb -1.568542 time 2019-02-23 14:05:43.017025
Model ind 640 epoch 548 head B head_i_epoch 0 batch 200: avg loss -1.872519 avg loss no lamb -1.872519 time 2019-02-23 14:07:32.342374
last batch sz 160
Model ind 640 epoch 548 head B head_i_epoch 1 batch 0: avg loss -1.789222 avg loss no lamb -1.789222 time 2019-02-23 14:08:51.559676
Model ind 640 epoch 548 head B head_i_epoch 1 batch 100: avg loss -1.740895 avg loss no lamb -1.740895 time 2019-02-23 14:10:40.644816
Model ind 640 epoch 548 head B head_i_epoch 1 batch 200: avg loss -1.834487 avg loss no lamb -1.834487 time 2019-02-23 14:12:29.590914
last batch sz 160
Pre: time 2019-02-23 14:14:08.113155: 
 	std: 0.053397883
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5024833, 0.61111665, 0.6113167, 0.61113334, 0.5019]
	train_accs: [0.5024833, 0.61111665, 0.6113167, 0.61113334, 0.5019]
	best_train_sub_head: 2
	worst: 0.5019
	avg: 0.56759
	best: 0.6113167

Starting e_i: 549
Model ind 640 epoch 549 head A head_i_epoch 0 batch 0: avg loss -3.102520 avg loss no lamb -3.102520 time 2019-02-23 14:14:10.283798
Model ind 640 epoch 549 head A head_i_epoch 0 batch 100: avg loss -3.004857 avg loss no lamb -3.004857 time 2019-02-23 14:16:02.002013
Model ind 640 epoch 549 head A head_i_epoch 0 batch 200: avg loss -3.176914 avg loss no lamb -3.176914 time 2019-02-23 14:17:51.448464
last batch sz 160
Model ind 640 epoch 549 head B head_i_epoch 0 batch 0: avg loss -1.746423 avg loss no lamb -1.746423 time 2019-02-23 14:19:10.773734
Model ind 640 epoch 549 head B head_i_epoch 0 batch 100: avg loss -1.607785 avg loss no lamb -1.607785 time 2019-02-23 14:20:59.850364
Model ind 640 epoch 549 head B head_i_epoch 0 batch 200: avg loss -1.850535 avg loss no lamb -1.850535 time 2019-02-23 14:22:48.656221
last batch sz 160
Model ind 640 epoch 549 head B head_i_epoch 1 batch 0: avg loss -1.740707 avg loss no lamb -1.740707 time 2019-02-23 14:24:07.945463
Model ind 640 epoch 549 head B head_i_epoch 1 batch 100: avg loss -1.648843 avg loss no lamb -1.648843 time 2019-02-23 14:25:57.825511
Model ind 640 epoch 549 head B head_i_epoch 1 batch 200: avg loss -1.863559 avg loss no lamb -1.863559 time 2019-02-23 14:27:47.323605
last batch sz 160
Pre: time 2019-02-23 14:29:26.758927: 
 	std: 0.0517008
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5044, 0.60971665, 0.6096, 0.6099333, 0.5040333]
	train_accs: [0.5044, 0.60971665, 0.6096, 0.6099333, 0.5040333]
	best_train_sub_head: 3
	worst: 0.5040333
	avg: 0.5675367
	best: 0.6099333

Starting e_i: 550
Model ind 640 epoch 550 head A head_i_epoch 0 batch 0: avg loss -3.074000 avg loss no lamb -3.074000 time 2019-02-23 14:29:28.996498
Model ind 640 epoch 550 head A head_i_epoch 0 batch 100: avg loss -2.968247 avg loss no lamb -2.968247 time 2019-02-23 14:31:18.768032
Model ind 640 epoch 550 head A head_i_epoch 0 batch 200: avg loss -3.085156 avg loss no lamb -3.085156 time 2019-02-23 14:33:07.654632
last batch sz 160
Model ind 640 epoch 550 head B head_i_epoch 0 batch 0: avg loss -1.678517 avg loss no lamb -1.678517 time 2019-02-23 14:34:27.245331
Model ind 640 epoch 550 head B head_i_epoch 0 batch 100: avg loss -1.677369 avg loss no lamb -1.677369 time 2019-02-23 14:36:16.231998
Model ind 640 epoch 550 head B head_i_epoch 0 batch 200: avg loss -1.791843 avg loss no lamb -1.791843 time 2019-02-23 14:38:04.927094
last batch sz 160
Model ind 640 epoch 550 head B head_i_epoch 1 batch 0: avg loss -1.769816 avg loss no lamb -1.769816 time 2019-02-23 14:39:23.682904
Model ind 640 epoch 550 head B head_i_epoch 1 batch 100: avg loss -1.641290 avg loss no lamb -1.641290 time 2019-02-23 14:41:12.760103
Model ind 640 epoch 550 head B head_i_epoch 1 batch 200: avg loss -1.767488 avg loss no lamb -1.767488 time 2019-02-23 14:43:01.308246
last batch sz 160
Pre: time 2019-02-23 14:44:42.873980: 
 	std: 0.053457513
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5021, 0.6109833, 0.61113334, 0.6113167, 0.50195]
	train_accs: [0.5021, 0.6109833, 0.61113334, 0.6113167, 0.50195]
	best_train_sub_head: 3
	worst: 0.50195
	avg: 0.56749666
	best: 0.6113167

Starting e_i: 551
Model ind 640 epoch 551 head A head_i_epoch 0 batch 0: avg loss -3.036188 avg loss no lamb -3.036188 time 2019-02-23 14:44:50.274797
Model ind 640 epoch 551 head A head_i_epoch 0 batch 100: avg loss -2.964412 avg loss no lamb -2.964412 time 2019-02-23 14:46:40.826812
Model ind 640 epoch 551 head A head_i_epoch 0 batch 200: avg loss -3.079817 avg loss no lamb -3.079817 time 2019-02-23 14:48:30.048948
last batch sz 160
Model ind 640 epoch 551 head B head_i_epoch 0 batch 0: avg loss -1.765259 avg loss no lamb -1.765259 time 2019-02-23 14:49:49.595222
Model ind 640 epoch 551 head B head_i_epoch 0 batch 100: avg loss -1.678453 avg loss no lamb -1.678453 time 2019-02-23 14:51:39.010653
Model ind 640 epoch 551 head B head_i_epoch 0 batch 200: avg loss -1.900870 avg loss no lamb -1.900870 time 2019-02-23 14:53:27.948199
last batch sz 160
Model ind 640 epoch 551 head B head_i_epoch 1 batch 0: avg loss -1.733952 avg loss no lamb -1.733952 time 2019-02-23 14:54:47.713585
Model ind 640 epoch 551 head B head_i_epoch 1 batch 100: avg loss -1.709078 avg loss no lamb -1.709078 time 2019-02-23 14:56:36.473270
Model ind 640 epoch 551 head B head_i_epoch 1 batch 200: avg loss -1.825123 avg loss no lamb -1.825123 time 2019-02-23 14:58:26.015804
last batch sz 160
Pre: time 2019-02-23 15:00:04.927507: 
 	std: 0.051808786
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50703335, 0.61285, 0.61263335, 0.6134, 0.50738335]
	train_accs: [0.50703335, 0.61285, 0.61263335, 0.6134, 0.50738335]
	best_train_sub_head: 3
	worst: 0.50703335
	avg: 0.57066
	best: 0.6134

Starting e_i: 552
Model ind 640 epoch 552 head A head_i_epoch 0 batch 0: avg loss -3.044751 avg loss no lamb -3.044751 time 2019-02-23 15:00:07.085701
Model ind 640 epoch 552 head A head_i_epoch 0 batch 100: avg loss -2.974787 avg loss no lamb -2.974787 time 2019-02-23 15:01:56.509618
Model ind 640 epoch 552 head A head_i_epoch 0 batch 200: avg loss -3.056345 avg loss no lamb -3.056345 time 2019-02-23 15:03:48.416923
last batch sz 160
Model ind 640 epoch 552 head B head_i_epoch 0 batch 0: avg loss -1.787911 avg loss no lamb -1.787911 time 2019-02-23 15:05:11.816084
Model ind 640 epoch 552 head B head_i_epoch 0 batch 100: avg loss -1.637321 avg loss no lamb -1.637321 time 2019-02-23 15:08:39.849314
Model ind 640 epoch 552 head B head_i_epoch 0 batch 200: avg loss -1.830889 avg loss no lamb -1.830889 time 2019-02-23 15:11:53.910552
last batch sz 160
Model ind 640 epoch 552 head B head_i_epoch 1 batch 0: avg loss -1.722353 avg loss no lamb -1.722353 time 2019-02-23 15:14:21.707715
Model ind 640 epoch 552 head B head_i_epoch 1 batch 100: avg loss -1.633652 avg loss no lamb -1.633652 time 2019-02-23 15:17:53.809973
Model ind 640 epoch 552 head B head_i_epoch 1 batch 200: avg loss -1.817497 avg loss no lamb -1.817497 time 2019-02-23 15:21:07.445254
last batch sz 160
Pre: time 2019-02-23 15:23:29.708216: 
 	std: 0.05137053
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50305, 0.6074167, 0.6075, 0.60763335, 0.50226665]
	train_accs: [0.50305, 0.6074167, 0.6075, 0.60763335, 0.50226665]
	best_train_sub_head: 3
	worst: 0.50226665
	avg: 0.56557333
	best: 0.60763335

Starting e_i: 553
Model ind 640 epoch 553 head A head_i_epoch 0 batch 0: avg loss -3.044045 avg loss no lamb -3.044045 time 2019-02-23 15:23:32.888534
Model ind 640 epoch 553 head A head_i_epoch 0 batch 100: avg loss -3.020908 avg loss no lamb -3.020908 time 2019-02-23 15:27:01.684976
Model ind 640 epoch 553 head A head_i_epoch 0 batch 200: avg loss -3.056403 avg loss no lamb -3.056403 time 2019-02-23 15:29:46.218869
last batch sz 160
Model ind 640 epoch 553 head B head_i_epoch 0 batch 0: avg loss -1.761546 avg loss no lamb -1.761546 time 2019-02-23 15:32:31.270238
Model ind 640 epoch 553 head B head_i_epoch 0 batch 100: avg loss -1.603657 avg loss no lamb -1.603657 time 2019-02-23 15:35:45.381844
Model ind 640 epoch 553 head B head_i_epoch 0 batch 200: avg loss -1.759909 avg loss no lamb -1.759909 time 2019-02-23 15:39:01.452485
last batch sz 160
Model ind 640 epoch 553 head B head_i_epoch 1 batch 0: avg loss -1.697798 avg loss no lamb -1.697798 time 2019-02-23 15:41:46.847281
Model ind 640 epoch 553 head B head_i_epoch 1 batch 100: avg loss -1.658764 avg loss no lamb -1.658764 time 2019-02-23 15:45:01.098686
Model ind 640 epoch 553 head B head_i_epoch 1 batch 200: avg loss -1.734823 avg loss no lamb -1.734823 time 2019-02-23 15:48:19.722610
last batch sz 160
Pre: time 2019-02-23 15:51:36.957773: 
 	std: 0.05200136
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50298333, 0.60926664, 0.6092333, 0.60926664, 0.5032333]
	train_accs: [0.50298333, 0.60926664, 0.6092333, 0.60926664, 0.5032333]
	best_train_sub_head: 1
	worst: 0.50298333
	avg: 0.56679666
	best: 0.60926664

Starting e_i: 554
Model ind 640 epoch 554 head A head_i_epoch 0 batch 0: avg loss -3.017573 avg loss no lamb -3.017573 time 2019-02-23 15:51:39.454056
Model ind 640 epoch 554 head A head_i_epoch 0 batch 100: avg loss -3.056231 avg loss no lamb -3.056231 time 2019-02-23 15:55:06.015856
Model ind 640 epoch 554 head A head_i_epoch 0 batch 200: avg loss -3.107278 avg loss no lamb -3.107278 time 2019-02-23 15:58:22.235414
last batch sz 160
Model ind 640 epoch 554 head B head_i_epoch 0 batch 0: avg loss -1.693137 avg loss no lamb -1.693137 time 2019-02-23 16:00:53.804969
Model ind 640 epoch 554 head B head_i_epoch 0 batch 100: avg loss -1.633019 avg loss no lamb -1.633019 time 2019-02-23 16:04:26.704230
Model ind 640 epoch 554 head B head_i_epoch 0 batch 200: avg loss -1.787794 avg loss no lamb -1.787794 time 2019-02-23 16:07:42.067655
last batch sz 160
Model ind 640 epoch 554 head B head_i_epoch 1 batch 0: avg loss -1.765753 avg loss no lamb -1.765753 time 2019-02-23 16:10:08.946089
Model ind 640 epoch 554 head B head_i_epoch 1 batch 100: avg loss -1.699881 avg loss no lamb -1.699881 time 2019-02-23 16:13:43.734624
Model ind 640 epoch 554 head B head_i_epoch 1 batch 200: avg loss -1.824476 avg loss no lamb -1.824476 time 2019-02-23 16:17:02.358508
last batch sz 160
Pre: time 2019-02-23 16:19:58.683253: 
 	std: 0.05139871
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50553334, 0.61046666, 0.61016667, 0.61016667, 0.50516665]
	train_accs: [0.50553334, 0.61046666, 0.61016667, 0.61016667, 0.50516665]
	best_train_sub_head: 1
	worst: 0.50516665
	avg: 0.56829995
	best: 0.61046666

Starting e_i: 555
Model ind 640 epoch 555 head A head_i_epoch 0 batch 0: avg loss -3.144062 avg loss no lamb -3.144062 time 2019-02-23 16:20:01.719698
Model ind 640 epoch 555 head A head_i_epoch 0 batch 100: avg loss -2.957173 avg loss no lamb -2.957173 time 2019-02-23 16:23:33.496789
Model ind 640 epoch 555 head A head_i_epoch 0 batch 200: avg loss -3.072439 avg loss no lamb -3.072439 time 2019-02-23 16:27:03.680055
last batch sz 160
Model ind 640 epoch 555 head B head_i_epoch 0 batch 0: avg loss -1.861965 avg loss no lamb -1.861965 time 2019-02-23 16:29:18.850415
Model ind 640 epoch 555 head B head_i_epoch 0 batch 100: avg loss -1.707976 avg loss no lamb -1.707976 time 2019-02-23 16:32:48.197817
Model ind 640 epoch 555 head B head_i_epoch 0 batch 200: avg loss -1.852626 avg loss no lamb -1.852626 time 2019-02-23 16:36:23.889901
last batch sz 160
Model ind 640 epoch 555 head B head_i_epoch 1 batch 0: avg loss -1.702208 avg loss no lamb -1.702208 time 2019-02-23 16:38:40.864849
Model ind 640 epoch 555 head B head_i_epoch 1 batch 100: avg loss -1.637238 avg loss no lamb -1.637238 time 2019-02-23 16:42:02.661309
Model ind 640 epoch 555 head B head_i_epoch 1 batch 200: avg loss -1.718308 avg loss no lamb -1.718308 time 2019-02-23 16:45:45.966770
last batch sz 160
Pre: time 2019-02-23 16:48:38.137162: 
 	std: 0.050886948
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5056667, 0.60941666, 0.60945, 0.60925, 0.5053333]
	train_accs: [0.5056667, 0.60941666, 0.60945, 0.60925, 0.5053333]
	best_train_sub_head: 2
	worst: 0.5053333
	avg: 0.5678233
	best: 0.60945

Starting e_i: 556
Model ind 640 epoch 556 head A head_i_epoch 0 batch 0: avg loss -3.068227 avg loss no lamb -3.068227 time 2019-02-23 16:48:41.211968
Model ind 640 epoch 556 head A head_i_epoch 0 batch 100: avg loss -2.918562 avg loss no lamb -2.918562 time 2019-02-23 16:51:57.478823
Model ind 640 epoch 556 head A head_i_epoch 0 batch 200: avg loss -3.090302 avg loss no lamb -3.090302 time 2019-02-23 16:55:15.606094
last batch sz 160
Model ind 640 epoch 556 head B head_i_epoch 0 batch 0: avg loss -1.749224 avg loss no lamb -1.749224 time 2019-02-23 16:57:33.432367
Model ind 640 epoch 556 head B head_i_epoch 0 batch 100: avg loss -1.591937 avg loss no lamb -1.591937 time 2019-02-23 17:00:35.203913
Model ind 640 epoch 556 head B head_i_epoch 0 batch 200: avg loss -1.758945 avg loss no lamb -1.758945 time 2019-02-23 17:04:03.327649
last batch sz 160
Model ind 640 epoch 556 head B head_i_epoch 1 batch 0: avg loss -1.704133 avg loss no lamb -1.704133 time 2019-02-23 17:06:13.010615
Model ind 640 epoch 556 head B head_i_epoch 1 batch 100: avg loss -1.692688 avg loss no lamb -1.692688 time 2019-02-23 17:09:13.279280
Model ind 640 epoch 556 head B head_i_epoch 1 batch 200: avg loss -1.819697 avg loss no lamb -1.819697 time 2019-02-23 17:12:45.412589
last batch sz 160
Pre: time 2019-02-23 17:15:37.944118: 
 	std: 0.05155402
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5057333, 0.6102333, 0.6105833, 0.61015, 0.50445]
	train_accs: [0.5057333, 0.6102333, 0.6105833, 0.61015, 0.50445]
	best_train_sub_head: 2
	worst: 0.50445
	avg: 0.56823003
	best: 0.6105833

Starting e_i: 557
Model ind 640 epoch 557 head A head_i_epoch 0 batch 0: avg loss -3.033417 avg loss no lamb -3.033417 time 2019-02-23 17:15:41.513075
Model ind 640 epoch 557 head A head_i_epoch 0 batch 100: avg loss -2.928407 avg loss no lamb -2.928407 time 2019-02-23 17:18:44.583982
Model ind 640 epoch 557 head A head_i_epoch 0 batch 200: avg loss -3.032758 avg loss no lamb -3.032758 time 2019-02-23 17:22:03.372123
last batch sz 160
Model ind 640 epoch 557 head B head_i_epoch 0 batch 0: avg loss -1.663828 avg loss no lamb -1.663828 time 2019-02-23 17:24:16.900486
Model ind 640 epoch 557 head B head_i_epoch 0 batch 100: avg loss -1.621733 avg loss no lamb -1.621733 time 2019-02-23 17:27:12.997244
Model ind 640 epoch 557 head B head_i_epoch 0 batch 200: avg loss -1.778541 avg loss no lamb -1.778541 time 2019-02-23 17:30:41.429209
last batch sz 160
Model ind 640 epoch 557 head B head_i_epoch 1 batch 0: avg loss -1.705468 avg loss no lamb -1.705468 time 2019-02-23 17:32:47.077065
Model ind 640 epoch 557 head B head_i_epoch 1 batch 100: avg loss -1.664444 avg loss no lamb -1.664444 time 2019-02-23 17:35:59.190556
Model ind 640 epoch 557 head B head_i_epoch 1 batch 200: avg loss -1.832732 avg loss no lamb -1.832732 time 2019-02-23 17:39:13.147120
last batch sz 160
Pre: time 2019-02-23 17:41:54.075627: 
 	std: 0.052502573
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50483334, 0.6116833, 0.61175, 0.61155, 0.50415]
	train_accs: [0.50483334, 0.6116833, 0.61175, 0.61155, 0.50415]
	best_train_sub_head: 2
	worst: 0.50415
	avg: 0.5687933
	best: 0.61175

Starting e_i: 558
Model ind 640 epoch 558 head A head_i_epoch 0 batch 0: avg loss -3.043932 avg loss no lamb -3.043932 time 2019-02-23 17:41:56.556074
Model ind 640 epoch 558 head A head_i_epoch 0 batch 100: avg loss -2.976979 avg loss no lamb -2.976979 time 2019-02-23 17:45:05.974199
Model ind 640 epoch 558 head A head_i_epoch 0 batch 200: avg loss -3.070868 avg loss no lamb -3.070868 time 2019-02-23 17:48:24.397986
last batch sz 160
Model ind 640 epoch 558 head B head_i_epoch 0 batch 0: avg loss -1.653623 avg loss no lamb -1.653623 time 2019-02-23 17:50:28.104487
Model ind 640 epoch 558 head B head_i_epoch 0 batch 100: avg loss -1.655254 avg loss no lamb -1.655254 time 2019-02-23 17:53:50.658272
Model ind 640 epoch 558 head B head_i_epoch 0 batch 200: avg loss -1.883581 avg loss no lamb -1.883581 time 2019-02-23 17:56:55.628791
last batch sz 160
Model ind 640 epoch 558 head B head_i_epoch 1 batch 0: avg loss -1.757539 avg loss no lamb -1.757539 time 2019-02-23 17:58:58.504570
Model ind 640 epoch 558 head B head_i_epoch 1 batch 100: avg loss -1.625892 avg loss no lamb -1.625892 time 2019-02-23 18:02:24.520282
Model ind 640 epoch 558 head B head_i_epoch 1 batch 200: avg loss -1.789277 avg loss no lamb -1.789277 time 2019-02-23 18:05:28.044999
last batch sz 160
Pre: time 2019-02-23 18:08:06.536635: 
 	std: 0.05167214
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50378335, 0.6091833, 0.60908335, 0.60898334, 0.50343335]
	train_accs: [0.50378335, 0.6091833, 0.60908335, 0.60898334, 0.50343335]
	best_train_sub_head: 1
	worst: 0.50343335
	avg: 0.5668933
	best: 0.6091833

Starting e_i: 559
Model ind 640 epoch 559 head A head_i_epoch 0 batch 0: avg loss -3.052612 avg loss no lamb -3.052612 time 2019-02-23 18:08:08.963531
Model ind 640 epoch 559 head A head_i_epoch 0 batch 100: avg loss -2.996950 avg loss no lamb -2.996950 time 2019-02-23 18:11:37.008981
Model ind 640 epoch 559 head A head_i_epoch 0 batch 200: avg loss -3.100783 avg loss no lamb -3.100783 time 2019-02-23 18:14:38.617860
last batch sz 160
Model ind 640 epoch 559 head B head_i_epoch 0 batch 0: avg loss -1.770728 avg loss no lamb -1.770728 time 2019-02-23 18:16:53.297642
Model ind 640 epoch 559 head B head_i_epoch 0 batch 100: avg loss -1.564191 avg loss no lamb -1.564191 time 2019-02-23 18:20:05.973923
Model ind 640 epoch 559 head B head_i_epoch 0 batch 200: avg loss -1.847370 avg loss no lamb -1.847370 time 2019-02-23 18:23:06.068905
last batch sz 160
Model ind 640 epoch 559 head B head_i_epoch 1 batch 0: avg loss -1.757704 avg loss no lamb -1.757704 time 2019-02-23 18:25:36.861740
Model ind 640 epoch 559 head B head_i_epoch 1 batch 100: avg loss -1.619841 avg loss no lamb -1.619841 time 2019-02-23 18:28:44.642263
Model ind 640 epoch 559 head B head_i_epoch 1 batch 200: avg loss -1.871412 avg loss no lamb -1.871412 time 2019-02-23 18:31:45.959320
last batch sz 160
Pre: time 2019-02-23 18:34:51.408321: 
 	std: 0.051691152
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5050667, 0.61073333, 0.6108, 0.61053336, 0.50528336]
	train_accs: [0.5050667, 0.61073333, 0.6108, 0.61053336, 0.50528336]
	best_train_sub_head: 2
	worst: 0.5050667
	avg: 0.56848335
	best: 0.6108

Starting e_i: 560
Model ind 640 epoch 560 head A head_i_epoch 0 batch 0: avg loss -3.123388 avg loss no lamb -3.123388 time 2019-02-23 18:34:53.567135
Model ind 640 epoch 560 head A head_i_epoch 0 batch 100: avg loss -3.067116 avg loss no lamb -3.067116 time 2019-02-23 18:38:05.200435
Model ind 640 epoch 560 head A head_i_epoch 0 batch 200: avg loss -3.063168 avg loss no lamb -3.063168 time 2019-02-23 18:41:10.530246
last batch sz 160
Model ind 640 epoch 560 head B head_i_epoch 0 batch 0: avg loss -1.725289 avg loss no lamb -1.725289 time 2019-02-23 18:43:45.365617
Model ind 640 epoch 560 head B head_i_epoch 0 batch 100: avg loss -1.645090 avg loss no lamb -1.645090 time 2019-02-23 18:46:52.854179
Model ind 640 epoch 560 head B head_i_epoch 0 batch 200: avg loss -1.779388 avg loss no lamb -1.779388 time 2019-02-23 18:50:00.808684
last batch sz 160
Model ind 640 epoch 560 head B head_i_epoch 1 batch 0: avg loss -1.733832 avg loss no lamb -1.733832 time 2019-02-23 18:52:31.915837
Model ind 640 epoch 560 head B head_i_epoch 1 batch 100: avg loss -1.581170 avg loss no lamb -1.581170 time 2019-02-23 18:55:38.588421
Model ind 640 epoch 560 head B head_i_epoch 1 batch 200: avg loss -1.854205 avg loss no lamb -1.854205 time 2019-02-23 18:58:44.817749
last batch sz 160
Pre: time 2019-02-23 19:01:48.744563: 
 	std: 0.05229807
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5042667, 0.61125, 0.61113334, 0.6111, 0.50455]
	train_accs: [0.5042667, 0.61125, 0.61113334, 0.6111, 0.50455]
	best_train_sub_head: 1
	worst: 0.5042667
	avg: 0.56846
	best: 0.61125

Starting e_i: 561
Model ind 640 epoch 561 head A head_i_epoch 0 batch 0: avg loss -3.041792 avg loss no lamb -3.041792 time 2019-02-23 19:01:55.016630
Model ind 640 epoch 561 head A head_i_epoch 0 batch 100: avg loss -2.976645 avg loss no lamb -2.976645 time 2019-02-23 19:05:11.081902
Model ind 640 epoch 561 head A head_i_epoch 0 batch 200: avg loss -3.081636 avg loss no lamb -3.081636 time 2019-02-23 19:08:16.951394
last batch sz 160
Model ind 640 epoch 561 head B head_i_epoch 0 batch 0: avg loss -1.771578 avg loss no lamb -1.771578 time 2019-02-23 19:10:48.033348
Model ind 640 epoch 561 head B head_i_epoch 0 batch 100: avg loss -1.647891 avg loss no lamb -1.647891 time 2019-02-23 19:13:54.871906
Model ind 640 epoch 561 head B head_i_epoch 0 batch 200: avg loss -1.810008 avg loss no lamb -1.810008 time 2019-02-23 19:16:59.679197
last batch sz 160
Model ind 640 epoch 561 head B head_i_epoch 1 batch 0: avg loss -1.722183 avg loss no lamb -1.722183 time 2019-02-23 19:19:32.400415
Model ind 640 epoch 561 head B head_i_epoch 1 batch 100: avg loss -1.654868 avg loss no lamb -1.654868 time 2019-02-23 19:22:37.447336
Model ind 640 epoch 561 head B head_i_epoch 1 batch 200: avg loss -1.901765 avg loss no lamb -1.901765 time 2019-02-23 19:25:41.384180
last batch sz 160
Pre: time 2019-02-23 19:28:48.336540: 
 	std: 0.05194023
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5045, 0.6105833, 0.61025, 0.61038333, 0.5042667]
	train_accs: [0.5045, 0.6105833, 0.61025, 0.61038333, 0.5042667]
	best_train_sub_head: 1
	worst: 0.5042667
	avg: 0.5679966
	best: 0.6105833

Starting e_i: 562
Model ind 640 epoch 562 head A head_i_epoch 0 batch 0: avg loss -3.061494 avg loss no lamb -3.061494 time 2019-02-23 19:28:50.476474
Model ind 640 epoch 562 head A head_i_epoch 0 batch 100: avg loss -2.976995 avg loss no lamb -2.976995 time 2019-02-23 19:32:02.091338
Model ind 640 epoch 562 head A head_i_epoch 0 batch 200: avg loss -3.119581 avg loss no lamb -3.119581 time 2019-02-23 19:35:05.511972
last batch sz 160
Model ind 640 epoch 562 head B head_i_epoch 0 batch 0: avg loss -1.730245 avg loss no lamb -1.730245 time 2019-02-23 19:37:39.300110
Model ind 640 epoch 562 head B head_i_epoch 0 batch 100: avg loss -1.694736 avg loss no lamb -1.694736 time 2019-02-23 19:40:42.521248
Model ind 640 epoch 562 head B head_i_epoch 0 batch 200: avg loss -1.770116 avg loss no lamb -1.770116 time 2019-02-23 19:43:43.933046
last batch sz 160
Model ind 640 epoch 562 head B head_i_epoch 1 batch 0: avg loss -1.837878 avg loss no lamb -1.837878 time 2019-02-23 19:46:16.738754
Model ind 640 epoch 562 head B head_i_epoch 1 batch 100: avg loss -1.663210 avg loss no lamb -1.663210 time 2019-02-23 19:49:25.291945
Model ind 640 epoch 562 head B head_i_epoch 1 batch 200: avg loss -1.809295 avg loss no lamb -1.809295 time 2019-02-23 19:52:28.839989
last batch sz 160
Pre: time 2019-02-23 19:55:40.123294: 
 	std: 0.05054147
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50726664, 0.61071664, 0.61016667, 0.6103167, 0.5072]
	train_accs: [0.50726664, 0.61071664, 0.61016667, 0.6103167, 0.5072]
	best_train_sub_head: 1
	worst: 0.5072
	avg: 0.56913334
	best: 0.61071664

Starting e_i: 563
Model ind 640 epoch 563 head A head_i_epoch 0 batch 0: avg loss -3.119376 avg loss no lamb -3.119376 time 2019-02-23 19:55:42.771781
Model ind 640 epoch 563 head A head_i_epoch 0 batch 100: avg loss -3.003695 avg loss no lamb -3.003695 time 2019-02-23 19:58:50.098914
Model ind 640 epoch 563 head A head_i_epoch 0 batch 200: avg loss -3.063748 avg loss no lamb -3.063748 time 2019-02-23 20:01:54.813559
last batch sz 160
Model ind 640 epoch 563 head B head_i_epoch 0 batch 0: avg loss -1.801708 avg loss no lamb -1.801708 time 2019-02-23 20:04:29.011987
Model ind 640 epoch 563 head B head_i_epoch 0 batch 100: avg loss -1.579033 avg loss no lamb -1.579033 time 2019-02-23 20:07:33.235160
Model ind 640 epoch 563 head B head_i_epoch 0 batch 200: avg loss -1.817579 avg loss no lamb -1.817579 time 2019-02-23 20:10:32.436442
last batch sz 160
Model ind 640 epoch 563 head B head_i_epoch 1 batch 0: avg loss -1.715367 avg loss no lamb -1.715367 time 2019-02-23 20:13:06.743463
Model ind 640 epoch 563 head B head_i_epoch 1 batch 100: avg loss -1.658635 avg loss no lamb -1.658635 time 2019-02-23 20:16:12.663442
Model ind 640 epoch 563 head B head_i_epoch 1 batch 200: avg loss -1.841468 avg loss no lamb -1.841468 time 2019-02-23 20:19:12.761790
last batch sz 160
Pre: time 2019-02-23 20:22:25.202185: 
 	std: 0.05189483
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5013, 0.60796666, 0.6075, 0.60756665, 0.5022]
	train_accs: [0.5013, 0.60796666, 0.6075, 0.60756665, 0.5022]
	best_train_sub_head: 1
	worst: 0.5013
	avg: 0.56530666
	best: 0.60796666

Starting e_i: 564
Model ind 640 epoch 564 head A head_i_epoch 0 batch 0: avg loss -3.080071 avg loss no lamb -3.080071 time 2019-02-23 20:22:27.732633
Model ind 640 epoch 564 head A head_i_epoch 0 batch 100: avg loss -2.986025 avg loss no lamb -2.986025 time 2019-02-23 20:25:35.850653
Model ind 640 epoch 564 head A head_i_epoch 0 batch 200: avg loss -3.103140 avg loss no lamb -3.103140 time 2019-02-23 20:28:39.443848
last batch sz 160
Model ind 640 epoch 564 head B head_i_epoch 0 batch 0: avg loss -1.791944 avg loss no lamb -1.791944 time 2019-02-23 20:31:14.508145
Model ind 640 epoch 564 head B head_i_epoch 0 batch 100: avg loss -1.719653 avg loss no lamb -1.719653 time 2019-02-23 20:34:23.585396
Model ind 640 epoch 564 head B head_i_epoch 0 batch 200: avg loss -1.808403 avg loss no lamb -1.808403 time 2019-02-23 20:37:34.010054
last batch sz 160
Model ind 640 epoch 564 head B head_i_epoch 1 batch 0: avg loss -1.665770 avg loss no lamb -1.665770 time 2019-02-23 20:40:13.439907
Model ind 640 epoch 564 head B head_i_epoch 1 batch 100: avg loss -1.619166 avg loss no lamb -1.619166 time 2019-02-23 20:43:21.722811
Model ind 640 epoch 564 head B head_i_epoch 1 batch 200: avg loss -1.832269 avg loss no lamb -1.832269 time 2019-02-23 20:46:42.431876
last batch sz 160
Pre: time 2019-02-23 20:50:07.285708: 
 	std: 0.052613944
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5042, 0.61185, 0.61191666, 0.61175, 0.5046833]
	train_accs: [0.5042, 0.61185, 0.61191666, 0.61175, 0.5046833]
	best_train_sub_head: 2
	worst: 0.5042
	avg: 0.56887996
	best: 0.61191666

Starting e_i: 565
Model ind 640 epoch 565 head A head_i_epoch 0 batch 0: avg loss -3.108582 avg loss no lamb -3.108582 time 2019-02-23 20:50:10.484994
Model ind 640 epoch 565 head A head_i_epoch 0 batch 100: avg loss -2.958158 avg loss no lamb -2.958158 time 2019-02-23 20:53:49.489782
Model ind 640 epoch 565 head A head_i_epoch 0 batch 200: avg loss -3.129375 avg loss no lamb -3.129375 time 2019-02-23 20:57:12.113796
last batch sz 160
Model ind 640 epoch 565 head B head_i_epoch 0 batch 0: avg loss -1.715438 avg loss no lamb -1.715438 time 2019-02-23 20:59:37.080960
Model ind 640 epoch 565 head B head_i_epoch 0 batch 100: avg loss -1.522602 avg loss no lamb -1.522602 time 2019-02-23 21:03:23.454593
Model ind 640 epoch 565 head B head_i_epoch 0 batch 200: avg loss -1.821185 avg loss no lamb -1.821185 time 2019-02-23 21:06:46.482897
last batch sz 160
Model ind 640 epoch 565 head B head_i_epoch 1 batch 0: avg loss -1.777218 avg loss no lamb -1.777218 time 2019-02-23 21:09:10.236531
Model ind 640 epoch 565 head B head_i_epoch 1 batch 100: avg loss -1.543234 avg loss no lamb -1.543234 time 2019-02-23 21:12:59.100734
Model ind 640 epoch 565 head B head_i_epoch 1 batch 200: avg loss -1.874301 avg loss no lamb -1.874301 time 2019-02-23 21:16:25.411391
last batch sz 160
Pre: time 2019-02-23 21:19:23.309784: 
 	std: 0.052385543
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5028333, 0.6105833, 0.61013335, 0.61051667, 0.50413334]
	train_accs: [0.5028333, 0.6105833, 0.61013335, 0.61051667, 0.50413334]
	best_train_sub_head: 1
	worst: 0.5028333
	avg: 0.56764
	best: 0.6105833

Starting e_i: 566
Model ind 640 epoch 566 head A head_i_epoch 0 batch 0: avg loss -3.081095 avg loss no lamb -3.081095 time 2019-02-23 21:19:27.095942
Model ind 640 epoch 566 head A head_i_epoch 0 batch 100: avg loss -2.897918 avg loss no lamb -2.897918 time 2019-02-23 21:22:51.569984
Model ind 640 epoch 566 head A head_i_epoch 0 batch 200: avg loss -3.046912 avg loss no lamb -3.046912 time 2019-02-23 21:26:40.081813
last batch sz 160
Model ind 640 epoch 566 head B head_i_epoch 0 batch 0: avg loss -1.767646 avg loss no lamb -1.767646 time 2019-02-23 21:29:05.562169
Model ind 640 epoch 566 head B head_i_epoch 0 batch 100: avg loss -1.580337 avg loss no lamb -1.580337 time 2019-02-23 21:32:28.435573
Model ind 640 epoch 566 head B head_i_epoch 0 batch 200: avg loss -1.840451 avg loss no lamb -1.840451 time 2019-02-23 21:36:05.217169
last batch sz 160
Model ind 640 epoch 566 head B head_i_epoch 1 batch 0: avg loss -1.800735 avg loss no lamb -1.800735 time 2019-02-23 21:38:36.724252
Model ind 640 epoch 566 head B head_i_epoch 1 batch 100: avg loss -1.633337 avg loss no lamb -1.633337 time 2019-02-23 21:41:57.448193
Model ind 640 epoch 566 head B head_i_epoch 1 batch 200: avg loss -1.778268 avg loss no lamb -1.778268 time 2019-02-23 21:45:30.457671
last batch sz 160
Pre: time 2019-02-23 21:48:52.523171: 
 	std: 0.053752962
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50085, 0.6109, 0.61055, 0.61086667, 0.50125]
	train_accs: [0.50085, 0.6109, 0.61055, 0.61086667, 0.50125]
	best_train_sub_head: 1
	worst: 0.50085
	avg: 0.5668833
	best: 0.6109

Starting e_i: 567
Model ind 640 epoch 567 head A head_i_epoch 0 batch 0: avg loss -3.032182 avg loss no lamb -3.032182 time 2019-02-23 21:48:56.058503
Model ind 640 epoch 567 head A head_i_epoch 0 batch 100: avg loss -2.976929 avg loss no lamb -2.976929 time 2019-02-23 21:52:18.786414
Model ind 640 epoch 567 head A head_i_epoch 0 batch 200: avg loss -3.114721 avg loss no lamb -3.114721 time 2019-02-23 21:55:37.540643
last batch sz 160
Model ind 640 epoch 567 head B head_i_epoch 0 batch 0: avg loss -1.782964 avg loss no lamb -1.782964 time 2019-02-23 21:58:27.097557
Model ind 640 epoch 567 head B head_i_epoch 0 batch 100: avg loss -1.598355 avg loss no lamb -1.598355 time 2019-02-23 22:01:54.029656
Model ind 640 epoch 567 head B head_i_epoch 0 batch 200: avg loss -1.911396 avg loss no lamb -1.911396 time 2019-02-23 22:05:15.300264
last batch sz 160
Model ind 640 epoch 567 head B head_i_epoch 1 batch 0: avg loss -1.699540 avg loss no lamb -1.699540 time 2019-02-23 22:07:56.212301
Model ind 640 epoch 567 head B head_i_epoch 1 batch 100: avg loss -1.676397 avg loss no lamb -1.676397 time 2019-02-23 22:11:27.416353
Model ind 640 epoch 567 head B head_i_epoch 1 batch 200: avg loss -1.823810 avg loss no lamb -1.823810 time 2019-02-23 22:14:48.512237
last batch sz 160
Pre: time 2019-02-23 22:17:53.397943: 
 	std: 0.05153414
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5071333, 0.6122, 0.61165, 0.61255, 0.50675]
	train_accs: [0.5071333, 0.6122, 0.61165, 0.61255, 0.50675]
	best_train_sub_head: 3
	worst: 0.50675
	avg: 0.5700566
	best: 0.61255

Starting e_i: 568
Model ind 640 epoch 568 head A head_i_epoch 0 batch 0: avg loss -3.101483 avg loss no lamb -3.101483 time 2019-02-23 22:17:58.874133
Model ind 640 epoch 568 head A head_i_epoch 0 batch 100: avg loss -3.037744 avg loss no lamb -3.037744 time 2019-02-23 22:21:43.452035
Model ind 640 epoch 568 head A head_i_epoch 0 batch 200: avg loss -3.143530 avg loss no lamb -3.143530 time 2019-02-23 22:25:16.178684
last batch sz 160
Model ind 640 epoch 568 head B head_i_epoch 0 batch 0: avg loss -1.660640 avg loss no lamb -1.660640 time 2019-02-23 22:27:35.472954
Model ind 640 epoch 568 head B head_i_epoch 0 batch 100: avg loss -1.624155 avg loss no lamb -1.624155 time 2019-02-23 22:31:10.546980
Model ind 640 epoch 568 head B head_i_epoch 0 batch 200: avg loss -1.886877 avg loss no lamb -1.886877 time 2019-02-23 22:34:47.133725
last batch sz 160
Model ind 640 epoch 568 head B head_i_epoch 1 batch 0: avg loss -1.840578 avg loss no lamb -1.840578 time 2019-02-23 22:37:10.598374
Model ind 640 epoch 568 head B head_i_epoch 1 batch 100: avg loss -1.660597 avg loss no lamb -1.660597 time 2019-02-23 22:40:30.784893
Model ind 640 epoch 568 head B head_i_epoch 1 batch 200: avg loss -1.805503 avg loss no lamb -1.805503 time 2019-02-23 22:44:17.072895
last batch sz 160
Pre: time 2019-02-23 22:47:07.905876: 
 	std: 0.053794034
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50333333, 0.61305, 0.6127167, 0.61345, 0.5032]
	train_accs: [0.50333333, 0.61305, 0.6127167, 0.61345, 0.5032]
	best_train_sub_head: 3
	worst: 0.5032
	avg: 0.56915003
	best: 0.61345

Starting e_i: 569
Model ind 640 epoch 569 head A head_i_epoch 0 batch 0: avg loss -3.120567 avg loss no lamb -3.120567 time 2019-02-23 22:47:11.062125
Model ind 640 epoch 569 head A head_i_epoch 0 batch 100: avg loss -3.027872 avg loss no lamb -3.027872 time 2019-02-23 22:50:32.742454
Model ind 640 epoch 569 head A head_i_epoch 0 batch 200: avg loss -3.127103 avg loss no lamb -3.127103 time 2019-02-23 22:53:56.188393
last batch sz 160
Model ind 640 epoch 569 head B head_i_epoch 0 batch 0: avg loss -1.756749 avg loss no lamb -1.756749 time 2019-02-23 22:56:34.940715
Model ind 640 epoch 569 head B head_i_epoch 0 batch 100: avg loss -1.660179 avg loss no lamb -1.660179 time 2019-02-23 22:59:53.065244
Model ind 640 epoch 569 head B head_i_epoch 0 batch 200: avg loss -1.772381 avg loss no lamb -1.772381 time 2019-02-23 23:03:12.590813
last batch sz 160
Model ind 640 epoch 569 head B head_i_epoch 1 batch 0: avg loss -1.655914 avg loss no lamb -1.655914 time 2019-02-23 23:04:34.397661
Model ind 640 epoch 569 head B head_i_epoch 1 batch 100: avg loss -1.683136 avg loss no lamb -1.683136 time 2019-02-23 23:06:27.573518
Model ind 640 epoch 569 head B head_i_epoch 1 batch 200: avg loss -1.817585 avg loss no lamb -1.817585 time 2019-02-23 23:08:20.097395
last batch sz 160
Pre: time 2019-02-23 23:10:03.774443: 
 	std: 0.05051134
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50621665, 0.60938334, 0.6091, 0.60943335, 0.5061833]
	train_accs: [0.50621665, 0.60938334, 0.6091, 0.60943335, 0.5061833]
	best_train_sub_head: 3
	worst: 0.5061833
	avg: 0.5680634
	best: 0.60943335

Starting e_i: 570
Model ind 640 epoch 570 head A head_i_epoch 0 batch 0: avg loss -3.109105 avg loss no lamb -3.109105 time 2019-02-23 23:10:06.145444
Model ind 640 epoch 570 head A head_i_epoch 0 batch 100: avg loss -2.951880 avg loss no lamb -2.951880 time 2019-02-23 23:11:58.693021
Model ind 640 epoch 570 head A head_i_epoch 0 batch 200: avg loss -3.110007 avg loss no lamb -3.110007 time 2019-02-23 23:13:51.732630
last batch sz 160
Model ind 640 epoch 570 head B head_i_epoch 0 batch 0: avg loss -1.738530 avg loss no lamb -1.738530 time 2019-02-23 23:15:14.220745
Model ind 640 epoch 570 head B head_i_epoch 0 batch 100: avg loss -1.693061 avg loss no lamb -1.693061 time 2019-02-23 23:17:07.928358
Model ind 640 epoch 570 head B head_i_epoch 0 batch 200: avg loss -1.831207 avg loss no lamb -1.831207 time 2019-02-23 23:19:02.468814
last batch sz 160
Model ind 640 epoch 570 head B head_i_epoch 1 batch 0: avg loss -1.742148 avg loss no lamb -1.742148 time 2019-02-23 23:20:24.806610
Model ind 640 epoch 570 head B head_i_epoch 1 batch 100: avg loss -1.705917 avg loss no lamb -1.705917 time 2019-02-23 23:22:20.697829
Model ind 640 epoch 570 head B head_i_epoch 1 batch 200: avg loss -1.778212 avg loss no lamb -1.778212 time 2019-02-23 23:24:14.542959
last batch sz 160
Pre: time 2019-02-23 23:25:59.871887: 
 	std: 0.051496517
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5050167, 0.6101, 0.6102, 0.61035, 0.50518334]
	train_accs: [0.5050167, 0.6101, 0.6102, 0.61035, 0.50518334]
	best_train_sub_head: 3
	worst: 0.5050167
	avg: 0.56816995
	best: 0.61035

Starting e_i: 571
Model ind 640 epoch 571 head A head_i_epoch 0 batch 0: avg loss -3.072169 avg loss no lamb -3.072169 time 2019-02-23 23:26:07.217539
Model ind 640 epoch 571 head A head_i_epoch 0 batch 100: avg loss -3.084749 avg loss no lamb -3.084749 time 2019-02-23 23:28:00.718721
Model ind 640 epoch 571 head A head_i_epoch 0 batch 200: avg loss -3.096940 avg loss no lamb -3.096940 time 2019-02-23 23:29:54.811007
last batch sz 160
Model ind 640 epoch 571 head B head_i_epoch 0 batch 0: avg loss -1.681057 avg loss no lamb -1.681057 time 2019-02-23 23:31:17.825067
Model ind 640 epoch 571 head B head_i_epoch 0 batch 100: avg loss -1.680025 avg loss no lamb -1.680025 time 2019-02-23 23:33:11.929893
Model ind 640 epoch 571 head B head_i_epoch 0 batch 200: avg loss -1.796177 avg loss no lamb -1.796177 time 2019-02-23 23:35:05.316637
last batch sz 160
Model ind 640 epoch 571 head B head_i_epoch 1 batch 0: avg loss -1.739778 avg loss no lamb -1.739778 time 2019-02-23 23:36:26.887857
Model ind 640 epoch 571 head B head_i_epoch 1 batch 100: avg loss -1.657467 avg loss no lamb -1.657467 time 2019-02-23 23:38:20.888478
Model ind 640 epoch 571 head B head_i_epoch 1 batch 200: avg loss -1.806973 avg loss no lamb -1.806973 time 2019-02-23 23:40:14.297731
last batch sz 160
Pre: time 2019-02-23 23:41:56.759589: 
 	std: 0.052334998
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5068333, 0.61406666, 0.61365, 0.6137667, 0.5071667]
	train_accs: [0.5068333, 0.61406666, 0.61365, 0.6137667, 0.5071667]
	best_train_sub_head: 1
	worst: 0.5068333
	avg: 0.57109666
	best: 0.61406666

Starting e_i: 572
Model ind 640 epoch 572 head A head_i_epoch 0 batch 0: avg loss -3.057353 avg loss no lamb -3.057353 time 2019-02-23 23:42:06.605241
Model ind 640 epoch 572 head A head_i_epoch 0 batch 100: avg loss -2.981162 avg loss no lamb -2.981162 time 2019-02-23 23:44:00.472271
Model ind 640 epoch 572 head A head_i_epoch 0 batch 200: avg loss -3.051748 avg loss no lamb -3.051748 time 2019-02-23 23:45:56.148852
last batch sz 160
Model ind 640 epoch 572 head B head_i_epoch 0 batch 0: avg loss -1.731953 avg loss no lamb -1.731953 time 2019-02-23 23:47:19.148874
Model ind 640 epoch 572 head B head_i_epoch 0 batch 100: avg loss -1.723588 avg loss no lamb -1.723588 time 2019-02-23 23:49:12.336693
Model ind 640 epoch 572 head B head_i_epoch 0 batch 200: avg loss -1.862182 avg loss no lamb -1.862182 time 2019-02-23 23:51:05.999949
last batch sz 160
Model ind 640 epoch 572 head B head_i_epoch 1 batch 0: avg loss -1.738646 avg loss no lamb -1.738646 time 2019-02-23 23:52:27.790795
Model ind 640 epoch 572 head B head_i_epoch 1 batch 100: avg loss -1.669894 avg loss no lamb -1.669894 time 2019-02-23 23:54:22.080357
Model ind 640 epoch 572 head B head_i_epoch 1 batch 200: avg loss -1.940791 avg loss no lamb -1.940791 time 2019-02-23 23:56:15.848526
last batch sz 160
Pre: time 2019-02-23 23:58:00.028646: 
 	std: 0.051107656
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5078833, 0.61253333, 0.61205, 0.61253333, 0.5082167]
	train_accs: [0.5078833, 0.61253333, 0.61205, 0.61253333, 0.5082167]
	best_train_sub_head: 1
	worst: 0.5078833
	avg: 0.5706433
	best: 0.61253333

Starting e_i: 573
Model ind 640 epoch 573 head A head_i_epoch 0 batch 0: avg loss -3.035330 avg loss no lamb -3.035330 time 2019-02-23 23:58:02.437646
Model ind 640 epoch 573 head A head_i_epoch 0 batch 100: avg loss -2.978066 avg loss no lamb -2.978066 time 2019-02-23 23:59:56.445997
Model ind 640 epoch 573 head A head_i_epoch 0 batch 200: avg loss -3.060515 avg loss no lamb -3.060515 time 2019-02-24 00:01:50.211121
last batch sz 160
Model ind 640 epoch 573 head B head_i_epoch 0 batch 0: avg loss -1.799538 avg loss no lamb -1.799538 time 2019-02-24 00:03:11.387732
Model ind 640 epoch 573 head B head_i_epoch 0 batch 100: avg loss -1.746250 avg loss no lamb -1.746250 time 2019-02-24 00:05:05.799376
Model ind 640 epoch 573 head B head_i_epoch 0 batch 200: avg loss -1.797858 avg loss no lamb -1.797858 time 2019-02-24 00:06:59.895330
last batch sz 160
Model ind 640 epoch 573 head B head_i_epoch 1 batch 0: avg loss -1.735283 avg loss no lamb -1.735283 time 2019-02-24 00:08:21.581885
Model ind 640 epoch 573 head B head_i_epoch 1 batch 100: avg loss -1.647380 avg loss no lamb -1.647380 time 2019-02-24 00:10:13.910473
Model ind 640 epoch 573 head B head_i_epoch 1 batch 200: avg loss -1.845859 avg loss no lamb -1.845859 time 2019-02-24 00:12:09.263346
last batch sz 160
Pre: time 2019-02-24 00:14:26.821969: 
 	std: 0.05201808
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50481665, 0.61106664, 0.6105, 0.61095, 0.5045]
	train_accs: [0.50481665, 0.61106664, 0.6105, 0.61095, 0.5045]
	best_train_sub_head: 1
	worst: 0.5045
	avg: 0.56836665
	best: 0.61106664

Starting e_i: 574
Model ind 640 epoch 574 head A head_i_epoch 0 batch 0: avg loss -3.020362 avg loss no lamb -3.020362 time 2019-02-24 00:14:30.390303
Model ind 640 epoch 574 head A head_i_epoch 0 batch 100: avg loss -2.915461 avg loss no lamb -2.915461 time 2019-02-24 00:17:01.174674
Model ind 640 epoch 574 head A head_i_epoch 0 batch 200: avg loss -3.130920 avg loss no lamb -3.130920 time 2019-02-24 00:19:27.355583
last batch sz 160
Model ind 640 epoch 574 head B head_i_epoch 0 batch 0: avg loss -1.751719 avg loss no lamb -1.751719 time 2019-02-24 00:21:15.204280
Model ind 640 epoch 574 head B head_i_epoch 0 batch 100: avg loss -1.677601 avg loss no lamb -1.677601 time 2019-02-24 00:23:42.654059
Model ind 640 epoch 574 head B head_i_epoch 0 batch 200: avg loss -1.845869 avg loss no lamb -1.845869 time 2019-02-24 00:26:07.673011
last batch sz 160
Model ind 640 epoch 574 head B head_i_epoch 1 batch 0: avg loss -1.750459 avg loss no lamb -1.750459 time 2019-02-24 00:28:01.749750
Model ind 640 epoch 574 head B head_i_epoch 1 batch 100: avg loss -1.568421 avg loss no lamb -1.568421 time 2019-02-24 00:30:28.835344
Model ind 640 epoch 574 head B head_i_epoch 1 batch 200: avg loss -1.879606 avg loss no lamb -1.879606 time 2019-02-24 00:32:58.704983
last batch sz 160
Pre: time 2019-02-24 00:35:13.171116: 
 	std: 0.052172773
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5044, 0.6109, 0.61095, 0.6109667, 0.50448334]
	train_accs: [0.5044, 0.6109, 0.61095, 0.6109667, 0.50448334]
	best_train_sub_head: 3
	worst: 0.5044
	avg: 0.56834
	best: 0.6109667

Starting e_i: 575
Model ind 640 epoch 575 head A head_i_epoch 0 batch 0: avg loss -3.049194 avg loss no lamb -3.049194 time 2019-02-24 00:35:16.469762
Model ind 640 epoch 575 head A head_i_epoch 0 batch 100: avg loss -3.021268 avg loss no lamb -3.021268 time 2019-02-24 00:37:43.846046
Model ind 640 epoch 575 head A head_i_epoch 0 batch 200: avg loss -3.089700 avg loss no lamb -3.089700 time 2019-02-24 00:40:09.964283
last batch sz 160
Model ind 640 epoch 575 head B head_i_epoch 0 batch 0: avg loss -1.816385 avg loss no lamb -1.816385 time 2019-02-24 00:42:03.455477
Model ind 640 epoch 575 head B head_i_epoch 0 batch 100: avg loss -1.607626 avg loss no lamb -1.607626 time 2019-02-24 00:44:30.572455
Model ind 640 epoch 575 head B head_i_epoch 0 batch 200: avg loss -1.839957 avg loss no lamb -1.839957 time 2019-02-24 00:47:00.686845
last batch sz 160
Model ind 640 epoch 575 head B head_i_epoch 1 batch 0: avg loss -1.748914 avg loss no lamb -1.748914 time 2019-02-24 00:48:51.670613
Model ind 640 epoch 575 head B head_i_epoch 1 batch 100: avg loss -1.713666 avg loss no lamb -1.713666 time 2019-02-24 00:51:20.114499
Model ind 640 epoch 575 head B head_i_epoch 1 batch 200: avg loss -1.870642 avg loss no lamb -1.870642 time 2019-02-24 00:53:48.239096
last batch sz 160
Pre: time 2019-02-24 00:56:00.868780: 
 	std: 0.05171012
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50581664, 0.6113, 0.6113833, 0.61125, 0.5057]
	train_accs: [0.50581664, 0.6113, 0.6113833, 0.61125, 0.5057]
	best_train_sub_head: 2
	worst: 0.5057
	avg: 0.56909
	best: 0.6113833

Starting e_i: 576
Model ind 640 epoch 576 head A head_i_epoch 0 batch 0: avg loss -3.031729 avg loss no lamb -3.031729 time 2019-02-24 00:56:03.951844
Model ind 640 epoch 576 head A head_i_epoch 0 batch 100: avg loss -3.008006 avg loss no lamb -3.008006 time 2019-02-24 00:58:31.730079
Model ind 640 epoch 576 head A head_i_epoch 0 batch 200: avg loss -3.085582 avg loss no lamb -3.085582 time 2019-02-24 01:00:59.988479
last batch sz 160
Model ind 640 epoch 576 head B head_i_epoch 0 batch 0: avg loss -1.795243 avg loss no lamb -1.795243 time 2019-02-24 01:02:49.442727
Model ind 640 epoch 576 head B head_i_epoch 0 batch 100: avg loss -1.720760 avg loss no lamb -1.720760 time 2019-02-24 01:05:17.236485
Model ind 640 epoch 576 head B head_i_epoch 0 batch 200: avg loss -1.808632 avg loss no lamb -1.808632 time 2019-02-24 01:07:47.800959
last batch sz 160
Model ind 640 epoch 576 head B head_i_epoch 1 batch 0: avg loss -1.684858 avg loss no lamb -1.684858 time 2019-02-24 01:09:29.889679
Model ind 640 epoch 576 head B head_i_epoch 1 batch 100: avg loss -1.581758 avg loss no lamb -1.581758 time 2019-02-24 01:12:00.134333
Model ind 640 epoch 576 head B head_i_epoch 1 batch 200: avg loss -1.809049 avg loss no lamb -1.809049 time 2019-02-24 01:14:28.354859
last batch sz 160
Pre: time 2019-02-24 01:16:39.389891: 
 	std: 0.05045565
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101, 0.61298335, 0.6128, 0.61326665, 0.50995]
	train_accs: [0.5101, 0.61298335, 0.6128, 0.61326665, 0.50995]
	best_train_sub_head: 3
	worst: 0.50995
	avg: 0.57181996
	best: 0.61326665

Starting e_i: 577
Model ind 640 epoch 577 head A head_i_epoch 0 batch 0: avg loss -2.999292 avg loss no lamb -2.999292 time 2019-02-24 01:16:43.035234
Model ind 640 epoch 577 head A head_i_epoch 0 batch 100: avg loss -2.962615 avg loss no lamb -2.962615 time 2019-02-24 01:19:12.908239
Model ind 640 epoch 577 head A head_i_epoch 0 batch 200: avg loss -3.092818 avg loss no lamb -3.092818 time 2019-02-24 01:21:40.489915
last batch sz 160
Model ind 640 epoch 577 head B head_i_epoch 0 batch 0: avg loss -1.771977 avg loss no lamb -1.771977 time 2019-02-24 01:23:24.178893
Model ind 640 epoch 577 head B head_i_epoch 0 batch 100: avg loss -1.642783 avg loss no lamb -1.642783 time 2019-02-24 01:25:53.367044
Model ind 640 epoch 577 head B head_i_epoch 0 batch 200: avg loss -1.806617 avg loss no lamb -1.806617 time 2019-02-24 01:28:22.029797
last batch sz 160
Model ind 640 epoch 577 head B head_i_epoch 1 batch 0: avg loss -1.683776 avg loss no lamb -1.683776 time 2019-02-24 01:30:04.344372
Model ind 640 epoch 577 head B head_i_epoch 1 batch 100: avg loss -1.626174 avg loss no lamb -1.626174 time 2019-02-24 01:32:31.042615
Model ind 640 epoch 577 head B head_i_epoch 1 batch 200: avg loss -1.925550 avg loss no lamb -1.925550 time 2019-02-24 01:35:00.335332
last batch sz 160
Pre: time 2019-02-24 01:37:16.867651: 
 	std: 0.050729044
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5093667, 0.61305, 0.6127167, 0.6128333, 0.5092667]
	train_accs: [0.5093667, 0.61305, 0.6127167, 0.6128333, 0.5092667]
	best_train_sub_head: 1
	worst: 0.5092667
	avg: 0.57144666
	best: 0.61305

Starting e_i: 578
Model ind 640 epoch 578 head A head_i_epoch 0 batch 0: avg loss -3.088240 avg loss no lamb -3.088240 time 2019-02-24 01:37:19.595245
Model ind 640 epoch 578 head A head_i_epoch 0 batch 100: avg loss -2.906441 avg loss no lamb -2.906441 time 2019-02-24 01:39:50.568780
Model ind 640 epoch 578 head A head_i_epoch 0 batch 200: avg loss -3.105050 avg loss no lamb -3.105050 time 2019-02-24 01:42:19.911615
last batch sz 160
Model ind 640 epoch 578 head B head_i_epoch 0 batch 0: avg loss -1.801868 avg loss no lamb -1.801868 time 2019-02-24 01:44:08.393344
Model ind 640 epoch 578 head B head_i_epoch 0 batch 100: avg loss -1.629575 avg loss no lamb -1.629575 time 2019-02-24 01:46:37.125189
Model ind 640 epoch 578 head B head_i_epoch 0 batch 200: avg loss -1.842393 avg loss no lamb -1.842393 time 2019-02-24 01:49:04.169094
last batch sz 160
Model ind 640 epoch 578 head B head_i_epoch 1 batch 0: avg loss -1.755686 avg loss no lamb -1.755686 time 2019-02-24 01:50:55.089039
Model ind 640 epoch 578 head B head_i_epoch 1 batch 100: avg loss -1.631503 avg loss no lamb -1.631503 time 2019-02-24 01:53:24.070494
Model ind 640 epoch 578 head B head_i_epoch 1 batch 200: avg loss -1.816162 avg loss no lamb -1.816162 time 2019-02-24 01:55:54.295320
last batch sz 160
Pre: time 2019-02-24 01:58:10.727712: 
 	std: 0.05272586
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5039333, 0.6117333, 0.6116667, 0.61215, 0.50451666]
	train_accs: [0.5039333, 0.6117333, 0.6116667, 0.61215, 0.50451666]
	best_train_sub_head: 3
	worst: 0.5039333
	avg: 0.5688
	best: 0.61215

Starting e_i: 579
Model ind 640 epoch 579 head A head_i_epoch 0 batch 0: avg loss -3.065991 avg loss no lamb -3.065991 time 2019-02-24 01:58:14.115330
Model ind 640 epoch 579 head A head_i_epoch 0 batch 100: avg loss -2.933489 avg loss no lamb -2.933489 time 2019-02-24 02:00:41.763450
Model ind 640 epoch 579 head A head_i_epoch 0 batch 200: avg loss -3.122751 avg loss no lamb -3.122751 time 2019-02-24 02:03:07.679655
last batch sz 160
Model ind 640 epoch 579 head B head_i_epoch 0 batch 0: avg loss -1.746145 avg loss no lamb -1.746145 time 2019-02-24 02:05:01.567342
Model ind 640 epoch 579 head B head_i_epoch 0 batch 100: avg loss -1.612841 avg loss no lamb -1.612841 time 2019-02-24 02:07:27.676848
Model ind 640 epoch 579 head B head_i_epoch 0 batch 200: avg loss -1.839311 avg loss no lamb -1.839311 time 2019-02-24 02:09:53.501893
last batch sz 160
Model ind 640 epoch 579 head B head_i_epoch 1 batch 0: avg loss -1.815155 avg loss no lamb -1.815155 time 2019-02-24 02:11:43.108125
Model ind 640 epoch 579 head B head_i_epoch 1 batch 100: avg loss -1.651462 avg loss no lamb -1.651462 time 2019-02-24 02:14:12.145797
Model ind 640 epoch 579 head B head_i_epoch 1 batch 200: avg loss -1.829000 avg loss no lamb -1.829000 time 2019-02-24 02:16:37.942732
last batch sz 160
Pre: time 2019-02-24 02:18:49.263118: 
 	std: 0.052668218
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5018833, 0.6096, 0.6095167, 0.6095333, 0.5022]
	train_accs: [0.5018833, 0.6096, 0.6095167, 0.6095333, 0.5022]
	best_train_sub_head: 1
	worst: 0.5018833
	avg: 0.5665466
	best: 0.6096

Starting e_i: 580
Model ind 640 epoch 580 head A head_i_epoch 0 batch 0: avg loss -3.050967 avg loss no lamb -3.050967 time 2019-02-24 02:18:52.666765
Model ind 640 epoch 580 head A head_i_epoch 0 batch 100: avg loss -2.954378 avg loss no lamb -2.954378 time 2019-02-24 02:21:20.349347
Model ind 640 epoch 580 head A head_i_epoch 0 batch 200: avg loss -3.098580 avg loss no lamb -3.098580 time 2019-02-24 02:23:49.749050
last batch sz 160
Model ind 640 epoch 580 head B head_i_epoch 0 batch 0: avg loss -1.729373 avg loss no lamb -1.729373 time 2019-02-24 02:25:37.167659
Model ind 640 epoch 580 head B head_i_epoch 0 batch 100: avg loss -1.561416 avg loss no lamb -1.561416 time 2019-02-24 02:28:04.587675
Model ind 640 epoch 580 head B head_i_epoch 0 batch 200: avg loss -1.834195 avg loss no lamb -1.834195 time 2019-02-24 02:30:32.614620
last batch sz 160
Model ind 640 epoch 580 head B head_i_epoch 1 batch 0: avg loss -1.693895 avg loss no lamb -1.693895 time 2019-02-24 02:32:15.058256
Model ind 640 epoch 580 head B head_i_epoch 1 batch 100: avg loss -1.618477 avg loss no lamb -1.618477 time 2019-02-24 02:34:46.973009
Model ind 640 epoch 580 head B head_i_epoch 1 batch 200: avg loss -1.818488 avg loss no lamb -1.818488 time 2019-02-24 02:37:13.446302
last batch sz 160
Pre: time 2019-02-24 02:39:27.483229: 
 	std: 0.052237134
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5050333, 0.6112, 0.61151665, 0.61141664, 0.50446665]
	train_accs: [0.5050333, 0.6112, 0.61151665, 0.61141664, 0.50446665]
	best_train_sub_head: 2
	worst: 0.50446665
	avg: 0.56872666
	best: 0.61151665

Starting e_i: 581
Model ind 640 epoch 581 head A head_i_epoch 0 batch 0: avg loss -3.074814 avg loss no lamb -3.074814 time 2019-02-24 02:39:38.604278
Model ind 640 epoch 581 head A head_i_epoch 0 batch 100: avg loss -2.982920 avg loss no lamb -2.982920 time 2019-02-24 02:42:07.079120
Model ind 640 epoch 581 head A head_i_epoch 0 batch 200: avg loss -3.051730 avg loss no lamb -3.051730 time 2019-02-24 02:44:38.999951
last batch sz 160
Model ind 640 epoch 581 head B head_i_epoch 0 batch 0: avg loss -1.733073 avg loss no lamb -1.733073 time 2019-02-24 02:46:22.331581
Model ind 640 epoch 581 head B head_i_epoch 0 batch 100: avg loss -1.632408 avg loss no lamb -1.632408 time 2019-02-24 02:48:50.941779
Model ind 640 epoch 581 head B head_i_epoch 0 batch 200: avg loss -1.824649 avg loss no lamb -1.824649 time 2019-02-24 02:51:20.619105
last batch sz 160
Model ind 640 epoch 581 head B head_i_epoch 1 batch 0: avg loss -1.769656 avg loss no lamb -1.769656 time 2019-02-24 02:53:06.552655
Model ind 640 epoch 581 head B head_i_epoch 1 batch 100: avg loss -1.691496 avg loss no lamb -1.691496 time 2019-02-24 02:55:34.654062
Model ind 640 epoch 581 head B head_i_epoch 1 batch 200: avg loss -1.802355 avg loss no lamb -1.802355 time 2019-02-24 02:58:01.173406
last batch sz 160
Pre: time 2019-02-24 03:00:19.952790: 
 	std: 0.051276077
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5067, 0.61158335, 0.61145, 0.61141664, 0.50693333]
	train_accs: [0.5067, 0.61158335, 0.61145, 0.61141664, 0.50693333]
	best_train_sub_head: 1
	worst: 0.5067
	avg: 0.5696166
	best: 0.61158335

Starting e_i: 582
Model ind 640 epoch 582 head A head_i_epoch 0 batch 0: avg loss -3.053143 avg loss no lamb -3.053143 time 2019-02-24 03:00:23.172085
Model ind 640 epoch 582 head A head_i_epoch 0 batch 100: avg loss -3.037015 avg loss no lamb -3.037015 time 2019-02-24 03:02:52.963903
Model ind 640 epoch 582 head A head_i_epoch 0 batch 200: avg loss -3.077538 avg loss no lamb -3.077538 time 2019-02-24 03:05:21.926087
last batch sz 160
Model ind 640 epoch 582 head B head_i_epoch 0 batch 0: avg loss -1.765217 avg loss no lamb -1.765217 time 2019-02-24 03:07:10.072971
Model ind 640 epoch 582 head B head_i_epoch 0 batch 100: avg loss -1.706402 avg loss no lamb -1.706402 time 2019-02-24 03:09:36.532114
Model ind 640 epoch 582 head B head_i_epoch 0 batch 200: avg loss -1.834725 avg loss no lamb -1.834725 time 2019-02-24 03:12:04.972572
last batch sz 160
Model ind 640 epoch 582 head B head_i_epoch 1 batch 0: avg loss -1.704976 avg loss no lamb -1.704976 time 2019-02-24 03:13:56.572856
Model ind 640 epoch 582 head B head_i_epoch 1 batch 100: avg loss -1.680040 avg loss no lamb -1.680040 time 2019-02-24 03:16:23.632045
Model ind 640 epoch 582 head B head_i_epoch 1 batch 200: avg loss -1.748994 avg loss no lamb -1.748994 time 2019-02-24 03:18:52.969966
last batch sz 160
Pre: time 2019-02-24 03:21:08.158439: 
 	std: 0.05205275
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50505, 0.61025, 0.61065, 0.61071664, 0.5035333]
	train_accs: [0.50505, 0.61025, 0.61065, 0.61071664, 0.5035333]
	best_train_sub_head: 3
	worst: 0.5035333
	avg: 0.56804
	best: 0.61071664

Starting e_i: 583
Model ind 640 epoch 583 head A head_i_epoch 0 batch 0: avg loss -3.035875 avg loss no lamb -3.035875 time 2019-02-24 03:21:11.326767
Model ind 640 epoch 583 head A head_i_epoch 0 batch 100: avg loss -2.991576 avg loss no lamb -2.991576 time 2019-02-24 03:23:39.181110
Model ind 640 epoch 583 head A head_i_epoch 0 batch 200: avg loss -3.046567 avg loss no lamb -3.046567 time 2019-02-24 03:26:07.723823
last batch sz 160
Model ind 640 epoch 583 head B head_i_epoch 0 batch 0: avg loss -1.715677 avg loss no lamb -1.715677 time 2019-02-24 03:27:59.939626
Model ind 640 epoch 583 head B head_i_epoch 0 batch 100: avg loss -1.599842 avg loss no lamb -1.599842 time 2019-02-24 03:30:25.614405
Model ind 640 epoch 583 head B head_i_epoch 0 batch 200: avg loss -1.837597 avg loss no lamb -1.837597 time 2019-02-24 03:32:51.876046
last batch sz 160
Model ind 640 epoch 583 head B head_i_epoch 1 batch 0: avg loss -1.711412 avg loss no lamb -1.711412 time 2019-02-24 03:34:43.343525
Model ind 640 epoch 583 head B head_i_epoch 1 batch 100: avg loss -1.643256 avg loss no lamb -1.643256 time 2019-02-24 03:37:11.857441
Model ind 640 epoch 583 head B head_i_epoch 1 batch 200: avg loss -1.836357 avg loss no lamb -1.836357 time 2019-02-24 03:39:38.693766
last batch sz 160
Pre: time 2019-02-24 03:41:53.001370: 
 	std: 0.05218368
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5064167, 0.61223334, 0.6123, 0.61251664, 0.50525]
	train_accs: [0.5064167, 0.61223334, 0.6123, 0.61251664, 0.50525]
	best_train_sub_head: 3
	worst: 0.50525
	avg: 0.5697433
	best: 0.61251664

Starting e_i: 584
Model ind 640 epoch 584 head A head_i_epoch 0 batch 0: avg loss -3.045799 avg loss no lamb -3.045799 time 2019-02-24 03:41:55.764588
Model ind 640 epoch 584 head A head_i_epoch 0 batch 100: avg loss -2.975582 avg loss no lamb -2.975582 time 2019-02-24 03:44:23.323925
Model ind 640 epoch 584 head A head_i_epoch 0 batch 200: avg loss -3.079125 avg loss no lamb -3.079125 time 2019-02-24 03:46:50.354743
last batch sz 160
Model ind 640 epoch 584 head B head_i_epoch 0 batch 0: avg loss -1.721547 avg loss no lamb -1.721547 time 2019-02-24 03:48:43.304328
Model ind 640 epoch 584 head B head_i_epoch 0 batch 100: avg loss -1.689694 avg loss no lamb -1.689694 time 2019-02-24 03:51:12.515854
Model ind 640 epoch 584 head B head_i_epoch 0 batch 200: avg loss -1.812174 avg loss no lamb -1.812174 time 2019-02-24 03:53:41.835520
last batch sz 160
Model ind 640 epoch 584 head B head_i_epoch 1 batch 0: avg loss -1.775963 avg loss no lamb -1.775963 time 2019-02-24 03:55:24.901192
Model ind 640 epoch 584 head B head_i_epoch 1 batch 100: avg loss -1.773323 avg loss no lamb -1.773323 time 2019-02-24 03:57:57.118556
Model ind 640 epoch 584 head B head_i_epoch 1 batch 200: avg loss -1.810823 avg loss no lamb -1.810823 time 2019-02-24 04:00:23.938837
last batch sz 160
Pre: time 2019-02-24 04:02:40.447150: 
 	std: 0.051311888
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5096833, 0.6135333, 0.61378336, 0.61396664, 0.50836664]
	train_accs: [0.5096833, 0.6135333, 0.61378336, 0.61396664, 0.50836664]
	best_train_sub_head: 3
	worst: 0.50836664
	avg: 0.57186663
	best: 0.61396664

Starting e_i: 585
Model ind 640 epoch 585 head A head_i_epoch 0 batch 0: avg loss -3.158508 avg loss no lamb -3.158508 time 2019-02-24 04:02:43.362696
Model ind 640 epoch 585 head A head_i_epoch 0 batch 100: avg loss -3.040601 avg loss no lamb -3.040601 time 2019-02-24 04:05:11.800601
Model ind 640 epoch 585 head A head_i_epoch 0 batch 200: avg loss -3.076439 avg loss no lamb -3.076439 time 2019-02-24 04:07:42.757166
last batch sz 160
Model ind 640 epoch 585 head B head_i_epoch 0 batch 0: avg loss -1.745788 avg loss no lamb -1.745788 time 2019-02-24 04:09:24.640583
Model ind 640 epoch 585 head B head_i_epoch 0 batch 100: avg loss -1.722160 avg loss no lamb -1.722160 time 2019-02-24 04:11:50.863924
Model ind 640 epoch 585 head B head_i_epoch 0 batch 200: avg loss -1.796791 avg loss no lamb -1.796791 time 2019-02-24 04:14:23.730471
last batch sz 160
Model ind 640 epoch 585 head B head_i_epoch 1 batch 0: avg loss -1.740870 avg loss no lamb -1.740870 time 2019-02-24 04:16:05.488120
Model ind 640 epoch 585 head B head_i_epoch 1 batch 100: avg loss -1.624159 avg loss no lamb -1.624159 time 2019-02-24 04:18:32.506558
Model ind 640 epoch 585 head B head_i_epoch 1 batch 200: avg loss -1.803783 avg loss no lamb -1.803783 time 2019-02-24 04:21:01.069065
last batch sz 160
Pre: time 2019-02-24 04:23:11.116282: 
 	std: 0.051416323
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50336665, 0.6081167, 0.6080667, 0.60825, 0.50301665]
	train_accs: [0.50336665, 0.6081167, 0.6080667, 0.60825, 0.50301665]
	best_train_sub_head: 3
	worst: 0.50301665
	avg: 0.56616336
	best: 0.60825

Starting e_i: 586
Model ind 640 epoch 586 head A head_i_epoch 0 batch 0: avg loss -3.082861 avg loss no lamb -3.082861 time 2019-02-24 04:23:14.271194
Model ind 640 epoch 586 head A head_i_epoch 0 batch 100: avg loss -2.949926 avg loss no lamb -2.949926 time 2019-02-24 04:25:47.359111
Model ind 640 epoch 586 head A head_i_epoch 0 batch 200: avg loss -3.053372 avg loss no lamb -3.053372 time 2019-02-24 04:28:18.777291
last batch sz 160
Model ind 640 epoch 586 head B head_i_epoch 0 batch 0: avg loss -1.768421 avg loss no lamb -1.768421 time 2019-02-24 04:30:03.361417
Model ind 640 epoch 586 head B head_i_epoch 0 batch 100: avg loss -1.705805 avg loss no lamb -1.705805 time 2019-02-24 04:32:33.637342
Model ind 640 epoch 586 head B head_i_epoch 0 batch 200: avg loss -1.892220 avg loss no lamb -1.892220 time 2019-02-24 04:35:01.592100
last batch sz 160
Model ind 640 epoch 586 head B head_i_epoch 1 batch 0: avg loss -1.802698 avg loss no lamb -1.802698 time 2019-02-24 04:36:49.021788
Model ind 640 epoch 586 head B head_i_epoch 1 batch 100: avg loss -1.570488 avg loss no lamb -1.570488 time 2019-02-24 04:39:15.994667
Model ind 640 epoch 586 head B head_i_epoch 1 batch 200: avg loss -1.793193 avg loss no lamb -1.793193 time 2019-02-24 04:41:42.425679
last batch sz 160
Pre: time 2019-02-24 04:44:00.421420: 
 	std: 0.05148977
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5074833, 0.6124, 0.61275, 0.61263335, 0.5075]
	train_accs: [0.5074833, 0.6124, 0.61275, 0.61263335, 0.5075]
	best_train_sub_head: 2
	worst: 0.5074833
	avg: 0.57055336
	best: 0.61275

Starting e_i: 587
Model ind 640 epoch 587 head A head_i_epoch 0 batch 0: avg loss -3.066262 avg loss no lamb -3.066262 time 2019-02-24 04:44:03.134279
Model ind 640 epoch 587 head A head_i_epoch 0 batch 100: avg loss -2.996925 avg loss no lamb -2.996925 time 2019-02-24 04:46:33.115854
Model ind 640 epoch 587 head A head_i_epoch 0 batch 200: avg loss -3.104787 avg loss no lamb -3.104787 time 2019-02-24 04:49:04.895140
last batch sz 160
Model ind 640 epoch 587 head B head_i_epoch 0 batch 0: avg loss -1.767526 avg loss no lamb -1.767526 time 2019-02-24 04:50:54.992863
Model ind 640 epoch 587 head B head_i_epoch 0 batch 100: avg loss -1.710688 avg loss no lamb -1.710688 time 2019-02-24 04:53:22.668093
Model ind 640 epoch 587 head B head_i_epoch 0 batch 200: avg loss -1.787135 avg loss no lamb -1.787135 time 2019-02-24 04:55:51.009827
last batch sz 160
Model ind 640 epoch 587 head B head_i_epoch 1 batch 0: avg loss -1.766355 avg loss no lamb -1.766355 time 2019-02-24 04:57:43.788269
Model ind 640 epoch 587 head B head_i_epoch 1 batch 100: avg loss -1.729734 avg loss no lamb -1.729734 time 2019-02-24 05:00:10.204206
Model ind 640 epoch 587 head B head_i_epoch 1 batch 200: avg loss -1.879253 avg loss no lamb -1.879253 time 2019-02-24 05:02:39.001847
last batch sz 160
Pre: time 2019-02-24 05:04:53.957598: 
 	std: 0.052266713
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50561666, 0.6123833, 0.6124333, 0.61245, 0.50585]
	train_accs: [0.50561666, 0.6123833, 0.6124333, 0.61245, 0.50585]
	best_train_sub_head: 3
	worst: 0.50561666
	avg: 0.5697467
	best: 0.61245

Starting e_i: 588
Model ind 640 epoch 588 head A head_i_epoch 0 batch 0: avg loss -2.993812 avg loss no lamb -2.993812 time 2019-02-24 05:04:57.361105
Model ind 640 epoch 588 head A head_i_epoch 0 batch 100: avg loss -2.983398 avg loss no lamb -2.983398 time 2019-02-24 05:07:25.011704
Model ind 640 epoch 588 head A head_i_epoch 0 batch 200: avg loss -3.036001 avg loss no lamb -3.036001 time 2019-02-24 05:09:52.021753
last batch sz 160
Model ind 640 epoch 588 head B head_i_epoch 0 batch 0: avg loss -1.788519 avg loss no lamb -1.788519 time 2019-02-24 05:11:44.040363
Model ind 640 epoch 588 head B head_i_epoch 0 batch 100: avg loss -1.709942 avg loss no lamb -1.709942 time 2019-02-24 05:14:10.882721
Model ind 640 epoch 588 head B head_i_epoch 0 batch 200: avg loss -1.870820 avg loss no lamb -1.870820 time 2019-02-24 05:16:37.559116
last batch sz 160
Model ind 640 epoch 588 head B head_i_epoch 1 batch 0: avg loss -1.676652 avg loss no lamb -1.676652 time 2019-02-24 05:18:31.363945
Model ind 640 epoch 588 head B head_i_epoch 1 batch 100: avg loss -1.593706 avg loss no lamb -1.593706 time 2019-02-24 05:21:00.343217
Model ind 640 epoch 588 head B head_i_epoch 1 batch 200: avg loss -1.816514 avg loss no lamb -1.816514 time 2019-02-24 05:23:27.805794
last batch sz 160
Pre: time 2019-02-24 05:25:43.083702: 
 	std: 0.0520898
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50365, 0.60983336, 0.6099667, 0.6098833, 0.50348336]
	train_accs: [0.50365, 0.60983336, 0.6099667, 0.6098833, 0.50348336]
	best_train_sub_head: 2
	worst: 0.50348336
	avg: 0.5673634
	best: 0.6099667

Starting e_i: 589
Model ind 640 epoch 589 head A head_i_epoch 0 batch 0: avg loss -3.057739 avg loss no lamb -3.057739 time 2019-02-24 05:25:45.456906
Model ind 640 epoch 589 head A head_i_epoch 0 batch 100: avg loss -3.059641 avg loss no lamb -3.059641 time 2019-02-24 05:28:12.651723
Model ind 640 epoch 589 head A head_i_epoch 0 batch 200: avg loss -3.036035 avg loss no lamb -3.036035 time 2019-02-24 05:30:39.133501
last batch sz 160
Model ind 640 epoch 589 head B head_i_epoch 0 batch 0: avg loss -1.792339 avg loss no lamb -1.792339 time 2019-02-24 05:32:31.937313
Model ind 640 epoch 589 head B head_i_epoch 0 batch 100: avg loss -1.701066 avg loss no lamb -1.701066 time 2019-02-24 05:35:00.563020
Model ind 640 epoch 589 head B head_i_epoch 0 batch 200: avg loss -1.897513 avg loss no lamb -1.897513 time 2019-02-24 05:37:26.464113
last batch sz 160
Model ind 640 epoch 589 head B head_i_epoch 1 batch 0: avg loss -1.824671 avg loss no lamb -1.824671 time 2019-02-24 05:39:14.652663
Model ind 640 epoch 589 head B head_i_epoch 1 batch 100: avg loss -1.665531 avg loss no lamb -1.665531 time 2019-02-24 05:41:42.763798
Model ind 640 epoch 589 head B head_i_epoch 1 batch 200: avg loss -1.834045 avg loss no lamb -1.834045 time 2019-02-24 05:44:13.426646
last batch sz 160
Pre: time 2019-02-24 05:46:27.001629: 
 	std: 0.05176327
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5060667, 0.61163336, 0.6116667, 0.61148334, 0.5058]
	train_accs: [0.5060667, 0.61163336, 0.6116667, 0.61148334, 0.5058]
	best_train_sub_head: 2
	worst: 0.5058
	avg: 0.56933004
	best: 0.6116667

Starting e_i: 590
Model ind 640 epoch 590 head A head_i_epoch 0 batch 0: avg loss -3.128305 avg loss no lamb -3.128305 time 2019-02-24 05:46:30.283148
Model ind 640 epoch 590 head A head_i_epoch 0 batch 100: avg loss -3.050853 avg loss no lamb -3.050853 time 2019-02-24 05:48:56.966164
Model ind 640 epoch 590 head A head_i_epoch 0 batch 200: avg loss -3.097411 avg loss no lamb -3.097411 time 2019-02-24 05:51:26.538282
last batch sz 160
Model ind 640 epoch 590 head B head_i_epoch 0 batch 0: avg loss -1.810040 avg loss no lamb -1.810040 time 2019-02-24 05:53:08.910195
Model ind 640 epoch 590 head B head_i_epoch 0 batch 100: avg loss -1.576792 avg loss no lamb -1.576792 time 2019-02-24 05:55:36.027488
Model ind 640 epoch 590 head B head_i_epoch 0 batch 200: avg loss -1.816221 avg loss no lamb -1.816221 time 2019-02-24 05:58:06.620015
last batch sz 160
Model ind 640 epoch 590 head B head_i_epoch 1 batch 0: avg loss -1.695359 avg loss no lamb -1.695359 time 2019-02-24 05:59:48.136845
Model ind 640 epoch 590 head B head_i_epoch 1 batch 100: avg loss -1.715181 avg loss no lamb -1.715181 time 2019-02-24 06:02:16.221873
Model ind 640 epoch 590 head B head_i_epoch 1 batch 200: avg loss -1.837522 avg loss no lamb -1.837522 time 2019-02-24 06:04:47.840602
last batch sz 160
Pre: time 2019-02-24 06:07:00.450044: 
 	std: 0.052217137
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5056, 0.6117, 0.6120667, 0.6116167, 0.50481665]
	train_accs: [0.5056, 0.6117, 0.6120667, 0.6116167, 0.50481665]
	best_train_sub_head: 2
	worst: 0.50481665
	avg: 0.56916
	best: 0.6120667

Starting e_i: 591
Model ind 640 epoch 591 head A head_i_epoch 0 batch 0: avg loss -3.034592 avg loss no lamb -3.034592 time 2019-02-24 06:07:08.080121
Model ind 640 epoch 591 head A head_i_epoch 0 batch 100: avg loss -2.958340 avg loss no lamb -2.958340 time 2019-02-24 06:09:35.693084
Model ind 640 epoch 591 head A head_i_epoch 0 batch 200: avg loss -3.122641 avg loss no lamb -3.122641 time 2019-02-24 06:12:08.022674
last batch sz 160
Model ind 640 epoch 591 head B head_i_epoch 0 batch 0: avg loss -1.785330 avg loss no lamb -1.785330 time 2019-02-24 06:13:52.530916
Model ind 640 epoch 591 head B head_i_epoch 0 batch 100: avg loss -1.664117 avg loss no lamb -1.664117 time 2019-02-24 06:16:19.379935
Model ind 640 epoch 591 head B head_i_epoch 0 batch 200: avg loss -1.770535 avg loss no lamb -1.770535 time 2019-02-24 06:18:50.128004
last batch sz 160
Model ind 640 epoch 591 head B head_i_epoch 1 batch 0: avg loss -1.770559 avg loss no lamb -1.770559 time 2019-02-24 06:20:34.364813
Model ind 640 epoch 591 head B head_i_epoch 1 batch 100: avg loss -1.626666 avg loss no lamb -1.626666 time 2019-02-24 06:23:03.113524
Model ind 640 epoch 591 head B head_i_epoch 1 batch 200: avg loss -1.807842 avg loss no lamb -1.807842 time 2019-02-24 06:25:31.760829
last batch sz 160
Pre: time 2019-02-24 06:27:46.684865: 
 	std: 0.050372653
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51276666, 0.6153167, 0.6153833, 0.6153167, 0.5122667]
	train_accs: [0.51276666, 0.6153167, 0.6153833, 0.6153167, 0.5122667]
	best_train_sub_head: 2
	worst: 0.5122667
	avg: 0.57421
	best: 0.6153833

Starting e_i: 592
Model ind 640 epoch 592 head A head_i_epoch 0 batch 0: avg loss -3.100410 avg loss no lamb -3.100410 time 2019-02-24 06:27:54.622630
Model ind 640 epoch 592 head A head_i_epoch 0 batch 100: avg loss -2.943213 avg loss no lamb -2.943213 time 2019-02-24 06:30:21.539903
Model ind 640 epoch 592 head A head_i_epoch 0 batch 200: avg loss -3.108764 avg loss no lamb -3.108764 time 2019-02-24 06:32:54.262224
last batch sz 160
Model ind 640 epoch 592 head B head_i_epoch 0 batch 0: avg loss -1.805973 avg loss no lamb -1.805973 time 2019-02-24 06:34:37.273207
Model ind 640 epoch 592 head B head_i_epoch 0 batch 100: avg loss -1.722028 avg loss no lamb -1.722028 time 2019-02-24 06:37:03.365798
Model ind 640 epoch 592 head B head_i_epoch 0 batch 200: avg loss -1.829293 avg loss no lamb -1.829293 time 2019-02-24 06:39:35.148419
last batch sz 160
Model ind 640 epoch 592 head B head_i_epoch 1 batch 0: avg loss -1.759329 avg loss no lamb -1.759329 time 2019-02-24 06:41:23.506676
Model ind 640 epoch 592 head B head_i_epoch 1 batch 100: avg loss -1.698054 avg loss no lamb -1.698054 time 2019-02-24 06:43:51.623687
Model ind 640 epoch 592 head B head_i_epoch 1 batch 200: avg loss -1.806602 avg loss no lamb -1.806602 time 2019-02-24 06:46:17.995175
last batch sz 160
Pre: time 2019-02-24 06:48:34.685216: 
 	std: 0.05218386
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50265, 0.60913336, 0.6101, 0.6095167, 0.50348336]
	train_accs: [0.50265, 0.60913336, 0.6101, 0.6095167, 0.50348336]
	best_train_sub_head: 2
	worst: 0.50265
	avg: 0.56697667
	best: 0.6101

Starting e_i: 593
Model ind 640 epoch 593 head A head_i_epoch 0 batch 0: avg loss -3.149950 avg loss no lamb -3.149950 time 2019-02-24 06:48:37.315804
Model ind 640 epoch 593 head A head_i_epoch 0 batch 100: avg loss -3.000795 avg loss no lamb -3.000795 time 2019-02-24 06:51:04.532163
Model ind 640 epoch 593 head A head_i_epoch 0 batch 200: avg loss -3.151258 avg loss no lamb -3.151258 time 2019-02-24 06:53:30.215959
last batch sz 160
Model ind 640 epoch 593 head B head_i_epoch 0 batch 0: avg loss -1.708486 avg loss no lamb -1.708486 time 2019-02-24 06:55:21.548474
Model ind 640 epoch 593 head B head_i_epoch 0 batch 100: avg loss -1.685666 avg loss no lamb -1.685666 time 2019-02-24 06:57:51.466028
Model ind 640 epoch 593 head B head_i_epoch 0 batch 200: avg loss -1.818719 avg loss no lamb -1.818719 time 2019-02-24 07:00:18.351217
last batch sz 160
Model ind 640 epoch 593 head B head_i_epoch 1 batch 0: avg loss -1.716480 avg loss no lamb -1.716480 time 2019-02-24 07:02:12.837514
Model ind 640 epoch 593 head B head_i_epoch 1 batch 100: avg loss -1.682974 avg loss no lamb -1.682974 time 2019-02-24 07:04:40.815082
Model ind 640 epoch 593 head B head_i_epoch 1 batch 200: avg loss -1.869364 avg loss no lamb -1.869364 time 2019-02-24 07:07:07.543302
last batch sz 160
Pre: time 2019-02-24 07:09:26.339638: 
 	std: 0.051694166
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50605, 0.61155, 0.612, 0.6113833, 0.5062]
	train_accs: [0.50605, 0.61155, 0.612, 0.6113833, 0.5062]
	best_train_sub_head: 2
	worst: 0.50605
	avg: 0.56943667
	best: 0.612

Starting e_i: 594
Model ind 640 epoch 594 head A head_i_epoch 0 batch 0: avg loss -3.101128 avg loss no lamb -3.101128 time 2019-02-24 07:09:28.742021
Model ind 640 epoch 594 head A head_i_epoch 0 batch 100: avg loss -2.955432 avg loss no lamb -2.955432 time 2019-02-24 07:11:55.414513
Model ind 640 epoch 594 head A head_i_epoch 0 batch 200: avg loss -3.099716 avg loss no lamb -3.099716 time 2019-02-24 07:14:25.615176
last batch sz 160
Model ind 640 epoch 594 head B head_i_epoch 0 batch 0: avg loss -1.755637 avg loss no lamb -1.755637 time 2019-02-24 07:16:17.857467
Model ind 640 epoch 594 head B head_i_epoch 0 batch 100: avg loss -1.642893 avg loss no lamb -1.642893 time 2019-02-24 07:18:45.570540
Model ind 640 epoch 594 head B head_i_epoch 0 batch 200: avg loss -1.858176 avg loss no lamb -1.858176 time 2019-02-24 07:21:12.346807
last batch sz 160
Model ind 640 epoch 594 head B head_i_epoch 1 batch 0: avg loss -1.716184 avg loss no lamb -1.716184 time 2019-02-24 07:22:59.494194
Model ind 640 epoch 594 head B head_i_epoch 1 batch 100: avg loss -1.713107 avg loss no lamb -1.713107 time 2019-02-24 07:25:29.602257
Model ind 640 epoch 594 head B head_i_epoch 1 batch 200: avg loss -1.844473 avg loss no lamb -1.844473 time 2019-02-24 07:28:00.117255
last batch sz 160
Pre: time 2019-02-24 07:30:15.297881: 
 	std: 0.052340135
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5039833, 0.61088336, 0.6117167, 0.61088336, 0.5046667]
	train_accs: [0.5039833, 0.61088336, 0.6117167, 0.61088336, 0.5046667]
	best_train_sub_head: 2
	worst: 0.5039833
	avg: 0.5684267
	best: 0.6117167

Starting e_i: 595
Model ind 640 epoch 595 head A head_i_epoch 0 batch 0: avg loss -3.059415 avg loss no lamb -3.059415 time 2019-02-24 07:30:17.996786
Model ind 640 epoch 595 head A head_i_epoch 0 batch 100: avg loss -2.987589 avg loss no lamb -2.987589 time 2019-02-24 07:32:43.725552
Model ind 640 epoch 595 head A head_i_epoch 0 batch 200: avg loss -3.066760 avg loss no lamb -3.066760 time 2019-02-24 07:35:12.694392
last batch sz 160
Model ind 640 epoch 595 head B head_i_epoch 0 batch 0: avg loss -1.756341 avg loss no lamb -1.756341 time 2019-02-24 07:36:59.866633
Model ind 640 epoch 595 head B head_i_epoch 0 batch 100: avg loss -1.697718 avg loss no lamb -1.697718 time 2019-02-24 07:39:26.949537
Model ind 640 epoch 595 head B head_i_epoch 0 batch 200: avg loss -1.853914 avg loss no lamb -1.853914 time 2019-02-24 07:41:54.264888
last batch sz 160
Model ind 640 epoch 595 head B head_i_epoch 1 batch 0: avg loss -1.724393 avg loss no lamb -1.724393 time 2019-02-24 07:43:42.156806
Model ind 640 epoch 595 head B head_i_epoch 1 batch 100: avg loss -1.636663 avg loss no lamb -1.636663 time 2019-02-24 07:46:12.618773
Model ind 640 epoch 595 head B head_i_epoch 1 batch 200: avg loss -1.788931 avg loss no lamb -1.788931 time 2019-02-24 07:48:40.825289
last batch sz 160
Pre: time 2019-02-24 07:50:55.229940: 
 	std: 0.05260436
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50378335, 0.61121666, 0.6120833, 0.6116, 0.5047333]
	train_accs: [0.50378335, 0.61121666, 0.6120833, 0.6116, 0.5047333]
	best_train_sub_head: 2
	worst: 0.50378335
	avg: 0.5686833
	best: 0.6120833

Starting e_i: 596
Model ind 640 epoch 596 head A head_i_epoch 0 batch 0: avg loss -3.078153 avg loss no lamb -3.078153 time 2019-02-24 07:50:57.677602
Model ind 640 epoch 596 head A head_i_epoch 0 batch 100: avg loss -3.017992 avg loss no lamb -3.017992 time 2019-02-24 07:53:32.394846
Model ind 640 epoch 596 head A head_i_epoch 0 batch 200: avg loss -3.127219 avg loss no lamb -3.127219 time 2019-02-24 07:55:59.423250
last batch sz 160
Model ind 640 epoch 596 head B head_i_epoch 0 batch 0: avg loss -1.797803 avg loss no lamb -1.797803 time 2019-02-24 07:57:43.057276
Model ind 640 epoch 596 head B head_i_epoch 0 batch 100: avg loss -1.661580 avg loss no lamb -1.661580 time 2019-02-24 08:00:15.980143
Model ind 640 epoch 596 head B head_i_epoch 0 batch 200: avg loss -1.884419 avg loss no lamb -1.884419 time 2019-02-24 08:02:43.100325
last batch sz 160
Model ind 640 epoch 596 head B head_i_epoch 1 batch 0: avg loss -1.731545 avg loss no lamb -1.731545 time 2019-02-24 08:04:23.701455
Model ind 640 epoch 596 head B head_i_epoch 1 batch 100: avg loss -1.694856 avg loss no lamb -1.694856 time 2019-02-24 08:06:16.869912
Model ind 640 epoch 596 head B head_i_epoch 1 batch 200: avg loss -1.811202 avg loss no lamb -1.811202 time 2019-02-24 08:08:09.545788
last batch sz 160
Pre: time 2019-02-24 08:09:53.221776: 
 	std: 0.053012464
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5015333, 0.60961664, 0.60973334, 0.6098833, 0.5015333]
	train_accs: [0.5015333, 0.60961664, 0.60973334, 0.6098833, 0.5015333]
	best_train_sub_head: 3
	worst: 0.5015333
	avg: 0.56646
	best: 0.6098833

Starting e_i: 597
Model ind 640 epoch 597 head A head_i_epoch 0 batch 0: avg loss -3.034957 avg loss no lamb -3.034957 time 2019-02-24 08:09:55.579114
Model ind 640 epoch 597 head A head_i_epoch 0 batch 100: avg loss -3.018234 avg loss no lamb -3.018234 time 2019-02-24 08:11:49.471702
Model ind 640 epoch 597 head A head_i_epoch 0 batch 200: avg loss -3.159692 avg loss no lamb -3.159692 time 2019-02-24 08:13:40.215527
last batch sz 160
Model ind 640 epoch 597 head B head_i_epoch 0 batch 0: avg loss -1.737014 avg loss no lamb -1.737014 time 2019-02-24 08:15:04.801586
Model ind 640 epoch 597 head B head_i_epoch 0 batch 100: avg loss -1.756651 avg loss no lamb -1.756651 time 2019-02-24 08:16:59.159055
Model ind 640 epoch 597 head B head_i_epoch 0 batch 200: avg loss -1.760233 avg loss no lamb -1.760233 time 2019-02-24 08:18:53.061415
last batch sz 160
Model ind 640 epoch 597 head B head_i_epoch 1 batch 0: avg loss -1.740209 avg loss no lamb -1.740209 time 2019-02-24 08:20:15.431318
Model ind 640 epoch 597 head B head_i_epoch 1 batch 100: avg loss -1.610899 avg loss no lamb -1.610899 time 2019-02-24 08:22:08.883396
Model ind 640 epoch 597 head B head_i_epoch 1 batch 200: avg loss -1.825124 avg loss no lamb -1.825124 time 2019-02-24 08:24:01.494061
last batch sz 160
Pre: time 2019-02-24 08:25:45.500258: 
 	std: 0.051092397
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51065, 0.615, 0.61495, 0.6147, 0.51053333]
	train_accs: [0.51065, 0.615, 0.61495, 0.6147, 0.51053333]
	best_train_sub_head: 1
	worst: 0.51053333
	avg: 0.57316667
	best: 0.615

Starting e_i: 598
Model ind 640 epoch 598 head A head_i_epoch 0 batch 0: avg loss -3.064738 avg loss no lamb -3.064738 time 2019-02-24 08:25:48.038013
Model ind 640 epoch 598 head A head_i_epoch 0 batch 100: avg loss -2.986217 avg loss no lamb -2.986217 time 2019-02-24 08:27:40.918298
Model ind 640 epoch 598 head A head_i_epoch 0 batch 200: avg loss -3.098843 avg loss no lamb -3.098843 time 2019-02-24 08:29:34.856612
last batch sz 160
Model ind 640 epoch 598 head B head_i_epoch 0 batch 0: avg loss -1.782968 avg loss no lamb -1.782968 time 2019-02-24 08:30:57.484378
Model ind 640 epoch 598 head B head_i_epoch 0 batch 100: avg loss -1.695643 avg loss no lamb -1.695643 time 2019-02-24 08:32:49.681561
Model ind 640 epoch 598 head B head_i_epoch 0 batch 200: avg loss -1.879707 avg loss no lamb -1.879707 time 2019-02-24 08:34:43.053164
last batch sz 160
Model ind 640 epoch 598 head B head_i_epoch 1 batch 0: avg loss -1.775567 avg loss no lamb -1.775567 time 2019-02-24 08:36:04.672199
Model ind 640 epoch 598 head B head_i_epoch 1 batch 100: avg loss -1.645851 avg loss no lamb -1.645851 time 2019-02-24 08:37:58.257907
Model ind 640 epoch 598 head B head_i_epoch 1 batch 200: avg loss -1.901127 avg loss no lamb -1.901127 time 2019-02-24 08:39:52.182013
last batch sz 160
Pre: time 2019-02-24 08:41:35.996102: 
 	std: 0.052380275
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5043, 0.6114333, 0.61083335, 0.6116167, 0.50445]
	train_accs: [0.5043, 0.6114333, 0.61083335, 0.6116167, 0.50445]
	best_train_sub_head: 3
	worst: 0.5043
	avg: 0.5685267
	best: 0.6116167

Starting e_i: 599
Model ind 640 epoch 599 head A head_i_epoch 0 batch 0: avg loss -3.005375 avg loss no lamb -3.005375 time 2019-02-24 08:41:38.375044
Model ind 640 epoch 599 head A head_i_epoch 0 batch 100: avg loss -2.966151 avg loss no lamb -2.966151 time 2019-02-24 08:43:34.692962
Model ind 640 epoch 599 head A head_i_epoch 0 batch 200: avg loss -3.127251 avg loss no lamb -3.127251 time 2019-02-24 08:45:29.242878
last batch sz 160
Model ind 640 epoch 599 head B head_i_epoch 0 batch 0: avg loss -1.728804 avg loss no lamb -1.728804 time 2019-02-24 08:46:51.877532
Model ind 640 epoch 599 head B head_i_epoch 0 batch 100: avg loss -1.671959 avg loss no lamb -1.671959 time 2019-02-24 08:48:45.117305
Model ind 640 epoch 599 head B head_i_epoch 0 batch 200: avg loss -1.846760 avg loss no lamb -1.846760 time 2019-02-24 08:50:39.278906
last batch sz 160
Model ind 640 epoch 599 head B head_i_epoch 1 batch 0: avg loss -1.702804 avg loss no lamb -1.702804 time 2019-02-24 08:52:02.207800
Model ind 640 epoch 599 head B head_i_epoch 1 batch 100: avg loss -1.590742 avg loss no lamb -1.590742 time 2019-02-24 08:53:55.767331
Model ind 640 epoch 599 head B head_i_epoch 1 batch 200: avg loss -1.865206 avg loss no lamb -1.865206 time 2019-02-24 08:55:49.344419
last batch sz 160
Pre: time 2019-02-24 08:57:31.809972: 
 	std: 0.05282973
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50401664, 0.6116, 0.61116666, 0.6113667, 0.50306666]
	train_accs: [0.50401664, 0.6116, 0.61116666, 0.6113667, 0.50306666]
	best_train_sub_head: 1
	worst: 0.50306666
	avg: 0.5682433
	best: 0.6116

Starting e_i: 600
Model ind 640 epoch 600 head A head_i_epoch 0 batch 0: avg loss -3.100311 avg loss no lamb -3.100311 time 2019-02-24 08:57:34.049256
Model ind 640 epoch 600 head A head_i_epoch 0 batch 100: avg loss -3.030940 avg loss no lamb -3.030940 time 2019-02-24 08:59:29.114474
Model ind 640 epoch 600 head A head_i_epoch 0 batch 200: avg loss -3.152124 avg loss no lamb -3.152124 time 2019-02-24 09:01:21.724449
last batch sz 160
Model ind 640 epoch 600 head B head_i_epoch 0 batch 0: avg loss -1.749261 avg loss no lamb -1.749261 time 2019-02-24 09:02:43.285065
Model ind 640 epoch 600 head B head_i_epoch 0 batch 100: avg loss -1.627469 avg loss no lamb -1.627469 time 2019-02-24 09:04:36.558989
Model ind 640 epoch 600 head B head_i_epoch 0 batch 200: avg loss -1.810251 avg loss no lamb -1.810251 time 2019-02-24 09:06:30.852530
last batch sz 160
Model ind 640 epoch 600 head B head_i_epoch 1 batch 0: avg loss -1.712906 avg loss no lamb -1.712906 time 2019-02-24 09:07:53.844902
Model ind 640 epoch 600 head B head_i_epoch 1 batch 100: avg loss -1.638667 avg loss no lamb -1.638667 time 2019-02-24 09:09:47.504841
Model ind 640 epoch 600 head B head_i_epoch 1 batch 200: avg loss -1.851983 avg loss no lamb -1.851983 time 2019-02-24 09:11:41.273976
last batch sz 160
Pre: time 2019-02-24 09:13:25.333679: 
 	std: 0.051391747
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50621665, 0.61111665, 0.6112667, 0.6113, 0.5064333]
	train_accs: [0.50621665, 0.61111665, 0.6112667, 0.6113, 0.5064333]
	best_train_sub_head: 3
	worst: 0.50621665
	avg: 0.5692667
	best: 0.6113

Starting e_i: 601
Model ind 640 epoch 601 head A head_i_epoch 0 batch 0: avg loss -3.107509 avg loss no lamb -3.107509 time 2019-02-24 09:13:31.785305
Model ind 640 epoch 601 head A head_i_epoch 0 batch 100: avg loss -2.913086 avg loss no lamb -2.913086 time 2019-02-24 09:15:24.435207
Model ind 640 epoch 601 head A head_i_epoch 0 batch 200: avg loss -3.083192 avg loss no lamb -3.083192 time 2019-02-24 09:17:17.734594
last batch sz 160
Model ind 640 epoch 601 head B head_i_epoch 0 batch 0: avg loss -1.802601 avg loss no lamb -1.802601 time 2019-02-24 09:18:39.905012
Model ind 640 epoch 601 head B head_i_epoch 0 batch 100: avg loss -1.687377 avg loss no lamb -1.687377 time 2019-02-24 09:20:31.279648
Model ind 640 epoch 601 head B head_i_epoch 0 batch 200: avg loss -1.871976 avg loss no lamb -1.871976 time 2019-02-24 09:22:25.788721
last batch sz 160
Model ind 640 epoch 601 head B head_i_epoch 1 batch 0: avg loss -1.694903 avg loss no lamb -1.694903 time 2019-02-24 09:23:47.860038
Model ind 640 epoch 601 head B head_i_epoch 1 batch 100: avg loss -1.611568 avg loss no lamb -1.611568 time 2019-02-24 09:25:40.557511
Model ind 640 epoch 601 head B head_i_epoch 1 batch 200: avg loss -1.835612 avg loss no lamb -1.835612 time 2019-02-24 09:27:33.423438
last batch sz 160
Pre: time 2019-02-24 09:29:17.510587: 
 	std: 0.052211683
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50295, 0.61005, 0.6098167, 0.61003333, 0.50383335]
	train_accs: [0.50295, 0.61005, 0.6098167, 0.61003333, 0.50383335]
	best_train_sub_head: 1
	worst: 0.50295
	avg: 0.5673367
	best: 0.61005

Starting e_i: 602
Model ind 640 epoch 602 head A head_i_epoch 0 batch 0: avg loss -3.087106 avg loss no lamb -3.087106 time 2019-02-24 09:29:20.123679
Model ind 640 epoch 602 head A head_i_epoch 0 batch 100: avg loss -3.035915 avg loss no lamb -3.035915 time 2019-02-24 09:31:14.049406
Model ind 640 epoch 602 head A head_i_epoch 0 batch 200: avg loss -3.126445 avg loss no lamb -3.126445 time 2019-02-24 09:33:06.473361
last batch sz 160
Model ind 640 epoch 602 head B head_i_epoch 0 batch 0: avg loss -1.676666 avg loss no lamb -1.676666 time 2019-02-24 09:34:28.521131
Model ind 640 epoch 602 head B head_i_epoch 0 batch 100: avg loss -1.669053 avg loss no lamb -1.669053 time 2019-02-24 09:36:24.767630
Model ind 640 epoch 602 head B head_i_epoch 0 batch 200: avg loss -1.796900 avg loss no lamb -1.796900 time 2019-02-24 09:38:20.944930
last batch sz 160
Model ind 640 epoch 602 head B head_i_epoch 1 batch 0: avg loss -1.750539 avg loss no lamb -1.750539 time 2019-02-24 09:39:43.967899
Model ind 640 epoch 602 head B head_i_epoch 1 batch 100: avg loss -1.586116 avg loss no lamb -1.586116 time 2019-02-24 09:41:35.569542
Model ind 640 epoch 602 head B head_i_epoch 1 batch 200: avg loss -1.760679 avg loss no lamb -1.760679 time 2019-02-24 09:43:32.985034
last batch sz 160
Pre: time 2019-02-24 09:45:17.647126: 
 	std: 0.052096073
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50661665, 0.6123833, 0.6124333, 0.61275, 0.50575]
	train_accs: [0.50661665, 0.6123833, 0.6124333, 0.61275, 0.50575]
	best_train_sub_head: 3
	worst: 0.50575
	avg: 0.56998664
	best: 0.61275

Starting e_i: 603
Model ind 640 epoch 603 head A head_i_epoch 0 batch 0: avg loss -3.107867 avg loss no lamb -3.107867 time 2019-02-24 09:45:20.070541
Model ind 640 epoch 603 head A head_i_epoch 0 batch 100: avg loss -3.088368 avg loss no lamb -3.088368 time 2019-02-24 09:47:14.134174
Model ind 640 epoch 603 head A head_i_epoch 0 batch 200: avg loss -3.088122 avg loss no lamb -3.088122 time 2019-02-24 09:49:07.481869
last batch sz 160
Model ind 640 epoch 603 head B head_i_epoch 0 batch 0: avg loss -1.825027 avg loss no lamb -1.825027 time 2019-02-24 09:50:30.616641
Model ind 640 epoch 603 head B head_i_epoch 0 batch 100: avg loss -1.671718 avg loss no lamb -1.671718 time 2019-02-24 09:52:24.385069
Model ind 640 epoch 603 head B head_i_epoch 0 batch 200: avg loss -1.833197 avg loss no lamb -1.833197 time 2019-02-24 09:54:18.579799
last batch sz 160
Model ind 640 epoch 603 head B head_i_epoch 1 batch 0: avg loss -1.767325 avg loss no lamb -1.767325 time 2019-02-24 09:55:41.497681
Model ind 640 epoch 603 head B head_i_epoch 1 batch 100: avg loss -1.688191 avg loss no lamb -1.688191 time 2019-02-24 09:57:35.833187
Model ind 640 epoch 603 head B head_i_epoch 1 batch 200: avg loss -1.869810 avg loss no lamb -1.869810 time 2019-02-24 09:59:29.758531
last batch sz 160
Pre: time 2019-02-24 10:01:14.157645: 
 	std: 0.051925503
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50493336, 0.6106333, 0.61128336, 0.6109833, 0.5050167]
	train_accs: [0.50493336, 0.6106333, 0.61128336, 0.6109833, 0.5050167]
	best_train_sub_head: 2
	worst: 0.50493336
	avg: 0.56857
	best: 0.61128336

Starting e_i: 604
Model ind 640 epoch 604 head A head_i_epoch 0 batch 0: avg loss -3.062387 avg loss no lamb -3.062387 time 2019-02-24 10:01:16.760821
Model ind 640 epoch 604 head A head_i_epoch 0 batch 100: avg loss -3.104880 avg loss no lamb -3.104880 time 2019-02-24 10:03:10.217571
Model ind 640 epoch 604 head A head_i_epoch 0 batch 200: avg loss -3.152881 avg loss no lamb -3.152881 time 2019-02-24 10:05:03.641990
last batch sz 160
Model ind 640 epoch 604 head B head_i_epoch 0 batch 0: avg loss -1.772079 avg loss no lamb -1.772079 time 2019-02-24 10:06:26.546057
Model ind 640 epoch 604 head B head_i_epoch 0 batch 100: avg loss -1.646200 avg loss no lamb -1.646200 time 2019-02-24 10:08:20.687813
Model ind 640 epoch 604 head B head_i_epoch 0 batch 200: avg loss -1.806878 avg loss no lamb -1.806878 time 2019-02-24 10:10:13.680236
last batch sz 160
Model ind 640 epoch 604 head B head_i_epoch 1 batch 0: avg loss -1.660281 avg loss no lamb -1.660281 time 2019-02-24 10:11:35.921676
Model ind 640 epoch 604 head B head_i_epoch 1 batch 100: avg loss -1.755664 avg loss no lamb -1.755664 time 2019-02-24 10:13:28.923554
Model ind 640 epoch 604 head B head_i_epoch 1 batch 200: avg loss -1.817036 avg loss no lamb -1.817036 time 2019-02-24 10:15:22.542014
last batch sz 160
Pre: time 2019-02-24 10:17:06.780528: 
 	std: 0.05283299
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5060833, 0.61385, 0.6142167, 0.61411667, 0.50635]
	train_accs: [0.5060833, 0.61385, 0.6142167, 0.61411667, 0.50635]
	best_train_sub_head: 2
	worst: 0.5060833
	avg: 0.5709233
	best: 0.6142167

Starting e_i: 605
Model ind 640 epoch 605 head A head_i_epoch 0 batch 0: avg loss -3.127379 avg loss no lamb -3.127379 time 2019-02-24 10:17:09.208687
Model ind 640 epoch 605 head A head_i_epoch 0 batch 100: avg loss -3.035382 avg loss no lamb -3.035382 time 2019-02-24 10:19:03.680322
Model ind 640 epoch 605 head A head_i_epoch 0 batch 200: avg loss -3.199898 avg loss no lamb -3.199898 time 2019-02-24 10:20:58.079924
last batch sz 160
Model ind 640 epoch 605 head B head_i_epoch 0 batch 0: avg loss -1.737026 avg loss no lamb -1.737026 time 2019-02-24 10:22:21.322769
Model ind 640 epoch 605 head B head_i_epoch 0 batch 100: avg loss -1.696754 avg loss no lamb -1.696754 time 2019-02-24 10:24:15.998532
Model ind 640 epoch 605 head B head_i_epoch 0 batch 200: avg loss -1.850911 avg loss no lamb -1.850911 time 2019-02-24 10:26:07.986016
last batch sz 160
Model ind 640 epoch 605 head B head_i_epoch 1 batch 0: avg loss -1.744965 avg loss no lamb -1.744965 time 2019-02-24 10:27:32.109838
Model ind 640 epoch 605 head B head_i_epoch 1 batch 100: avg loss -1.633559 avg loss no lamb -1.633559 time 2019-02-24 10:29:26.644242
Model ind 640 epoch 605 head B head_i_epoch 1 batch 200: avg loss -1.817375 avg loss no lamb -1.817375 time 2019-02-24 10:31:19.740332
last batch sz 160
Pre: time 2019-02-24 10:33:04.906385: 
 	std: 0.051394533
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50841665, 0.6131333, 0.6135167, 0.6134, 0.50846666]
	train_accs: [0.50841665, 0.6131333, 0.6135167, 0.6134, 0.50846666]
	best_train_sub_head: 2
	worst: 0.50841665
	avg: 0.5713867
	best: 0.6135167

Starting e_i: 606
Model ind 640 epoch 606 head A head_i_epoch 0 batch 0: avg loss -3.054425 avg loss no lamb -3.054425 time 2019-02-24 10:33:07.290098
Model ind 640 epoch 606 head A head_i_epoch 0 batch 100: avg loss -3.039396 avg loss no lamb -3.039396 time 2019-02-24 10:35:01.184185
Model ind 640 epoch 606 head A head_i_epoch 0 batch 200: avg loss -3.122362 avg loss no lamb -3.122362 time 2019-02-24 10:36:55.020815
last batch sz 160
Model ind 640 epoch 606 head B head_i_epoch 0 batch 0: avg loss -1.619377 avg loss no lamb -1.619377 time 2019-02-24 10:38:17.613605
Model ind 640 epoch 606 head B head_i_epoch 0 batch 100: avg loss -1.597194 avg loss no lamb -1.597194 time 2019-02-24 10:40:10.225763
Model ind 640 epoch 606 head B head_i_epoch 0 batch 200: avg loss -1.778634 avg loss no lamb -1.778634 time 2019-02-24 10:42:04.908553
last batch sz 160
Model ind 640 epoch 606 head B head_i_epoch 1 batch 0: avg loss -1.787708 avg loss no lamb -1.787708 time 2019-02-24 10:43:28.016308
Model ind 640 epoch 606 head B head_i_epoch 1 batch 100: avg loss -1.719273 avg loss no lamb -1.719273 time 2019-02-24 10:45:21.524072
Model ind 640 epoch 606 head B head_i_epoch 1 batch 200: avg loss -1.812432 avg loss no lamb -1.812432 time 2019-02-24 10:47:14.479287
last batch sz 160
Pre: time 2019-02-24 10:48:57.793028: 
 	std: 0.051717
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5061167, 0.6117167, 0.61188334, 0.61155, 0.5061833]
	train_accs: [0.5061167, 0.6117167, 0.61188334, 0.61155, 0.5061833]
	best_train_sub_head: 2
	worst: 0.5061167
	avg: 0.56949
	best: 0.61188334

Starting e_i: 607
Model ind 640 epoch 607 head A head_i_epoch 0 batch 0: avg loss -3.035190 avg loss no lamb -3.035190 time 2019-02-24 10:49:00.402675
Model ind 640 epoch 607 head A head_i_epoch 0 batch 100: avg loss -3.012371 avg loss no lamb -3.012371 time 2019-02-24 10:50:54.574689
Model ind 640 epoch 607 head A head_i_epoch 0 batch 200: avg loss -3.124355 avg loss no lamb -3.124355 time 2019-02-24 10:52:47.341131
last batch sz 160
Model ind 640 epoch 607 head B head_i_epoch 0 batch 0: avg loss -1.766543 avg loss no lamb -1.766543 time 2019-02-24 10:54:09.009344
Model ind 640 epoch 607 head B head_i_epoch 0 batch 100: avg loss -1.704334 avg loss no lamb -1.704334 time 2019-02-24 10:56:02.057888
Model ind 640 epoch 607 head B head_i_epoch 0 batch 200: avg loss -1.926250 avg loss no lamb -1.926250 time 2019-02-24 10:57:54.627944
last batch sz 160
Model ind 640 epoch 607 head B head_i_epoch 1 batch 0: avg loss -1.727593 avg loss no lamb -1.727593 time 2019-02-24 10:59:17.140740
Model ind 640 epoch 607 head B head_i_epoch 1 batch 100: avg loss -1.667660 avg loss no lamb -1.667660 time 2019-02-24 11:01:10.037935
Model ind 640 epoch 607 head B head_i_epoch 1 batch 200: avg loss -1.857889 avg loss no lamb -1.857889 time 2019-02-24 11:03:03.359155
last batch sz 160
Pre: time 2019-02-24 11:04:47.092713: 
 	std: 0.051708106
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5073, 0.61325, 0.6131, 0.6134167, 0.50811666]
	train_accs: [0.5073, 0.61325, 0.6131, 0.6134167, 0.50811666]
	best_train_sub_head: 3
	worst: 0.5073
	avg: 0.5710367
	best: 0.6134167

Starting e_i: 608
Model ind 640 epoch 608 head A head_i_epoch 0 batch 0: avg loss -3.103891 avg loss no lamb -3.103891 time 2019-02-24 11:04:49.383104
Model ind 640 epoch 608 head A head_i_epoch 0 batch 100: avg loss -2.964899 avg loss no lamb -2.964899 time 2019-02-24 11:06:43.565071
Model ind 640 epoch 608 head A head_i_epoch 0 batch 200: avg loss -3.180145 avg loss no lamb -3.180145 time 2019-02-24 11:08:37.275248
last batch sz 160
Model ind 640 epoch 608 head B head_i_epoch 0 batch 0: avg loss -1.790249 avg loss no lamb -1.790249 time 2019-02-24 11:09:58.707121
Model ind 640 epoch 608 head B head_i_epoch 0 batch 100: avg loss -1.698727 avg loss no lamb -1.698727 time 2019-02-24 11:11:52.942707
Model ind 640 epoch 608 head B head_i_epoch 0 batch 200: avg loss -1.872980 avg loss no lamb -1.872980 time 2019-02-24 11:13:46.961540
last batch sz 160
Model ind 640 epoch 608 head B head_i_epoch 1 batch 0: avg loss -1.770322 avg loss no lamb -1.770322 time 2019-02-24 11:15:09.722772
Model ind 640 epoch 608 head B head_i_epoch 1 batch 100: avg loss -1.634836 avg loss no lamb -1.634836 time 2019-02-24 11:17:04.271221
Model ind 640 epoch 608 head B head_i_epoch 1 batch 200: avg loss -1.861502 avg loss no lamb -1.861502 time 2019-02-24 11:18:57.729567
last batch sz 160
Pre: time 2019-02-24 11:20:42.485012: 
 	std: 0.05129566
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50835, 0.6131333, 0.61365, 0.61328334, 0.50895]
	train_accs: [0.50835, 0.6131333, 0.61365, 0.61328334, 0.50895]
	best_train_sub_head: 2
	worst: 0.50835
	avg: 0.57147336
	best: 0.61365

Starting e_i: 609
Model ind 640 epoch 609 head A head_i_epoch 0 batch 0: avg loss -3.049627 avg loss no lamb -3.049627 time 2019-02-24 11:20:44.896622
Model ind 640 epoch 609 head A head_i_epoch 0 batch 100: avg loss -2.987938 avg loss no lamb -2.987938 time 2019-02-24 11:22:39.173641
Model ind 640 epoch 609 head A head_i_epoch 0 batch 200: avg loss -3.125299 avg loss no lamb -3.125299 time 2019-02-24 11:24:33.032019
last batch sz 160
Model ind 640 epoch 609 head B head_i_epoch 0 batch 0: avg loss -1.733737 avg loss no lamb -1.733737 time 2019-02-24 11:25:55.852732
Model ind 640 epoch 609 head B head_i_epoch 0 batch 100: avg loss -1.669803 avg loss no lamb -1.669803 time 2019-02-24 11:27:49.399082
Model ind 640 epoch 609 head B head_i_epoch 0 batch 200: avg loss -1.851154 avg loss no lamb -1.851154 time 2019-02-24 11:29:43.885574
last batch sz 160
Model ind 640 epoch 609 head B head_i_epoch 1 batch 0: avg loss -1.743879 avg loss no lamb -1.743879 time 2019-02-24 11:31:06.559113
Model ind 640 epoch 609 head B head_i_epoch 1 batch 100: avg loss -1.670207 avg loss no lamb -1.670207 time 2019-02-24 11:32:57.908684
Model ind 640 epoch 609 head B head_i_epoch 1 batch 200: avg loss -1.849390 avg loss no lamb -1.849390 time 2019-02-24 11:34:52.662008
last batch sz 160
Pre: time 2019-02-24 11:36:36.975528: 
 	std: 0.051298503
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50635, 0.61078334, 0.6105833, 0.6105667, 0.50551665]
	train_accs: [0.50635, 0.61078334, 0.6105833, 0.6105667, 0.50551665]
	best_train_sub_head: 1
	worst: 0.50551665
	avg: 0.56876004
	best: 0.61078334

Starting e_i: 610
Model ind 640 epoch 610 head A head_i_epoch 0 batch 0: avg loss -3.081198 avg loss no lamb -3.081198 time 2019-02-24 11:36:39.495979
Model ind 640 epoch 610 head A head_i_epoch 0 batch 100: avg loss -3.072294 avg loss no lamb -3.072294 time 2019-02-24 11:38:33.147550
Model ind 640 epoch 610 head A head_i_epoch 0 batch 200: avg loss -3.063633 avg loss no lamb -3.063633 time 2019-02-24 11:40:25.253396
last batch sz 160
Model ind 640 epoch 610 head B head_i_epoch 0 batch 0: avg loss -1.813996 avg loss no lamb -1.813996 time 2019-02-24 11:41:50.056298
Model ind 640 epoch 610 head B head_i_epoch 0 batch 100: avg loss -1.611218 avg loss no lamb -1.611218 time 2019-02-24 11:43:43.948065
Model ind 640 epoch 610 head B head_i_epoch 0 batch 200: avg loss -1.803407 avg loss no lamb -1.803407 time 2019-02-24 11:45:38.478190
last batch sz 160
Model ind 640 epoch 610 head B head_i_epoch 1 batch 0: avg loss -1.850916 avg loss no lamb -1.850916 time 2019-02-24 11:47:01.080602
Model ind 640 epoch 610 head B head_i_epoch 1 batch 100: avg loss -1.676350 avg loss no lamb -1.676350 time 2019-02-24 11:48:53.957080
Model ind 640 epoch 610 head B head_i_epoch 1 batch 200: avg loss -1.870515 avg loss no lamb -1.870515 time 2019-02-24 11:50:47.096113
last batch sz 160
Pre: time 2019-02-24 11:52:30.751184: 
 	std: 0.051323712
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5071667, 0.61186665, 0.6117333, 0.6117667, 0.5068833]
	train_accs: [0.5071667, 0.61186665, 0.6117333, 0.6117667, 0.5068833]
	best_train_sub_head: 1
	worst: 0.5068833
	avg: 0.56988335
	best: 0.61186665

Starting e_i: 611
Model ind 640 epoch 611 head A head_i_epoch 0 batch 0: avg loss -3.117144 avg loss no lamb -3.117144 time 2019-02-24 11:52:37.374469
Model ind 640 epoch 611 head A head_i_epoch 0 batch 100: avg loss -3.014825 avg loss no lamb -3.014825 time 2019-02-24 11:54:29.425372
Model ind 640 epoch 611 head A head_i_epoch 0 batch 200: avg loss -3.093430 avg loss no lamb -3.093430 time 2019-02-24 11:56:25.502868
last batch sz 160
Model ind 640 epoch 611 head B head_i_epoch 0 batch 0: avg loss -1.733520 avg loss no lamb -1.733520 time 2019-02-24 11:57:48.285532
Model ind 640 epoch 611 head B head_i_epoch 0 batch 100: avg loss -1.639706 avg loss no lamb -1.639706 time 2019-02-24 11:59:42.051700
Model ind 640 epoch 611 head B head_i_epoch 0 batch 200: avg loss -1.754042 avg loss no lamb -1.754042 time 2019-02-24 12:01:36.101431
last batch sz 160
Model ind 640 epoch 611 head B head_i_epoch 1 batch 0: avg loss -1.789415 avg loss no lamb -1.789415 time 2019-02-24 12:02:58.271808
Model ind 640 epoch 611 head B head_i_epoch 1 batch 100: avg loss -1.668509 avg loss no lamb -1.668509 time 2019-02-24 12:04:51.703197
Model ind 640 epoch 611 head B head_i_epoch 1 batch 200: avg loss -1.850155 avg loss no lamb -1.850155 time 2019-02-24 12:06:45.814444
last batch sz 160
Pre: time 2019-02-24 12:08:30.559772: 
 	std: 0.050861318
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5092667, 0.6134167, 0.6134667, 0.6132, 0.50981665]
	train_accs: [0.5092667, 0.6134167, 0.6134667, 0.6132, 0.50981665]
	best_train_sub_head: 2
	worst: 0.5092667
	avg: 0.5718333
	best: 0.6134667

Starting e_i: 612
Model ind 640 epoch 612 head A head_i_epoch 0 batch 0: avg loss -3.124079 avg loss no lamb -3.124079 time 2019-02-24 12:08:32.981042
Model ind 640 epoch 612 head A head_i_epoch 0 batch 100: avg loss -2.976124 avg loss no lamb -2.976124 time 2019-02-24 12:10:25.882045
Model ind 640 epoch 612 head A head_i_epoch 0 batch 200: avg loss -3.144771 avg loss no lamb -3.144771 time 2019-02-24 12:12:22.287558
last batch sz 160
Model ind 640 epoch 612 head B head_i_epoch 0 batch 0: avg loss -1.734309 avg loss no lamb -1.734309 time 2019-02-24 12:13:46.206816
Model ind 640 epoch 612 head B head_i_epoch 0 batch 100: avg loss -1.709360 avg loss no lamb -1.709360 time 2019-02-24 12:15:39.030509
Model ind 640 epoch 612 head B head_i_epoch 0 batch 200: avg loss -1.834483 avg loss no lamb -1.834483 time 2019-02-24 12:17:32.823136
last batch sz 160
Model ind 640 epoch 612 head B head_i_epoch 1 batch 0: avg loss -1.737075 avg loss no lamb -1.737075 time 2019-02-24 12:18:55.440265
Model ind 640 epoch 612 head B head_i_epoch 1 batch 100: avg loss -1.733032 avg loss no lamb -1.733032 time 2019-02-24 12:20:49.202964
Model ind 640 epoch 612 head B head_i_epoch 1 batch 200: avg loss -1.809831 avg loss no lamb -1.809831 time 2019-02-24 12:22:42.318609
last batch sz 160
Pre: time 2019-02-24 12:24:25.675471: 
 	std: 0.05070738
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51066667, 0.61406666, 0.6139167, 0.6138833, 0.51023334]
	train_accs: [0.51066667, 0.61406666, 0.6139167, 0.6138833, 0.51023334]
	best_train_sub_head: 1
	worst: 0.51023334
	avg: 0.57255334
	best: 0.61406666

Starting e_i: 613
Model ind 640 epoch 613 head A head_i_epoch 0 batch 0: avg loss -3.040059 avg loss no lamb -3.040059 time 2019-02-24 12:24:28.131353
Model ind 640 epoch 613 head A head_i_epoch 0 batch 100: avg loss -3.025033 avg loss no lamb -3.025033 time 2019-02-24 12:26:21.583119
Model ind 640 epoch 613 head A head_i_epoch 0 batch 200: avg loss -3.086990 avg loss no lamb -3.086990 time 2019-02-24 12:28:14.986179
last batch sz 160
Model ind 640 epoch 613 head B head_i_epoch 0 batch 0: avg loss -1.769582 avg loss no lamb -1.769582 time 2019-02-24 12:29:37.390736
Model ind 640 epoch 613 head B head_i_epoch 0 batch 100: avg loss -1.641621 avg loss no lamb -1.641621 time 2019-02-24 12:31:31.521741
Model ind 640 epoch 613 head B head_i_epoch 0 batch 200: avg loss -1.802896 avg loss no lamb -1.802896 time 2019-02-24 12:33:24.940529
last batch sz 160
Model ind 640 epoch 613 head B head_i_epoch 1 batch 0: avg loss -1.825118 avg loss no lamb -1.825118 time 2019-02-24 12:34:47.235484
Model ind 640 epoch 613 head B head_i_epoch 1 batch 100: avg loss -1.630786 avg loss no lamb -1.630786 time 2019-02-24 12:36:40.569052
Model ind 640 epoch 613 head B head_i_epoch 1 batch 200: avg loss -1.782536 avg loss no lamb -1.782536 time 2019-02-24 12:38:32.468851
last batch sz 160
Pre: time 2019-02-24 12:40:19.019323: 
 	std: 0.051110156
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5078833, 0.61193335, 0.61231667, 0.61228335, 0.5078167]
	train_accs: [0.5078833, 0.61193335, 0.61231667, 0.61228335, 0.5078167]
	best_train_sub_head: 2
	worst: 0.5078167
	avg: 0.57044667
	best: 0.61231667

Starting e_i: 614
Model ind 640 epoch 614 head A head_i_epoch 0 batch 0: avg loss -3.096250 avg loss no lamb -3.096250 time 2019-02-24 12:40:21.705603
Model ind 640 epoch 614 head A head_i_epoch 0 batch 100: avg loss -3.057964 avg loss no lamb -3.057964 time 2019-02-24 12:42:17.978459
Model ind 640 epoch 614 head A head_i_epoch 0 batch 200: avg loss -3.128539 avg loss no lamb -3.128539 time 2019-02-24 12:44:11.350991
last batch sz 160
Model ind 640 epoch 614 head B head_i_epoch 0 batch 0: avg loss -1.838648 avg loss no lamb -1.838648 time 2019-02-24 12:45:33.777888
Model ind 640 epoch 614 head B head_i_epoch 0 batch 100: avg loss -1.753517 avg loss no lamb -1.753517 time 2019-02-24 12:47:27.352612
Model ind 640 epoch 614 head B head_i_epoch 0 batch 200: avg loss -1.832350 avg loss no lamb -1.832350 time 2019-02-24 12:49:21.074666
last batch sz 160
Model ind 640 epoch 614 head B head_i_epoch 1 batch 0: avg loss -1.769272 avg loss no lamb -1.769272 time 2019-02-24 12:50:43.062691
Model ind 640 epoch 614 head B head_i_epoch 1 batch 100: avg loss -1.668743 avg loss no lamb -1.668743 time 2019-02-24 12:52:37.385545
Model ind 640 epoch 614 head B head_i_epoch 1 batch 200: avg loss -1.859672 avg loss no lamb -1.859672 time 2019-02-24 12:54:31.573447
last batch sz 160
Pre: time 2019-02-24 12:56:15.968154: 
 	std: 0.05037517
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104333, 0.6132333, 0.6131833, 0.6134167, 0.5104667]
	train_accs: [0.5104333, 0.6132333, 0.6131833, 0.6134167, 0.5104667]
	best_train_sub_head: 3
	worst: 0.5104333
	avg: 0.5721467
	best: 0.6134167

Starting e_i: 615
Model ind 640 epoch 615 head A head_i_epoch 0 batch 0: avg loss -3.068241 avg loss no lamb -3.068241 time 2019-02-24 12:56:18.652996
Model ind 640 epoch 615 head A head_i_epoch 0 batch 100: avg loss -3.039904 avg loss no lamb -3.039904 time 2019-02-24 12:58:13.219809
Model ind 640 epoch 615 head A head_i_epoch 0 batch 200: avg loss -3.193551 avg loss no lamb -3.193551 time 2019-02-24 13:00:05.432690
last batch sz 160
Model ind 640 epoch 615 head B head_i_epoch 0 batch 0: avg loss -1.776365 avg loss no lamb -1.776365 time 2019-02-24 13:01:28.692194
Model ind 640 epoch 615 head B head_i_epoch 0 batch 100: avg loss -1.606030 avg loss no lamb -1.606030 time 2019-02-24 13:03:22.792281
Model ind 640 epoch 615 head B head_i_epoch 0 batch 200: avg loss -1.714563 avg loss no lamb -1.714563 time 2019-02-24 13:05:17.031009
last batch sz 160
Model ind 640 epoch 615 head B head_i_epoch 1 batch 0: avg loss -1.793265 avg loss no lamb -1.793265 time 2019-02-24 13:06:39.945839
Model ind 640 epoch 615 head B head_i_epoch 1 batch 100: avg loss -1.831394 avg loss no lamb -1.831394 time 2019-02-24 13:08:33.712041
Model ind 640 epoch 615 head B head_i_epoch 1 batch 200: avg loss -1.834656 avg loss no lamb -1.834656 time 2019-02-24 13:10:27.870939
last batch sz 160
Pre: time 2019-02-24 13:12:12.093467: 
 	std: 0.051008012
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50818336, 0.6124, 0.6125, 0.6124833, 0.5085]
	train_accs: [0.50818336, 0.6124, 0.6125, 0.6124833, 0.5085]
	best_train_sub_head: 2
	worst: 0.50818336
	avg: 0.5708133
	best: 0.6125

Starting e_i: 616
Model ind 640 epoch 616 head A head_i_epoch 0 batch 0: avg loss -3.143363 avg loss no lamb -3.143363 time 2019-02-24 13:12:15.155629
Model ind 640 epoch 616 head A head_i_epoch 0 batch 100: avg loss -3.007400 avg loss no lamb -3.007400 time 2019-02-24 13:14:08.232201
Model ind 640 epoch 616 head A head_i_epoch 0 batch 200: avg loss -3.180949 avg loss no lamb -3.180949 time 2019-02-24 13:16:02.487577
last batch sz 160
Model ind 640 epoch 616 head B head_i_epoch 0 batch 0: avg loss -1.811194 avg loss no lamb -1.811194 time 2019-02-24 13:17:25.681919
Model ind 640 epoch 616 head B head_i_epoch 0 batch 100: avg loss -1.704206 avg loss no lamb -1.704206 time 2019-02-24 13:19:19.142164
Model ind 640 epoch 616 head B head_i_epoch 0 batch 200: avg loss -1.788676 avg loss no lamb -1.788676 time 2019-02-24 13:21:12.316838
last batch sz 160
Model ind 640 epoch 616 head B head_i_epoch 1 batch 0: avg loss -1.724856 avg loss no lamb -1.724856 time 2019-02-24 13:22:32.061987
Model ind 640 epoch 616 head B head_i_epoch 1 batch 100: avg loss -1.694048 avg loss no lamb -1.694048 time 2019-02-24 13:24:27.027115
Model ind 640 epoch 616 head B head_i_epoch 1 batch 200: avg loss -1.783560 avg loss no lamb -1.783560 time 2019-02-24 13:26:20.871824
last batch sz 160
Pre: time 2019-02-24 13:28:05.282092: 
 	std: 0.050750986
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50915, 0.6124167, 0.61251664, 0.6125, 0.5086167]
	train_accs: [0.50915, 0.6124167, 0.61251664, 0.6125, 0.5086167]
	best_train_sub_head: 2
	worst: 0.5086167
	avg: 0.57104003
	best: 0.61251664

Starting e_i: 617
Model ind 640 epoch 617 head A head_i_epoch 0 batch 0: avg loss -3.089378 avg loss no lamb -3.089378 time 2019-02-24 13:28:07.816072
Model ind 640 epoch 617 head A head_i_epoch 0 batch 100: avg loss -2.995212 avg loss no lamb -2.995212 time 2019-02-24 13:30:02.189606
Model ind 640 epoch 617 head A head_i_epoch 0 batch 200: avg loss -3.231817 avg loss no lamb -3.231817 time 2019-02-24 13:31:56.535161
last batch sz 160
Model ind 640 epoch 617 head B head_i_epoch 0 batch 0: avg loss -1.772342 avg loss no lamb -1.772342 time 2019-02-24 13:33:19.750698
Model ind 640 epoch 617 head B head_i_epoch 0 batch 100: avg loss -1.743149 avg loss no lamb -1.743149 time 2019-02-24 13:35:13.694342
Model ind 640 epoch 617 head B head_i_epoch 0 batch 200: avg loss -1.846863 avg loss no lamb -1.846863 time 2019-02-24 13:37:07.598841
last batch sz 160
Model ind 640 epoch 617 head B head_i_epoch 1 batch 0: avg loss -1.759549 avg loss no lamb -1.759549 time 2019-02-24 13:38:29.990193
Model ind 640 epoch 617 head B head_i_epoch 1 batch 100: avg loss -1.707053 avg loss no lamb -1.707053 time 2019-02-24 13:40:24.627046
Model ind 640 epoch 617 head B head_i_epoch 1 batch 200: avg loss -1.844958 avg loss no lamb -1.844958 time 2019-02-24 13:42:20.007193
last batch sz 160
Pre: time 2019-02-24 13:44:02.683355: 
 	std: 0.05187891
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50581664, 0.61155, 0.6117333, 0.61183333, 0.5058]
	train_accs: [0.50581664, 0.61155, 0.6117333, 0.61183333, 0.5058]
	best_train_sub_head: 3
	worst: 0.5058
	avg: 0.56934667
	best: 0.61183333

Starting e_i: 618
Model ind 640 epoch 618 head A head_i_epoch 0 batch 0: avg loss -3.111180 avg loss no lamb -3.111180 time 2019-02-24 13:44:04.881410
Model ind 640 epoch 618 head A head_i_epoch 0 batch 100: avg loss -3.031954 avg loss no lamb -3.031954 time 2019-02-24 13:45:58.686983
Model ind 640 epoch 618 head A head_i_epoch 0 batch 200: avg loss -3.065546 avg loss no lamb -3.065546 time 2019-02-24 13:47:52.680114
last batch sz 160
Model ind 640 epoch 618 head B head_i_epoch 0 batch 0: avg loss -1.724134 avg loss no lamb -1.724134 time 2019-02-24 13:49:15.860278
Model ind 640 epoch 618 head B head_i_epoch 0 batch 100: avg loss -1.693961 avg loss no lamb -1.693961 time 2019-02-24 13:51:09.943534
Model ind 640 epoch 618 head B head_i_epoch 0 batch 200: avg loss -1.834395 avg loss no lamb -1.834395 time 2019-02-24 13:53:02.900308
last batch sz 160
Model ind 640 epoch 618 head B head_i_epoch 1 batch 0: avg loss -1.809098 avg loss no lamb -1.809098 time 2019-02-24 13:54:25.394073
Model ind 640 epoch 618 head B head_i_epoch 1 batch 100: avg loss -1.621733 avg loss no lamb -1.621733 time 2019-02-24 13:56:19.783935
Model ind 640 epoch 618 head B head_i_epoch 1 batch 200: avg loss -1.831455 avg loss no lamb -1.831455 time 2019-02-24 13:58:13.376585
last batch sz 160
Pre: time 2019-02-24 13:59:58.674760: 
 	std: 0.050882827
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5075333, 0.61116666, 0.61128336, 0.61146665, 0.50735]
	train_accs: [0.5075333, 0.61116666, 0.61128336, 0.61146665, 0.50735]
	best_train_sub_head: 3
	worst: 0.50735
	avg: 0.56975996
	best: 0.61146665

Starting e_i: 619
Model ind 640 epoch 619 head A head_i_epoch 0 batch 0: avg loss -3.031682 avg loss no lamb -3.031682 time 2019-02-24 14:00:01.264714
Model ind 640 epoch 619 head A head_i_epoch 0 batch 100: avg loss -3.089902 avg loss no lamb -3.089902 time 2019-02-24 14:01:56.328128
Model ind 640 epoch 619 head A head_i_epoch 0 batch 200: avg loss -3.081672 avg loss no lamb -3.081672 time 2019-02-24 14:03:51.055708
last batch sz 160
Model ind 640 epoch 619 head B head_i_epoch 0 batch 0: avg loss -1.849950 avg loss no lamb -1.849950 time 2019-02-24 14:05:46.584868
Model ind 640 epoch 619 head B head_i_epoch 0 batch 100: avg loss -1.687695 avg loss no lamb -1.687695 time 2019-02-24 14:07:59.376410
Model ind 640 epoch 619 head B head_i_epoch 0 batch 200: avg loss -1.822514 avg loss no lamb -1.822514 time 2019-02-24 14:10:13.873478
last batch sz 160
Model ind 640 epoch 619 head B head_i_epoch 1 batch 0: avg loss -1.831671 avg loss no lamb -1.831671 time 2019-02-24 14:12:07.199663
Model ind 640 epoch 619 head B head_i_epoch 1 batch 100: avg loss -1.719536 avg loss no lamb -1.719536 time 2019-02-24 14:14:32.203975
Model ind 640 epoch 619 head B head_i_epoch 1 batch 200: avg loss -1.853715 avg loss no lamb -1.853715 time 2019-02-24 14:16:46.340817
last batch sz 160
Pre: time 2019-02-24 14:18:50.955080: 
 	std: 0.05097544
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5065333, 0.6107, 0.6103167, 0.6106667, 0.5064833]
	train_accs: [0.5065333, 0.6107, 0.6103167, 0.6106667, 0.5064833]
	best_train_sub_head: 1
	worst: 0.5064833
	avg: 0.56894004
	best: 0.6107

Starting e_i: 620
Model ind 640 epoch 620 head A head_i_epoch 0 batch 0: avg loss -3.062712 avg loss no lamb -3.062712 time 2019-02-24 14:18:53.895690
Model ind 640 epoch 620 head A head_i_epoch 0 batch 100: avg loss -2.970421 avg loss no lamb -2.970421 time 2019-02-24 14:21:19.488831
Model ind 640 epoch 620 head A head_i_epoch 0 batch 200: avg loss -3.198477 avg loss no lamb -3.198477 time 2019-02-24 14:23:47.767133
last batch sz 160
Model ind 640 epoch 620 head B head_i_epoch 0 batch 0: avg loss -1.759531 avg loss no lamb -1.759531 time 2019-02-24 14:25:15.821351
Model ind 640 epoch 620 head B head_i_epoch 0 batch 100: avg loss -1.646673 avg loss no lamb -1.646673 time 2019-02-24 14:27:43.620381
Model ind 640 epoch 620 head B head_i_epoch 0 batch 200: avg loss -1.803537 avg loss no lamb -1.803537 time 2019-02-24 14:30:10.171255
last batch sz 160
Model ind 640 epoch 620 head B head_i_epoch 1 batch 0: avg loss -1.766007 avg loss no lamb -1.766007 time 2019-02-24 14:31:52.648652
Model ind 640 epoch 620 head B head_i_epoch 1 batch 100: avg loss -1.743492 avg loss no lamb -1.743492 time 2019-02-24 14:34:08.033842
Model ind 640 epoch 620 head B head_i_epoch 1 batch 200: avg loss -1.859214 avg loss no lamb -1.859214 time 2019-02-24 14:36:35.868324
last batch sz 160
Pre: time 2019-02-24 14:38:50.828243: 
 	std: 0.052643694
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50631666, 0.6137, 0.61356664, 0.61363333, 0.50603336]
	train_accs: [0.50631666, 0.6137, 0.61356664, 0.61363333, 0.50603336]
	best_train_sub_head: 1
	worst: 0.50603336
	avg: 0.57065
	best: 0.6137

Starting e_i: 621
Model ind 640 epoch 621 head A head_i_epoch 0 batch 0: avg loss -3.214751 avg loss no lamb -3.214751 time 2019-02-24 14:38:56.942973
Model ind 640 epoch 621 head A head_i_epoch 0 batch 100: avg loss -3.073860 avg loss no lamb -3.073860 time 2019-02-24 14:41:15.917246
Model ind 640 epoch 621 head A head_i_epoch 0 batch 200: avg loss -3.143967 avg loss no lamb -3.143967 time 2019-02-24 14:43:26.900479
last batch sz 160
Model ind 640 epoch 621 head B head_i_epoch 0 batch 0: avg loss -1.755110 avg loss no lamb -1.755110 time 2019-02-24 14:45:18.670937
Model ind 640 epoch 621 head B head_i_epoch 0 batch 100: avg loss -1.785923 avg loss no lamb -1.785923 time 2019-02-24 14:47:46.355792
Model ind 640 epoch 621 head B head_i_epoch 0 batch 200: avg loss -1.844499 avg loss no lamb -1.844499 time 2019-02-24 14:50:02.973865
last batch sz 160
Model ind 640 epoch 621 head B head_i_epoch 1 batch 0: avg loss -1.788139 avg loss no lamb -1.788139 time 2019-02-24 14:51:45.581257
Model ind 640 epoch 621 head B head_i_epoch 1 batch 100: avg loss -1.718379 avg loss no lamb -1.718379 time 2019-02-24 14:54:12.165785
Model ind 640 epoch 621 head B head_i_epoch 1 batch 200: avg loss -1.838034 avg loss no lamb -1.838034 time 2019-02-24 14:56:33.035755
last batch sz 160
Pre: time 2019-02-24 14:58:30.742477: 
 	std: 0.050355706
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51021665, 0.61303335, 0.61235, 0.61255, 0.5095]
	train_accs: [0.51021665, 0.61303335, 0.61235, 0.61255, 0.5095]
	best_train_sub_head: 1
	worst: 0.5095
	avg: 0.57153
	best: 0.61303335

Starting e_i: 622
Model ind 640 epoch 622 head A head_i_epoch 0 batch 0: avg loss -3.067836 avg loss no lamb -3.067836 time 2019-02-24 14:58:33.207563
Model ind 640 epoch 622 head A head_i_epoch 0 batch 100: avg loss -2.975071 avg loss no lamb -2.975071 time 2019-02-24 15:01:00.674028
Model ind 640 epoch 622 head A head_i_epoch 0 batch 200: avg loss -3.112343 avg loss no lamb -3.112343 time 2019-02-24 15:03:26.454345
last batch sz 160
Model ind 640 epoch 622 head B head_i_epoch 0 batch 0: avg loss -1.766281 avg loss no lamb -1.766281 time 2019-02-24 15:05:07.763213
Model ind 640 epoch 622 head B head_i_epoch 0 batch 100: avg loss -1.682733 avg loss no lamb -1.682733 time 2019-02-24 15:07:23.126166
Model ind 640 epoch 622 head B head_i_epoch 0 batch 200: avg loss -1.859874 avg loss no lamb -1.859874 time 2019-02-24 15:09:49.794551
last batch sz 160
Model ind 640 epoch 622 head B head_i_epoch 1 batch 0: avg loss -1.834945 avg loss no lamb -1.834945 time 2019-02-24 15:11:37.081033
Model ind 640 epoch 622 head B head_i_epoch 1 batch 100: avg loss -1.749559 avg loss no lamb -1.749559 time 2019-02-24 15:13:44.751237
Model ind 640 epoch 622 head B head_i_epoch 1 batch 200: avg loss -1.825526 avg loss no lamb -1.825526 time 2019-02-24 15:16:11.487930
last batch sz 160
Pre: time 2019-02-24 15:18:27.210659: 
 	std: 0.05230215
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50568336, 0.6124167, 0.6122, 0.6123667, 0.50545]
	train_accs: [0.50568336, 0.6124167, 0.6122, 0.6123667, 0.50545]
	best_train_sub_head: 1
	worst: 0.50545
	avg: 0.56962335
	best: 0.6124167

Starting e_i: 623
Model ind 640 epoch 623 head A head_i_epoch 0 batch 0: avg loss -3.080685 avg loss no lamb -3.080685 time 2019-02-24 15:18:29.698669
Model ind 640 epoch 623 head A head_i_epoch 0 batch 100: avg loss -3.075281 avg loss no lamb -3.075281 time 2019-02-24 15:20:56.795380
Model ind 640 epoch 623 head A head_i_epoch 0 batch 200: avg loss -3.142640 avg loss no lamb -3.142640 time 2019-02-24 15:23:12.991101
last batch sz 160
Model ind 640 epoch 623 head B head_i_epoch 0 batch 0: avg loss -1.791697 avg loss no lamb -1.791697 time 2019-02-24 15:24:55.636654
Model ind 640 epoch 623 head B head_i_epoch 0 batch 100: avg loss -1.718868 avg loss no lamb -1.718868 time 2019-02-24 15:27:21.410985
Model ind 640 epoch 623 head B head_i_epoch 0 batch 200: avg loss -1.804533 avg loss no lamb -1.804533 time 2019-02-24 15:29:41.322761
last batch sz 160
Model ind 640 epoch 623 head B head_i_epoch 1 batch 0: avg loss -1.861368 avg loss no lamb -1.861368 time 2019-02-24 15:31:16.857070
Model ind 640 epoch 623 head B head_i_epoch 1 batch 100: avg loss -1.686657 avg loss no lamb -1.686657 time 2019-02-24 15:33:41.258845
Model ind 640 epoch 623 head B head_i_epoch 1 batch 200: avg loss -1.831469 avg loss no lamb -1.831469 time 2019-02-24 15:36:08.789898
last batch sz 160
Pre: time 2019-02-24 15:38:10.553987: 
 	std: 0.051936116
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50605, 0.61193335, 0.6118, 0.6120333, 0.5057667]
	train_accs: [0.50605, 0.61193335, 0.6118, 0.6120333, 0.5057667]
	best_train_sub_head: 3
	worst: 0.5057667
	avg: 0.56951666
	best: 0.6120333

Starting e_i: 624
Model ind 640 epoch 624 head A head_i_epoch 0 batch 0: avg loss -3.134111 avg loss no lamb -3.134111 time 2019-02-24 15:38:13.355930
Model ind 640 epoch 624 head A head_i_epoch 0 batch 100: avg loss -3.018610 avg loss no lamb -3.018610 time 2019-02-24 15:40:31.013210
Model ind 640 epoch 624 head A head_i_epoch 0 batch 200: avg loss -3.063960 avg loss no lamb -3.063960 time 2019-02-24 15:42:58.677790
last batch sz 160
Model ind 640 epoch 624 head B head_i_epoch 0 batch 0: avg loss -1.813590 avg loss no lamb -1.813590 time 2019-02-24 15:44:44.153540
Model ind 640 epoch 624 head B head_i_epoch 0 batch 100: avg loss -1.693294 avg loss no lamb -1.693294 time 2019-02-24 15:46:58.284008
Model ind 640 epoch 624 head B head_i_epoch 0 batch 200: avg loss -1.822823 avg loss no lamb -1.822823 time 2019-02-24 15:49:17.566422
last batch sz 160
Model ind 640 epoch 624 head B head_i_epoch 1 batch 0: avg loss -1.772932 avg loss no lamb -1.772932 time 2019-02-24 15:51:10.199393
Model ind 640 epoch 624 head B head_i_epoch 1 batch 100: avg loss -1.673141 avg loss no lamb -1.673141 time 2019-02-24 15:53:23.208084
Model ind 640 epoch 624 head B head_i_epoch 1 batch 200: avg loss -1.776686 avg loss no lamb -1.776686 time 2019-02-24 15:55:37.444587
last batch sz 160
Pre: time 2019-02-24 15:57:50.784087: 
 	std: 0.051829457
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50813335, 0.61345, 0.61335, 0.6134833, 0.5071333]
	train_accs: [0.50813335, 0.61345, 0.61335, 0.6134833, 0.5071333]
	best_train_sub_head: 3
	worst: 0.5071333
	avg: 0.57111
	best: 0.6134833

Starting e_i: 625
Model ind 640 epoch 625 head A head_i_epoch 0 batch 0: avg loss -3.102880 avg loss no lamb -3.102880 time 2019-02-24 15:57:54.080287
Model ind 640 epoch 625 head A head_i_epoch 0 batch 100: avg loss -2.940097 avg loss no lamb -2.940097 time 2019-02-24 16:00:22.352338
Model ind 640 epoch 625 head A head_i_epoch 0 batch 200: avg loss -3.223644 avg loss no lamb -3.223644 time 2019-02-24 16:02:38.910685
last batch sz 160
Model ind 640 epoch 625 head B head_i_epoch 0 batch 0: avg loss -1.812016 avg loss no lamb -1.812016 time 2019-02-24 16:04:19.076415
Model ind 640 epoch 625 head B head_i_epoch 0 batch 100: avg loss -1.678002 avg loss no lamb -1.678002 time 2019-02-24 16:06:44.061301
Model ind 640 epoch 625 head B head_i_epoch 0 batch 200: avg loss -1.864351 avg loss no lamb -1.864351 time 2019-02-24 16:09:13.143539
last batch sz 160
Model ind 640 epoch 625 head B head_i_epoch 1 batch 0: avg loss -1.840136 avg loss no lamb -1.840136 time 2019-02-24 16:10:41.191249
Model ind 640 epoch 625 head B head_i_epoch 1 batch 100: avg loss -1.636676 avg loss no lamb -1.636676 time 2019-02-24 16:13:08.969230
Model ind 640 epoch 625 head B head_i_epoch 1 batch 200: avg loss -1.832993 avg loss no lamb -1.832993 time 2019-02-24 16:15:34.762773
last batch sz 160
Pre: time 2019-02-24 16:17:37.365851: 
 	std: 0.051806748
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50515, 0.6110833, 0.6109333, 0.6109333, 0.5053167]
	train_accs: [0.50515, 0.6110833, 0.6109333, 0.6109333, 0.5053167]
	best_train_sub_head: 1
	worst: 0.50515
	avg: 0.5686833
	best: 0.6110833

Starting e_i: 626
Model ind 640 epoch 626 head A head_i_epoch 0 batch 0: avg loss -3.113866 avg loss no lamb -3.113866 time 2019-02-24 16:17:40.465831
Model ind 640 epoch 626 head A head_i_epoch 0 batch 100: avg loss -2.995540 avg loss no lamb -2.995540 time 2019-02-24 16:19:54.193972
Model ind 640 epoch 626 head A head_i_epoch 0 batch 200: avg loss -3.138365 avg loss no lamb -3.138365 time 2019-02-24 16:22:23.489751
last batch sz 160
Model ind 640 epoch 626 head B head_i_epoch 0 batch 0: avg loss -1.724002 avg loss no lamb -1.724002 time 2019-02-24 16:24:15.109704
Model ind 640 epoch 626 head B head_i_epoch 0 batch 100: avg loss -1.705450 avg loss no lamb -1.705450 time 2019-02-24 16:26:29.076522
Model ind 640 epoch 626 head B head_i_epoch 0 batch 200: avg loss -1.843014 avg loss no lamb -1.843014 time 2019-02-24 16:28:43.746182
last batch sz 160
Model ind 640 epoch 626 head B head_i_epoch 1 batch 0: avg loss -1.773771 avg loss no lamb -1.773771 time 2019-02-24 16:30:36.517100
Model ind 640 epoch 626 head B head_i_epoch 1 batch 100: avg loss -1.737924 avg loss no lamb -1.737924 time 2019-02-24 16:32:54.841945
Model ind 640 epoch 626 head B head_i_epoch 1 batch 200: avg loss -1.805586 avg loss no lamb -1.805586 time 2019-02-24 16:35:11.540564
last batch sz 160
Pre: time 2019-02-24 16:37:20.910457: 
 	std: 0.051202543
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50876665, 0.61326665, 0.6131833, 0.61315, 0.5086]
	train_accs: [0.50876665, 0.61326665, 0.6131833, 0.61315, 0.5086]
	best_train_sub_head: 1
	worst: 0.5086
	avg: 0.5713934
	best: 0.61326665

Starting e_i: 627
Model ind 640 epoch 627 head A head_i_epoch 0 batch 0: avg loss -3.089384 avg loss no lamb -3.089384 time 2019-02-24 16:37:27.449669
Model ind 640 epoch 627 head A head_i_epoch 0 batch 100: avg loss -3.010348 avg loss no lamb -3.010348 time 2019-02-24 16:39:54.581232
Model ind 640 epoch 627 head A head_i_epoch 0 batch 200: avg loss -3.124844 avg loss no lamb -3.124844 time 2019-02-24 16:42:19.890524
last batch sz 160
Model ind 640 epoch 627 head B head_i_epoch 0 batch 0: avg loss -1.796764 avg loss no lamb -1.796764 time 2019-02-24 16:43:50.547553
Model ind 640 epoch 627 head B head_i_epoch 0 batch 100: avg loss -1.820247 avg loss no lamb -1.820247 time 2019-02-24 16:46:17.515291
Model ind 640 epoch 627 head B head_i_epoch 0 batch 200: avg loss -1.776172 avg loss no lamb -1.776172 time 2019-02-24 16:48:44.145525
last batch sz 160
Model ind 640 epoch 627 head B head_i_epoch 1 batch 0: avg loss -1.779830 avg loss no lamb -1.779830 time 2019-02-24 16:50:15.158832
Model ind 640 epoch 627 head B head_i_epoch 1 batch 100: avg loss -1.649583 avg loss no lamb -1.649583 time 2019-02-24 16:52:39.323546
Model ind 640 epoch 627 head B head_i_epoch 1 batch 200: avg loss -1.841718 avg loss no lamb -1.841718 time 2019-02-24 16:55:05.200799
last batch sz 160
Pre: time 2019-02-24 16:57:19.044223: 
 	std: 0.051168982
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5104167, 0.615, 0.6147, 0.61436665, 0.5100667]
	train_accs: [0.5104167, 0.615, 0.6147, 0.61436665, 0.5100667]
	best_train_sub_head: 1
	worst: 0.5100667
	avg: 0.5729101
	best: 0.615

Starting e_i: 628
Model ind 640 epoch 628 head A head_i_epoch 0 batch 0: avg loss -3.043785 avg loss no lamb -3.043785 time 2019-02-24 16:57:21.477678
Model ind 640 epoch 628 head A head_i_epoch 0 batch 100: avg loss -3.003038 avg loss no lamb -3.003038 time 2019-02-24 16:59:36.556686
Model ind 640 epoch 628 head A head_i_epoch 0 batch 200: avg loss -3.064692 avg loss no lamb -3.064692 time 2019-02-24 17:01:52.626912
last batch sz 160
Model ind 640 epoch 628 head B head_i_epoch 0 batch 0: avg loss -1.822747 avg loss no lamb -1.822747 time 2019-02-24 17:03:46.051173
Model ind 640 epoch 628 head B head_i_epoch 0 batch 100: avg loss -1.761819 avg loss no lamb -1.761819 time 2019-02-24 17:06:03.065033
Model ind 640 epoch 628 head B head_i_epoch 0 batch 200: avg loss -1.881504 avg loss no lamb -1.881504 time 2019-02-24 17:08:19.056211
last batch sz 160
Model ind 640 epoch 628 head B head_i_epoch 1 batch 0: avg loss -1.752436 avg loss no lamb -1.752436 time 2019-02-24 17:10:06.511418
Model ind 640 epoch 628 head B head_i_epoch 1 batch 100: avg loss -1.646509 avg loss no lamb -1.646509 time 2019-02-24 17:12:35.601064
Model ind 640 epoch 628 head B head_i_epoch 1 batch 200: avg loss -1.848232 avg loss no lamb -1.848232 time 2019-02-24 17:14:48.341117
last batch sz 160
Pre: time 2019-02-24 17:16:53.344739: 
 	std: 0.051444333
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50725, 0.6117667, 0.61165, 0.61193335, 0.5063]
	train_accs: [0.50725, 0.6117667, 0.61165, 0.61193335, 0.5063]
	best_train_sub_head: 3
	worst: 0.5063
	avg: 0.56978
	best: 0.61193335

Starting e_i: 629
Model ind 640 epoch 629 head A head_i_epoch 0 batch 0: avg loss -3.173736 avg loss no lamb -3.173736 time 2019-02-24 17:16:56.854756
Model ind 640 epoch 629 head A head_i_epoch 0 batch 100: avg loss -2.986930 avg loss no lamb -2.986930 time 2019-02-24 17:18:50.134272
Model ind 640 epoch 629 head A head_i_epoch 0 batch 200: avg loss -3.106406 avg loss no lamb -3.106406 time 2019-02-24 17:20:42.996335
last batch sz 160
Model ind 640 epoch 629 head B head_i_epoch 0 batch 0: avg loss -1.838304 avg loss no lamb -1.838304 time 2019-02-24 17:22:05.225417
Model ind 640 epoch 629 head B head_i_epoch 0 batch 100: avg loss -1.657629 avg loss no lamb -1.657629 time 2019-02-24 17:24:01.423848
Model ind 640 epoch 629 head B head_i_epoch 0 batch 200: avg loss -1.843082 avg loss no lamb -1.843082 time 2019-02-24 17:25:55.119653
last batch sz 160
Model ind 640 epoch 629 head B head_i_epoch 1 batch 0: avg loss -1.731686 avg loss no lamb -1.731686 time 2019-02-24 17:27:16.871138
Model ind 640 epoch 629 head B head_i_epoch 1 batch 100: avg loss -1.740532 avg loss no lamb -1.740532 time 2019-02-24 17:29:12.421350
Model ind 640 epoch 629 head B head_i_epoch 1 batch 200: avg loss -1.808216 avg loss no lamb -1.808216 time 2019-02-24 17:31:06.451254
last batch sz 160
Pre: time 2019-02-24 17:32:50.615297: 
 	std: 0.051332846
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101333, 0.61413336, 0.6142167, 0.61403334, 0.5085667]
	train_accs: [0.5101333, 0.61413336, 0.6142167, 0.61403334, 0.5085667]
	best_train_sub_head: 2
	worst: 0.5085667
	avg: 0.57221663
	best: 0.6142167

Starting e_i: 630
Model ind 640 epoch 630 head A head_i_epoch 0 batch 0: avg loss -3.151541 avg loss no lamb -3.151541 time 2019-02-24 17:32:53.330233
Model ind 640 epoch 630 head A head_i_epoch 0 batch 100: avg loss -2.990988 avg loss no lamb -2.990988 time 2019-02-24 17:34:47.273012
Model ind 640 epoch 630 head A head_i_epoch 0 batch 200: avg loss -3.197285 avg loss no lamb -3.197285 time 2019-02-24 17:36:41.518944
last batch sz 160
Model ind 640 epoch 630 head B head_i_epoch 0 batch 0: avg loss -1.838931 avg loss no lamb -1.838931 time 2019-02-24 17:38:03.838053
Model ind 640 epoch 630 head B head_i_epoch 0 batch 100: avg loss -1.745627 avg loss no lamb -1.745627 time 2019-02-24 17:40:00.251251
Model ind 640 epoch 630 head B head_i_epoch 0 batch 200: avg loss -1.890980 avg loss no lamb -1.890980 time 2019-02-24 17:41:54.543872
last batch sz 160
Model ind 640 epoch 630 head B head_i_epoch 1 batch 0: avg loss -1.733585 avg loss no lamb -1.733585 time 2019-02-24 17:43:17.720125
Model ind 640 epoch 630 head B head_i_epoch 1 batch 100: avg loss -1.727238 avg loss no lamb -1.727238 time 2019-02-24 17:45:12.080793
Model ind 640 epoch 630 head B head_i_epoch 1 batch 200: avg loss -1.816872 avg loss no lamb -1.816872 time 2019-02-24 17:47:05.906520
last batch sz 160
Pre: time 2019-02-24 17:48:48.226932: 
 	std: 0.052865025
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50703335, 0.61408335, 0.61408335, 0.61435, 0.5055]
	train_accs: [0.50703335, 0.61408335, 0.61408335, 0.61435, 0.5055]
	best_train_sub_head: 3
	worst: 0.5055
	avg: 0.57101
	best: 0.61435

Starting e_i: 631
Model ind 640 epoch 631 head A head_i_epoch 0 batch 0: avg loss -3.096621 avg loss no lamb -3.096621 time 2019-02-24 17:48:57.822272
Model ind 640 epoch 631 head A head_i_epoch 0 batch 100: avg loss -3.056032 avg loss no lamb -3.056032 time 2019-02-24 17:50:53.784929
Model ind 640 epoch 631 head A head_i_epoch 0 batch 200: avg loss -3.148200 avg loss no lamb -3.148200 time 2019-02-24 17:52:48.080663
last batch sz 160
Model ind 640 epoch 631 head B head_i_epoch 0 batch 0: avg loss -1.795639 avg loss no lamb -1.795639 time 2019-02-24 17:54:11.111334
Model ind 640 epoch 631 head B head_i_epoch 0 batch 100: avg loss -1.749070 avg loss no lamb -1.749070 time 2019-02-24 17:56:05.593779
Model ind 640 epoch 631 head B head_i_epoch 0 batch 200: avg loss -1.773994 avg loss no lamb -1.773994 time 2019-02-24 17:57:59.444854
last batch sz 160
Model ind 640 epoch 631 head B head_i_epoch 1 batch 0: avg loss -1.816598 avg loss no lamb -1.816598 time 2019-02-24 17:59:22.795721
Model ind 640 epoch 631 head B head_i_epoch 1 batch 100: avg loss -1.668399 avg loss no lamb -1.668399 time 2019-02-24 18:01:16.171126
Model ind 640 epoch 631 head B head_i_epoch 1 batch 200: avg loss -1.844029 avg loss no lamb -1.844029 time 2019-02-24 18:03:10.585860
last batch sz 160
Pre: time 2019-02-24 18:04:55.675183: 
 	std: 0.050347846
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50773335, 0.60978335, 0.6098167, 0.6095, 0.5061333]
	train_accs: [0.50773335, 0.60978335, 0.6098167, 0.6095, 0.5061333]
	best_train_sub_head: 2
	worst: 0.5061333
	avg: 0.5685934
	best: 0.6098167

Starting e_i: 632
Model ind 640 epoch 632 head A head_i_epoch 0 batch 0: avg loss -3.085372 avg loss no lamb -3.085372 time 2019-02-24 18:04:58.396545
Model ind 640 epoch 632 head A head_i_epoch 0 batch 100: avg loss -2.929124 avg loss no lamb -2.929124 time 2019-02-24 18:06:52.314898
Model ind 640 epoch 632 head A head_i_epoch 0 batch 200: avg loss -3.138318 avg loss no lamb -3.138318 time 2019-02-24 18:08:45.643248
last batch sz 160
Model ind 640 epoch 632 head B head_i_epoch 0 batch 0: avg loss -1.663936 avg loss no lamb -1.663936 time 2019-02-24 18:10:07.310669
Model ind 640 epoch 632 head B head_i_epoch 0 batch 100: avg loss -1.689211 avg loss no lamb -1.689211 time 2019-02-24 18:12:01.302580
Model ind 640 epoch 632 head B head_i_epoch 0 batch 200: avg loss -1.908676 avg loss no lamb -1.908676 time 2019-02-24 18:13:55.269753
last batch sz 160
Model ind 640 epoch 632 head B head_i_epoch 1 batch 0: avg loss -1.731544 avg loss no lamb -1.731544 time 2019-02-24 18:15:17.819922
Model ind 640 epoch 632 head B head_i_epoch 1 batch 100: avg loss -1.717857 avg loss no lamb -1.717857 time 2019-02-24 18:17:11.389547
Model ind 640 epoch 632 head B head_i_epoch 1 batch 200: avg loss -1.902478 avg loss no lamb -1.902478 time 2019-02-24 18:19:04.456401
last batch sz 160
Pre: time 2019-02-24 18:20:49.109545: 
 	std: 0.052035756
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50715, 0.6128333, 0.6125, 0.6127833, 0.5058333]
	train_accs: [0.50715, 0.6128333, 0.6125, 0.6127833, 0.5058333]
	best_train_sub_head: 1
	worst: 0.5058333
	avg: 0.57022005
	best: 0.6128333

Starting e_i: 633
Model ind 640 epoch 633 head A head_i_epoch 0 batch 0: avg loss -3.108910 avg loss no lamb -3.108910 time 2019-02-24 18:20:51.574658
Model ind 640 epoch 633 head A head_i_epoch 0 batch 100: avg loss -2.974310 avg loss no lamb -2.974310 time 2019-02-24 18:22:45.676209
Model ind 640 epoch 633 head A head_i_epoch 0 batch 200: avg loss -3.161659 avg loss no lamb -3.161659 time 2019-02-24 18:24:40.057904
last batch sz 160
Model ind 640 epoch 633 head B head_i_epoch 0 batch 0: avg loss -1.828829 avg loss no lamb -1.828829 time 2019-02-24 18:26:03.093445
Model ind 640 epoch 633 head B head_i_epoch 0 batch 100: avg loss -1.732203 avg loss no lamb -1.732203 time 2019-02-24 18:27:57.619993
Model ind 640 epoch 633 head B head_i_epoch 0 batch 200: avg loss -1.843935 avg loss no lamb -1.843935 time 2019-02-24 18:29:51.936648
last batch sz 160
Model ind 640 epoch 633 head B head_i_epoch 1 batch 0: avg loss -1.710159 avg loss no lamb -1.710159 time 2019-02-24 18:31:15.348083
Model ind 640 epoch 633 head B head_i_epoch 1 batch 100: avg loss -1.761034 avg loss no lamb -1.761034 time 2019-02-24 18:33:07.237798
Model ind 640 epoch 633 head B head_i_epoch 1 batch 200: avg loss -1.845011 avg loss no lamb -1.845011 time 2019-02-24 18:35:02.471230
last batch sz 160
Pre: time 2019-02-24 18:36:46.809527: 
 	std: 0.050960336
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5085, 0.61251664, 0.61268336, 0.6124667, 0.5085667]
	train_accs: [0.5085, 0.61251664, 0.61268336, 0.6124667, 0.5085667]
	best_train_sub_head: 2
	worst: 0.5085
	avg: 0.57094663
	best: 0.61268336

Starting e_i: 634
Model ind 640 epoch 634 head A head_i_epoch 0 batch 0: avg loss -3.103407 avg loss no lamb -3.103407 time 2019-02-24 18:36:49.568614
Model ind 640 epoch 634 head A head_i_epoch 0 batch 100: avg loss -3.062109 avg loss no lamb -3.062109 time 2019-02-24 18:38:45.777323
Model ind 640 epoch 634 head A head_i_epoch 0 batch 200: avg loss -3.207352 avg loss no lamb -3.207352 time 2019-02-24 18:40:40.571391
last batch sz 160
Model ind 640 epoch 634 head B head_i_epoch 0 batch 0: avg loss -1.772789 avg loss no lamb -1.772789 time 2019-02-24 18:42:03.318442
Model ind 640 epoch 634 head B head_i_epoch 0 batch 100: avg loss -1.668577 avg loss no lamb -1.668577 time 2019-02-24 18:43:57.280366
Model ind 640 epoch 634 head B head_i_epoch 0 batch 200: avg loss -1.890559 avg loss no lamb -1.890559 time 2019-02-24 18:45:51.633144
last batch sz 160
Model ind 640 epoch 634 head B head_i_epoch 1 batch 0: avg loss -1.753999 avg loss no lamb -1.753999 time 2019-02-24 18:47:14.776522
Model ind 640 epoch 634 head B head_i_epoch 1 batch 100: avg loss -1.740152 avg loss no lamb -1.740152 time 2019-02-24 18:49:09.829589
Model ind 640 epoch 634 head B head_i_epoch 1 batch 200: avg loss -1.847029 avg loss no lamb -1.847029 time 2019-02-24 18:51:03.914347
last batch sz 160
Pre: time 2019-02-24 18:52:49.192071: 
 	std: 0.051594157
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5083, 0.61258334, 0.6124333, 0.61235, 0.506]
	train_accs: [0.5083, 0.61258334, 0.6124333, 0.61235, 0.506]
	best_train_sub_head: 1
	worst: 0.506
	avg: 0.57033336
	best: 0.61258334

Starting e_i: 635
Model ind 640 epoch 635 head A head_i_epoch 0 batch 0: avg loss -3.116860 avg loss no lamb -3.116860 time 2019-02-24 18:52:52.069677
Model ind 640 epoch 635 head A head_i_epoch 0 batch 100: avg loss -2.928812 avg loss no lamb -2.928812 time 2019-02-24 18:54:45.426814
Model ind 640 epoch 635 head A head_i_epoch 0 batch 200: avg loss -3.072940 avg loss no lamb -3.072940 time 2019-02-24 18:56:40.998404
last batch sz 160
Model ind 640 epoch 635 head B head_i_epoch 0 batch 0: avg loss -1.784020 avg loss no lamb -1.784020 time 2019-02-24 18:58:03.703004
Model ind 640 epoch 635 head B head_i_epoch 0 batch 100: avg loss -1.673101 avg loss no lamb -1.673101 time 2019-02-24 18:59:57.536283
Model ind 640 epoch 635 head B head_i_epoch 0 batch 200: avg loss -1.855965 avg loss no lamb -1.855965 time 2019-02-24 19:01:52.191629
last batch sz 160
Model ind 640 epoch 635 head B head_i_epoch 1 batch 0: avg loss -1.852087 avg loss no lamb -1.852087 time 2019-02-24 19:03:15.191836
Model ind 640 epoch 635 head B head_i_epoch 1 batch 100: avg loss -1.704408 avg loss no lamb -1.704408 time 2019-02-24 19:05:09.573526
Model ind 640 epoch 635 head B head_i_epoch 1 batch 200: avg loss -1.840117 avg loss no lamb -1.840117 time 2019-02-24 19:07:03.366570
last batch sz 160
Pre: time 2019-02-24 19:08:48.247328: 
 	std: 0.050588127
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.512, 0.6148667, 0.61501664, 0.6147, 0.5112]
	train_accs: [0.512, 0.6148667, 0.61501664, 0.6147, 0.5112]
	best_train_sub_head: 2
	worst: 0.5112
	avg: 0.57355666
	best: 0.61501664

Starting e_i: 636
Model ind 640 epoch 636 head A head_i_epoch 0 batch 0: avg loss -3.202275 avg loss no lamb -3.202275 time 2019-02-24 19:08:51.006644
Model ind 640 epoch 636 head A head_i_epoch 0 batch 100: avg loss -3.026382 avg loss no lamb -3.026382 time 2019-02-24 19:10:45.021879
Model ind 640 epoch 636 head A head_i_epoch 0 batch 200: avg loss -3.141777 avg loss no lamb -3.141777 time 2019-02-24 19:12:39.277229
last batch sz 160
Model ind 640 epoch 636 head B head_i_epoch 0 batch 0: avg loss -1.746570 avg loss no lamb -1.746570 time 2019-02-24 19:14:02.329970
Model ind 640 epoch 636 head B head_i_epoch 0 batch 100: avg loss -1.697099 avg loss no lamb -1.697099 time 2019-02-24 19:15:55.983615
Model ind 640 epoch 636 head B head_i_epoch 0 batch 200: avg loss -1.867136 avg loss no lamb -1.867136 time 2019-02-24 19:17:47.637594
last batch sz 160
Model ind 640 epoch 636 head B head_i_epoch 1 batch 0: avg loss -1.754365 avg loss no lamb -1.754365 time 2019-02-24 19:19:10.852389
Model ind 640 epoch 636 head B head_i_epoch 1 batch 100: avg loss -1.723618 avg loss no lamb -1.723618 time 2019-02-24 19:21:05.078946
Model ind 640 epoch 636 head B head_i_epoch 1 batch 200: avg loss -1.730302 avg loss no lamb -1.730302 time 2019-02-24 19:22:59.183569
last batch sz 160
Pre: time 2019-02-24 19:24:44.375536: 
 	std: 0.051303923
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5071833, 0.61141664, 0.61095, 0.6112667, 0.5058]
	train_accs: [0.5071833, 0.61141664, 0.61095, 0.6112667, 0.5058]
	best_train_sub_head: 1
	worst: 0.5058
	avg: 0.5693233
	best: 0.61141664

Starting e_i: 637
Model ind 640 epoch 637 head A head_i_epoch 0 batch 0: avg loss -3.047422 avg loss no lamb -3.047422 time 2019-02-24 19:24:47.012327
Model ind 640 epoch 637 head A head_i_epoch 0 batch 100: avg loss -3.032902 avg loss no lamb -3.032902 time 2019-02-24 19:26:41.529852
Model ind 640 epoch 637 head A head_i_epoch 0 batch 200: avg loss -3.009244 avg loss no lamb -3.009244 time 2019-02-24 19:28:35.529733
last batch sz 160
Model ind 640 epoch 637 head B head_i_epoch 0 batch 0: avg loss -1.767711 avg loss no lamb -1.767711 time 2019-02-24 19:29:57.971994
Model ind 640 epoch 637 head B head_i_epoch 0 batch 100: avg loss -1.746259 avg loss no lamb -1.746259 time 2019-02-24 19:31:51.677561
Model ind 640 epoch 637 head B head_i_epoch 0 batch 200: avg loss -1.817166 avg loss no lamb -1.817166 time 2019-02-24 19:33:46.663747
last batch sz 160
Model ind 640 epoch 637 head B head_i_epoch 1 batch 0: avg loss -1.781666 avg loss no lamb -1.781666 time 2019-02-24 19:35:09.456369
Model ind 640 epoch 637 head B head_i_epoch 1 batch 100: avg loss -1.731532 avg loss no lamb -1.731532 time 2019-02-24 19:37:02.540917
Model ind 640 epoch 637 head B head_i_epoch 1 batch 200: avg loss -1.852896 avg loss no lamb -1.852896 time 2019-02-24 19:38:55.403643
last batch sz 160
Pre: time 2019-02-24 19:40:41.131954: 
 	std: 0.052452665
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5053, 0.6117833, 0.61195, 0.61128336, 0.5039167]
	train_accs: [0.5053, 0.6117833, 0.61195, 0.61128336, 0.5039167]
	best_train_sub_head: 2
	worst: 0.5039167
	avg: 0.56884664
	best: 0.61195

Starting e_i: 638
Model ind 640 epoch 638 head A head_i_epoch 0 batch 0: avg loss -3.170730 avg loss no lamb -3.170730 time 2019-02-24 19:40:43.709340
Model ind 640 epoch 638 head A head_i_epoch 0 batch 100: avg loss -3.076770 avg loss no lamb -3.076770 time 2019-02-24 19:42:37.767137
Model ind 640 epoch 638 head A head_i_epoch 0 batch 200: avg loss -3.173978 avg loss no lamb -3.173978 time 2019-02-24 19:44:32.424031
last batch sz 160
Model ind 640 epoch 638 head B head_i_epoch 0 batch 0: avg loss -1.853928 avg loss no lamb -1.853928 time 2019-02-24 19:45:55.639837
Model ind 640 epoch 638 head B head_i_epoch 0 batch 100: avg loss -1.725117 avg loss no lamb -1.725117 time 2019-02-24 19:47:49.622379
Model ind 640 epoch 638 head B head_i_epoch 0 batch 200: avg loss -1.809673 avg loss no lamb -1.809673 time 2019-02-24 19:49:43.111093
last batch sz 160
Model ind 640 epoch 638 head B head_i_epoch 1 batch 0: avg loss -1.816945 avg loss no lamb -1.816945 time 2019-02-24 19:51:06.289912
Model ind 640 epoch 638 head B head_i_epoch 1 batch 100: avg loss -1.713825 avg loss no lamb -1.713825 time 2019-02-24 19:53:00.327642
Model ind 640 epoch 638 head B head_i_epoch 1 batch 200: avg loss -1.857775 avg loss no lamb -1.857775 time 2019-02-24 19:54:55.150711
last batch sz 160
Pre: time 2019-02-24 19:56:39.183798: 
 	std: 0.05073703
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50998336, 0.613, 0.61295, 0.61296666, 0.50883335]
	train_accs: [0.50998336, 0.613, 0.61295, 0.61296666, 0.50883335]
	best_train_sub_head: 1
	worst: 0.50883335
	avg: 0.5715467
	best: 0.613

Starting e_i: 639
Model ind 640 epoch 639 head A head_i_epoch 0 batch 0: avg loss -3.183492 avg loss no lamb -3.183492 time 2019-02-24 19:56:41.712877
Model ind 640 epoch 639 head A head_i_epoch 0 batch 100: avg loss -2.931470 avg loss no lamb -2.931470 time 2019-02-24 19:58:36.323406
Model ind 640 epoch 639 head A head_i_epoch 0 batch 200: avg loss -3.122393 avg loss no lamb -3.122393 time 2019-02-24 20:00:30.862400
last batch sz 160
Model ind 640 epoch 639 head B head_i_epoch 0 batch 0: avg loss -1.797455 avg loss no lamb -1.797455 time 2019-02-24 20:01:53.998843
Model ind 640 epoch 639 head B head_i_epoch 0 batch 100: avg loss -1.759554 avg loss no lamb -1.759554 time 2019-02-24 20:03:49.161563
Model ind 640 epoch 639 head B head_i_epoch 0 batch 200: avg loss -1.897134 avg loss no lamb -1.897134 time 2019-02-24 20:05:43.323268
last batch sz 160
Model ind 640 epoch 639 head B head_i_epoch 1 batch 0: avg loss -1.794141 avg loss no lamb -1.794141 time 2019-02-24 20:07:05.967866
Model ind 640 epoch 639 head B head_i_epoch 1 batch 100: avg loss -1.699785 avg loss no lamb -1.699785 time 2019-02-24 20:09:00.356455
Model ind 640 epoch 639 head B head_i_epoch 1 batch 200: avg loss -1.858410 avg loss no lamb -1.858410 time 2019-02-24 20:10:54.597829
last batch sz 160
Pre: time 2019-02-24 20:12:38.872453: 
 	std: 0.05164774
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50895, 0.61436665, 0.61446667, 0.61401665, 0.50876665]
	train_accs: [0.50895, 0.61436665, 0.61446667, 0.61401665, 0.50876665]
	best_train_sub_head: 2
	worst: 0.50876665
	avg: 0.57211334
	best: 0.61446667

Starting e_i: 640
Model ind 640 epoch 640 head A head_i_epoch 0 batch 0: avg loss -3.124348 avg loss no lamb -3.124348 time 2019-02-24 20:12:41.721270
Model ind 640 epoch 640 head A head_i_epoch 0 batch 100: avg loss -2.992416 avg loss no lamb -2.992416 time 2019-02-24 20:14:35.408914
Model ind 640 epoch 640 head A head_i_epoch 0 batch 200: avg loss -3.156548 avg loss no lamb -3.156548 time 2019-02-24 20:16:29.613689
last batch sz 160
Model ind 640 epoch 640 head B head_i_epoch 0 batch 0: avg loss -1.781260 avg loss no lamb -1.781260 time 2019-02-24 20:17:52.755963
Model ind 640 epoch 640 head B head_i_epoch 0 batch 100: avg loss -1.712547 avg loss no lamb -1.712547 time 2019-02-24 20:19:46.420278
Model ind 640 epoch 640 head B head_i_epoch 0 batch 200: avg loss -1.820612 avg loss no lamb -1.820612 time 2019-02-24 20:21:40.590408
last batch sz 160
Model ind 640 epoch 640 head B head_i_epoch 1 batch 0: avg loss -1.837402 avg loss no lamb -1.837402 time 2019-02-24 20:23:01.380431
Model ind 640 epoch 640 head B head_i_epoch 1 batch 100: avg loss -1.698601 avg loss no lamb -1.698601 time 2019-02-24 20:24:56.669455
Model ind 640 epoch 640 head B head_i_epoch 1 batch 200: avg loss -1.799446 avg loss no lamb -1.799446 time 2019-02-24 20:26:51.268926
last batch sz 160
Pre: time 2019-02-24 20:28:35.816349: 
 	std: 0.049988735
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107667, 0.6128, 0.61298335, 0.61268336, 0.5108]
	train_accs: [0.5107667, 0.6128, 0.61298335, 0.61268336, 0.5108]
	best_train_sub_head: 2
	worst: 0.5107667
	avg: 0.5720067
	best: 0.61298335

Starting e_i: 641
Model ind 640 epoch 641 head A head_i_epoch 0 batch 0: avg loss -3.102591 avg loss no lamb -3.102591 time 2019-02-24 20:28:48.262693
Model ind 640 epoch 641 head A head_i_epoch 0 batch 100: avg loss -3.041058 avg loss no lamb -3.041058 time 2019-02-24 20:30:42.558257
Model ind 640 epoch 641 head A head_i_epoch 0 batch 200: avg loss -3.180089 avg loss no lamb -3.180089 time 2019-02-24 20:32:37.313446
last batch sz 160
Model ind 640 epoch 641 head B head_i_epoch 0 batch 0: avg loss -1.806937 avg loss no lamb -1.806937 time 2019-02-24 20:34:00.827666
Model ind 640 epoch 641 head B head_i_epoch 0 batch 100: avg loss -1.678297 avg loss no lamb -1.678297 time 2019-02-24 20:35:53.215093
Model ind 640 epoch 641 head B head_i_epoch 0 batch 200: avg loss -1.786312 avg loss no lamb -1.786312 time 2019-02-24 20:37:48.660656
last batch sz 160
Model ind 640 epoch 641 head B head_i_epoch 1 batch 0: avg loss -1.782690 avg loss no lamb -1.782690 time 2019-02-24 20:39:12.072292
Model ind 640 epoch 641 head B head_i_epoch 1 batch 100: avg loss -1.663761 avg loss no lamb -1.663761 time 2019-02-24 20:41:05.681282
Model ind 640 epoch 641 head B head_i_epoch 1 batch 200: avg loss -1.799309 avg loss no lamb -1.799309 time 2019-02-24 20:42:59.112365
last batch sz 160
Pre: time 2019-02-24 20:44:41.450077: 
 	std: 0.051202916
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5039, 0.60885, 0.60831666, 0.60828334, 0.5040333]
	train_accs: [0.5039, 0.60885, 0.60831666, 0.60828334, 0.5040333]
	best_train_sub_head: 1
	worst: 0.5039
	avg: 0.5666767
	best: 0.60885

Starting e_i: 642
Model ind 640 epoch 642 head A head_i_epoch 0 batch 0: avg loss -3.088765 avg loss no lamb -3.088765 time 2019-02-24 20:44:43.839311
Model ind 640 epoch 642 head A head_i_epoch 0 batch 100: avg loss -3.022190 avg loss no lamb -3.022190 time 2019-02-24 20:46:39.175336
Model ind 640 epoch 642 head A head_i_epoch 0 batch 200: avg loss -3.160833 avg loss no lamb -3.160833 time 2019-02-24 20:48:33.235232
last batch sz 160
Model ind 640 epoch 642 head B head_i_epoch 0 batch 0: avg loss -1.823770 avg loss no lamb -1.823770 time 2019-02-24 20:49:57.043398
Model ind 640 epoch 642 head B head_i_epoch 0 batch 100: avg loss -1.712885 avg loss no lamb -1.712885 time 2019-02-24 20:51:50.998918
Model ind 640 epoch 642 head B head_i_epoch 0 batch 200: avg loss -1.881988 avg loss no lamb -1.881988 time 2019-02-24 20:53:44.271443
last batch sz 160
Model ind 640 epoch 642 head B head_i_epoch 1 batch 0: avg loss -1.875795 avg loss no lamb -1.875795 time 2019-02-24 20:55:06.842007
Model ind 640 epoch 642 head B head_i_epoch 1 batch 100: avg loss -1.687305 avg loss no lamb -1.687305 time 2019-02-24 20:57:00.714319
Model ind 640 epoch 642 head B head_i_epoch 1 batch 200: avg loss -1.817568 avg loss no lamb -1.817568 time 2019-02-24 20:58:54.540468
last batch sz 160
Pre: time 2019-02-24 21:00:38.842201: 
 	std: 0.050568588
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5102, 0.6134833, 0.6134167, 0.6130667, 0.51]
	train_accs: [0.5102, 0.6134833, 0.6134167, 0.6130667, 0.51]
	best_train_sub_head: 1
	worst: 0.51
	avg: 0.57203335
	best: 0.6134833

Starting e_i: 643
Model ind 640 epoch 643 head A head_i_epoch 0 batch 0: avg loss -3.115384 avg loss no lamb -3.115384 time 2019-02-24 21:00:41.638350
Model ind 640 epoch 643 head A head_i_epoch 0 batch 100: avg loss -3.014485 avg loss no lamb -3.014485 time 2019-02-24 21:02:35.732815
Model ind 640 epoch 643 head A head_i_epoch 0 batch 200: avg loss -3.133739 avg loss no lamb -3.133739 time 2019-02-24 21:04:29.957164
last batch sz 160
Model ind 640 epoch 643 head B head_i_epoch 0 batch 0: avg loss -1.816561 avg loss no lamb -1.816561 time 2019-02-24 21:05:52.573455
Model ind 640 epoch 643 head B head_i_epoch 0 batch 100: avg loss -1.737329 avg loss no lamb -1.737329 time 2019-02-24 21:07:45.398018
Model ind 640 epoch 643 head B head_i_epoch 0 batch 200: avg loss -1.895733 avg loss no lamb -1.895733 time 2019-02-24 21:09:39.954692
last batch sz 160
Model ind 640 epoch 643 head B head_i_epoch 1 batch 0: avg loss -1.835756 avg loss no lamb -1.835756 time 2019-02-24 21:11:03.099473
Model ind 640 epoch 643 head B head_i_epoch 1 batch 100: avg loss -1.634869 avg loss no lamb -1.634869 time 2019-02-24 21:12:56.071890
Model ind 640 epoch 643 head B head_i_epoch 1 batch 200: avg loss -1.873209 avg loss no lamb -1.873209 time 2019-02-24 21:14:49.571301
last batch sz 160
Pre: time 2019-02-24 21:16:34.482467: 
 	std: 0.0514821
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50715, 0.61186665, 0.61225, 0.6117167, 0.50656664]
	train_accs: [0.50715, 0.61186665, 0.61225, 0.6117167, 0.50656664]
	best_train_sub_head: 2
	worst: 0.50656664
	avg: 0.56990993
	best: 0.61225

Starting e_i: 644
Model ind 640 epoch 644 head A head_i_epoch 0 batch 0: avg loss -3.087850 avg loss no lamb -3.087850 time 2019-02-24 21:16:37.449808
Model ind 640 epoch 644 head A head_i_epoch 0 batch 100: avg loss -3.079328 avg loss no lamb -3.079328 time 2019-02-24 21:18:31.605744
Model ind 640 epoch 644 head A head_i_epoch 0 batch 200: avg loss -3.133920 avg loss no lamb -3.133920 time 2019-02-24 21:20:25.146612
last batch sz 160
Model ind 640 epoch 644 head B head_i_epoch 0 batch 0: avg loss -1.746078 avg loss no lamb -1.746078 time 2019-02-24 21:21:48.541606
Model ind 640 epoch 644 head B head_i_epoch 0 batch 100: avg loss -1.664753 avg loss no lamb -1.664753 time 2019-02-24 21:23:42.146682
Model ind 640 epoch 644 head B head_i_epoch 0 batch 200: avg loss -1.754588 avg loss no lamb -1.754588 time 2019-02-24 21:25:35.637571
last batch sz 160
Model ind 640 epoch 644 head B head_i_epoch 1 batch 0: avg loss -1.748033 avg loss no lamb -1.748033 time 2019-02-24 21:26:58.208061
Model ind 640 epoch 644 head B head_i_epoch 1 batch 100: avg loss -1.723814 avg loss no lamb -1.723814 time 2019-02-24 21:28:50.877264
Model ind 640 epoch 644 head B head_i_epoch 1 batch 200: avg loss -1.886614 avg loss no lamb -1.886614 time 2019-02-24 21:30:45.090178
last batch sz 160
Pre: time 2019-02-24 21:32:29.977773: 
 	std: 0.05010592
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51155, 0.61355, 0.61403334, 0.61375, 0.51145]
	train_accs: [0.51155, 0.61355, 0.61403334, 0.61375, 0.51145]
	best_train_sub_head: 2
	worst: 0.51145
	avg: 0.5728667
	best: 0.61403334

Starting e_i: 645
Model ind 640 epoch 645 head A head_i_epoch 0 batch 0: avg loss -3.050813 avg loss no lamb -3.050813 time 2019-02-24 21:32:33.387414
Model ind 640 epoch 645 head A head_i_epoch 0 batch 100: avg loss -3.020390 avg loss no lamb -3.020390 time 2019-02-24 21:34:26.614055
Model ind 640 epoch 645 head A head_i_epoch 0 batch 200: avg loss -3.117068 avg loss no lamb -3.117068 time 2019-02-24 21:36:18.161779
last batch sz 160
Model ind 640 epoch 645 head B head_i_epoch 0 batch 0: avg loss -1.822728 avg loss no lamb -1.822728 time 2019-02-24 21:37:42.208555
Model ind 640 epoch 645 head B head_i_epoch 0 batch 100: avg loss -1.642740 avg loss no lamb -1.642740 time 2019-02-24 21:39:35.135848
Model ind 640 epoch 645 head B head_i_epoch 0 batch 200: avg loss -1.790979 avg loss no lamb -1.790979 time 2019-02-24 21:41:27.785939
last batch sz 160
Model ind 640 epoch 645 head B head_i_epoch 1 batch 0: avg loss -1.835341 avg loss no lamb -1.835341 time 2019-02-24 21:42:49.316110
Model ind 640 epoch 645 head B head_i_epoch 1 batch 100: avg loss -1.799844 avg loss no lamb -1.799844 time 2019-02-24 21:44:41.827117
Model ind 640 epoch 645 head B head_i_epoch 1 batch 200: avg loss -1.814643 avg loss no lamb -1.814643 time 2019-02-24 21:46:34.832913
last batch sz 160
Pre: time 2019-02-24 21:48:17.793832: 
 	std: 0.050447717
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5086333, 0.6114, 0.61113334, 0.6113667, 0.50801665]
	train_accs: [0.5086333, 0.6114, 0.61113334, 0.6113667, 0.50801665]
	best_train_sub_head: 1
	worst: 0.50801665
	avg: 0.57010996
	best: 0.6114

Starting e_i: 646
Model ind 640 epoch 646 head A head_i_epoch 0 batch 0: avg loss -3.099695 avg loss no lamb -3.099695 time 2019-02-24 21:48:20.241510
Model ind 640 epoch 646 head A head_i_epoch 0 batch 100: avg loss -3.002729 avg loss no lamb -3.002729 time 2019-02-24 21:50:13.145207
Model ind 640 epoch 646 head A head_i_epoch 0 batch 200: avg loss -3.156144 avg loss no lamb -3.156144 time 2019-02-24 21:52:04.722614
last batch sz 160
Model ind 640 epoch 646 head B head_i_epoch 0 batch 0: avg loss -1.787063 avg loss no lamb -1.787063 time 2019-02-24 21:53:28.330901
Model ind 640 epoch 646 head B head_i_epoch 0 batch 100: avg loss -1.701014 avg loss no lamb -1.701014 time 2019-02-24 21:55:21.325717
Model ind 640 epoch 646 head B head_i_epoch 0 batch 200: avg loss -1.773660 avg loss no lamb -1.773660 time 2019-02-24 21:57:14.079900
last batch sz 160
Model ind 640 epoch 646 head B head_i_epoch 1 batch 0: avg loss -1.833239 avg loss no lamb -1.833239 time 2019-02-24 21:58:36.098037
Model ind 640 epoch 646 head B head_i_epoch 1 batch 100: avg loss -1.683187 avg loss no lamb -1.683187 time 2019-02-24 22:00:29.269983
Model ind 640 epoch 646 head B head_i_epoch 1 batch 200: avg loss -1.798831 avg loss no lamb -1.798831 time 2019-02-24 22:02:22.273219
last batch sz 160
Pre: time 2019-02-24 22:04:05.875214: 
 	std: 0.05056718
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51, 0.61336666, 0.6135, 0.6131667, 0.51025]
	train_accs: [0.51, 0.61336666, 0.6135, 0.6131667, 0.51025]
	best_train_sub_head: 2
	worst: 0.51
	avg: 0.57205665
	best: 0.6135

Starting e_i: 647
Model ind 640 epoch 647 head A head_i_epoch 0 batch 0: avg loss -3.049437 avg loss no lamb -3.049437 time 2019-02-24 22:04:08.831714
Model ind 640 epoch 647 head A head_i_epoch 0 batch 100: avg loss -3.013313 avg loss no lamb -3.013313 time 2019-02-24 22:06:01.784417
Model ind 640 epoch 647 head A head_i_epoch 0 batch 200: avg loss -3.127652 avg loss no lamb -3.127652 time 2019-02-24 22:07:58.056230
last batch sz 160
Model ind 640 epoch 647 head B head_i_epoch 0 batch 0: avg loss -1.783048 avg loss no lamb -1.783048 time 2019-02-24 22:09:19.710336
Model ind 640 epoch 647 head B head_i_epoch 0 batch 100: avg loss -1.676676 avg loss no lamb -1.676676 time 2019-02-24 22:11:12.353430
Model ind 640 epoch 647 head B head_i_epoch 0 batch 200: avg loss -1.858534 avg loss no lamb -1.858534 time 2019-02-24 22:13:03.976164
last batch sz 160
Model ind 640 epoch 647 head B head_i_epoch 1 batch 0: avg loss -1.772282 avg loss no lamb -1.772282 time 2019-02-24 22:14:26.428091
Model ind 640 epoch 647 head B head_i_epoch 1 batch 100: avg loss -1.682268 avg loss no lamb -1.682268 time 2019-02-24 22:16:20.314180
Model ind 640 epoch 647 head B head_i_epoch 1 batch 200: avg loss -1.854879 avg loss no lamb -1.854879 time 2019-02-24 22:18:13.106834
last batch sz 160
Pre: time 2019-02-24 22:19:56.596555: 
 	std: 0.050923813
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5079333, 0.6120167, 0.61221665, 0.6117333, 0.50815]
	train_accs: [0.5079333, 0.6120167, 0.61221665, 0.6117333, 0.50815]
	best_train_sub_head: 2
	worst: 0.5079333
	avg: 0.5704101
	best: 0.61221665

Starting e_i: 648
Model ind 640 epoch 648 head A head_i_epoch 0 batch 0: avg loss -3.119859 avg loss no lamb -3.119859 time 2019-02-24 22:19:59.605596
Model ind 640 epoch 648 head A head_i_epoch 0 batch 100: avg loss -2.906463 avg loss no lamb -2.906463 time 2019-02-24 22:21:53.098909
Model ind 640 epoch 648 head A head_i_epoch 0 batch 200: avg loss -3.111664 avg loss no lamb -3.111664 time 2019-02-24 22:23:46.251689
last batch sz 160
Model ind 640 epoch 648 head B head_i_epoch 0 batch 0: avg loss -1.693541 avg loss no lamb -1.693541 time 2019-02-24 22:25:08.730071
Model ind 640 epoch 648 head B head_i_epoch 0 batch 100: avg loss -1.749749 avg loss no lamb -1.749749 time 2019-02-24 22:27:01.916737
Model ind 640 epoch 648 head B head_i_epoch 0 batch 200: avg loss -1.834839 avg loss no lamb -1.834839 time 2019-02-24 22:28:54.090213
last batch sz 160
Model ind 640 epoch 648 head B head_i_epoch 1 batch 0: avg loss -1.859195 avg loss no lamb -1.859195 time 2019-02-24 22:30:16.298419
Model ind 640 epoch 648 head B head_i_epoch 1 batch 100: avg loss -1.709297 avg loss no lamb -1.709297 time 2019-02-24 22:32:09.001449
Model ind 640 epoch 648 head B head_i_epoch 1 batch 200: avg loss -1.813087 avg loss no lamb -1.813087 time 2019-02-24 22:34:02.144072
last batch sz 160
Pre: time 2019-02-24 22:35:42.785620: 
 	std: 0.05158822
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.505, 0.6110333, 0.61111665, 0.61085, 0.5064]
	train_accs: [0.505, 0.6110333, 0.61111665, 0.61085, 0.5064]
	best_train_sub_head: 2
	worst: 0.505
	avg: 0.56887996
	best: 0.61111665

Starting e_i: 649
Model ind 640 epoch 649 head A head_i_epoch 0 batch 0: avg loss -3.108372 avg loss no lamb -3.108372 time 2019-02-24 22:35:45.125535
Model ind 640 epoch 649 head A head_i_epoch 0 batch 100: avg loss -3.114973 avg loss no lamb -3.114973 time 2019-02-24 22:37:43.099948
Model ind 640 epoch 649 head A head_i_epoch 0 batch 200: avg loss -3.159862 avg loss no lamb -3.159862 time 2019-02-24 22:39:35.741208
last batch sz 160
Model ind 640 epoch 649 head B head_i_epoch 0 batch 0: avg loss -1.816338 avg loss no lamb -1.816338 time 2019-02-24 22:40:57.891472
Model ind 640 epoch 649 head B head_i_epoch 0 batch 100: avg loss -1.697529 avg loss no lamb -1.697529 time 2019-02-24 22:42:51.304989
Model ind 640 epoch 649 head B head_i_epoch 0 batch 200: avg loss -1.838890 avg loss no lamb -1.838890 time 2019-02-24 22:44:44.084463
last batch sz 160
Model ind 640 epoch 649 head B head_i_epoch 1 batch 0: avg loss -1.706069 avg loss no lamb -1.706069 time 2019-02-24 22:46:05.714246
Model ind 640 epoch 649 head B head_i_epoch 1 batch 100: avg loss -1.626118 avg loss no lamb -1.626118 time 2019-02-24 22:47:59.168299
Model ind 640 epoch 649 head B head_i_epoch 1 batch 200: avg loss -1.838908 avg loss no lamb -1.838908 time 2019-02-24 22:49:52.291448
last batch sz 160
Pre: time 2019-02-24 22:51:35.519598: 
 	std: 0.050806753
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50811666, 0.612, 0.61156666, 0.61163336, 0.5079333]
	train_accs: [0.50811666, 0.612, 0.61156666, 0.61163336, 0.5079333]
	best_train_sub_head: 1
	worst: 0.5079333
	avg: 0.57025003
	best: 0.612

Starting e_i: 650
Model ind 640 epoch 650 head A head_i_epoch 0 batch 0: avg loss -3.162080 avg loss no lamb -3.162080 time 2019-02-24 22:51:37.963493
Model ind 640 epoch 650 head A head_i_epoch 0 batch 100: avg loss -3.046399 avg loss no lamb -3.046399 time 2019-02-24 22:53:30.647791
Model ind 640 epoch 650 head A head_i_epoch 0 batch 200: avg loss -3.124194 avg loss no lamb -3.124194 time 2019-02-24 22:55:24.017842
last batch sz 160
Model ind 640 epoch 650 head B head_i_epoch 0 batch 0: avg loss -1.795996 avg loss no lamb -1.795996 time 2019-02-24 22:56:46.311296
Model ind 640 epoch 650 head B head_i_epoch 0 batch 100: avg loss -1.683805 avg loss no lamb -1.683805 time 2019-02-24 22:58:38.396822
Model ind 640 epoch 650 head B head_i_epoch 0 batch 200: avg loss -1.851502 avg loss no lamb -1.851502 time 2019-02-24 23:00:31.802015
last batch sz 160
Model ind 640 epoch 650 head B head_i_epoch 1 batch 0: avg loss -1.860161 avg loss no lamb -1.860161 time 2019-02-24 23:01:53.690778
Model ind 640 epoch 650 head B head_i_epoch 1 batch 100: avg loss -1.706187 avg loss no lamb -1.706187 time 2019-02-24 23:03:46.418458
Model ind 640 epoch 650 head B head_i_epoch 1 batch 200: avg loss -1.903298 avg loss no lamb -1.903298 time 2019-02-24 23:05:39.678627
last batch sz 160
Pre: time 2019-02-24 23:07:23.019182: 
 	std: 0.051007096
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50913334, 0.61275, 0.61303335, 0.61291665, 0.50843334]
	train_accs: [0.50913334, 0.61275, 0.61303335, 0.61291665, 0.50843334]
	best_train_sub_head: 2
	worst: 0.50843334
	avg: 0.57125336
	best: 0.61303335

Starting e_i: 651
Model ind 640 epoch 651 head A head_i_epoch 0 batch 0: avg loss -3.056994 avg loss no lamb -3.056994 time 2019-02-24 23:07:33.526538
Model ind 640 epoch 651 head A head_i_epoch 0 batch 100: avg loss -3.091179 avg loss no lamb -3.091179 time 2019-02-24 23:09:27.635856
Model ind 640 epoch 651 head A head_i_epoch 0 batch 200: avg loss -3.069850 avg loss no lamb -3.069850 time 2019-02-24 23:11:20.285655
last batch sz 160
Model ind 640 epoch 651 head B head_i_epoch 0 batch 0: avg loss -1.815131 avg loss no lamb -1.815131 time 2019-02-24 23:12:42.004291
Model ind 640 epoch 651 head B head_i_epoch 0 batch 100: avg loss -1.609432 avg loss no lamb -1.609432 time 2019-02-24 23:14:34.991609
Model ind 640 epoch 651 head B head_i_epoch 0 batch 200: avg loss -1.818972 avg loss no lamb -1.818972 time 2019-02-24 23:16:27.684496
last batch sz 160
Model ind 640 epoch 651 head B head_i_epoch 1 batch 0: avg loss -1.804727 avg loss no lamb -1.804727 time 2019-02-24 23:17:49.913291
Model ind 640 epoch 651 head B head_i_epoch 1 batch 100: avg loss -1.747234 avg loss no lamb -1.747234 time 2019-02-24 23:19:41.802848
Model ind 640 epoch 651 head B head_i_epoch 1 batch 200: avg loss -1.770462 avg loss no lamb -1.770462 time 2019-02-24 23:21:36.724071
last batch sz 160
Pre: time 2019-02-24 23:23:17.885911: 
 	std: 0.05067449
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5097, 0.6124333, 0.61253333, 0.6127667, 0.5085833]
	train_accs: [0.5097, 0.6124333, 0.61253333, 0.6127667, 0.5085833]
	best_train_sub_head: 3
	worst: 0.5085833
	avg: 0.57120335
	best: 0.6127667

Starting e_i: 652
Model ind 640 epoch 652 head A head_i_epoch 0 batch 0: avg loss -3.070237 avg loss no lamb -3.070237 time 2019-02-24 23:23:20.463450
Model ind 640 epoch 652 head A head_i_epoch 0 batch 100: avg loss -3.076186 avg loss no lamb -3.076186 time 2019-02-24 23:25:17.087070
Model ind 640 epoch 652 head A head_i_epoch 0 batch 200: avg loss -3.106521 avg loss no lamb -3.106521 time 2019-02-24 23:27:10.662952
last batch sz 160
Model ind 640 epoch 652 head B head_i_epoch 0 batch 0: avg loss -1.813732 avg loss no lamb -1.813732 time 2019-02-24 23:28:32.997959
Model ind 640 epoch 652 head B head_i_epoch 0 batch 100: avg loss -1.699982 avg loss no lamb -1.699982 time 2019-02-24 23:30:26.444522
Model ind 640 epoch 652 head B head_i_epoch 0 batch 200: avg loss -1.813236 avg loss no lamb -1.813236 time 2019-02-24 23:32:19.445268
last batch sz 160
Model ind 640 epoch 652 head B head_i_epoch 1 batch 0: avg loss -1.842112 avg loss no lamb -1.842112 time 2019-02-24 23:33:41.720185
Model ind 640 epoch 652 head B head_i_epoch 1 batch 100: avg loss -1.688529 avg loss no lamb -1.688529 time 2019-02-24 23:35:34.585318
Model ind 640 epoch 652 head B head_i_epoch 1 batch 200: avg loss -1.786458 avg loss no lamb -1.786458 time 2019-02-24 23:37:29.882739
last batch sz 160
Pre: time 2019-02-24 23:39:13.789269: 
 	std: 0.051007617
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50923336, 0.6127333, 0.6127833, 0.61303335, 0.5082333]
	train_accs: [0.50923336, 0.6127333, 0.6127833, 0.61303335, 0.5082333]
	best_train_sub_head: 3
	worst: 0.5082333
	avg: 0.57120335
	best: 0.61303335

Starting e_i: 653
Model ind 640 epoch 653 head A head_i_epoch 0 batch 0: avg loss -3.156289 avg loss no lamb -3.156289 time 2019-02-24 23:39:16.797748
Model ind 640 epoch 653 head A head_i_epoch 0 batch 100: avg loss -2.988593 avg loss no lamb -2.988593 time 2019-02-24 23:41:09.548674
Model ind 640 epoch 653 head A head_i_epoch 0 batch 200: avg loss -3.215255 avg loss no lamb -3.215255 time 2019-02-24 23:43:01.920090
last batch sz 160
Model ind 640 epoch 653 head B head_i_epoch 0 batch 0: avg loss -1.799141 avg loss no lamb -1.799141 time 2019-02-24 23:44:24.351938
Model ind 640 epoch 653 head B head_i_epoch 0 batch 100: avg loss -1.780483 avg loss no lamb -1.780483 time 2019-02-24 23:46:17.221486
Model ind 640 epoch 653 head B head_i_epoch 0 batch 200: avg loss -1.797817 avg loss no lamb -1.797817 time 2019-02-24 23:48:09.625118
last batch sz 160
Model ind 640 epoch 653 head B head_i_epoch 1 batch 0: avg loss -1.801728 avg loss no lamb -1.801728 time 2019-02-24 23:49:32.320530
Model ind 640 epoch 653 head B head_i_epoch 1 batch 100: avg loss -1.813214 avg loss no lamb -1.813214 time 2019-02-24 23:51:24.341397
Model ind 640 epoch 653 head B head_i_epoch 1 batch 200: avg loss -1.837413 avg loss no lamb -1.837413 time 2019-02-24 23:53:16.921195
last batch sz 160
Pre: time 2019-02-24 23:55:01.488993: 
 	std: 0.051073354
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5097833, 0.614, 0.61403334, 0.6143, 0.50993335]
	train_accs: [0.5097833, 0.614, 0.61403334, 0.6143, 0.50993335]
	best_train_sub_head: 3
	worst: 0.5097833
	avg: 0.57241
	best: 0.6143

Starting e_i: 654
Model ind 640 epoch 654 head A head_i_epoch 0 batch 0: avg loss -3.158193 avg loss no lamb -3.158193 time 2019-02-24 23:55:04.091806
Model ind 640 epoch 654 head A head_i_epoch 0 batch 100: avg loss -3.152901 avg loss no lamb -3.152901 time 2019-02-24 23:56:57.366695
Model ind 640 epoch 654 head A head_i_epoch 0 batch 200: avg loss -3.118249 avg loss no lamb -3.118249 time 2019-02-24 23:58:50.389858
last batch sz 160
Model ind 640 epoch 654 head B head_i_epoch 0 batch 0: avg loss -1.781341 avg loss no lamb -1.781341 time 2019-02-25 00:00:12.627439
Model ind 640 epoch 654 head B head_i_epoch 0 batch 100: avg loss -1.649996 avg loss no lamb -1.649996 time 2019-02-25 00:02:06.257718
Model ind 640 epoch 654 head B head_i_epoch 0 batch 200: avg loss -1.777339 avg loss no lamb -1.777339 time 2019-02-25 00:03:57.044693
last batch sz 160
Model ind 640 epoch 654 head B head_i_epoch 1 batch 0: avg loss -1.756356 avg loss no lamb -1.756356 time 2019-02-25 00:05:20.438329
Model ind 640 epoch 654 head B head_i_epoch 1 batch 100: avg loss -1.653717 avg loss no lamb -1.653717 time 2019-02-25 00:07:13.630573
Model ind 640 epoch 654 head B head_i_epoch 1 batch 200: avg loss -1.893712 avg loss no lamb -1.893712 time 2019-02-25 00:09:06.913988
last batch sz 160
Pre: time 2019-02-25 00:10:50.938924: 
 	std: 0.051191628
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50765, 0.6128833, 0.6127, 0.61256665, 0.5088]
	train_accs: [0.50765, 0.6128833, 0.6127, 0.61256665, 0.5088]
	best_train_sub_head: 1
	worst: 0.50765
	avg: 0.57092
	best: 0.6128833

Starting e_i: 655
Model ind 640 epoch 655 head A head_i_epoch 0 batch 0: avg loss -3.166931 avg loss no lamb -3.166931 time 2019-02-25 00:10:53.640949
Model ind 640 epoch 655 head A head_i_epoch 0 batch 100: avg loss -3.122953 avg loss no lamb -3.122953 time 2019-02-25 00:12:47.247548
Model ind 640 epoch 655 head A head_i_epoch 0 batch 200: avg loss -3.127199 avg loss no lamb -3.127199 time 2019-02-25 00:14:40.744659
last batch sz 160
Model ind 640 epoch 655 head B head_i_epoch 0 batch 0: avg loss -1.734787 avg loss no lamb -1.734787 time 2019-02-25 00:16:02.906660
Model ind 640 epoch 655 head B head_i_epoch 0 batch 100: avg loss -1.571928 avg loss no lamb -1.571928 time 2019-02-25 00:17:55.029528
Model ind 640 epoch 655 head B head_i_epoch 0 batch 200: avg loss -1.840149 avg loss no lamb -1.840149 time 2019-02-25 00:19:48.009296
last batch sz 160
Model ind 640 epoch 655 head B head_i_epoch 1 batch 0: avg loss -1.746021 avg loss no lamb -1.746021 time 2019-02-25 00:21:09.906458
Model ind 640 epoch 655 head B head_i_epoch 1 batch 100: avg loss -1.606366 avg loss no lamb -1.606366 time 2019-02-25 00:23:02.882241
Model ind 640 epoch 655 head B head_i_epoch 1 batch 200: avg loss -1.888692 avg loss no lamb -1.888692 time 2019-02-25 00:24:55.092614
last batch sz 160
Pre: time 2019-02-25 00:26:37.326998: 
 	std: 0.049648516
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5106, 0.61263335, 0.6124833, 0.6124833, 0.51178336]
	train_accs: [0.5106, 0.61263335, 0.6124833, 0.6124833, 0.51178336]
	best_train_sub_head: 1
	worst: 0.5106
	avg: 0.5719966
	best: 0.61263335

Starting e_i: 656
Model ind 640 epoch 656 head A head_i_epoch 0 batch 0: avg loss -3.112529 avg loss no lamb -3.112529 time 2019-02-25 00:26:40.330179
Model ind 640 epoch 656 head A head_i_epoch 0 batch 100: avg loss -3.116094 avg loss no lamb -3.116094 time 2019-02-25 00:28:34.932682
Model ind 640 epoch 656 head A head_i_epoch 0 batch 200: avg loss -3.168483 avg loss no lamb -3.168483 time 2019-02-25 00:30:28.165324
last batch sz 160
Model ind 640 epoch 656 head B head_i_epoch 0 batch 0: avg loss -1.806326 avg loss no lamb -1.806326 time 2019-02-25 00:31:50.675238
Model ind 640 epoch 656 head B head_i_epoch 0 batch 100: avg loss -1.749308 avg loss no lamb -1.749308 time 2019-02-25 00:33:43.640552
Model ind 640 epoch 656 head B head_i_epoch 0 batch 200: avg loss -1.893161 avg loss no lamb -1.893161 time 2019-02-25 00:35:36.061050
last batch sz 160
Model ind 640 epoch 656 head B head_i_epoch 1 batch 0: avg loss -1.758290 avg loss no lamb -1.758290 time 2019-02-25 00:36:59.521534
Model ind 640 epoch 656 head B head_i_epoch 1 batch 100: avg loss -1.694840 avg loss no lamb -1.694840 time 2019-02-25 00:38:51.845257
Model ind 640 epoch 656 head B head_i_epoch 1 batch 200: avg loss -1.887538 avg loss no lamb -1.887538 time 2019-02-25 00:40:43.630835
last batch sz 160
Pre: time 2019-02-25 00:42:31.054622: 
 	std: 0.05102703
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50808334, 0.6124, 0.6121333, 0.61231667, 0.5081667]
	train_accs: [0.50808334, 0.6124, 0.6121333, 0.61231667, 0.5081667]
	best_train_sub_head: 1
	worst: 0.50808334
	avg: 0.57061994
	best: 0.6124

Starting e_i: 657
Model ind 640 epoch 657 head A head_i_epoch 0 batch 0: avg loss -3.157774 avg loss no lamb -3.157774 time 2019-02-25 00:42:33.678271
Model ind 640 epoch 657 head A head_i_epoch 0 batch 100: avg loss -3.071578 avg loss no lamb -3.071578 time 2019-02-25 00:44:27.139928
Model ind 640 epoch 657 head A head_i_epoch 0 batch 200: avg loss -3.085246 avg loss no lamb -3.085246 time 2019-02-25 00:46:19.607369
last batch sz 160
Model ind 640 epoch 657 head B head_i_epoch 0 batch 0: avg loss -1.737116 avg loss no lamb -1.737116 time 2019-02-25 00:47:41.366684
Model ind 640 epoch 657 head B head_i_epoch 0 batch 100: avg loss -1.704943 avg loss no lamb -1.704943 time 2019-02-25 00:49:34.876451
Model ind 640 epoch 657 head B head_i_epoch 0 batch 200: avg loss -1.783453 avg loss no lamb -1.783453 time 2019-02-25 00:51:28.134176
last batch sz 160
Model ind 640 epoch 657 head B head_i_epoch 1 batch 0: avg loss -1.813001 avg loss no lamb -1.813001 time 2019-02-25 00:52:49.909950
Model ind 640 epoch 657 head B head_i_epoch 1 batch 100: avg loss -1.647936 avg loss no lamb -1.647936 time 2019-02-25 00:54:42.455207
Model ind 640 epoch 657 head B head_i_epoch 1 batch 200: avg loss -1.868806 avg loss no lamb -1.868806 time 2019-02-25 00:56:35.714825
last batch sz 160
Pre: time 2019-02-25 00:58:20.721527: 
 	std: 0.051054284
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50941664, 0.6138167, 0.6138333, 0.61366665, 0.5097]
	train_accs: [0.50941664, 0.6138167, 0.6138333, 0.61366665, 0.5097]
	best_train_sub_head: 2
	worst: 0.50941664
	avg: 0.5720867
	best: 0.6138333

Starting e_i: 658
Model ind 640 epoch 658 head A head_i_epoch 0 batch 0: avg loss -3.124899 avg loss no lamb -3.124899 time 2019-02-25 00:58:23.289074
Model ind 640 epoch 658 head A head_i_epoch 0 batch 100: avg loss -3.110792 avg loss no lamb -3.110792 time 2019-02-25 01:00:16.629197
Model ind 640 epoch 658 head A head_i_epoch 0 batch 200: avg loss -3.180282 avg loss no lamb -3.180282 time 2019-02-25 01:02:09.357248
last batch sz 160
Model ind 640 epoch 658 head B head_i_epoch 0 batch 0: avg loss -1.850930 avg loss no lamb -1.850930 time 2019-02-25 01:03:31.227655
Model ind 640 epoch 658 head B head_i_epoch 0 batch 100: avg loss -1.720016 avg loss no lamb -1.720016 time 2019-02-25 01:05:24.172105
Model ind 640 epoch 658 head B head_i_epoch 0 batch 200: avg loss -1.846675 avg loss no lamb -1.846675 time 2019-02-25 01:07:17.652512
last batch sz 160
Model ind 640 epoch 658 head B head_i_epoch 1 batch 0: avg loss -1.726683 avg loss no lamb -1.726683 time 2019-02-25 01:08:39.640390
Model ind 640 epoch 658 head B head_i_epoch 1 batch 100: avg loss -1.719560 avg loss no lamb -1.719560 time 2019-02-25 01:10:31.028683
Model ind 640 epoch 658 head B head_i_epoch 1 batch 200: avg loss -1.831653 avg loss no lamb -1.831653 time 2019-02-25 01:12:25.990930
last batch sz 160
Pre: time 2019-02-25 01:14:10.627642: 
 	std: 0.050576765
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5104, 0.6138333, 0.61396664, 0.61356664, 0.5107]
	train_accs: [0.5104, 0.6138333, 0.61396664, 0.61356664, 0.5107]
	best_train_sub_head: 2
	worst: 0.5104
	avg: 0.5724933
	best: 0.61396664

Starting e_i: 659
Model ind 640 epoch 659 head A head_i_epoch 0 batch 0: avg loss -3.103910 avg loss no lamb -3.103910 time 2019-02-25 01:14:13.214440
Model ind 640 epoch 659 head A head_i_epoch 0 batch 100: avg loss -3.018235 avg loss no lamb -3.018235 time 2019-02-25 01:16:06.334741
Model ind 640 epoch 659 head A head_i_epoch 0 batch 200: avg loss -3.137925 avg loss no lamb -3.137925 time 2019-02-25 01:18:00.026262
last batch sz 160
Model ind 640 epoch 659 head B head_i_epoch 0 batch 0: avg loss -1.825168 avg loss no lamb -1.825168 time 2019-02-25 01:19:22.198585
Model ind 640 epoch 659 head B head_i_epoch 0 batch 100: avg loss -1.722185 avg loss no lamb -1.722185 time 2019-02-25 01:21:15.831219
Model ind 640 epoch 659 head B head_i_epoch 0 batch 200: avg loss -1.803998 avg loss no lamb -1.803998 time 2019-02-25 01:23:08.546596
last batch sz 160
Model ind 640 epoch 659 head B head_i_epoch 1 batch 0: avg loss -1.841240 avg loss no lamb -1.841240 time 2019-02-25 01:24:30.805553
Model ind 640 epoch 659 head B head_i_epoch 1 batch 100: avg loss -1.593954 avg loss no lamb -1.593954 time 2019-02-25 01:26:23.713602
Model ind 640 epoch 659 head B head_i_epoch 1 batch 200: avg loss -1.860017 avg loss no lamb -1.860017 time 2019-02-25 01:28:17.273469
last batch sz 160
Pre: time 2019-02-25 01:30:00.768512: 
 	std: 0.052254774
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50668335, 0.61345, 0.6136, 0.61301666, 0.5067]
	train_accs: [0.50668335, 0.61345, 0.6136, 0.61301666, 0.5067]
	best_train_sub_head: 2
	worst: 0.50668335
	avg: 0.57069004
	best: 0.6136

Starting e_i: 660
Model ind 640 epoch 660 head A head_i_epoch 0 batch 0: avg loss -3.088400 avg loss no lamb -3.088400 time 2019-02-25 01:30:03.631417
Model ind 640 epoch 660 head A head_i_epoch 0 batch 100: avg loss -2.946522 avg loss no lamb -2.946522 time 2019-02-25 01:31:55.712369
Model ind 640 epoch 660 head A head_i_epoch 0 batch 200: avg loss -3.099463 avg loss no lamb -3.099463 time 2019-02-25 01:33:49.028253
last batch sz 160
Model ind 640 epoch 660 head B head_i_epoch 0 batch 0: avg loss -1.792000 avg loss no lamb -1.792000 time 2019-02-25 01:35:09.937196
Model ind 640 epoch 660 head B head_i_epoch 0 batch 100: avg loss -1.697504 avg loss no lamb -1.697504 time 2019-02-25 01:37:04.912018
Model ind 640 epoch 660 head B head_i_epoch 0 batch 200: avg loss -1.828983 avg loss no lamb -1.828983 time 2019-02-25 01:38:57.833484
last batch sz 160
Model ind 640 epoch 660 head B head_i_epoch 1 batch 0: avg loss -1.801410 avg loss no lamb -1.801410 time 2019-02-25 01:40:19.533100
Model ind 640 epoch 660 head B head_i_epoch 1 batch 100: avg loss -1.684376 avg loss no lamb -1.684376 time 2019-02-25 01:42:12.115075
Model ind 640 epoch 660 head B head_i_epoch 1 batch 200: avg loss -1.869937 avg loss no lamb -1.869937 time 2019-02-25 01:44:04.333442
last batch sz 160
Pre: time 2019-02-25 01:45:47.742667: 
 	std: 0.05034011
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5096667, 0.6128, 0.61275, 0.61256665, 0.51023334]
	train_accs: [0.5096667, 0.6128, 0.61275, 0.61256665, 0.51023334]
	best_train_sub_head: 1
	worst: 0.5096667
	avg: 0.57160336
	best: 0.6128

Starting e_i: 661
Model ind 640 epoch 661 head A head_i_epoch 0 batch 0: avg loss -3.116229 avg loss no lamb -3.116229 time 2019-02-25 01:46:01.577328
Model ind 640 epoch 661 head A head_i_epoch 0 batch 100: avg loss -3.006459 avg loss no lamb -3.006459 time 2019-02-25 01:47:55.072584
Model ind 640 epoch 661 head A head_i_epoch 0 batch 200: avg loss -3.145664 avg loss no lamb -3.145664 time 2019-02-25 01:49:48.290722
last batch sz 160
Model ind 640 epoch 661 head B head_i_epoch 0 batch 0: avg loss -1.778531 avg loss no lamb -1.778531 time 2019-02-25 01:51:10.192233
Model ind 640 epoch 661 head B head_i_epoch 0 batch 100: avg loss -1.658648 avg loss no lamb -1.658648 time 2019-02-25 01:53:03.088656
Model ind 640 epoch 661 head B head_i_epoch 0 batch 200: avg loss -1.762926 avg loss no lamb -1.762926 time 2019-02-25 01:54:54.670036
last batch sz 160
Model ind 640 epoch 661 head B head_i_epoch 1 batch 0: avg loss -1.752483 avg loss no lamb -1.752483 time 2019-02-25 01:56:17.595312
Model ind 640 epoch 661 head B head_i_epoch 1 batch 100: avg loss -1.590006 avg loss no lamb -1.590006 time 2019-02-25 01:58:10.448934
Model ind 640 epoch 661 head B head_i_epoch 1 batch 200: avg loss -1.856258 avg loss no lamb -1.856258 time 2019-02-25 02:00:05.355299
last batch sz 160
Pre: time 2019-02-25 02:01:48.718174: 
 	std: 0.05100669
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50846666, 0.61333334, 0.61321664, 0.61296666, 0.50965]
	train_accs: [0.50846666, 0.61333334, 0.61321664, 0.61296666, 0.50965]
	best_train_sub_head: 1
	worst: 0.50846666
	avg: 0.57152665
	best: 0.61333334

Starting e_i: 662
Model ind 640 epoch 662 head A head_i_epoch 0 batch 0: avg loss -3.098689 avg loss no lamb -3.098689 time 2019-02-25 02:01:54.374916
Model ind 640 epoch 662 head A head_i_epoch 0 batch 100: avg loss -3.037528 avg loss no lamb -3.037528 time 2019-02-25 02:03:48.501211
Model ind 640 epoch 662 head A head_i_epoch 0 batch 200: avg loss -3.167476 avg loss no lamb -3.167476 time 2019-02-25 02:05:41.781037
last batch sz 160
Model ind 640 epoch 662 head B head_i_epoch 0 batch 0: avg loss -1.688979 avg loss no lamb -1.688979 time 2019-02-25 02:07:03.932755
Model ind 640 epoch 662 head B head_i_epoch 0 batch 100: avg loss -1.649602 avg loss no lamb -1.649602 time 2019-02-25 02:08:56.457616
Model ind 640 epoch 662 head B head_i_epoch 0 batch 200: avg loss -1.844061 avg loss no lamb -1.844061 time 2019-02-25 02:10:49.723906
last batch sz 160
Model ind 640 epoch 662 head B head_i_epoch 1 batch 0: avg loss -1.866222 avg loss no lamb -1.866222 time 2019-02-25 02:12:11.940937
Model ind 640 epoch 662 head B head_i_epoch 1 batch 100: avg loss -1.629154 avg loss no lamb -1.629154 time 2019-02-25 02:14:05.219249
Model ind 640 epoch 662 head B head_i_epoch 1 batch 200: avg loss -1.869587 avg loss no lamb -1.869587 time 2019-02-25 02:15:58.302055
last batch sz 160
Pre: time 2019-02-25 02:17:41.568433: 
 	std: 0.05028791
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5108, 0.614, 0.61411667, 0.61385, 0.5118833]
	train_accs: [0.5108, 0.614, 0.61411667, 0.61385, 0.5118833]
	best_train_sub_head: 2
	worst: 0.5108
	avg: 0.57293
	best: 0.61411667

Starting e_i: 663
Model ind 640 epoch 663 head A head_i_epoch 0 batch 0: avg loss -3.132755 avg loss no lamb -3.132755 time 2019-02-25 02:17:44.816845
Model ind 640 epoch 663 head A head_i_epoch 0 batch 100: avg loss -3.049958 avg loss no lamb -3.049958 time 2019-02-25 02:19:38.620099
Model ind 640 epoch 663 head A head_i_epoch 0 batch 200: avg loss -3.091292 avg loss no lamb -3.091292 time 2019-02-25 02:21:30.928437
last batch sz 160
Model ind 640 epoch 663 head B head_i_epoch 0 batch 0: avg loss -1.709845 avg loss no lamb -1.709845 time 2019-02-25 02:22:53.133891
Model ind 640 epoch 663 head B head_i_epoch 0 batch 100: avg loss -1.676077 avg loss no lamb -1.676077 time 2019-02-25 02:24:46.701548
Model ind 640 epoch 663 head B head_i_epoch 0 batch 200: avg loss -1.819509 avg loss no lamb -1.819509 time 2019-02-25 02:26:39.646459
last batch sz 160
Model ind 640 epoch 663 head B head_i_epoch 1 batch 0: avg loss -1.800919 avg loss no lamb -1.800919 time 2019-02-25 02:28:01.931419
Model ind 640 epoch 663 head B head_i_epoch 1 batch 100: avg loss -1.745704 avg loss no lamb -1.745704 time 2019-02-25 02:29:54.565842
Model ind 640 epoch 663 head B head_i_epoch 1 batch 200: avg loss -1.918534 avg loss no lamb -1.918534 time 2019-02-25 02:31:47.576468
last batch sz 160
Pre: time 2019-02-25 02:33:31.195981: 
 	std: 0.051782507
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5072167, 0.6130667, 0.6133, 0.6131333, 0.50771666]
	train_accs: [0.5072167, 0.6130667, 0.6133, 0.6131333, 0.50771666]
	best_train_sub_head: 2
	worst: 0.5072167
	avg: 0.57088673
	best: 0.6133

Starting e_i: 664
Model ind 640 epoch 664 head A head_i_epoch 0 batch 0: avg loss -3.066626 avg loss no lamb -3.066626 time 2019-02-25 02:33:33.658963
Model ind 640 epoch 664 head A head_i_epoch 0 batch 100: avg loss -3.090682 avg loss no lamb -3.090682 time 2019-02-25 02:35:29.054764
Model ind 640 epoch 664 head A head_i_epoch 0 batch 200: avg loss -3.159778 avg loss no lamb -3.159778 time 2019-02-25 02:37:22.605434
last batch sz 160
Model ind 640 epoch 664 head B head_i_epoch 0 batch 0: avg loss -1.893020 avg loss no lamb -1.893020 time 2019-02-25 02:38:43.524075
Model ind 640 epoch 664 head B head_i_epoch 0 batch 100: avg loss -1.614681 avg loss no lamb -1.614681 time 2019-02-25 02:40:38.136359
Model ind 640 epoch 664 head B head_i_epoch 0 batch 200: avg loss -1.911075 avg loss no lamb -1.911075 time 2019-02-25 02:42:30.836808
last batch sz 160
Model ind 640 epoch 664 head B head_i_epoch 1 batch 0: avg loss -1.857670 avg loss no lamb -1.857670 time 2019-02-25 02:43:53.474253
Model ind 640 epoch 664 head B head_i_epoch 1 batch 100: avg loss -1.700519 avg loss no lamb -1.700519 time 2019-02-25 02:45:46.342039
Model ind 640 epoch 664 head B head_i_epoch 1 batch 200: avg loss -1.836715 avg loss no lamb -1.836715 time 2019-02-25 02:47:40.507322
last batch sz 160
Pre: time 2019-02-25 02:49:24.148997: 
 	std: 0.050910596
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50925, 0.6138333, 0.61395, 0.6138667, 0.51068336]
	train_accs: [0.50925, 0.6138333, 0.61395, 0.6138667, 0.51068336]
	best_train_sub_head: 2
	worst: 0.50925
	avg: 0.5723167
	best: 0.61395

Starting e_i: 665
Model ind 640 epoch 665 head A head_i_epoch 0 batch 0: avg loss -3.100322 avg loss no lamb -3.100322 time 2019-02-25 02:49:27.110419
Model ind 640 epoch 665 head A head_i_epoch 0 batch 100: avg loss -3.053371 avg loss no lamb -3.053371 time 2019-02-25 02:51:20.467903
Model ind 640 epoch 665 head A head_i_epoch 0 batch 200: avg loss -3.050303 avg loss no lamb -3.050303 time 2019-02-25 02:53:13.325476
last batch sz 160
Model ind 640 epoch 665 head B head_i_epoch 0 batch 0: avg loss -1.852116 avg loss no lamb -1.852116 time 2019-02-25 02:54:35.627268
Model ind 640 epoch 665 head B head_i_epoch 0 batch 100: avg loss -1.715186 avg loss no lamb -1.715186 time 2019-02-25 02:56:27.560628
Model ind 640 epoch 665 head B head_i_epoch 0 batch 200: avg loss -1.778332 avg loss no lamb -1.778332 time 2019-02-25 02:58:21.451301
last batch sz 160
Model ind 640 epoch 665 head B head_i_epoch 1 batch 0: avg loss -1.795873 avg loss no lamb -1.795873 time 2019-02-25 02:59:43.660856
Model ind 640 epoch 665 head B head_i_epoch 1 batch 100: avg loss -1.740487 avg loss no lamb -1.740487 time 2019-02-25 03:01:35.268992
Model ind 640 epoch 665 head B head_i_epoch 1 batch 200: avg loss -1.800444 avg loss no lamb -1.800444 time 2019-02-25 03:03:29.637533
last batch sz 160
Pre: time 2019-02-25 03:05:12.869658: 
 	std: 0.051392317
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5086167, 0.6139167, 0.61403334, 0.61378336, 0.5094]
	train_accs: [0.5086167, 0.6139167, 0.61403334, 0.61378336, 0.5094]
	best_train_sub_head: 2
	worst: 0.5086167
	avg: 0.57194996
	best: 0.61403334

Starting e_i: 666
Model ind 640 epoch 666 head A head_i_epoch 0 batch 0: avg loss -3.132355 avg loss no lamb -3.132355 time 2019-02-25 03:05:15.309067
Model ind 640 epoch 666 head A head_i_epoch 0 batch 100: avg loss -3.019756 avg loss no lamb -3.019756 time 2019-02-25 03:07:08.359484
Model ind 640 epoch 666 head A head_i_epoch 0 batch 200: avg loss -3.154353 avg loss no lamb -3.154353 time 2019-02-25 03:09:01.526448
last batch sz 160
Model ind 640 epoch 666 head B head_i_epoch 0 batch 0: avg loss -1.749869 avg loss no lamb -1.749869 time 2019-02-25 03:10:23.516893
Model ind 640 epoch 666 head B head_i_epoch 0 batch 100: avg loss -1.698090 avg loss no lamb -1.698090 time 2019-02-25 03:12:17.307866
Model ind 640 epoch 666 head B head_i_epoch 0 batch 200: avg loss -1.837912 avg loss no lamb -1.837912 time 2019-02-25 03:14:09.779558
last batch sz 160
Model ind 640 epoch 666 head B head_i_epoch 1 batch 0: avg loss -1.740091 avg loss no lamb -1.740091 time 2019-02-25 03:15:33.344818
Model ind 640 epoch 666 head B head_i_epoch 1 batch 100: avg loss -1.645550 avg loss no lamb -1.645550 time 2019-02-25 03:17:27.531292
Model ind 640 epoch 666 head B head_i_epoch 1 batch 200: avg loss -1.875803 avg loss no lamb -1.875803 time 2019-02-25 03:19:21.855576
last batch sz 160
Pre: time 2019-02-25 03:21:06.367578: 
 	std: 0.051613953
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5071833, 0.6124667, 0.61303335, 0.6127167, 0.5075833]
	train_accs: [0.5071833, 0.6124667, 0.61303335, 0.6127167, 0.5075833]
	best_train_sub_head: 2
	worst: 0.5071833
	avg: 0.5705967
	best: 0.61303335

Starting e_i: 667
Model ind 640 epoch 667 head A head_i_epoch 0 batch 0: avg loss -3.114008 avg loss no lamb -3.114008 time 2019-02-25 03:21:08.845613
Model ind 640 epoch 667 head A head_i_epoch 0 batch 100: avg loss -3.029608 avg loss no lamb -3.029608 time 2019-02-25 03:23:00.417108
Model ind 640 epoch 667 head A head_i_epoch 0 batch 200: avg loss -3.076379 avg loss no lamb -3.076379 time 2019-02-25 03:24:54.566331
last batch sz 160
Model ind 640 epoch 667 head B head_i_epoch 0 batch 0: avg loss -1.750710 avg loss no lamb -1.750710 time 2019-02-25 03:26:17.926842
Model ind 640 epoch 667 head B head_i_epoch 0 batch 100: avg loss -1.690682 avg loss no lamb -1.690682 time 2019-02-25 03:28:11.147324
Model ind 640 epoch 667 head B head_i_epoch 0 batch 200: avg loss -1.792964 avg loss no lamb -1.792964 time 2019-02-25 03:30:05.458823
last batch sz 160
Model ind 640 epoch 667 head B head_i_epoch 1 batch 0: avg loss -1.769937 avg loss no lamb -1.769937 time 2019-02-25 03:31:27.569516
Model ind 640 epoch 667 head B head_i_epoch 1 batch 100: avg loss -1.651045 avg loss no lamb -1.651045 time 2019-02-25 03:33:20.420293
Model ind 640 epoch 667 head B head_i_epoch 1 batch 200: avg loss -1.816492 avg loss no lamb -1.816492 time 2019-02-25 03:35:14.614243
last batch sz 160
Pre: time 2019-02-25 03:36:58.823923: 
 	std: 0.052001607
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50516665, 0.6113833, 0.6117333, 0.6113, 0.5054833]
	train_accs: [0.50516665, 0.6113833, 0.6117333, 0.6113, 0.5054833]
	best_train_sub_head: 2
	worst: 0.50516665
	avg: 0.56901336
	best: 0.6117333

Starting e_i: 668
Model ind 640 epoch 668 head A head_i_epoch 0 batch 0: avg loss -3.071751 avg loss no lamb -3.071751 time 2019-02-25 03:37:01.208739
Model ind 640 epoch 668 head A head_i_epoch 0 batch 100: avg loss -3.078344 avg loss no lamb -3.078344 time 2019-02-25 03:38:53.414579
Model ind 640 epoch 668 head A head_i_epoch 0 batch 200: avg loss -3.140608 avg loss no lamb -3.140608 time 2019-02-25 03:40:47.999525
last batch sz 160
Model ind 640 epoch 668 head B head_i_epoch 0 batch 0: avg loss -1.736970 avg loss no lamb -1.736970 time 2019-02-25 03:42:10.324857
Model ind 640 epoch 668 head B head_i_epoch 0 batch 100: avg loss -1.667456 avg loss no lamb -1.667456 time 2019-02-25 03:44:03.829581
Model ind 640 epoch 668 head B head_i_epoch 0 batch 200: avg loss -1.834911 avg loss no lamb -1.834911 time 2019-02-25 03:45:56.227919
last batch sz 160
Model ind 640 epoch 668 head B head_i_epoch 1 batch 0: avg loss -1.801566 avg loss no lamb -1.801566 time 2019-02-25 03:47:19.235480
Model ind 640 epoch 668 head B head_i_epoch 1 batch 100: avg loss -1.713030 avg loss no lamb -1.713030 time 2019-02-25 03:49:12.490299
Model ind 640 epoch 668 head B head_i_epoch 1 batch 200: avg loss -1.836005 avg loss no lamb -1.836005 time 2019-02-25 03:51:05.189165
last batch sz 160
Pre: time 2019-02-25 03:52:49.017582: 
 	std: 0.05220286
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50685, 0.6137, 0.61356664, 0.61343336, 0.5071667]
	train_accs: [0.50685, 0.6137, 0.61356664, 0.61343336, 0.5071667]
	best_train_sub_head: 1
	worst: 0.50685
	avg: 0.5709433
	best: 0.6137

Starting e_i: 669
Model ind 640 epoch 669 head A head_i_epoch 0 batch 0: avg loss -3.149328 avg loss no lamb -3.149328 time 2019-02-25 03:52:51.586191
Model ind 640 epoch 669 head A head_i_epoch 0 batch 100: avg loss -3.064785 avg loss no lamb -3.064785 time 2019-02-25 03:54:45.026407
Model ind 640 epoch 669 head A head_i_epoch 0 batch 200: avg loss -3.088424 avg loss no lamb -3.088424 time 2019-02-25 03:56:37.701072
last batch sz 160
Model ind 640 epoch 669 head B head_i_epoch 0 batch 0: avg loss -1.837546 avg loss no lamb -1.837546 time 2019-02-25 03:58:00.480642
Model ind 640 epoch 669 head B head_i_epoch 0 batch 100: avg loss -1.723155 avg loss no lamb -1.723155 time 2019-02-25 03:59:54.150163
Model ind 640 epoch 669 head B head_i_epoch 0 batch 200: avg loss -1.873741 avg loss no lamb -1.873741 time 2019-02-25 04:01:47.858121
last batch sz 160
Model ind 640 epoch 669 head B head_i_epoch 1 batch 0: avg loss -1.743630 avg loss no lamb -1.743630 time 2019-02-25 04:03:10.127656
Model ind 640 epoch 669 head B head_i_epoch 1 batch 100: avg loss -1.649361 avg loss no lamb -1.649361 time 2019-02-25 04:05:03.057314
Model ind 640 epoch 669 head B head_i_epoch 1 batch 200: avg loss -1.827636 avg loss no lamb -1.827636 time 2019-02-25 04:06:54.384465
last batch sz 160
Pre: time 2019-02-25 04:08:39.666334: 
 	std: 0.051325448
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5086833, 0.61365, 0.61373335, 0.6139167, 0.5093167]
	train_accs: [0.5086833, 0.61365, 0.61373335, 0.6139167, 0.5093167]
	best_train_sub_head: 3
	worst: 0.5086833
	avg: 0.57185996
	best: 0.6139167

Starting e_i: 670
Model ind 640 epoch 670 head A head_i_epoch 0 batch 0: avg loss -3.098808 avg loss no lamb -3.098808 time 2019-02-25 04:08:42.460942
Model ind 640 epoch 670 head A head_i_epoch 0 batch 100: avg loss -3.052636 avg loss no lamb -3.052636 time 2019-02-25 04:10:35.768074
Model ind 640 epoch 670 head A head_i_epoch 0 batch 200: avg loss -3.176119 avg loss no lamb -3.176119 time 2019-02-25 04:12:29.144860
last batch sz 160
Model ind 640 epoch 670 head B head_i_epoch 0 batch 0: avg loss -1.775967 avg loss no lamb -1.775967 time 2019-02-25 04:13:51.761777
Model ind 640 epoch 670 head B head_i_epoch 0 batch 100: avg loss -1.737904 avg loss no lamb -1.737904 time 2019-02-25 04:15:44.860144
Model ind 640 epoch 670 head B head_i_epoch 0 batch 200: avg loss -1.854460 avg loss no lamb -1.854460 time 2019-02-25 04:17:37.843432
last batch sz 160
Model ind 640 epoch 670 head B head_i_epoch 1 batch 0: avg loss -1.846298 avg loss no lamb -1.846298 time 2019-02-25 04:18:59.861032
Model ind 640 epoch 670 head B head_i_epoch 1 batch 100: avg loss -1.785094 avg loss no lamb -1.785094 time 2019-02-25 04:20:52.582090
Model ind 640 epoch 670 head B head_i_epoch 1 batch 200: avg loss -1.866340 avg loss no lamb -1.866340 time 2019-02-25 04:22:45.389985
last batch sz 160
Pre: time 2019-02-25 04:24:30.070441: 
 	std: 0.051480196
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50595, 0.61151665, 0.61156666, 0.6117833, 0.5071333]
	train_accs: [0.50595, 0.61151665, 0.61156666, 0.6117833, 0.5071333]
	best_train_sub_head: 3
	worst: 0.50595
	avg: 0.56959
	best: 0.6117833

Starting e_i: 671
Model ind 640 epoch 671 head A head_i_epoch 0 batch 0: avg loss -3.126623 avg loss no lamb -3.126623 time 2019-02-25 04:24:37.743278
Model ind 640 epoch 671 head A head_i_epoch 0 batch 100: avg loss -3.049312 avg loss no lamb -3.049312 time 2019-02-25 04:26:31.308480
Model ind 640 epoch 671 head A head_i_epoch 0 batch 200: avg loss -3.131505 avg loss no lamb -3.131505 time 2019-02-25 04:28:24.278410
last batch sz 160
Model ind 640 epoch 671 head B head_i_epoch 0 batch 0: avg loss -1.833806 avg loss no lamb -1.833806 time 2019-02-25 04:29:45.795672
Model ind 640 epoch 671 head B head_i_epoch 0 batch 100: avg loss -1.635118 avg loss no lamb -1.635118 time 2019-02-25 04:31:38.931447
Model ind 640 epoch 671 head B head_i_epoch 0 batch 200: avg loss -1.817328 avg loss no lamb -1.817328 time 2019-02-25 04:33:34.807642
last batch sz 160
Model ind 640 epoch 671 head B head_i_epoch 1 batch 0: avg loss -1.801894 avg loss no lamb -1.801894 time 2019-02-25 04:34:59.373928
Model ind 640 epoch 671 head B head_i_epoch 1 batch 100: avg loss -1.644821 avg loss no lamb -1.644821 time 2019-02-25 04:36:52.001834
Model ind 640 epoch 671 head B head_i_epoch 1 batch 200: avg loss -1.902469 avg loss no lamb -1.902469 time 2019-02-25 04:38:45.794398
last batch sz 160
Pre: time 2019-02-25 04:40:30.225566: 
 	std: 0.05126394
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50626665, 0.6116, 0.6111, 0.6116667, 0.50736666]
	train_accs: [0.50626665, 0.6116, 0.6111, 0.6116667, 0.50736666]
	best_train_sub_head: 3
	worst: 0.50626665
	avg: 0.5696
	best: 0.6116667

Starting e_i: 672
Model ind 640 epoch 672 head A head_i_epoch 0 batch 0: avg loss -3.235978 avg loss no lamb -3.235978 time 2019-02-25 04:40:32.724141
Model ind 640 epoch 672 head A head_i_epoch 0 batch 100: avg loss -3.016524 avg loss no lamb -3.016524 time 2019-02-25 04:42:26.982037
Model ind 640 epoch 672 head A head_i_epoch 0 batch 200: avg loss -3.192648 avg loss no lamb -3.192648 time 2019-02-25 04:44:20.433054
last batch sz 160
Model ind 640 epoch 672 head B head_i_epoch 0 batch 0: avg loss -1.758728 avg loss no lamb -1.758728 time 2019-02-25 04:45:43.563475
Model ind 640 epoch 672 head B head_i_epoch 0 batch 100: avg loss -1.779146 avg loss no lamb -1.779146 time 2019-02-25 04:47:36.964856
Model ind 640 epoch 672 head B head_i_epoch 0 batch 200: avg loss -1.896241 avg loss no lamb -1.896241 time 2019-02-25 04:49:29.708185
last batch sz 160
Model ind 640 epoch 672 head B head_i_epoch 1 batch 0: avg loss -1.754603 avg loss no lamb -1.754603 time 2019-02-25 04:50:50.718877
Model ind 640 epoch 672 head B head_i_epoch 1 batch 100: avg loss -1.787079 avg loss no lamb -1.787079 time 2019-02-25 04:52:44.879188
Model ind 640 epoch 672 head B head_i_epoch 1 batch 200: avg loss -1.821829 avg loss no lamb -1.821829 time 2019-02-25 04:54:37.909693
last batch sz 160
Pre: time 2019-02-25 04:56:21.924322: 
 	std: 0.051012047
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50673336, 0.6109, 0.6108, 0.6110333, 0.5068333]
	train_accs: [0.50673336, 0.6109, 0.6108, 0.6110333, 0.5068333]
	best_train_sub_head: 3
	worst: 0.50673336
	avg: 0.56926
	best: 0.6110333

Starting e_i: 673
Model ind 640 epoch 673 head A head_i_epoch 0 batch 0: avg loss -3.126035 avg loss no lamb -3.126035 time 2019-02-25 04:56:24.508020
Model ind 640 epoch 673 head A head_i_epoch 0 batch 100: avg loss -3.069848 avg loss no lamb -3.069848 time 2019-02-25 04:58:17.856063
Model ind 640 epoch 673 head A head_i_epoch 0 batch 200: avg loss -3.093193 avg loss no lamb -3.093193 time 2019-02-25 05:00:11.159088
last batch sz 160
Model ind 640 epoch 673 head B head_i_epoch 0 batch 0: avg loss -1.781358 avg loss no lamb -1.781358 time 2019-02-25 05:01:32.836633
Model ind 640 epoch 673 head B head_i_epoch 0 batch 100: avg loss -1.650050 avg loss no lamb -1.650050 time 2019-02-25 05:03:26.098327
Model ind 640 epoch 673 head B head_i_epoch 0 batch 200: avg loss -1.810923 avg loss no lamb -1.810923 time 2019-02-25 05:05:18.271138
last batch sz 160
Model ind 640 epoch 673 head B head_i_epoch 1 batch 0: avg loss -1.771955 avg loss no lamb -1.771955 time 2019-02-25 05:06:41.009528
Model ind 640 epoch 673 head B head_i_epoch 1 batch 100: avg loss -1.705028 avg loss no lamb -1.705028 time 2019-02-25 05:08:33.762261
Model ind 640 epoch 673 head B head_i_epoch 1 batch 200: avg loss -1.873202 avg loss no lamb -1.873202 time 2019-02-25 05:10:26.334541
last batch sz 160
Pre: time 2019-02-25 05:12:09.906147: 
 	std: 0.05101775
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50693333, 0.61145, 0.6113333, 0.6110833, 0.50736666]
	train_accs: [0.50693333, 0.61145, 0.6113333, 0.6110833, 0.50736666]
	best_train_sub_head: 1
	worst: 0.50693333
	avg: 0.56963336
	best: 0.61145

Starting e_i: 674
Model ind 640 epoch 674 head A head_i_epoch 0 batch 0: avg loss -3.112537 avg loss no lamb -3.112537 time 2019-02-25 05:12:12.369519
Model ind 640 epoch 674 head A head_i_epoch 0 batch 100: avg loss -3.083649 avg loss no lamb -3.083649 time 2019-02-25 05:14:04.617783
Model ind 640 epoch 674 head A head_i_epoch 0 batch 200: avg loss -3.179300 avg loss no lamb -3.179300 time 2019-02-25 05:15:58.353740
last batch sz 160
Model ind 640 epoch 674 head B head_i_epoch 0 batch 0: avg loss -1.822132 avg loss no lamb -1.822132 time 2019-02-25 05:17:20.846656
Model ind 640 epoch 674 head B head_i_epoch 0 batch 100: avg loss -1.706538 avg loss no lamb -1.706538 time 2019-02-25 05:19:14.209787
Model ind 640 epoch 674 head B head_i_epoch 0 batch 200: avg loss -1.851110 avg loss no lamb -1.851110 time 2019-02-25 05:21:07.514723
last batch sz 160
Model ind 640 epoch 674 head B head_i_epoch 1 batch 0: avg loss -1.783723 avg loss no lamb -1.783723 time 2019-02-25 05:22:29.723936
Model ind 640 epoch 674 head B head_i_epoch 1 batch 100: avg loss -1.768010 avg loss no lamb -1.768010 time 2019-02-25 05:24:23.103673
Model ind 640 epoch 674 head B head_i_epoch 1 batch 200: avg loss -1.836924 avg loss no lamb -1.836924 time 2019-02-25 05:26:15.984605
last batch sz 160
Pre: time 2019-02-25 05:28:00.609640: 
 	std: 0.052188158
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50551665, 0.6128167, 0.61268336, 0.61265, 0.5068667]
	train_accs: [0.50551665, 0.6128167, 0.61268336, 0.61265, 0.5068667]
	best_train_sub_head: 1
	worst: 0.50551665
	avg: 0.5701066
	best: 0.6128167

Starting e_i: 675
Model ind 640 epoch 675 head A head_i_epoch 0 batch 0: avg loss -3.141986 avg loss no lamb -3.141986 time 2019-02-25 05:28:03.305911
Model ind 640 epoch 675 head A head_i_epoch 0 batch 100: avg loss -3.131814 avg loss no lamb -3.131814 time 2019-02-25 05:29:56.336489
Model ind 640 epoch 675 head A head_i_epoch 0 batch 200: avg loss -3.105066 avg loss no lamb -3.105066 time 2019-02-25 05:31:49.476486
last batch sz 160
Model ind 640 epoch 675 head B head_i_epoch 0 batch 0: avg loss -1.829010 avg loss no lamb -1.829010 time 2019-02-25 05:33:10.757942
Model ind 640 epoch 675 head B head_i_epoch 0 batch 100: avg loss -1.697817 avg loss no lamb -1.697817 time 2019-02-25 05:35:05.435937
Model ind 640 epoch 675 head B head_i_epoch 0 batch 200: avg loss -1.752783 avg loss no lamb -1.752783 time 2019-02-25 05:36:59.296724
last batch sz 160
Model ind 640 epoch 675 head B head_i_epoch 1 batch 0: avg loss -1.794718 avg loss no lamb -1.794718 time 2019-02-25 05:38:21.599713
Model ind 640 epoch 675 head B head_i_epoch 1 batch 100: avg loss -1.674258 avg loss no lamb -1.674258 time 2019-02-25 05:40:14.053288
Model ind 640 epoch 675 head B head_i_epoch 1 batch 200: avg loss -1.867000 avg loss no lamb -1.867000 time 2019-02-25 05:42:07.508645
last batch sz 160
Pre: time 2019-02-25 05:43:51.155331: 
 	std: 0.050399967
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50785, 0.6107, 0.6106333, 0.61115, 0.50805]
	train_accs: [0.50785, 0.6107, 0.6106333, 0.61115, 0.50805]
	best_train_sub_head: 3
	worst: 0.50785
	avg: 0.5696767
	best: 0.61115

Starting e_i: 676
Model ind 640 epoch 676 head A head_i_epoch 0 batch 0: avg loss -3.153309 avg loss no lamb -3.153309 time 2019-02-25 05:43:53.666216
Model ind 640 epoch 676 head A head_i_epoch 0 batch 100: avg loss -3.101207 avg loss no lamb -3.101207 time 2019-02-25 05:45:48.192894
Model ind 640 epoch 676 head A head_i_epoch 0 batch 200: avg loss -3.152560 avg loss no lamb -3.152560 time 2019-02-25 05:47:41.102472
last batch sz 160
Model ind 640 epoch 676 head B head_i_epoch 0 batch 0: avg loss -1.848182 avg loss no lamb -1.848182 time 2019-02-25 05:49:04.038615
Model ind 640 epoch 676 head B head_i_epoch 0 batch 100: avg loss -1.776509 avg loss no lamb -1.776509 time 2019-02-25 05:50:59.012717
Model ind 640 epoch 676 head B head_i_epoch 0 batch 200: avg loss -1.842957 avg loss no lamb -1.842957 time 2019-02-25 05:52:53.580958
last batch sz 160
Model ind 640 epoch 676 head B head_i_epoch 1 batch 0: avg loss -1.737870 avg loss no lamb -1.737870 time 2019-02-25 05:54:15.865802
Model ind 640 epoch 676 head B head_i_epoch 1 batch 100: avg loss -1.746687 avg loss no lamb -1.746687 time 2019-02-25 05:56:08.771698
Model ind 640 epoch 676 head B head_i_epoch 1 batch 200: avg loss -1.826033 avg loss no lamb -1.826033 time 2019-02-25 05:57:59.582214
last batch sz 160
Pre: time 2019-02-25 05:59:44.971881: 
 	std: 0.05111243
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50883335, 0.6138667, 0.61333334, 0.61366665, 0.50975]
	train_accs: [0.50883335, 0.6138667, 0.61333334, 0.61366665, 0.50975]
	best_train_sub_head: 1
	worst: 0.50883335
	avg: 0.57189
	best: 0.6138667

Starting e_i: 677
Model ind 640 epoch 677 head A head_i_epoch 0 batch 0: avg loss -3.066888 avg loss no lamb -3.066888 time 2019-02-25 05:59:47.733123
Model ind 640 epoch 677 head A head_i_epoch 0 batch 100: avg loss -3.095509 avg loss no lamb -3.095509 time 2019-02-25 06:01:39.911872
Model ind 640 epoch 677 head A head_i_epoch 0 batch 200: avg loss -3.132964 avg loss no lamb -3.132964 time 2019-02-25 06:03:33.354233
last batch sz 160
Model ind 640 epoch 677 head B head_i_epoch 0 batch 0: avg loss -1.790195 avg loss no lamb -1.790195 time 2019-02-25 06:04:55.570415
Model ind 640 epoch 677 head B head_i_epoch 0 batch 100: avg loss -1.670130 avg loss no lamb -1.670130 time 2019-02-25 06:06:48.179027
Model ind 640 epoch 677 head B head_i_epoch 0 batch 200: avg loss -1.896384 avg loss no lamb -1.896384 time 2019-02-25 06:08:41.329044
last batch sz 160
Model ind 640 epoch 677 head B head_i_epoch 1 batch 0: avg loss -1.672064 avg loss no lamb -1.672064 time 2019-02-25 06:10:03.403094
Model ind 640 epoch 677 head B head_i_epoch 1 batch 100: avg loss -1.722409 avg loss no lamb -1.722409 time 2019-02-25 06:11:56.343243
Model ind 640 epoch 677 head B head_i_epoch 1 batch 200: avg loss -1.867925 avg loss no lamb -1.867925 time 2019-02-25 06:13:50.101230
last batch sz 160
Pre: time 2019-02-25 06:15:34.955292: 
 	std: 0.050803307
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5103833, 0.61466664, 0.61436665, 0.6145667, 0.51128334]
	train_accs: [0.5103833, 0.61466664, 0.61436665, 0.6145667, 0.51128334]
	best_train_sub_head: 1
	worst: 0.5103833
	avg: 0.57305336
	best: 0.61466664

Starting e_i: 678
Model ind 640 epoch 678 head A head_i_epoch 0 batch 0: avg loss -3.069088 avg loss no lamb -3.069088 time 2019-02-25 06:15:37.536082
Model ind 640 epoch 678 head A head_i_epoch 0 batch 100: avg loss -3.084649 avg loss no lamb -3.084649 time 2019-02-25 06:17:30.712153
Model ind 640 epoch 678 head A head_i_epoch 0 batch 200: avg loss -3.206162 avg loss no lamb -3.206162 time 2019-02-25 06:19:23.002917
last batch sz 160
Model ind 640 epoch 678 head B head_i_epoch 0 batch 0: avg loss -1.751772 avg loss no lamb -1.751772 time 2019-02-25 06:20:46.263957
Model ind 640 epoch 678 head B head_i_epoch 0 batch 100: avg loss -1.740580 avg loss no lamb -1.740580 time 2019-02-25 06:22:39.817877
Model ind 640 epoch 678 head B head_i_epoch 0 batch 200: avg loss -1.858628 avg loss no lamb -1.858628 time 2019-02-25 06:24:33.588728
last batch sz 160
Model ind 640 epoch 678 head B head_i_epoch 1 batch 0: avg loss -1.798563 avg loss no lamb -1.798563 time 2019-02-25 06:25:56.475902
Model ind 640 epoch 678 head B head_i_epoch 1 batch 100: avg loss -1.662767 avg loss no lamb -1.662767 time 2019-02-25 06:27:50.157215
Model ind 640 epoch 678 head B head_i_epoch 1 batch 200: avg loss -1.875503 avg loss no lamb -1.875503 time 2019-02-25 06:29:43.661048
last batch sz 160
Pre: time 2019-02-25 06:31:27.557178: 
 	std: 0.049904656
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51201665, 0.61415, 0.61413336, 0.6142667, 0.5126167]
	train_accs: [0.51201665, 0.61415, 0.61413336, 0.6142667, 0.5126167]
	best_train_sub_head: 3
	worst: 0.51201665
	avg: 0.5734366
	best: 0.6142667

Starting e_i: 679
Model ind 640 epoch 679 head A head_i_epoch 0 batch 0: avg loss -3.207707 avg loss no lamb -3.207707 time 2019-02-25 06:31:30.353618
Model ind 640 epoch 679 head A head_i_epoch 0 batch 100: avg loss -3.044537 avg loss no lamb -3.044537 time 2019-02-25 06:33:24.849839
Model ind 640 epoch 679 head A head_i_epoch 0 batch 200: avg loss -3.159474 avg loss no lamb -3.159474 time 2019-02-25 06:35:18.701236
last batch sz 160
Model ind 640 epoch 679 head B head_i_epoch 0 batch 0: avg loss -1.783815 avg loss no lamb -1.783815 time 2019-02-25 06:36:40.314415
Model ind 640 epoch 679 head B head_i_epoch 0 batch 100: avg loss -1.706925 avg loss no lamb -1.706925 time 2019-02-25 06:38:32.536901
Model ind 640 epoch 679 head B head_i_epoch 0 batch 200: avg loss -1.876260 avg loss no lamb -1.876260 time 2019-02-25 06:40:24.553177
last batch sz 160
Model ind 640 epoch 679 head B head_i_epoch 1 batch 0: avg loss -1.739260 avg loss no lamb -1.739260 time 2019-02-25 06:41:45.239356
Model ind 640 epoch 679 head B head_i_epoch 1 batch 100: avg loss -1.666458 avg loss no lamb -1.666458 time 2019-02-25 06:43:39.715044
Model ind 640 epoch 679 head B head_i_epoch 1 batch 200: avg loss -1.823488 avg loss no lamb -1.823488 time 2019-02-25 06:45:32.023472
last batch sz 160
Pre: time 2019-02-25 06:47:15.784748: 
 	std: 0.05082888
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5069, 0.6109833, 0.6106667, 0.61118335, 0.5074833]
	train_accs: [0.5069, 0.6109833, 0.6106667, 0.61118335, 0.5074833]
	best_train_sub_head: 3
	worst: 0.5069
	avg: 0.56944335
	best: 0.61118335

Starting e_i: 680
Model ind 640 epoch 680 head A head_i_epoch 0 batch 0: avg loss -3.136499 avg loss no lamb -3.136499 time 2019-02-25 06:47:18.306439
Model ind 640 epoch 680 head A head_i_epoch 0 batch 100: avg loss -3.005934 avg loss no lamb -3.005934 time 2019-02-25 06:49:11.667058
Model ind 640 epoch 680 head A head_i_epoch 0 batch 200: avg loss -3.100373 avg loss no lamb -3.100373 time 2019-02-25 06:51:04.967565
last batch sz 160
Model ind 640 epoch 680 head B head_i_epoch 0 batch 0: avg loss -1.777466 avg loss no lamb -1.777466 time 2019-02-25 06:52:26.814354
Model ind 640 epoch 680 head B head_i_epoch 0 batch 100: avg loss -1.693299 avg loss no lamb -1.693299 time 2019-02-25 06:54:19.937221
Model ind 640 epoch 680 head B head_i_epoch 0 batch 200: avg loss -1.759626 avg loss no lamb -1.759626 time 2019-02-25 06:56:12.626369
last batch sz 160
Model ind 640 epoch 680 head B head_i_epoch 1 batch 0: avg loss -1.801177 avg loss no lamb -1.801177 time 2019-02-25 06:57:34.430881
Model ind 640 epoch 680 head B head_i_epoch 1 batch 100: avg loss -1.770416 avg loss no lamb -1.770416 time 2019-02-25 06:59:27.355084
Model ind 640 epoch 680 head B head_i_epoch 1 batch 200: avg loss -1.801322 avg loss no lamb -1.801322 time 2019-02-25 07:01:20.587797
last batch sz 160
Pre: time 2019-02-25 07:03:04.546708: 
 	std: 0.05173081
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50743335, 0.6134167, 0.613, 0.61326665, 0.50783336]
	train_accs: [0.50743335, 0.6134167, 0.613, 0.61326665, 0.50783336]
	best_train_sub_head: 1
	worst: 0.50743335
	avg: 0.57098997
	best: 0.6134167

Starting e_i: 681
Model ind 640 epoch 681 head A head_i_epoch 0 batch 0: avg loss -3.117616 avg loss no lamb -3.117616 time 2019-02-25 07:03:10.163489
Model ind 640 epoch 681 head A head_i_epoch 0 batch 100: avg loss -3.070879 avg loss no lamb -3.070879 time 2019-02-25 07:05:02.333794
Model ind 640 epoch 681 head A head_i_epoch 0 batch 200: avg loss -3.210278 avg loss no lamb -3.210278 time 2019-02-25 07:06:57.607254
last batch sz 160
Model ind 640 epoch 681 head B head_i_epoch 0 batch 0: avg loss -1.807295 avg loss no lamb -1.807295 time 2019-02-25 07:08:19.585853
Model ind 640 epoch 681 head B head_i_epoch 0 batch 100: avg loss -1.717886 avg loss no lamb -1.717886 time 2019-02-25 07:10:12.581303
Model ind 640 epoch 681 head B head_i_epoch 0 batch 200: avg loss -1.847237 avg loss no lamb -1.847237 time 2019-02-25 07:12:04.931101
last batch sz 160
Model ind 640 epoch 681 head B head_i_epoch 1 batch 0: avg loss -1.783550 avg loss no lamb -1.783550 time 2019-02-25 07:13:27.199825
Model ind 640 epoch 681 head B head_i_epoch 1 batch 100: avg loss -1.707396 avg loss no lamb -1.707396 time 2019-02-25 07:15:20.712976
Model ind 640 epoch 681 head B head_i_epoch 1 batch 200: avg loss -1.917220 avg loss no lamb -1.917220 time 2019-02-25 07:17:13.461046
last batch sz 160
Pre: time 2019-02-25 07:18:57.622593: 
 	std: 0.05085615
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51021665, 0.61455, 0.614, 0.6144, 0.5108]
	train_accs: [0.51021665, 0.61455, 0.614, 0.6144, 0.5108]
	best_train_sub_head: 1
	worst: 0.51021665
	avg: 0.5727933
	best: 0.61455

Starting e_i: 682
Model ind 640 epoch 682 head A head_i_epoch 0 batch 0: avg loss -3.101101 avg loss no lamb -3.101101 time 2019-02-25 07:19:00.839088
Model ind 640 epoch 682 head A head_i_epoch 0 batch 100: avg loss -2.966782 avg loss no lamb -2.966782 time 2019-02-25 07:20:53.971995
Model ind 640 epoch 682 head A head_i_epoch 0 batch 200: avg loss -3.179431 avg loss no lamb -3.179431 time 2019-02-25 07:22:47.360503
last batch sz 160
Model ind 640 epoch 682 head B head_i_epoch 0 batch 0: avg loss -1.829793 avg loss no lamb -1.829793 time 2019-02-25 07:24:09.883873
Model ind 640 epoch 682 head B head_i_epoch 0 batch 100: avg loss -1.729107 avg loss no lamb -1.729107 time 2019-02-25 07:26:01.356915
Model ind 640 epoch 682 head B head_i_epoch 0 batch 200: avg loss -1.791548 avg loss no lamb -1.791548 time 2019-02-25 07:27:55.065369
last batch sz 160
Model ind 640 epoch 682 head B head_i_epoch 1 batch 0: avg loss -1.796705 avg loss no lamb -1.796705 time 2019-02-25 07:29:16.958889
Model ind 640 epoch 682 head B head_i_epoch 1 batch 100: avg loss -1.730982 avg loss no lamb -1.730982 time 2019-02-25 07:31:09.453460
Model ind 640 epoch 682 head B head_i_epoch 1 batch 200: avg loss -1.885746 avg loss no lamb -1.885746 time 2019-02-25 07:33:04.588338
last batch sz 160
Pre: time 2019-02-25 07:34:49.315794: 
 	std: 0.051019225
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50888336, 0.6131333, 0.61308336, 0.6135333, 0.5093333]
	train_accs: [0.50888336, 0.6131333, 0.61308336, 0.6135333, 0.5093333]
	best_train_sub_head: 3
	worst: 0.50888336
	avg: 0.57159334
	best: 0.6135333

Starting e_i: 683
Model ind 640 epoch 683 head A head_i_epoch 0 batch 0: avg loss -3.090222 avg loss no lamb -3.090222 time 2019-02-25 07:34:51.731329
Model ind 640 epoch 683 head A head_i_epoch 0 batch 100: avg loss -3.003046 avg loss no lamb -3.003046 time 2019-02-25 07:36:45.283474
Model ind 640 epoch 683 head A head_i_epoch 0 batch 200: avg loss -3.156101 avg loss no lamb -3.156101 time 2019-02-25 07:38:38.353137
last batch sz 160
Model ind 640 epoch 683 head B head_i_epoch 0 batch 0: avg loss -1.778779 avg loss no lamb -1.778779 time 2019-02-25 07:40:00.713680
Model ind 640 epoch 683 head B head_i_epoch 0 batch 100: avg loss -1.688829 avg loss no lamb -1.688829 time 2019-02-25 07:41:53.955677
Model ind 640 epoch 683 head B head_i_epoch 0 batch 200: avg loss -1.889834 avg loss no lamb -1.889834 time 2019-02-25 07:43:47.068833
last batch sz 160
Model ind 640 epoch 683 head B head_i_epoch 1 batch 0: avg loss -1.741421 avg loss no lamb -1.741421 time 2019-02-25 07:45:09.638597
Model ind 640 epoch 683 head B head_i_epoch 1 batch 100: avg loss -1.740467 avg loss no lamb -1.740467 time 2019-02-25 07:47:03.370404
Model ind 640 epoch 683 head B head_i_epoch 1 batch 200: avg loss -1.916553 avg loss no lamb -1.916553 time 2019-02-25 07:48:55.347129
last batch sz 160
Pre: time 2019-02-25 07:50:39.964697: 
 	std: 0.051538307
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50526667, 0.61105, 0.6102833, 0.6109667, 0.50586665]
	train_accs: [0.50526667, 0.61105, 0.6102833, 0.6109667, 0.50586665]
	best_train_sub_head: 1
	worst: 0.50526667
	avg: 0.56868666
	best: 0.61105

Starting e_i: 684
Model ind 640 epoch 684 head A head_i_epoch 0 batch 0: avg loss -3.112602 avg loss no lamb -3.112602 time 2019-02-25 07:50:42.726922
Model ind 640 epoch 684 head A head_i_epoch 0 batch 100: avg loss -3.021570 avg loss no lamb -3.021570 time 2019-02-25 07:52:35.090368
Model ind 640 epoch 684 head A head_i_epoch 0 batch 200: avg loss -3.095787 avg loss no lamb -3.095787 time 2019-02-25 07:54:28.946973
last batch sz 160
Model ind 640 epoch 684 head B head_i_epoch 0 batch 0: avg loss -1.802343 avg loss no lamb -1.802343 time 2019-02-25 07:55:51.438695
Model ind 640 epoch 684 head B head_i_epoch 0 batch 100: avg loss -1.806198 avg loss no lamb -1.806198 time 2019-02-25 07:57:44.232211
Model ind 640 epoch 684 head B head_i_epoch 0 batch 200: avg loss -1.867082 avg loss no lamb -1.867082 time 2019-02-25 07:59:37.542009
last batch sz 160
Model ind 640 epoch 684 head B head_i_epoch 1 batch 0: avg loss -1.871183 avg loss no lamb -1.871183 time 2019-02-25 08:00:59.254135
Model ind 640 epoch 684 head B head_i_epoch 1 batch 100: avg loss -1.637181 avg loss no lamb -1.637181 time 2019-02-25 08:02:52.192409
Model ind 640 epoch 684 head B head_i_epoch 1 batch 200: avg loss -1.750664 avg loss no lamb -1.750664 time 2019-02-25 08:04:45.386015
last batch sz 160
Pre: time 2019-02-25 08:06:29.395264: 
 	std: 0.050376944
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50873333, 0.61181664, 0.61125, 0.61185, 0.50888336]
	train_accs: [0.50873333, 0.61181664, 0.61125, 0.61185, 0.50888336]
	best_train_sub_head: 3
	worst: 0.50873333
	avg: 0.5705067
	best: 0.61185

Starting e_i: 685
Model ind 640 epoch 685 head A head_i_epoch 0 batch 0: avg loss -3.173012 avg loss no lamb -3.173012 time 2019-02-25 08:06:31.989276
Model ind 640 epoch 685 head A head_i_epoch 0 batch 100: avg loss -3.090961 avg loss no lamb -3.090961 time 2019-02-25 08:08:25.026499
Model ind 640 epoch 685 head A head_i_epoch 0 batch 200: avg loss -3.199191 avg loss no lamb -3.199191 time 2019-02-25 08:10:17.069273
last batch sz 160
Model ind 640 epoch 685 head B head_i_epoch 0 batch 0: avg loss -1.774814 avg loss no lamb -1.774814 time 2019-02-25 08:11:37.046613
Model ind 640 epoch 685 head B head_i_epoch 0 batch 100: avg loss -1.690346 avg loss no lamb -1.690346 time 2019-02-25 08:13:24.617943
Model ind 640 epoch 685 head B head_i_epoch 0 batch 200: avg loss -1.800279 avg loss no lamb -1.800279 time 2019-02-25 08:15:12.248762
last batch sz 160
Model ind 640 epoch 685 head B head_i_epoch 1 batch 0: avg loss -1.871030 avg loss no lamb -1.871030 time 2019-02-25 08:16:30.314858
Model ind 640 epoch 685 head B head_i_epoch 1 batch 100: avg loss -1.682703 avg loss no lamb -1.682703 time 2019-02-25 08:18:17.068760
Model ind 640 epoch 685 head B head_i_epoch 1 batch 200: avg loss -1.847428 avg loss no lamb -1.847428 time 2019-02-25 08:20:04.478205
last batch sz 160
Pre: time 2019-02-25 08:21:41.998995: 
 	std: 0.050635114
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5093, 0.6127, 0.61245, 0.6126, 0.50915]
	train_accs: [0.5093, 0.6127, 0.61245, 0.6126, 0.50915]
	best_train_sub_head: 1
	worst: 0.50915
	avg: 0.57124
	best: 0.6127

Starting e_i: 686
Model ind 640 epoch 686 head A head_i_epoch 0 batch 0: avg loss -3.075580 avg loss no lamb -3.075580 time 2019-02-25 08:21:44.150910
Model ind 640 epoch 686 head A head_i_epoch 0 batch 100: avg loss -3.075950 avg loss no lamb -3.075950 time 2019-02-25 08:23:33.256041
Model ind 640 epoch 686 head A head_i_epoch 0 batch 200: avg loss -3.164329 avg loss no lamb -3.164329 time 2019-02-25 08:25:21.417759
last batch sz 160
Model ind 640 epoch 686 head B head_i_epoch 0 batch 0: avg loss -1.786000 avg loss no lamb -1.786000 time 2019-02-25 08:26:39.695151
Model ind 640 epoch 686 head B head_i_epoch 0 batch 100: avg loss -1.623400 avg loss no lamb -1.623400 time 2019-02-25 08:28:27.820165
Model ind 640 epoch 686 head B head_i_epoch 0 batch 200: avg loss -1.825804 avg loss no lamb -1.825804 time 2019-02-25 08:30:15.193333
last batch sz 160
Model ind 640 epoch 686 head B head_i_epoch 1 batch 0: avg loss -1.865691 avg loss no lamb -1.865691 time 2019-02-25 08:31:32.835042
Model ind 640 epoch 686 head B head_i_epoch 1 batch 100: avg loss -1.712286 avg loss no lamb -1.712286 time 2019-02-25 08:33:21.988322
Model ind 640 epoch 686 head B head_i_epoch 1 batch 200: avg loss -1.840531 avg loss no lamb -1.840531 time 2019-02-25 08:35:08.910930
last batch sz 160
Pre: time 2019-02-25 08:36:46.798121: 
 	std: 0.049622845
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51166666, 0.61308336, 0.6131667, 0.6133, 0.5121167]
	train_accs: [0.51166666, 0.61308336, 0.6131667, 0.6133, 0.5121167]
	best_train_sub_head: 3
	worst: 0.51166666
	avg: 0.5726667
	best: 0.6133

Starting e_i: 687
Model ind 640 epoch 687 head A head_i_epoch 0 batch 0: avg loss -3.051306 avg loss no lamb -3.051306 time 2019-02-25 08:36:48.965445
Model ind 640 epoch 687 head A head_i_epoch 0 batch 100: avg loss -3.060611 avg loss no lamb -3.060611 time 2019-02-25 08:38:36.318942
Model ind 640 epoch 687 head A head_i_epoch 0 batch 200: avg loss -3.094924 avg loss no lamb -3.094924 time 2019-02-25 08:40:24.324087
last batch sz 160
Model ind 640 epoch 687 head B head_i_epoch 0 batch 0: avg loss -1.809199 avg loss no lamb -1.809199 time 2019-02-25 08:41:43.329907
Model ind 640 epoch 687 head B head_i_epoch 0 batch 100: avg loss -1.738487 avg loss no lamb -1.738487 time 2019-02-25 08:43:31.231994
Model ind 640 epoch 687 head B head_i_epoch 0 batch 200: avg loss -1.756711 avg loss no lamb -1.756711 time 2019-02-25 08:45:18.047714
last batch sz 160
Model ind 640 epoch 687 head B head_i_epoch 1 batch 0: avg loss -1.770701 avg loss no lamb -1.770701 time 2019-02-25 08:46:36.205357
Model ind 640 epoch 687 head B head_i_epoch 1 batch 100: avg loss -1.771470 avg loss no lamb -1.771470 time 2019-02-25 08:48:23.535261
Model ind 640 epoch 687 head B head_i_epoch 1 batch 200: avg loss -1.832835 avg loss no lamb -1.832835 time 2019-02-25 08:50:10.925294
last batch sz 160
Pre: time 2019-02-25 08:51:49.224249: 
 	std: 0.050973434
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5067667, 0.61116666, 0.6112, 0.61145, 0.50768334]
	train_accs: [0.5067667, 0.61116666, 0.6112, 0.61145, 0.50768334]
	best_train_sub_head: 3
	worst: 0.5067667
	avg: 0.56965333
	best: 0.61145

Starting e_i: 688
Model ind 640 epoch 688 head A head_i_epoch 0 batch 0: avg loss -3.141822 avg loss no lamb -3.141822 time 2019-02-25 08:51:51.498532
Model ind 640 epoch 688 head A head_i_epoch 0 batch 100: avg loss -3.091207 avg loss no lamb -3.091207 time 2019-02-25 08:53:38.872546
Model ind 640 epoch 688 head A head_i_epoch 0 batch 200: avg loss -3.183713 avg loss no lamb -3.183713 time 2019-02-25 08:55:25.694185
last batch sz 160
Model ind 640 epoch 688 head B head_i_epoch 0 batch 0: avg loss -1.815292 avg loss no lamb -1.815292 time 2019-02-25 08:56:44.709866
Model ind 640 epoch 688 head B head_i_epoch 0 batch 100: avg loss -1.771238 avg loss no lamb -1.771238 time 2019-02-25 08:58:32.167859
Model ind 640 epoch 688 head B head_i_epoch 0 batch 200: avg loss -1.843564 avg loss no lamb -1.843564 time 2019-02-25 09:00:19.634322
last batch sz 160
Model ind 640 epoch 688 head B head_i_epoch 1 batch 0: avg loss -1.740346 avg loss no lamb -1.740346 time 2019-02-25 09:01:38.154842
Model ind 640 epoch 688 head B head_i_epoch 1 batch 100: avg loss -1.721478 avg loss no lamb -1.721478 time 2019-02-25 09:03:25.399355
Model ind 640 epoch 688 head B head_i_epoch 1 batch 200: avg loss -1.853701 avg loss no lamb -1.853701 time 2019-02-25 09:05:12.194331
last batch sz 160
Pre: time 2019-02-25 09:06:50.486942: 
 	std: 0.050526675
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5093833, 0.61268336, 0.61255, 0.61305, 0.50986665]
	train_accs: [0.5093833, 0.61268336, 0.61255, 0.61305, 0.50986665]
	best_train_sub_head: 3
	worst: 0.5093833
	avg: 0.5715067
	best: 0.61305

Starting e_i: 689
Model ind 640 epoch 689 head A head_i_epoch 0 batch 0: avg loss -3.125671 avg loss no lamb -3.125671 time 2019-02-25 09:06:52.964496
Model ind 640 epoch 689 head A head_i_epoch 0 batch 100: avg loss -2.997695 avg loss no lamb -2.997695 time 2019-02-25 09:08:40.631176
Model ind 640 epoch 689 head A head_i_epoch 0 batch 200: avg loss -3.109653 avg loss no lamb -3.109653 time 2019-02-25 09:10:28.957417
last batch sz 160
Model ind 640 epoch 689 head B head_i_epoch 0 batch 0: avg loss -1.801745 avg loss no lamb -1.801745 time 2019-02-25 09:11:46.445076
Model ind 640 epoch 689 head B head_i_epoch 0 batch 100: avg loss -1.618311 avg loss no lamb -1.618311 time 2019-02-25 09:13:33.127534
Model ind 640 epoch 689 head B head_i_epoch 0 batch 200: avg loss -1.918839 avg loss no lamb -1.918839 time 2019-02-25 09:15:20.615619
last batch sz 160
Model ind 640 epoch 689 head B head_i_epoch 1 batch 0: avg loss -1.773747 avg loss no lamb -1.773747 time 2019-02-25 09:16:38.337614
Model ind 640 epoch 689 head B head_i_epoch 1 batch 100: avg loss -1.838328 avg loss no lamb -1.838328 time 2019-02-25 09:18:25.220083
Model ind 640 epoch 689 head B head_i_epoch 1 batch 200: avg loss -1.813606 avg loss no lamb -1.813606 time 2019-02-25 09:20:12.422239
last batch sz 160
Pre: time 2019-02-25 09:21:50.595272: 
 	std: 0.050395347
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50813335, 0.6116, 0.6116, 0.6114, 0.5092]
	train_accs: [0.50813335, 0.6116, 0.6116, 0.6114, 0.5092]
	best_train_sub_head: 1
	worst: 0.50813335
	avg: 0.5703867
	best: 0.6116

Starting e_i: 690
Model ind 640 epoch 690 head A head_i_epoch 0 batch 0: avg loss -3.131203 avg loss no lamb -3.131203 time 2019-02-25 09:21:52.844799
Model ind 640 epoch 690 head A head_i_epoch 0 batch 100: avg loss -3.052802 avg loss no lamb -3.052802 time 2019-02-25 09:23:40.156709
Model ind 640 epoch 690 head A head_i_epoch 0 batch 200: avg loss -3.118134 avg loss no lamb -3.118134 time 2019-02-25 09:25:27.695782
last batch sz 160
Model ind 640 epoch 690 head B head_i_epoch 0 batch 0: avg loss -1.776704 avg loss no lamb -1.776704 time 2019-02-25 09:26:45.347815
Model ind 640 epoch 690 head B head_i_epoch 0 batch 100: avg loss -1.698525 avg loss no lamb -1.698525 time 2019-02-25 09:28:32.621299
Model ind 640 epoch 690 head B head_i_epoch 0 batch 200: avg loss -1.821399 avg loss no lamb -1.821399 time 2019-02-25 09:30:18.935551
last batch sz 160
Model ind 640 epoch 690 head B head_i_epoch 1 batch 0: avg loss -1.857473 avg loss no lamb -1.857473 time 2019-02-25 09:31:38.746338
Model ind 640 epoch 690 head B head_i_epoch 1 batch 100: avg loss -1.738528 avg loss no lamb -1.738528 time 2019-02-25 09:33:26.420027
Model ind 640 epoch 690 head B head_i_epoch 1 batch 200: avg loss -1.899683 avg loss no lamb -1.899683 time 2019-02-25 09:35:13.115255
last batch sz 160
Pre: time 2019-02-25 09:36:51.052145: 
 	std: 0.050617743
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5075833, 0.61123335, 0.61121666, 0.61121666, 0.5082167]
	train_accs: [0.5075833, 0.61123335, 0.61121666, 0.61121666, 0.5082167]
	best_train_sub_head: 1
	worst: 0.5075833
	avg: 0.5698933
	best: 0.61123335

Starting e_i: 691
Model ind 640 epoch 691 head A head_i_epoch 0 batch 0: avg loss -3.091454 avg loss no lamb -3.091454 time 2019-02-25 09:36:58.137544
Model ind 640 epoch 691 head A head_i_epoch 0 batch 100: avg loss -3.051750 avg loss no lamb -3.051750 time 2019-02-25 09:38:43.956091
Model ind 640 epoch 691 head A head_i_epoch 0 batch 200: avg loss -3.190340 avg loss no lamb -3.190340 time 2019-02-25 09:40:32.511826
last batch sz 160
Model ind 640 epoch 691 head B head_i_epoch 0 batch 0: avg loss -1.762149 avg loss no lamb -1.762149 time 2019-02-25 09:41:50.622237
Model ind 640 epoch 691 head B head_i_epoch 0 batch 100: avg loss -1.631338 avg loss no lamb -1.631338 time 2019-02-25 09:43:37.762343
Model ind 640 epoch 691 head B head_i_epoch 0 batch 200: avg loss -1.835439 avg loss no lamb -1.835439 time 2019-02-25 09:45:24.477207
last batch sz 160
Model ind 640 epoch 691 head B head_i_epoch 1 batch 0: avg loss -1.793483 avg loss no lamb -1.793483 time 2019-02-25 09:46:41.949680
Model ind 640 epoch 691 head B head_i_epoch 1 batch 100: avg loss -1.628424 avg loss no lamb -1.628424 time 2019-02-25 09:48:28.789455
Model ind 640 epoch 691 head B head_i_epoch 1 batch 200: avg loss -1.869305 avg loss no lamb -1.869305 time 2019-02-25 09:50:15.520374
last batch sz 160
Pre: time 2019-02-25 09:51:53.378566: 
 	std: 0.050313674
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5096167, 0.61275, 0.61293334, 0.61286664, 0.51068336]
	train_accs: [0.5096167, 0.61275, 0.61293334, 0.61286664, 0.51068336]
	best_train_sub_head: 2
	worst: 0.5096167
	avg: 0.57177
	best: 0.61293334

Starting e_i: 692
Model ind 640 epoch 692 head A head_i_epoch 0 batch 0: avg loss -3.129604 avg loss no lamb -3.129604 time 2019-02-25 09:51:55.596941
Model ind 640 epoch 692 head A head_i_epoch 0 batch 100: avg loss -3.076163 avg loss no lamb -3.076163 time 2019-02-25 09:53:43.453543
Model ind 640 epoch 692 head A head_i_epoch 0 batch 200: avg loss -3.142974 avg loss no lamb -3.142974 time 2019-02-25 09:55:31.265552
last batch sz 160
Model ind 640 epoch 692 head B head_i_epoch 0 batch 0: avg loss -1.838945 avg loss no lamb -1.838945 time 2019-02-25 09:56:49.818144
Model ind 640 epoch 692 head B head_i_epoch 0 batch 100: avg loss -1.722537 avg loss no lamb -1.722537 time 2019-02-25 09:58:38.070845
Model ind 640 epoch 692 head B head_i_epoch 0 batch 200: avg loss -1.802539 avg loss no lamb -1.802539 time 2019-02-25 10:00:25.716363
last batch sz 160
Model ind 640 epoch 692 head B head_i_epoch 1 batch 0: avg loss -1.879081 avg loss no lamb -1.879081 time 2019-02-25 10:01:43.741720
Model ind 640 epoch 692 head B head_i_epoch 1 batch 100: avg loss -1.788527 avg loss no lamb -1.788527 time 2019-02-25 10:03:31.507807
Model ind 640 epoch 692 head B head_i_epoch 1 batch 200: avg loss -1.782850 avg loss no lamb -1.782850 time 2019-02-25 10:05:19.735149
last batch sz 160
Pre: time 2019-02-25 10:06:58.359048: 
 	std: 0.051394492
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50836664, 0.61338335, 0.61335, 0.6135167, 0.50865]
	train_accs: [0.50836664, 0.61338335, 0.61335, 0.6135167, 0.50865]
	best_train_sub_head: 3
	worst: 0.50836664
	avg: 0.57145333
	best: 0.6135167

Starting e_i: 693
Model ind 640 epoch 693 head A head_i_epoch 0 batch 0: avg loss -3.123518 avg loss no lamb -3.123518 time 2019-02-25 10:07:00.694498
Model ind 640 epoch 693 head A head_i_epoch 0 batch 100: avg loss -3.080907 avg loss no lamb -3.080907 time 2019-02-25 10:08:47.786228
Model ind 640 epoch 693 head A head_i_epoch 0 batch 200: avg loss -3.156502 avg loss no lamb -3.156502 time 2019-02-25 10:10:34.565634
last batch sz 160
Model ind 640 epoch 693 head B head_i_epoch 0 batch 0: avg loss -1.892233 avg loss no lamb -1.892233 time 2019-02-25 10:11:52.812353
Model ind 640 epoch 693 head B head_i_epoch 0 batch 100: avg loss -1.684889 avg loss no lamb -1.684889 time 2019-02-25 10:13:39.931070
Model ind 640 epoch 693 head B head_i_epoch 0 batch 200: avg loss -1.836798 avg loss no lamb -1.836798 time 2019-02-25 10:15:27.414756
last batch sz 160
Model ind 640 epoch 693 head B head_i_epoch 1 batch 0: avg loss -1.802682 avg loss no lamb -1.802682 time 2019-02-25 10:16:45.479628
Model ind 640 epoch 693 head B head_i_epoch 1 batch 100: avg loss -1.747336 avg loss no lamb -1.747336 time 2019-02-25 10:18:32.339347
Model ind 640 epoch 693 head B head_i_epoch 1 batch 200: avg loss -1.883525 avg loss no lamb -1.883525 time 2019-02-25 10:20:19.289212
last batch sz 160
Pre: time 2019-02-25 10:21:56.911464: 
 	std: 0.051617287
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5076, 0.6134833, 0.6131, 0.61365, 0.5085]
	train_accs: [0.5076, 0.6134833, 0.6131, 0.61365, 0.5085]
	best_train_sub_head: 3
	worst: 0.5076
	avg: 0.57126665
	best: 0.61365

Starting e_i: 694
Model ind 640 epoch 694 head A head_i_epoch 0 batch 0: avg loss -3.105527 avg loss no lamb -3.105527 time 2019-02-25 10:21:59.435743
Model ind 640 epoch 694 head A head_i_epoch 0 batch 100: avg loss -3.028386 avg loss no lamb -3.028386 time 2019-02-25 10:23:47.030965
Model ind 640 epoch 694 head A head_i_epoch 0 batch 200: avg loss -3.167522 avg loss no lamb -3.167522 time 2019-02-25 10:25:34.459888
last batch sz 160
Model ind 640 epoch 694 head B head_i_epoch 0 batch 0: avg loss -1.790274 avg loss no lamb -1.790274 time 2019-02-25 10:26:52.224097
Model ind 640 epoch 694 head B head_i_epoch 0 batch 100: avg loss -1.752987 avg loss no lamb -1.752987 time 2019-02-25 10:28:39.311790
Model ind 640 epoch 694 head B head_i_epoch 0 batch 200: avg loss -1.777847 avg loss no lamb -1.777847 time 2019-02-25 10:30:27.030649
last batch sz 160
Model ind 640 epoch 694 head B head_i_epoch 1 batch 0: avg loss -1.791947 avg loss no lamb -1.791947 time 2019-02-25 10:31:46.352726
Model ind 640 epoch 694 head B head_i_epoch 1 batch 100: avg loss -1.700874 avg loss no lamb -1.700874 time 2019-02-25 10:33:33.151121
Model ind 640 epoch 694 head B head_i_epoch 1 batch 200: avg loss -1.881211 avg loss no lamb -1.881211 time 2019-02-25 10:35:19.994111
last batch sz 160
Pre: time 2019-02-25 10:36:58.922684: 
 	std: 0.051079385
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5071833, 0.61191666, 0.61181664, 0.61188334, 0.50803334]
	train_accs: [0.5071833, 0.61191666, 0.61181664, 0.61188334, 0.50803334]
	best_train_sub_head: 1
	worst: 0.5071833
	avg: 0.57016665
	best: 0.61191666

Starting e_i: 695
Model ind 640 epoch 695 head A head_i_epoch 0 batch 0: avg loss -3.157245 avg loss no lamb -3.157245 time 2019-02-25 10:37:01.136915
Model ind 640 epoch 695 head A head_i_epoch 0 batch 100: avg loss -3.043891 avg loss no lamb -3.043891 time 2019-02-25 10:38:48.491720
Model ind 640 epoch 695 head A head_i_epoch 0 batch 200: avg loss -3.153304 avg loss no lamb -3.153304 time 2019-02-25 10:40:35.625605
last batch sz 160
Model ind 640 epoch 695 head B head_i_epoch 0 batch 0: avg loss -1.793568 avg loss no lamb -1.793568 time 2019-02-25 10:41:53.582861
Model ind 640 epoch 695 head B head_i_epoch 0 batch 100: avg loss -1.762298 avg loss no lamb -1.762298 time 2019-02-25 10:43:40.592650
Model ind 640 epoch 695 head B head_i_epoch 0 batch 200: avg loss -1.837443 avg loss no lamb -1.837443 time 2019-02-25 10:45:28.021403
last batch sz 160
Model ind 640 epoch 695 head B head_i_epoch 1 batch 0: avg loss -1.798430 avg loss no lamb -1.798430 time 2019-02-25 10:46:45.708792
Model ind 640 epoch 695 head B head_i_epoch 1 batch 100: avg loss -1.738896 avg loss no lamb -1.738896 time 2019-02-25 10:48:32.139050
Model ind 640 epoch 695 head B head_i_epoch 1 batch 200: avg loss -1.904814 avg loss no lamb -1.904814 time 2019-02-25 10:50:19.523084
last batch sz 160
Pre: time 2019-02-25 10:51:58.202468: 
 	std: 0.05111323
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50835, 0.61308336, 0.61296666, 0.6131, 0.50908333]
	train_accs: [0.50835, 0.61308336, 0.61296666, 0.6131, 0.50908333]
	best_train_sub_head: 3
	worst: 0.50835
	avg: 0.57131666
	best: 0.6131

Starting e_i: 696
Model ind 640 epoch 696 head A head_i_epoch 0 batch 0: avg loss -3.108207 avg loss no lamb -3.108207 time 2019-02-25 10:52:00.783546
Model ind 640 epoch 696 head A head_i_epoch 0 batch 100: avg loss -3.019161 avg loss no lamb -3.019161 time 2019-02-25 10:53:48.252859
Model ind 640 epoch 696 head A head_i_epoch 0 batch 200: avg loss -3.169864 avg loss no lamb -3.169864 time 2019-02-25 10:55:34.482947
last batch sz 160
Model ind 640 epoch 696 head B head_i_epoch 0 batch 0: avg loss -1.801828 avg loss no lamb -1.801828 time 2019-02-25 10:56:52.111797
Model ind 640 epoch 696 head B head_i_epoch 0 batch 100: avg loss -1.686965 avg loss no lamb -1.686965 time 2019-02-25 10:58:40.885684
Model ind 640 epoch 696 head B head_i_epoch 0 batch 200: avg loss -1.871868 avg loss no lamb -1.871868 time 2019-02-25 11:00:28.510743
last batch sz 160
Model ind 640 epoch 696 head B head_i_epoch 1 batch 0: avg loss -1.855446 avg loss no lamb -1.855446 time 2019-02-25 11:01:45.866533
Model ind 640 epoch 696 head B head_i_epoch 1 batch 100: avg loss -1.768371 avg loss no lamb -1.768371 time 2019-02-25 11:03:32.577798
Model ind 640 epoch 696 head B head_i_epoch 1 batch 200: avg loss -1.778807 avg loss no lamb -1.778807 time 2019-02-25 11:05:19.398406
last batch sz 160
Pre: time 2019-02-25 11:06:57.807886: 
 	std: 0.05110083
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5086, 0.6135167, 0.61343336, 0.61371666, 0.5099]
	train_accs: [0.5086, 0.6135167, 0.61343336, 0.61371666, 0.5099]
	best_train_sub_head: 3
	worst: 0.5086
	avg: 0.5718333
	best: 0.61371666

Starting e_i: 697
Model ind 640 epoch 697 head A head_i_epoch 0 batch 0: avg loss -3.144731 avg loss no lamb -3.144731 time 2019-02-25 11:07:00.027088
Model ind 640 epoch 697 head A head_i_epoch 0 batch 100: avg loss -3.014241 avg loss no lamb -3.014241 time 2019-02-25 11:08:46.897164
Model ind 640 epoch 697 head A head_i_epoch 0 batch 200: avg loss -3.128007 avg loss no lamb -3.128007 time 2019-02-25 11:10:34.625150
last batch sz 160
Model ind 640 epoch 697 head B head_i_epoch 0 batch 0: avg loss -1.819686 avg loss no lamb -1.819686 time 2019-02-25 11:11:52.273323
Model ind 640 epoch 697 head B head_i_epoch 0 batch 100: avg loss -1.730172 avg loss no lamb -1.730172 time 2019-02-25 11:13:39.530903
Model ind 640 epoch 697 head B head_i_epoch 0 batch 200: avg loss -1.838643 avg loss no lamb -1.838643 time 2019-02-25 11:15:26.221896
last batch sz 160
Model ind 640 epoch 697 head B head_i_epoch 1 batch 0: avg loss -1.823982 avg loss no lamb -1.823982 time 2019-02-25 11:16:44.019449
Model ind 640 epoch 697 head B head_i_epoch 1 batch 100: avg loss -1.748837 avg loss no lamb -1.748837 time 2019-02-25 11:18:31.579284
Model ind 640 epoch 697 head B head_i_epoch 1 batch 200: avg loss -1.839157 avg loss no lamb -1.839157 time 2019-02-25 11:20:18.718812
last batch sz 160
Pre: time 2019-02-25 11:21:56.873661: 
 	std: 0.050715648
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50993335, 0.61373335, 0.6138, 0.61368334, 0.5105]
	train_accs: [0.50993335, 0.61373335, 0.6138, 0.61368334, 0.5105]
	best_train_sub_head: 2
	worst: 0.50993335
	avg: 0.57233
	best: 0.6138

Starting e_i: 698
Model ind 640 epoch 698 head A head_i_epoch 0 batch 0: avg loss -3.183163 avg loss no lamb -3.183163 time 2019-02-25 11:21:59.280365
Model ind 640 epoch 698 head A head_i_epoch 0 batch 100: avg loss -3.108375 avg loss no lamb -3.108375 time 2019-02-25 11:23:46.069052
Model ind 640 epoch 698 head A head_i_epoch 0 batch 200: avg loss -3.145010 avg loss no lamb -3.145010 time 2019-02-25 11:25:32.709059
last batch sz 160
Model ind 640 epoch 698 head B head_i_epoch 0 batch 0: avg loss -1.773568 avg loss no lamb -1.773568 time 2019-02-25 11:26:50.617639
Model ind 640 epoch 698 head B head_i_epoch 0 batch 100: avg loss -1.703378 avg loss no lamb -1.703378 time 2019-02-25 11:28:36.730574
Model ind 640 epoch 698 head B head_i_epoch 0 batch 200: avg loss -1.840175 avg loss no lamb -1.840175 time 2019-02-25 11:30:25.337235
last batch sz 160
Model ind 640 epoch 698 head B head_i_epoch 1 batch 0: avg loss -1.750400 avg loss no lamb -1.750400 time 2019-02-25 11:31:43.705643
Model ind 640 epoch 698 head B head_i_epoch 1 batch 100: avg loss -1.796996 avg loss no lamb -1.796996 time 2019-02-25 11:33:30.550325
Model ind 640 epoch 698 head B head_i_epoch 1 batch 200: avg loss -1.823819 avg loss no lamb -1.823819 time 2019-02-25 11:35:17.058904
last batch sz 160
Pre: time 2019-02-25 11:36:54.835477: 
 	std: 0.050893955
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5088, 0.61296666, 0.6129, 0.6130667, 0.5093833]
	train_accs: [0.5088, 0.61296666, 0.6129, 0.6130667, 0.5093833]
	best_train_sub_head: 3
	worst: 0.5088
	avg: 0.57142335
	best: 0.6130667

Starting e_i: 699
Model ind 640 epoch 699 head A head_i_epoch 0 batch 0: avg loss -3.116725 avg loss no lamb -3.116725 time 2019-02-25 11:36:57.041146
Model ind 640 epoch 699 head A head_i_epoch 0 batch 100: avg loss -3.070481 avg loss no lamb -3.070481 time 2019-02-25 11:38:44.930802
Model ind 640 epoch 699 head A head_i_epoch 0 batch 200: avg loss -3.193763 avg loss no lamb -3.193763 time 2019-02-25 11:40:31.912343
last batch sz 160
Model ind 640 epoch 699 head B head_i_epoch 0 batch 0: avg loss -1.795647 avg loss no lamb -1.795647 time 2019-02-25 11:41:49.940293
Model ind 640 epoch 699 head B head_i_epoch 0 batch 100: avg loss -1.719711 avg loss no lamb -1.719711 time 2019-02-25 11:43:37.650130
Model ind 640 epoch 699 head B head_i_epoch 0 batch 200: avg loss -1.844726 avg loss no lamb -1.844726 time 2019-02-25 11:45:24.773475
last batch sz 160
Model ind 640 epoch 699 head B head_i_epoch 1 batch 0: avg loss -1.846262 avg loss no lamb -1.846262 time 2019-02-25 11:46:42.653456
Model ind 640 epoch 699 head B head_i_epoch 1 batch 100: avg loss -1.686894 avg loss no lamb -1.686894 time 2019-02-25 11:48:29.510317
Model ind 640 epoch 699 head B head_i_epoch 1 batch 200: avg loss -1.782825 avg loss no lamb -1.782825 time 2019-02-25 11:50:16.141709
last batch sz 160
Pre: time 2019-02-25 11:51:54.389150: 
 	std: 0.05018774
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51096666, 0.6138167, 0.6136, 0.61371666, 0.51156664]
	train_accs: [0.51096666, 0.6138167, 0.6136, 0.61371666, 0.51156664]
	best_train_sub_head: 1
	worst: 0.51096666
	avg: 0.5727333
	best: 0.6138167

Starting e_i: 700
Model ind 640 epoch 700 head A head_i_epoch 0 batch 0: avg loss -3.174966 avg loss no lamb -3.174966 time 2019-02-25 11:51:56.845438
Model ind 640 epoch 700 head A head_i_epoch 0 batch 100: avg loss -3.044857 avg loss no lamb -3.044857 time 2019-02-25 11:53:43.478232
Model ind 640 epoch 700 head A head_i_epoch 0 batch 200: avg loss -3.150193 avg loss no lamb -3.150193 time 2019-02-25 11:55:30.622904
last batch sz 160
Model ind 640 epoch 700 head B head_i_epoch 0 batch 0: avg loss -1.790298 avg loss no lamb -1.790298 time 2019-02-25 11:56:48.306587
Model ind 640 epoch 700 head B head_i_epoch 0 batch 100: avg loss -1.740985 avg loss no lamb -1.740985 time 2019-02-25 11:58:34.937038
Model ind 640 epoch 700 head B head_i_epoch 0 batch 200: avg loss -1.791888 avg loss no lamb -1.791888 time 2019-02-25 12:00:21.642612
last batch sz 160
Model ind 640 epoch 700 head B head_i_epoch 1 batch 0: avg loss -1.750856 avg loss no lamb -1.750856 time 2019-02-25 12:01:38.699770
Model ind 640 epoch 700 head B head_i_epoch 1 batch 100: avg loss -1.771384 avg loss no lamb -1.771384 time 2019-02-25 12:03:25.697907
Model ind 640 epoch 700 head B head_i_epoch 1 batch 200: avg loss -1.882423 avg loss no lamb -1.882423 time 2019-02-25 12:05:12.440144
last batch sz 160
Pre: time 2019-02-25 12:06:50.189942: 
 	std: 0.050753474
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50876665, 0.61301666, 0.61298335, 0.61286664, 0.50995]
	train_accs: [0.50876665, 0.61301666, 0.61298335, 0.61286664, 0.50995]
	best_train_sub_head: 1
	worst: 0.50876665
	avg: 0.57151663
	best: 0.61301666

Starting e_i: 701
Model ind 640 epoch 701 head A head_i_epoch 0 batch 0: avg loss -3.223926 avg loss no lamb -3.223926 time 2019-02-25 12:06:56.086304
Model ind 640 epoch 701 head A head_i_epoch 0 batch 100: avg loss -3.048439 avg loss no lamb -3.048439 time 2019-02-25 12:08:43.578270
Model ind 640 epoch 701 head A head_i_epoch 0 batch 200: avg loss -3.205911 avg loss no lamb -3.205911 time 2019-02-25 12:10:30.865108
last batch sz 160
Model ind 640 epoch 701 head B head_i_epoch 0 batch 0: avg loss -1.778572 avg loss no lamb -1.778572 time 2019-02-25 12:11:48.651798
Model ind 640 epoch 701 head B head_i_epoch 0 batch 100: avg loss -1.782428 avg loss no lamb -1.782428 time 2019-02-25 12:13:34.233744
Model ind 640 epoch 701 head B head_i_epoch 0 batch 200: avg loss -1.885563 avg loss no lamb -1.885563 time 2019-02-25 12:15:24.453838
last batch sz 160
Model ind 640 epoch 701 head B head_i_epoch 1 batch 0: avg loss -1.751099 avg loss no lamb -1.751099 time 2019-02-25 12:16:42.197444
Model ind 640 epoch 701 head B head_i_epoch 1 batch 100: avg loss -1.746351 avg loss no lamb -1.746351 time 2019-02-25 12:18:29.299577
Model ind 640 epoch 701 head B head_i_epoch 1 batch 200: avg loss -1.821984 avg loss no lamb -1.821984 time 2019-02-25 12:20:15.864653
last batch sz 160
Pre: time 2019-02-25 12:21:54.019265: 
 	std: 0.050895274
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5086, 0.61255, 0.61266667, 0.6129, 0.5090333]
	train_accs: [0.5086, 0.61255, 0.61266667, 0.6129, 0.5090333]
	best_train_sub_head: 3
	worst: 0.5086
	avg: 0.57115
	best: 0.6129

Starting e_i: 702
Model ind 640 epoch 702 head A head_i_epoch 0 batch 0: avg loss -3.190232 avg loss no lamb -3.190232 time 2019-02-25 12:21:56.153762
Model ind 640 epoch 702 head A head_i_epoch 0 batch 100: avg loss -3.061820 avg loss no lamb -3.061820 time 2019-02-25 12:23:43.410935
Model ind 640 epoch 702 head A head_i_epoch 0 batch 200: avg loss -3.175709 avg loss no lamb -3.175709 time 2019-02-25 12:25:30.503947
last batch sz 160
Model ind 640 epoch 702 head B head_i_epoch 0 batch 0: avg loss -1.857683 avg loss no lamb -1.857683 time 2019-02-25 12:26:48.492182
Model ind 640 epoch 702 head B head_i_epoch 0 batch 100: avg loss -1.722952 avg loss no lamb -1.722952 time 2019-02-25 12:28:34.040050
Model ind 640 epoch 702 head B head_i_epoch 0 batch 200: avg loss -1.866876 avg loss no lamb -1.866876 time 2019-02-25 12:30:23.430033
last batch sz 160
Model ind 640 epoch 702 head B head_i_epoch 1 batch 0: avg loss -1.759362 avg loss no lamb -1.759362 time 2019-02-25 12:31:41.047496
Model ind 640 epoch 702 head B head_i_epoch 1 batch 100: avg loss -1.705340 avg loss no lamb -1.705340 time 2019-02-25 12:33:27.891203
Model ind 640 epoch 702 head B head_i_epoch 1 batch 200: avg loss -1.814296 avg loss no lamb -1.814296 time 2019-02-25 12:35:14.905101
last batch sz 160
Pre: time 2019-02-25 12:36:52.478105: 
 	std: 0.051062636
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5085, 0.6129, 0.6131, 0.61296666, 0.5090167]
	train_accs: [0.5085, 0.6129, 0.6131, 0.61296666, 0.5090167]
	best_train_sub_head: 2
	worst: 0.5085
	avg: 0.57129663
	best: 0.6131

Starting e_i: 703
Model ind 640 epoch 703 head A head_i_epoch 0 batch 0: avg loss -3.188793 avg loss no lamb -3.188793 time 2019-02-25 12:36:54.725528
Model ind 640 epoch 703 head A head_i_epoch 0 batch 100: avg loss -3.040246 avg loss no lamb -3.040246 time 2019-02-25 12:38:41.978882
Model ind 640 epoch 703 head A head_i_epoch 0 batch 200: avg loss -3.105473 avg loss no lamb -3.105473 time 2019-02-25 12:40:29.022629
last batch sz 160
Model ind 640 epoch 703 head B head_i_epoch 0 batch 0: avg loss -1.809520 avg loss no lamb -1.809520 time 2019-02-25 12:41:46.929881
Model ind 640 epoch 703 head B head_i_epoch 0 batch 100: avg loss -1.777503 avg loss no lamb -1.777503 time 2019-02-25 12:43:33.860520
Model ind 640 epoch 703 head B head_i_epoch 0 batch 200: avg loss -1.827288 avg loss no lamb -1.827288 time 2019-02-25 12:45:20.900293
last batch sz 160
Model ind 640 epoch 703 head B head_i_epoch 1 batch 0: avg loss -1.876683 avg loss no lamb -1.876683 time 2019-02-25 12:46:38.997464
Model ind 640 epoch 703 head B head_i_epoch 1 batch 100: avg loss -1.756815 avg loss no lamb -1.756815 time 2019-02-25 12:48:25.928185
Model ind 640 epoch 703 head B head_i_epoch 1 batch 200: avg loss -1.878340 avg loss no lamb -1.878340 time 2019-02-25 12:50:13.046057
last batch sz 160
Pre: time 2019-02-25 12:51:52.018017: 
 	std: 0.051115505
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50806665, 0.6123833, 0.61263335, 0.61245, 0.5082333]
	train_accs: [0.50806665, 0.6123833, 0.61263335, 0.61245, 0.5082333]
	best_train_sub_head: 2
	worst: 0.50806665
	avg: 0.57075334
	best: 0.61263335

Starting e_i: 704
Model ind 640 epoch 704 head A head_i_epoch 0 batch 0: avg loss -3.126320 avg loss no lamb -3.126320 time 2019-02-25 12:51:54.261404
Model ind 640 epoch 704 head A head_i_epoch 0 batch 100: avg loss -2.977902 avg loss no lamb -2.977902 time 2019-02-25 12:53:41.601727
Model ind 640 epoch 704 head A head_i_epoch 0 batch 200: avg loss -3.187012 avg loss no lamb -3.187012 time 2019-02-25 12:55:28.688949
last batch sz 160
Model ind 640 epoch 704 head B head_i_epoch 0 batch 0: avg loss -1.838049 avg loss no lamb -1.838049 time 2019-02-25 12:56:47.030504
Model ind 640 epoch 704 head B head_i_epoch 0 batch 100: avg loss -1.694428 avg loss no lamb -1.694428 time 2019-02-25 12:58:37.585318
Model ind 640 epoch 704 head B head_i_epoch 0 batch 200: avg loss -1.844838 avg loss no lamb -1.844838 time 2019-02-25 13:00:27.789560
last batch sz 160
Model ind 640 epoch 704 head B head_i_epoch 1 batch 0: avg loss -1.846746 avg loss no lamb -1.846746 time 2019-02-25 13:01:47.668254
Model ind 640 epoch 704 head B head_i_epoch 1 batch 100: avg loss -1.693651 avg loss no lamb -1.693651 time 2019-02-25 13:03:38.287182
Model ind 640 epoch 704 head B head_i_epoch 1 batch 200: avg loss -1.856055 avg loss no lamb -1.856055 time 2019-02-25 13:05:28.452435
last batch sz 160
Pre: time 2019-02-25 13:07:09.782347: 
 	std: 0.05070058
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5085833, 0.61228335, 0.6123667, 0.61225, 0.5090333]
	train_accs: [0.5085833, 0.61228335, 0.6123667, 0.61225, 0.5090333]
	best_train_sub_head: 2
	worst: 0.5085833
	avg: 0.5709033
	best: 0.6123667

Starting e_i: 705
Model ind 640 epoch 705 head A head_i_epoch 0 batch 0: avg loss -3.176145 avg loss no lamb -3.176145 time 2019-02-25 13:07:12.566294
Model ind 640 epoch 705 head A head_i_epoch 0 batch 100: avg loss -3.016953 avg loss no lamb -3.016953 time 2019-02-25 13:09:02.526934
Model ind 640 epoch 705 head A head_i_epoch 0 batch 200: avg loss -3.254088 avg loss no lamb -3.254088 time 2019-02-25 13:10:51.761489
last batch sz 160
Model ind 640 epoch 705 head B head_i_epoch 0 batch 0: avg loss -1.789372 avg loss no lamb -1.789372 time 2019-02-25 13:12:12.399092
Model ind 640 epoch 705 head B head_i_epoch 0 batch 100: avg loss -1.680514 avg loss no lamb -1.680514 time 2019-02-25 13:14:02.531234
Model ind 640 epoch 705 head B head_i_epoch 0 batch 200: avg loss -1.881169 avg loss no lamb -1.881169 time 2019-02-25 13:15:52.304114
last batch sz 160
Model ind 640 epoch 705 head B head_i_epoch 1 batch 0: avg loss -1.819170 avg loss no lamb -1.819170 time 2019-02-25 13:17:12.608895
Model ind 640 epoch 705 head B head_i_epoch 1 batch 100: avg loss -1.718130 avg loss no lamb -1.718130 time 2019-02-25 13:19:02.918552
Model ind 640 epoch 705 head B head_i_epoch 1 batch 200: avg loss -1.841314 avg loss no lamb -1.841314 time 2019-02-25 13:20:53.165497
last batch sz 160
Pre: time 2019-02-25 13:22:33.653588: 
 	std: 0.051218204
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50701666, 0.61225, 0.6124833, 0.61205, 0.50841665]
	train_accs: [0.50701666, 0.61225, 0.6124833, 0.61205, 0.50841665]
	best_train_sub_head: 2
	worst: 0.50701666
	avg: 0.57044333
	best: 0.6124833

Starting e_i: 706
Model ind 640 epoch 706 head A head_i_epoch 0 batch 0: avg loss -3.127238 avg loss no lamb -3.127238 time 2019-02-25 13:22:35.983602
Model ind 640 epoch 706 head A head_i_epoch 0 batch 100: avg loss -3.189403 avg loss no lamb -3.189403 time 2019-02-25 13:24:26.452170
Model ind 640 epoch 706 head A head_i_epoch 0 batch 200: avg loss -3.106779 avg loss no lamb -3.106779 time 2019-02-25 13:26:16.441251
last batch sz 160
Model ind 640 epoch 706 head B head_i_epoch 0 batch 0: avg loss -1.811672 avg loss no lamb -1.811672 time 2019-02-25 13:27:35.509840
Model ind 640 epoch 706 head B head_i_epoch 0 batch 100: avg loss -1.777425 avg loss no lamb -1.777425 time 2019-02-25 13:29:27.627459
Model ind 640 epoch 706 head B head_i_epoch 0 batch 200: avg loss -1.853060 avg loss no lamb -1.853060 time 2019-02-25 13:31:18.969925
last batch sz 160
Model ind 640 epoch 706 head B head_i_epoch 1 batch 0: avg loss -1.834539 avg loss no lamb -1.834539 time 2019-02-25 13:32:39.640926
Model ind 640 epoch 706 head B head_i_epoch 1 batch 100: avg loss -1.761615 avg loss no lamb -1.761615 time 2019-02-25 13:34:29.924785
Model ind 640 epoch 706 head B head_i_epoch 1 batch 200: avg loss -1.854636 avg loss no lamb -1.854636 time 2019-02-25 13:36:19.443562
last batch sz 160
Pre: time 2019-02-25 13:38:00.272839: 
 	std: 0.050648686
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50818336, 0.6120667, 0.6123, 0.6120833, 0.50935]
	train_accs: [0.50818336, 0.6120667, 0.6123, 0.6120833, 0.50935]
	best_train_sub_head: 2
	worst: 0.50818336
	avg: 0.5707966
	best: 0.6123

Starting e_i: 707
Model ind 640 epoch 707 head A head_i_epoch 0 batch 0: avg loss -3.095756 avg loss no lamb -3.095756 time 2019-02-25 13:38:03.201218
Model ind 640 epoch 707 head A head_i_epoch 0 batch 100: avg loss -3.061644 avg loss no lamb -3.061644 time 2019-02-25 13:39:53.629837
Model ind 640 epoch 707 head A head_i_epoch 0 batch 200: avg loss -3.101895 avg loss no lamb -3.101895 time 2019-02-25 13:41:44.237938
last batch sz 160
Model ind 640 epoch 707 head B head_i_epoch 0 batch 0: avg loss -1.888865 avg loss no lamb -1.888865 time 2019-02-25 13:43:04.262883
Model ind 640 epoch 707 head B head_i_epoch 0 batch 100: avg loss -1.770023 avg loss no lamb -1.770023 time 2019-02-25 13:44:54.363681
Model ind 640 epoch 707 head B head_i_epoch 0 batch 200: avg loss -1.892109 avg loss no lamb -1.892109 time 2019-02-25 13:46:44.637726
last batch sz 160
Model ind 640 epoch 707 head B head_i_epoch 1 batch 0: avg loss -1.814509 avg loss no lamb -1.814509 time 2019-02-25 13:48:04.968173
Model ind 640 epoch 707 head B head_i_epoch 1 batch 100: avg loss -1.692480 avg loss no lamb -1.692480 time 2019-02-25 13:49:55.376954
Model ind 640 epoch 707 head B head_i_epoch 1 batch 200: avg loss -1.898497 avg loss no lamb -1.898497 time 2019-02-25 13:51:45.473971
last batch sz 160
Pre: time 2019-02-25 13:53:26.082689: 
 	std: 0.05076739
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50813335, 0.61193335, 0.6122, 0.6119, 0.5086333]
	train_accs: [0.50813335, 0.61193335, 0.6122, 0.6119, 0.5086333]
	best_train_sub_head: 2
	worst: 0.50813335
	avg: 0.57056004
	best: 0.6122

Starting e_i: 708
Model ind 640 epoch 708 head A head_i_epoch 0 batch 0: avg loss -3.078426 avg loss no lamb -3.078426 time 2019-02-25 13:53:28.503037
Model ind 640 epoch 708 head A head_i_epoch 0 batch 100: avg loss -3.113667 avg loss no lamb -3.113667 time 2019-02-25 13:55:18.314845
Model ind 640 epoch 708 head A head_i_epoch 0 batch 200: avg loss -3.157200 avg loss no lamb -3.157200 time 2019-02-25 13:57:08.899630
last batch sz 160
Model ind 640 epoch 708 head B head_i_epoch 0 batch 0: avg loss -1.791581 avg loss no lamb -1.791581 time 2019-02-25 13:58:28.900152
Model ind 640 epoch 708 head B head_i_epoch 0 batch 100: avg loss -1.740013 avg loss no lamb -1.740013 time 2019-02-25 14:00:19.191016
Model ind 640 epoch 708 head B head_i_epoch 0 batch 200: avg loss -1.918065 avg loss no lamb -1.918065 time 2019-02-25 14:02:09.975357
last batch sz 160
Model ind 640 epoch 708 head B head_i_epoch 1 batch 0: avg loss -1.872917 avg loss no lamb -1.872917 time 2019-02-25 14:03:29.711482
Model ind 640 epoch 708 head B head_i_epoch 1 batch 100: avg loss -1.746287 avg loss no lamb -1.746287 time 2019-02-25 14:05:19.364854
Model ind 640 epoch 708 head B head_i_epoch 1 batch 200: avg loss -1.864304 avg loss no lamb -1.864304 time 2019-02-25 14:07:09.638919
last batch sz 160
Pre: time 2019-02-25 14:08:50.449032: 
 	std: 0.050045043
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101, 0.61261666, 0.61275, 0.61251664, 0.51085]
	train_accs: [0.5101, 0.61261666, 0.61275, 0.61251664, 0.51085]
	best_train_sub_head: 2
	worst: 0.5101
	avg: 0.5717667
	best: 0.61275

Starting e_i: 709
Model ind 640 epoch 709 head A head_i_epoch 0 batch 0: avg loss -3.164082 avg loss no lamb -3.164082 time 2019-02-25 14:08:53.314029
Model ind 640 epoch 709 head A head_i_epoch 0 batch 100: avg loss -3.124473 avg loss no lamb -3.124473 time 2019-02-25 14:10:43.382292
Model ind 640 epoch 709 head A head_i_epoch 0 batch 200: avg loss -3.146898 avg loss no lamb -3.146898 time 2019-02-25 14:12:32.622512
last batch sz 160
Model ind 640 epoch 709 head B head_i_epoch 0 batch 0: avg loss -1.844872 avg loss no lamb -1.844872 time 2019-02-25 14:13:52.023922
Model ind 640 epoch 709 head B head_i_epoch 0 batch 100: avg loss -1.711737 avg loss no lamb -1.711737 time 2019-02-25 14:15:42.447818
Model ind 640 epoch 709 head B head_i_epoch 0 batch 200: avg loss -1.827867 avg loss no lamb -1.827867 time 2019-02-25 14:17:32.817795
last batch sz 160
Model ind 640 epoch 709 head B head_i_epoch 1 batch 0: avg loss -1.778181 avg loss no lamb -1.778181 time 2019-02-25 14:18:53.413858
Model ind 640 epoch 709 head B head_i_epoch 1 batch 100: avg loss -1.704940 avg loss no lamb -1.704940 time 2019-02-25 14:20:42.750308
Model ind 640 epoch 709 head B head_i_epoch 1 batch 200: avg loss -1.897124 avg loss no lamb -1.897124 time 2019-02-25 14:22:31.680973
last batch sz 160
Pre: time 2019-02-25 14:24:11.885939: 
 	std: 0.050669473
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50951666, 0.6131, 0.61333334, 0.6133, 0.5101167]
	train_accs: [0.50951666, 0.6131, 0.61333334, 0.6133, 0.5101167]
	best_train_sub_head: 2
	worst: 0.50951666
	avg: 0.57187337
	best: 0.61333334

Starting e_i: 710
Model ind 640 epoch 710 head A head_i_epoch 0 batch 0: avg loss -3.099300 avg loss no lamb -3.099300 time 2019-02-25 14:24:14.581135
Model ind 640 epoch 710 head A head_i_epoch 0 batch 100: avg loss -3.018523 avg loss no lamb -3.018523 time 2019-02-25 14:26:05.564500
Model ind 640 epoch 710 head A head_i_epoch 0 batch 200: avg loss -3.193517 avg loss no lamb -3.193517 time 2019-02-25 14:27:55.895052
last batch sz 160
Model ind 640 epoch 710 head B head_i_epoch 0 batch 0: avg loss -1.897690 avg loss no lamb -1.897690 time 2019-02-25 14:29:17.157182
Model ind 640 epoch 710 head B head_i_epoch 0 batch 100: avg loss -1.734684 avg loss no lamb -1.734684 time 2019-02-25 14:31:07.355086
Model ind 640 epoch 710 head B head_i_epoch 0 batch 200: avg loss -1.920311 avg loss no lamb -1.920311 time 2019-02-25 14:32:57.438135
last batch sz 160
Model ind 640 epoch 710 head B head_i_epoch 1 batch 0: avg loss -1.828667 avg loss no lamb -1.828667 time 2019-02-25 14:34:17.491490
Model ind 640 epoch 710 head B head_i_epoch 1 batch 100: avg loss -1.855038 avg loss no lamb -1.855038 time 2019-02-25 14:36:07.247983
Model ind 640 epoch 710 head B head_i_epoch 1 batch 200: avg loss -1.926569 avg loss no lamb -1.926569 time 2019-02-25 14:37:57.153733
last batch sz 160
Pre: time 2019-02-25 14:39:38.168494: 
 	std: 0.050902043
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50698334, 0.6110333, 0.6112667, 0.6110833, 0.5074667]
	train_accs: [0.50698334, 0.6110333, 0.6112667, 0.6110833, 0.5074667]
	best_train_sub_head: 2
	worst: 0.50698334
	avg: 0.5695666
	best: 0.6112667

Starting e_i: 711
Model ind 640 epoch 711 head A head_i_epoch 0 batch 0: avg loss -3.137768 avg loss no lamb -3.137768 time 2019-02-25 14:39:44.755806
Model ind 640 epoch 711 head A head_i_epoch 0 batch 100: avg loss -3.072680 avg loss no lamb -3.072680 time 2019-02-25 14:41:34.571483
Model ind 640 epoch 711 head A head_i_epoch 0 batch 200: avg loss -3.129820 avg loss no lamb -3.129820 time 2019-02-25 14:43:25.004124
last batch sz 160
Model ind 640 epoch 711 head B head_i_epoch 0 batch 0: avg loss -1.729066 avg loss no lamb -1.729066 time 2019-02-25 14:44:44.982896
Model ind 640 epoch 711 head B head_i_epoch 0 batch 100: avg loss -1.742350 avg loss no lamb -1.742350 time 2019-02-25 14:46:34.891270
Model ind 640 epoch 711 head B head_i_epoch 0 batch 200: avg loss -1.795192 avg loss no lamb -1.795192 time 2019-02-25 14:48:26.779434
last batch sz 160
Model ind 640 epoch 711 head B head_i_epoch 1 batch 0: avg loss -1.798325 avg loss no lamb -1.798325 time 2019-02-25 14:49:47.813429
Model ind 640 epoch 711 head B head_i_epoch 1 batch 100: avg loss -1.747656 avg loss no lamb -1.747656 time 2019-02-25 14:51:38.288854
Model ind 640 epoch 711 head B head_i_epoch 1 batch 200: avg loss -1.887018 avg loss no lamb -1.887018 time 2019-02-25 14:53:28.444430
last batch sz 160
Pre: time 2019-02-25 14:55:09.449754: 
 	std: 0.0506048
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51025, 0.61378336, 0.61403334, 0.6142167, 0.5111833]
	train_accs: [0.51025, 0.61378336, 0.61403334, 0.6142167, 0.5111833]
	best_train_sub_head: 3
	worst: 0.51025
	avg: 0.57269335
	best: 0.6142167

Starting e_i: 712
Model ind 640 epoch 712 head A head_i_epoch 0 batch 0: avg loss -3.115407 avg loss no lamb -3.115407 time 2019-02-25 14:55:11.904630
Model ind 640 epoch 712 head A head_i_epoch 0 batch 100: avg loss -3.091737 avg loss no lamb -3.091737 time 2019-02-25 14:57:02.374105
Model ind 640 epoch 712 head A head_i_epoch 0 batch 200: avg loss -3.072520 avg loss no lamb -3.072520 time 2019-02-25 14:58:52.137208
last batch sz 160
Model ind 640 epoch 712 head B head_i_epoch 0 batch 0: avg loss -1.781105 avg loss no lamb -1.781105 time 2019-02-25 15:00:12.137498
Model ind 640 epoch 712 head B head_i_epoch 0 batch 100: avg loss -1.734174 avg loss no lamb -1.734174 time 2019-02-25 15:02:01.723154
Model ind 640 epoch 712 head B head_i_epoch 0 batch 200: avg loss -1.779620 avg loss no lamb -1.779620 time 2019-02-25 15:03:51.589772
last batch sz 160
Model ind 640 epoch 712 head B head_i_epoch 1 batch 0: avg loss -1.798027 avg loss no lamb -1.798027 time 2019-02-25 15:05:11.657811
Model ind 640 epoch 712 head B head_i_epoch 1 batch 100: avg loss -1.714844 avg loss no lamb -1.714844 time 2019-02-25 15:07:01.588046
Model ind 640 epoch 712 head B head_i_epoch 1 batch 200: avg loss -1.818065 avg loss no lamb -1.818065 time 2019-02-25 15:08:51.966096
last batch sz 160
Pre: time 2019-02-25 15:10:33.094983: 
 	std: 0.051393285
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5071333, 0.6120167, 0.61233336, 0.61231667, 0.5075]
	train_accs: [0.5071333, 0.6120167, 0.61233336, 0.61231667, 0.5075]
	best_train_sub_head: 2
	worst: 0.5071333
	avg: 0.5702599
	best: 0.61233336

Starting e_i: 713
Model ind 640 epoch 713 head A head_i_epoch 0 batch 0: avg loss -3.114890 avg loss no lamb -3.114890 time 2019-02-25 15:10:35.392441
Model ind 640 epoch 713 head A head_i_epoch 0 batch 100: avg loss -3.001895 avg loss no lamb -3.001895 time 2019-02-25 15:12:26.346812
Model ind 640 epoch 713 head A head_i_epoch 0 batch 200: avg loss -3.170250 avg loss no lamb -3.170250 time 2019-02-25 15:14:16.656026
last batch sz 160
Model ind 640 epoch 713 head B head_i_epoch 0 batch 0: avg loss -1.792689 avg loss no lamb -1.792689 time 2019-02-25 15:15:36.411368
Model ind 640 epoch 713 head B head_i_epoch 0 batch 100: avg loss -1.784904 avg loss no lamb -1.784904 time 2019-02-25 15:17:26.219204
Model ind 640 epoch 713 head B head_i_epoch 0 batch 200: avg loss -1.868415 avg loss no lamb -1.868415 time 2019-02-25 15:19:16.484260
last batch sz 160
Model ind 640 epoch 713 head B head_i_epoch 1 batch 0: avg loss -1.813354 avg loss no lamb -1.813354 time 2019-02-25 15:20:36.665757
Model ind 640 epoch 713 head B head_i_epoch 1 batch 100: avg loss -1.747916 avg loss no lamb -1.747916 time 2019-02-25 15:22:27.224713
Model ind 640 epoch 713 head B head_i_epoch 1 batch 200: avg loss -1.851999 avg loss no lamb -1.851999 time 2019-02-25 15:24:17.106605
last batch sz 160
Pre: time 2019-02-25 15:25:57.536505: 
 	std: 0.051143028
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5086333, 0.61336666, 0.6133, 0.61336666, 0.5092667]
	train_accs: [0.5086333, 0.61336666, 0.6133, 0.61336666, 0.5092667]
	best_train_sub_head: 1
	worst: 0.5086333
	avg: 0.57158667
	best: 0.61336666

Starting e_i: 714
Model ind 640 epoch 714 head A head_i_epoch 0 batch 0: avg loss -3.167110 avg loss no lamb -3.167110 time 2019-02-25 15:25:59.997365
Model ind 640 epoch 714 head A head_i_epoch 0 batch 100: avg loss -3.102704 avg loss no lamb -3.102704 time 2019-02-25 15:27:52.550230
Model ind 640 epoch 714 head A head_i_epoch 0 batch 200: avg loss -3.153266 avg loss no lamb -3.153266 time 2019-02-25 15:29:43.260816
last batch sz 160
Model ind 640 epoch 714 head B head_i_epoch 0 batch 0: avg loss -1.860915 avg loss no lamb -1.860915 time 2019-02-25 15:31:03.615656
Model ind 640 epoch 714 head B head_i_epoch 0 batch 100: avg loss -1.698675 avg loss no lamb -1.698675 time 2019-02-25 15:32:53.582823
Model ind 640 epoch 714 head B head_i_epoch 0 batch 200: avg loss -1.868671 avg loss no lamb -1.868671 time 2019-02-25 15:34:43.514979
last batch sz 160
Model ind 640 epoch 714 head B head_i_epoch 1 batch 0: avg loss -1.808764 avg loss no lamb -1.808764 time 2019-02-25 15:36:04.136879
Model ind 640 epoch 714 head B head_i_epoch 1 batch 100: avg loss -1.655224 avg loss no lamb -1.655224 time 2019-02-25 15:37:54.131944
Model ind 640 epoch 714 head B head_i_epoch 1 batch 200: avg loss -1.842884 avg loss no lamb -1.842884 time 2019-02-25 15:39:44.589235
last batch sz 160
Pre: time 2019-02-25 15:41:25.968598: 
 	std: 0.05166255
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50656664, 0.6120667, 0.61225, 0.61205, 0.5067667]
	train_accs: [0.50656664, 0.6120667, 0.61225, 0.61205, 0.5067667]
	best_train_sub_head: 2
	worst: 0.50656664
	avg: 0.56994
	best: 0.61225

Starting e_i: 715
Model ind 640 epoch 715 head A head_i_epoch 0 batch 0: avg loss -3.130805 avg loss no lamb -3.130805 time 2019-02-25 15:41:28.429437
Model ind 640 epoch 715 head A head_i_epoch 0 batch 100: avg loss -3.087488 avg loss no lamb -3.087488 time 2019-02-25 15:43:18.909450
Model ind 640 epoch 715 head A head_i_epoch 0 batch 200: avg loss -3.165616 avg loss no lamb -3.165616 time 2019-02-25 15:45:09.481470
last batch sz 160
Model ind 640 epoch 715 head B head_i_epoch 0 batch 0: avg loss -1.827653 avg loss no lamb -1.827653 time 2019-02-25 15:46:29.233520
Model ind 640 epoch 715 head B head_i_epoch 0 batch 100: avg loss -1.730892 avg loss no lamb -1.730892 time 2019-02-25 15:48:19.372384
Model ind 640 epoch 715 head B head_i_epoch 0 batch 200: avg loss -1.817236 avg loss no lamb -1.817236 time 2019-02-25 15:50:09.561610
last batch sz 160
Model ind 640 epoch 715 head B head_i_epoch 1 batch 0: avg loss -1.886065 avg loss no lamb -1.886065 time 2019-02-25 15:51:29.454410
Model ind 640 epoch 715 head B head_i_epoch 1 batch 100: avg loss -1.798604 avg loss no lamb -1.798604 time 2019-02-25 15:53:19.459797
Model ind 640 epoch 715 head B head_i_epoch 1 batch 200: avg loss -1.881979 avg loss no lamb -1.881979 time 2019-02-25 15:55:09.421131
last batch sz 160
Pre: time 2019-02-25 15:56:50.669458: 
 	std: 0.05078735
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50998336, 0.6144, 0.6141833, 0.6138667, 0.51098335]
	train_accs: [0.50998336, 0.6144, 0.6141833, 0.6138667, 0.51098335]
	best_train_sub_head: 1
	worst: 0.50998336
	avg: 0.57268333
	best: 0.6144

Starting e_i: 716
Model ind 640 epoch 716 head A head_i_epoch 0 batch 0: avg loss -3.114933 avg loss no lamb -3.114933 time 2019-02-25 15:56:53.631057
Model ind 640 epoch 716 head A head_i_epoch 0 batch 100: avg loss -3.058222 avg loss no lamb -3.058222 time 2019-02-25 15:58:43.685926
Model ind 640 epoch 716 head A head_i_epoch 0 batch 200: avg loss -3.154320 avg loss no lamb -3.154320 time 2019-02-25 16:00:34.594271
last batch sz 160
Model ind 640 epoch 716 head B head_i_epoch 0 batch 0: avg loss -1.843385 avg loss no lamb -1.843385 time 2019-02-25 16:01:54.670080
Model ind 640 epoch 716 head B head_i_epoch 0 batch 100: avg loss -1.723029 avg loss no lamb -1.723029 time 2019-02-25 16:03:44.074156
Model ind 640 epoch 716 head B head_i_epoch 0 batch 200: avg loss -1.913448 avg loss no lamb -1.913448 time 2019-02-25 16:05:35.663533
last batch sz 160
Model ind 640 epoch 716 head B head_i_epoch 1 batch 0: avg loss -1.843334 avg loss no lamb -1.843334 time 2019-02-25 16:06:56.869433
Model ind 640 epoch 716 head B head_i_epoch 1 batch 100: avg loss -1.734773 avg loss no lamb -1.734773 time 2019-02-25 16:08:47.583544
Model ind 640 epoch 716 head B head_i_epoch 1 batch 200: avg loss -1.863114 avg loss no lamb -1.863114 time 2019-02-25 16:10:38.436278
last batch sz 160
Pre: time 2019-02-25 16:12:19.423772: 
 	std: 0.05101748
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50918335, 0.61328334, 0.6134833, 0.6133, 0.50925]
	train_accs: [0.50918335, 0.61328334, 0.6134833, 0.6133, 0.50925]
	best_train_sub_head: 2
	worst: 0.50918335
	avg: 0.5717
	best: 0.6134833

Starting e_i: 717
Model ind 640 epoch 717 head A head_i_epoch 0 batch 0: avg loss -3.103855 avg loss no lamb -3.103855 time 2019-02-25 16:12:21.973391
Model ind 640 epoch 717 head A head_i_epoch 0 batch 100: avg loss -3.073167 avg loss no lamb -3.073167 time 2019-02-25 16:14:12.724343
Model ind 640 epoch 717 head A head_i_epoch 0 batch 200: avg loss -3.120422 avg loss no lamb -3.120422 time 2019-02-25 16:16:03.481082
last batch sz 160
Model ind 640 epoch 717 head B head_i_epoch 0 batch 0: avg loss -1.797550 avg loss no lamb -1.797550 time 2019-02-25 16:17:23.825181
Model ind 640 epoch 717 head B head_i_epoch 0 batch 100: avg loss -1.770756 avg loss no lamb -1.770756 time 2019-02-25 16:19:14.219155
Model ind 640 epoch 717 head B head_i_epoch 0 batch 200: avg loss -1.905325 avg loss no lamb -1.905325 time 2019-02-25 16:21:05.294753
last batch sz 160
Model ind 640 epoch 717 head B head_i_epoch 1 batch 0: avg loss -1.771302 avg loss no lamb -1.771302 time 2019-02-25 16:22:26.364296
Model ind 640 epoch 717 head B head_i_epoch 1 batch 100: avg loss -1.703020 avg loss no lamb -1.703020 time 2019-02-25 16:24:17.863090
Model ind 640 epoch 717 head B head_i_epoch 1 batch 200: avg loss -1.928949 avg loss no lamb -1.928949 time 2019-02-25 16:26:07.338127
last batch sz 160
Pre: time 2019-02-25 16:27:50.274459: 
 	std: 0.050542947
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50953335, 0.6131667, 0.6127833, 0.61298335, 0.5100833]
	train_accs: [0.50953335, 0.6131667, 0.6127833, 0.61298335, 0.5100833]
	best_train_sub_head: 1
	worst: 0.50953335
	avg: 0.57171
	best: 0.6131667

Starting e_i: 718
Model ind 640 epoch 718 head A head_i_epoch 0 batch 0: avg loss -3.198929 avg loss no lamb -3.198929 time 2019-02-25 16:27:53.000103
Model ind 640 epoch 718 head A head_i_epoch 0 batch 100: avg loss -3.132045 avg loss no lamb -3.132045 time 2019-02-25 16:29:43.175461
Model ind 640 epoch 718 head A head_i_epoch 0 batch 200: avg loss -3.083817 avg loss no lamb -3.083817 time 2019-02-25 16:31:32.947652
last batch sz 160
Model ind 640 epoch 718 head B head_i_epoch 0 batch 0: avg loss -1.807850 avg loss no lamb -1.807850 time 2019-02-25 16:32:52.894571
Model ind 640 epoch 718 head B head_i_epoch 0 batch 100: avg loss -1.700743 avg loss no lamb -1.700743 time 2019-02-25 16:34:43.591348
Model ind 640 epoch 718 head B head_i_epoch 0 batch 200: avg loss -1.868591 avg loss no lamb -1.868591 time 2019-02-25 16:36:34.114556
last batch sz 160
Model ind 640 epoch 718 head B head_i_epoch 1 batch 0: avg loss -1.761651 avg loss no lamb -1.761651 time 2019-02-25 16:37:57.225739
Model ind 640 epoch 718 head B head_i_epoch 1 batch 100: avg loss -1.678278 avg loss no lamb -1.678278 time 2019-02-25 16:39:51.285490
Model ind 640 epoch 718 head B head_i_epoch 1 batch 200: avg loss -1.805374 avg loss no lamb -1.805374 time 2019-02-25 16:41:42.456377
last batch sz 160
Pre: time 2019-02-25 16:43:29.286928: 
 	std: 0.049889337
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51075, 0.61255, 0.6127167, 0.61256665, 0.5108]
	train_accs: [0.51075, 0.61255, 0.6127167, 0.61256665, 0.5108]
	best_train_sub_head: 2
	worst: 0.51075
	avg: 0.5718767
	best: 0.6127167

Starting e_i: 719
Model ind 640 epoch 719 head A head_i_epoch 0 batch 0: avg loss -3.194134 avg loss no lamb -3.194134 time 2019-02-25 16:43:32.019149
Model ind 640 epoch 719 head A head_i_epoch 0 batch 100: avg loss -3.061692 avg loss no lamb -3.061692 time 2019-02-25 16:45:26.295774
Model ind 640 epoch 719 head A head_i_epoch 0 batch 200: avg loss -3.105605 avg loss no lamb -3.105605 time 2019-02-25 16:47:17.715408
last batch sz 160
Model ind 640 epoch 719 head B head_i_epoch 0 batch 0: avg loss -1.761076 avg loss no lamb -1.761076 time 2019-02-25 16:48:44.182856
Model ind 640 epoch 719 head B head_i_epoch 0 batch 100: avg loss -1.783587 avg loss no lamb -1.783587 time 2019-02-25 16:50:37.402775
Model ind 640 epoch 719 head B head_i_epoch 0 batch 200: avg loss -1.885230 avg loss no lamb -1.885230 time 2019-02-25 16:52:29.201435
last batch sz 160
Model ind 640 epoch 719 head B head_i_epoch 1 batch 0: avg loss -1.849667 avg loss no lamb -1.849667 time 2019-02-25 16:53:55.303569
Model ind 640 epoch 719 head B head_i_epoch 1 batch 100: avg loss -1.764268 avg loss no lamb -1.764268 time 2019-02-25 16:55:47.233729
Model ind 640 epoch 719 head B head_i_epoch 1 batch 200: avg loss -1.898638 avg loss no lamb -1.898638 time 2019-02-25 16:57:42.964053
last batch sz 160
Pre: time 2019-02-25 16:59:27.106309: 
 	std: 0.050492186
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50808334, 0.611, 0.6111, 0.6111, 0.5079167]
	train_accs: [0.50808334, 0.611, 0.6111, 0.6111, 0.5079167]
	best_train_sub_head: 2
	worst: 0.5079167
	avg: 0.56984
	best: 0.6111

Starting e_i: 720
Model ind 640 epoch 720 head A head_i_epoch 0 batch 0: avg loss -3.119627 avg loss no lamb -3.119627 time 2019-02-25 16:59:30.104824
Model ind 640 epoch 720 head A head_i_epoch 0 batch 100: avg loss -3.118265 avg loss no lamb -3.118265 time 2019-02-25 17:01:20.731561
Model ind 640 epoch 720 head A head_i_epoch 0 batch 200: avg loss -3.221779 avg loss no lamb -3.221779 time 2019-02-25 17:03:17.290829
last batch sz 160
Model ind 640 epoch 720 head B head_i_epoch 0 batch 0: avg loss -1.788250 avg loss no lamb -1.788250 time 2019-02-25 17:04:38.753071
Model ind 640 epoch 720 head B head_i_epoch 0 batch 100: avg loss -1.766636 avg loss no lamb -1.766636 time 2019-02-25 17:06:29.553525
Model ind 640 epoch 720 head B head_i_epoch 0 batch 200: avg loss -1.875360 avg loss no lamb -1.875360 time 2019-02-25 17:08:27.123315
last batch sz 160
Model ind 640 epoch 720 head B head_i_epoch 1 batch 0: avg loss -1.823189 avg loss no lamb -1.823189 time 2019-02-25 17:09:48.365976
Model ind 640 epoch 720 head B head_i_epoch 1 batch 100: avg loss -1.676742 avg loss no lamb -1.676742 time 2019-02-25 17:11:39.845613
Model ind 640 epoch 720 head B head_i_epoch 1 batch 200: avg loss -1.875595 avg loss no lamb -1.875595 time 2019-02-25 17:13:35.309144
last batch sz 160
Pre: time 2019-02-25 17:15:16.974223: 
 	std: 0.05034266
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51195, 0.61478335, 0.6145333, 0.6149667, 0.51205]
	train_accs: [0.51195, 0.61478335, 0.6145333, 0.6149667, 0.51205]
	best_train_sub_head: 3
	worst: 0.51195
	avg: 0.5736567
	best: 0.6149667

Starting e_i: 721
Model ind 640 epoch 721 head A head_i_epoch 0 batch 0: avg loss -3.141470 avg loss no lamb -3.141470 time 2019-02-25 17:15:23.414995
Model ind 640 epoch 721 head A head_i_epoch 0 batch 100: avg loss -3.090219 avg loss no lamb -3.090219 time 2019-02-25 17:17:21.788532
Model ind 640 epoch 721 head A head_i_epoch 0 batch 200: avg loss -3.210495 avg loss no lamb -3.210495 time 2019-02-25 17:19:15.968136
last batch sz 160
Model ind 640 epoch 721 head B head_i_epoch 0 batch 0: avg loss -1.796307 avg loss no lamb -1.796307 time 2019-02-25 17:20:36.737622
Model ind 640 epoch 721 head B head_i_epoch 0 batch 100: avg loss -1.763383 avg loss no lamb -1.763383 time 2019-02-25 17:22:35.861164
Model ind 640 epoch 721 head B head_i_epoch 0 batch 200: avg loss -1.823604 avg loss no lamb -1.823604 time 2019-02-25 17:24:29.527295
last batch sz 160
Model ind 640 epoch 721 head B head_i_epoch 1 batch 0: avg loss -1.840772 avg loss no lamb -1.840772 time 2019-02-25 17:25:50.920097
Model ind 640 epoch 721 head B head_i_epoch 1 batch 100: avg loss -1.780002 avg loss no lamb -1.780002 time 2019-02-25 17:27:50.532223
Model ind 640 epoch 721 head B head_i_epoch 1 batch 200: avg loss -1.857611 avg loss no lamb -1.857611 time 2019-02-25 17:29:41.308845
last batch sz 160
Pre: time 2019-02-25 17:31:29.259256: 
 	std: 0.050750565
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5096667, 0.6135833, 0.61395, 0.6138167, 0.5107167]
	train_accs: [0.5096667, 0.6135833, 0.61395, 0.6138167, 0.5107167]
	best_train_sub_head: 2
	worst: 0.5096667
	avg: 0.5723467
	best: 0.61395

Starting e_i: 722
Model ind 640 epoch 722 head A head_i_epoch 0 batch 0: avg loss -3.207743 avg loss no lamb -3.207743 time 2019-02-25 17:31:32.337930
Model ind 640 epoch 722 head A head_i_epoch 0 batch 100: avg loss -3.157182 avg loss no lamb -3.157182 time 2019-02-25 17:33:25.759434
Model ind 640 epoch 722 head A head_i_epoch 0 batch 200: avg loss -3.166543 avg loss no lamb -3.166543 time 2019-02-25 17:35:20.847625
last batch sz 160
Model ind 640 epoch 722 head B head_i_epoch 0 batch 0: avg loss -1.774029 avg loss no lamb -1.774029 time 2019-02-25 17:36:44.589773
Model ind 640 epoch 722 head B head_i_epoch 0 batch 100: avg loss -1.693555 avg loss no lamb -1.693555 time 2019-02-25 17:38:35.798080
Model ind 640 epoch 722 head B head_i_epoch 0 batch 200: avg loss -1.904776 avg loss no lamb -1.904776 time 2019-02-25 17:40:32.354064
last batch sz 160
Model ind 640 epoch 722 head B head_i_epoch 1 batch 0: avg loss -1.794353 avg loss no lamb -1.794353 time 2019-02-25 17:41:54.818411
Model ind 640 epoch 722 head B head_i_epoch 1 batch 100: avg loss -1.783217 avg loss no lamb -1.783217 time 2019-02-25 17:43:45.554509
Model ind 640 epoch 722 head B head_i_epoch 1 batch 200: avg loss -1.871611 avg loss no lamb -1.871611 time 2019-02-25 17:45:42.814210
last batch sz 160
Pre: time 2019-02-25 17:47:26.307189: 
 	std: 0.05084902
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5089333, 0.6135, 0.61336666, 0.6132333, 0.51021665]
	train_accs: [0.5089333, 0.6135, 0.61336666, 0.6132333, 0.51021665]
	best_train_sub_head: 1
	worst: 0.5089333
	avg: 0.57185
	best: 0.6135

Starting e_i: 723
Model ind 640 epoch 723 head A head_i_epoch 0 batch 0: avg loss -3.182854 avg loss no lamb -3.182854 time 2019-02-25 17:47:28.724128
Model ind 640 epoch 723 head A head_i_epoch 0 batch 100: avg loss -3.059678 avg loss no lamb -3.059678 time 2019-02-25 17:49:22.302874
Model ind 640 epoch 723 head A head_i_epoch 0 batch 200: avg loss -3.171531 avg loss no lamb -3.171531 time 2019-02-25 17:51:19.861396
last batch sz 160
Model ind 640 epoch 723 head B head_i_epoch 0 batch 0: avg loss -1.843148 avg loss no lamb -1.843148 time 2019-02-25 17:52:41.151526
Model ind 640 epoch 723 head B head_i_epoch 0 batch 100: avg loss -1.704707 avg loss no lamb -1.704707 time 2019-02-25 17:54:37.530900
Model ind 640 epoch 723 head B head_i_epoch 0 batch 200: avg loss -1.885885 avg loss no lamb -1.885885 time 2019-02-25 17:56:31.244932
last batch sz 160
Model ind 640 epoch 723 head B head_i_epoch 1 batch 0: avg loss -1.776496 avg loss no lamb -1.776496 time 2019-02-25 17:57:52.647231
Model ind 640 epoch 723 head B head_i_epoch 1 batch 100: avg loss -1.762170 avg loss no lamb -1.762170 time 2019-02-25 17:59:48.857311
Model ind 640 epoch 723 head B head_i_epoch 1 batch 200: avg loss -1.907735 avg loss no lamb -1.907735 time 2019-02-25 18:01:41.856611
last batch sz 160
Pre: time 2019-02-25 18:03:24.838898: 
 	std: 0.051014982
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50761664, 0.612, 0.61193335, 0.6120667, 0.50811666]
	train_accs: [0.50761664, 0.612, 0.61193335, 0.6120667, 0.50811666]
	best_train_sub_head: 3
	worst: 0.50761664
	avg: 0.5703467
	best: 0.6120667

Starting e_i: 724
Model ind 640 epoch 724 head A head_i_epoch 0 batch 0: avg loss -3.136137 avg loss no lamb -3.136137 time 2019-02-25 18:03:28.131271
Model ind 640 epoch 724 head A head_i_epoch 0 batch 100: avg loss -3.120723 avg loss no lamb -3.120723 time 2019-02-25 18:05:24.698348
Model ind 640 epoch 724 head A head_i_epoch 0 batch 200: avg loss -3.202723 avg loss no lamb -3.202723 time 2019-02-25 18:07:16.482214
last batch sz 160
Model ind 640 epoch 724 head B head_i_epoch 0 batch 0: avg loss -1.767048 avg loss no lamb -1.767048 time 2019-02-25 18:08:41.886056
Model ind 640 epoch 724 head B head_i_epoch 0 batch 100: avg loss -1.749408 avg loss no lamb -1.749408 time 2019-02-25 18:10:35.425454
Model ind 640 epoch 724 head B head_i_epoch 0 batch 200: avg loss -1.879713 avg loss no lamb -1.879713 time 2019-02-25 18:12:25.795391
last batch sz 160
Model ind 640 epoch 724 head B head_i_epoch 1 batch 0: avg loss -1.765084 avg loss no lamb -1.765084 time 2019-02-25 18:13:51.467529
Model ind 640 epoch 724 head B head_i_epoch 1 batch 100: avg loss -1.755963 avg loss no lamb -1.755963 time 2019-02-25 18:15:44.383029
Model ind 640 epoch 724 head B head_i_epoch 1 batch 200: avg loss -1.869690 avg loss no lamb -1.869690 time 2019-02-25 18:17:36.321463
last batch sz 160
Pre: time 2019-02-25 18:19:23.226229: 
 	std: 0.051213596
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50881666, 0.61406666, 0.6138667, 0.614, 0.5100667]
	train_accs: [0.50881666, 0.61406666, 0.6138667, 0.614, 0.5100667]
	best_train_sub_head: 1
	worst: 0.50881666
	avg: 0.57216334
	best: 0.61406666

Starting e_i: 725
Model ind 640 epoch 725 head A head_i_epoch 0 batch 0: avg loss -3.212565 avg loss no lamb -3.212565 time 2019-02-25 18:19:25.779314
Model ind 640 epoch 725 head A head_i_epoch 0 batch 100: avg loss -3.026490 avg loss no lamb -3.026490 time 2019-02-25 18:21:18.658991
Model ind 640 epoch 725 head A head_i_epoch 0 batch 200: avg loss -3.213485 avg loss no lamb -3.213485 time 2019-02-25 18:23:15.781523
last batch sz 160
Model ind 640 epoch 725 head B head_i_epoch 0 batch 0: avg loss -1.878484 avg loss no lamb -1.878484 time 2019-02-25 18:24:38.251324
Model ind 640 epoch 725 head B head_i_epoch 0 batch 100: avg loss -1.794847 avg loss no lamb -1.794847 time 2019-02-25 18:26:31.594668
Model ind 640 epoch 725 head B head_i_epoch 0 batch 200: avg loss -1.876944 avg loss no lamb -1.876944 time 2019-02-25 18:28:29.168080
last batch sz 160
Model ind 640 epoch 725 head B head_i_epoch 1 batch 0: avg loss -1.828956 avg loss no lamb -1.828956 time 2019-02-25 18:29:52.569909
Model ind 640 epoch 725 head B head_i_epoch 1 batch 100: avg loss -1.763206 avg loss no lamb -1.763206 time 2019-02-25 18:31:43.851460
Model ind 640 epoch 725 head B head_i_epoch 1 batch 200: avg loss -1.935906 avg loss no lamb -1.935906 time 2019-02-25 18:33:40.626849
last batch sz 160
Pre: time 2019-02-25 18:35:23.919505: 
 	std: 0.050432917
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50953335, 0.6127167, 0.61303335, 0.6127833, 0.51026666]
	train_accs: [0.50953335, 0.6127167, 0.61303335, 0.6127833, 0.51026666]
	best_train_sub_head: 2
	worst: 0.50953335
	avg: 0.5716667
	best: 0.61303335

Starting e_i: 726
Model ind 640 epoch 726 head A head_i_epoch 0 batch 0: avg loss -3.142095 avg loss no lamb -3.142095 time 2019-02-25 18:35:26.542784
Model ind 640 epoch 726 head A head_i_epoch 0 batch 100: avg loss -3.065102 avg loss no lamb -3.065102 time 2019-02-25 18:37:22.517204
Model ind 640 epoch 726 head A head_i_epoch 0 batch 200: avg loss -3.234755 avg loss no lamb -3.234755 time 2019-02-25 18:39:15.750087
last batch sz 160
Model ind 640 epoch 726 head B head_i_epoch 0 batch 0: avg loss -1.730368 avg loss no lamb -1.730368 time 2019-02-25 18:40:38.004085
Model ind 640 epoch 726 head B head_i_epoch 0 batch 100: avg loss -1.733540 avg loss no lamb -1.733540 time 2019-02-25 18:42:34.850771
Model ind 640 epoch 726 head B head_i_epoch 0 batch 200: avg loss -1.841537 avg loss no lamb -1.841537 time 2019-02-25 18:44:28.186559
last batch sz 160
Model ind 640 epoch 726 head B head_i_epoch 1 batch 0: avg loss -1.780242 avg loss no lamb -1.780242 time 2019-02-25 18:45:48.897765
Model ind 640 epoch 726 head B head_i_epoch 1 batch 100: avg loss -1.748687 avg loss no lamb -1.748687 time 2019-02-25 18:47:46.882927
Model ind 640 epoch 726 head B head_i_epoch 1 batch 200: avg loss -1.912291 avg loss no lamb -1.912291 time 2019-02-25 18:49:39.212352
last batch sz 160
Pre: time 2019-02-25 18:51:25.228311: 
 	std: 0.051115558
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50956666, 0.6141667, 0.61406666, 0.61403334, 0.50993335]
	train_accs: [0.50956666, 0.6141667, 0.61406666, 0.61403334, 0.50993335]
	best_train_sub_head: 1
	worst: 0.50956666
	avg: 0.57235336
	best: 0.6141667

Starting e_i: 727
Model ind 640 epoch 727 head A head_i_epoch 0 batch 0: avg loss -3.160483 avg loss no lamb -3.160483 time 2019-02-25 18:51:27.813479
Model ind 640 epoch 727 head A head_i_epoch 0 batch 100: avg loss -3.089700 avg loss no lamb -3.089700 time 2019-02-25 18:53:22.670523
Model ind 640 epoch 727 head A head_i_epoch 0 batch 200: avg loss -3.150527 avg loss no lamb -3.150527 time 2019-02-25 18:55:14.141776
last batch sz 160
Model ind 640 epoch 727 head B head_i_epoch 0 batch 0: avg loss -1.833215 avg loss no lamb -1.833215 time 2019-02-25 18:56:40.485412
Model ind 640 epoch 727 head B head_i_epoch 0 batch 100: avg loss -1.696335 avg loss no lamb -1.696335 time 2019-02-25 18:58:34.603625
Model ind 640 epoch 727 head B head_i_epoch 0 batch 200: avg loss -1.920065 avg loss no lamb -1.920065 time 2019-02-25 19:00:27.591451
last batch sz 160
Model ind 640 epoch 727 head B head_i_epoch 1 batch 0: avg loss -1.802409 avg loss no lamb -1.802409 time 2019-02-25 19:01:53.251887
Model ind 640 epoch 727 head B head_i_epoch 1 batch 100: avg loss -1.806680 avg loss no lamb -1.806680 time 2019-02-25 19:03:45.872717
Model ind 640 epoch 727 head B head_i_epoch 1 batch 200: avg loss -1.844262 avg loss no lamb -1.844262 time 2019-02-25 19:05:41.513220
last batch sz 160
Pre: time 2019-02-25 19:07:26.476451: 
 	std: 0.051178012
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50925, 0.61375, 0.61368334, 0.61366665, 0.50921667]
	train_accs: [0.50925, 0.61375, 0.61368334, 0.61366665, 0.50921667]
	best_train_sub_head: 1
	worst: 0.50921667
	avg: 0.57191336
	best: 0.61375

Starting e_i: 728
Model ind 640 epoch 728 head A head_i_epoch 0 batch 0: avg loss -3.167531 avg loss no lamb -3.167531 time 2019-02-25 19:07:28.996951
Model ind 640 epoch 728 head A head_i_epoch 0 batch 100: avg loss -3.117280 avg loss no lamb -3.117280 time 2019-02-25 19:09:20.460509
Model ind 640 epoch 728 head A head_i_epoch 0 batch 200: avg loss -3.215146 avg loss no lamb -3.215146 time 2019-02-25 19:11:17.784700
last batch sz 160
Model ind 640 epoch 728 head B head_i_epoch 0 batch 0: avg loss -1.809628 avg loss no lamb -1.809628 time 2019-02-25 19:12:40.287731
Model ind 640 epoch 728 head B head_i_epoch 0 batch 100: avg loss -1.815143 avg loss no lamb -1.815143 time 2019-02-25 19:14:31.885853
Model ind 640 epoch 728 head B head_i_epoch 0 batch 200: avg loss -1.896303 avg loss no lamb -1.896303 time 2019-02-25 19:16:29.523190
last batch sz 160
Model ind 640 epoch 728 head B head_i_epoch 1 batch 0: avg loss -1.930547 avg loss no lamb -1.930547 time 2019-02-25 19:17:51.123547
Model ind 640 epoch 728 head B head_i_epoch 1 batch 100: avg loss -1.654323 avg loss no lamb -1.654323 time 2019-02-25 19:19:48.022898
Model ind 640 epoch 728 head B head_i_epoch 1 batch 200: avg loss -1.913689 avg loss no lamb -1.913689 time 2019-02-25 19:21:42.877668
last batch sz 160
Pre: time 2019-02-25 19:23:24.089787: 
 	std: 0.050277684
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5097833, 0.6121, 0.61193335, 0.61215, 0.50908333]
	train_accs: [0.5097833, 0.6121, 0.61193335, 0.61215, 0.50908333]
	best_train_sub_head: 3
	worst: 0.50908333
	avg: 0.57101
	best: 0.61215

Starting e_i: 729
Model ind 640 epoch 729 head A head_i_epoch 0 batch 0: avg loss -3.133257 avg loss no lamb -3.133257 time 2019-02-25 19:23:26.625447
Model ind 640 epoch 729 head A head_i_epoch 0 batch 100: avg loss -3.085917 avg loss no lamb -3.085917 time 2019-02-25 19:25:23.881527
Model ind 640 epoch 729 head A head_i_epoch 0 batch 200: avg loss -3.145361 avg loss no lamb -3.145361 time 2019-02-25 19:27:18.227095
last batch sz 160
Model ind 640 epoch 729 head B head_i_epoch 0 batch 0: avg loss -1.871977 avg loss no lamb -1.871977 time 2019-02-25 19:28:39.206859
Model ind 640 epoch 729 head B head_i_epoch 0 batch 100: avg loss -1.777612 avg loss no lamb -1.777612 time 2019-02-25 19:30:35.938929
Model ind 640 epoch 729 head B head_i_epoch 0 batch 200: avg loss -1.843362 avg loss no lamb -1.843362 time 2019-02-25 19:32:27.330549
last batch sz 160
Model ind 640 epoch 729 head B head_i_epoch 1 batch 0: avg loss -1.811230 avg loss no lamb -1.811230 time 2019-02-25 19:33:51.433419
Model ind 640 epoch 729 head B head_i_epoch 1 batch 100: avg loss -1.755111 avg loss no lamb -1.755111 time 2019-02-25 19:35:46.593700
Model ind 640 epoch 729 head B head_i_epoch 1 batch 200: avg loss -1.886134 avg loss no lamb -1.886134 time 2019-02-25 19:37:37.120674
last batch sz 160
Pre: time 2019-02-25 19:39:25.560810: 
 	std: 0.051538635
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50776666, 0.61298335, 0.61293334, 0.61291665, 0.50771666]
	train_accs: [0.50776666, 0.61298335, 0.61293334, 0.61291665, 0.50771666]
	best_train_sub_head: 1
	worst: 0.50771666
	avg: 0.57086337
	best: 0.61298335

Starting e_i: 730
Model ind 640 epoch 730 head A head_i_epoch 0 batch 0: avg loss -3.141156 avg loss no lamb -3.141156 time 2019-02-25 19:39:28.505248
Model ind 640 epoch 730 head A head_i_epoch 0 batch 100: avg loss -3.066604 avg loss no lamb -3.066604 time 2019-02-25 19:41:20.903834
Model ind 640 epoch 730 head A head_i_epoch 0 batch 200: avg loss -3.262829 avg loss no lamb -3.262829 time 2019-02-25 19:43:17.340945
last batch sz 160
Model ind 640 epoch 730 head B head_i_epoch 0 batch 0: avg loss -1.828323 avg loss no lamb -1.828323 time 2019-02-25 19:44:39.802218
Model ind 640 epoch 730 head B head_i_epoch 0 batch 100: avg loss -1.772441 avg loss no lamb -1.772441 time 2019-02-25 19:46:35.065560
Model ind 640 epoch 730 head B head_i_epoch 0 batch 200: avg loss -1.883912 avg loss no lamb -1.883912 time 2019-02-25 19:48:31.055260
last batch sz 160
Model ind 640 epoch 730 head B head_i_epoch 1 batch 0: avg loss -1.894931 avg loss no lamb -1.894931 time 2019-02-25 19:49:52.246231
Model ind 640 epoch 730 head B head_i_epoch 1 batch 100: avg loss -1.696937 avg loss no lamb -1.696937 time 2019-02-25 19:51:48.746333
Model ind 640 epoch 730 head B head_i_epoch 1 batch 200: avg loss -1.806913 avg loss no lamb -1.806913 time 2019-02-25 19:53:41.982090
last batch sz 160
Pre: time 2019-02-25 19:55:22.762193: 
 	std: 0.051225822
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5093167, 0.61363333, 0.61373335, 0.61365, 0.5089]
	train_accs: [0.5093167, 0.61363333, 0.61373335, 0.61365, 0.5089]
	best_train_sub_head: 2
	worst: 0.5089
	avg: 0.57184666
	best: 0.61373335

Starting e_i: 731
Model ind 640 epoch 731 head A head_i_epoch 0 batch 0: avg loss -3.110454 avg loss no lamb -3.110454 time 2019-02-25 19:55:31.226870
Model ind 640 epoch 731 head A head_i_epoch 0 batch 100: avg loss -3.081983 avg loss no lamb -3.081983 time 2019-02-25 19:57:31.009041
Model ind 640 epoch 731 head A head_i_epoch 0 batch 200: avg loss -3.215967 avg loss no lamb -3.215967 time 2019-02-25 19:59:23.700959
last batch sz 160
Model ind 640 epoch 731 head B head_i_epoch 0 batch 0: avg loss -1.767100 avg loss no lamb -1.767100 time 2019-02-25 20:00:47.213965
Model ind 640 epoch 731 head B head_i_epoch 0 batch 100: avg loss -1.753263 avg loss no lamb -1.753263 time 2019-02-25 20:02:43.054559
Model ind 640 epoch 731 head B head_i_epoch 0 batch 200: avg loss -1.903458 avg loss no lamb -1.903458 time 2019-02-25 20:04:33.897627
last batch sz 160
Model ind 640 epoch 731 head B head_i_epoch 1 batch 0: avg loss -1.835699 avg loss no lamb -1.835699 time 2019-02-25 20:05:59.030240
Model ind 640 epoch 731 head B head_i_epoch 1 batch 100: avg loss -1.665838 avg loss no lamb -1.665838 time 2019-02-25 20:07:52.224340
Model ind 640 epoch 731 head B head_i_epoch 1 batch 200: avg loss -1.845317 avg loss no lamb -1.845317 time 2019-02-25 20:09:42.894233
last batch sz 160
Pre: time 2019-02-25 20:11:29.942325: 
 	std: 0.05167888
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50825, 0.61366665, 0.61366665, 0.6139333, 0.5082833]
	train_accs: [0.50825, 0.61366665, 0.61366665, 0.6139333, 0.5082833]
	best_train_sub_head: 3
	worst: 0.50825
	avg: 0.57156
	best: 0.6139333

Starting e_i: 732
Model ind 640 epoch 732 head A head_i_epoch 0 batch 0: avg loss -3.159308 avg loss no lamb -3.159308 time 2019-02-25 20:11:32.662960
Model ind 640 epoch 732 head A head_i_epoch 0 batch 100: avg loss -3.119545 avg loss no lamb -3.119545 time 2019-02-25 20:13:25.133048
Model ind 640 epoch 732 head A head_i_epoch 0 batch 200: avg loss -3.159121 avg loss no lamb -3.159121 time 2019-02-25 20:15:22.574484
last batch sz 160
Model ind 640 epoch 732 head B head_i_epoch 0 batch 0: avg loss -1.742718 avg loss no lamb -1.742718 time 2019-02-25 20:16:45.669618
Model ind 640 epoch 732 head B head_i_epoch 0 batch 100: avg loss -1.763342 avg loss no lamb -1.763342 time 2019-02-25 20:18:36.786941
Model ind 640 epoch 732 head B head_i_epoch 0 batch 200: avg loss -1.830961 avg loss no lamb -1.830961 time 2019-02-25 20:20:33.613490
last batch sz 160
Model ind 640 epoch 732 head B head_i_epoch 1 batch 0: avg loss -1.765574 avg loss no lamb -1.765574 time 2019-02-25 20:21:57.450001
Model ind 640 epoch 732 head B head_i_epoch 1 batch 100: avg loss -1.764485 avg loss no lamb -1.764485 time 2019-02-25 20:23:48.406045
Model ind 640 epoch 732 head B head_i_epoch 1 batch 200: avg loss -1.847206 avg loss no lamb -1.847206 time 2019-02-25 20:25:47.828949
last batch sz 160
Pre: time 2019-02-25 20:27:31.385777: 
 	std: 0.050866604
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50908333, 0.6127667, 0.6132, 0.6128, 0.5091]
	train_accs: [0.50908333, 0.6127667, 0.6132, 0.6128, 0.5091]
	best_train_sub_head: 2
	worst: 0.50908333
	avg: 0.57139
	best: 0.6132

Starting e_i: 733
Model ind 640 epoch 733 head A head_i_epoch 0 batch 0: avg loss -3.127300 avg loss no lamb -3.127300 time 2019-02-25 20:27:34.203123
Model ind 640 epoch 733 head A head_i_epoch 0 batch 100: avg loss -3.156823 avg loss no lamb -3.156823 time 2019-02-25 20:29:29.501704
Model ind 640 epoch 733 head A head_i_epoch 0 batch 200: avg loss -3.156999 avg loss no lamb -3.156999 time 2019-02-25 20:31:24.171923
last batch sz 160
Model ind 640 epoch 733 head B head_i_epoch 0 batch 0: avg loss -1.871570 avg loss no lamb -1.871570 time 2019-02-25 20:32:44.677602
Model ind 640 epoch 733 head B head_i_epoch 0 batch 100: avg loss -1.652762 avg loss no lamb -1.652762 time 2019-02-25 20:34:41.218688
Model ind 640 epoch 733 head B head_i_epoch 0 batch 200: avg loss -1.837878 avg loss no lamb -1.837878 time 2019-02-25 20:36:34.818478
last batch sz 160
Model ind 640 epoch 733 head B head_i_epoch 1 batch 0: avg loss -1.859206 avg loss no lamb -1.859206 time 2019-02-25 20:37:55.285761
Model ind 640 epoch 733 head B head_i_epoch 1 batch 100: avg loss -1.684161 avg loss no lamb -1.684161 time 2019-02-25 20:39:52.241812
Model ind 640 epoch 733 head B head_i_epoch 1 batch 200: avg loss -1.817450 avg loss no lamb -1.817450 time 2019-02-25 20:41:43.982108
last batch sz 160
Pre: time 2019-02-25 20:43:28.936446: 
 	std: 0.04984217
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50953335, 0.61081666, 0.61148334, 0.61116666, 0.5093]
	train_accs: [0.50953335, 0.61081666, 0.61148334, 0.61116666, 0.5093]
	best_train_sub_head: 2
	worst: 0.5093
	avg: 0.57045996
	best: 0.61148334

Starting e_i: 734
Model ind 640 epoch 734 head A head_i_epoch 0 batch 0: avg loss -3.078944 avg loss no lamb -3.078944 time 2019-02-25 20:43:32.106715
Model ind 640 epoch 734 head A head_i_epoch 0 batch 100: avg loss -3.095366 avg loss no lamb -3.095366 time 2019-02-25 20:45:28.191648
Model ind 640 epoch 734 head A head_i_epoch 0 batch 200: avg loss -3.181875 avg loss no lamb -3.181875 time 2019-02-25 20:47:20.379856
last batch sz 160
Model ind 640 epoch 734 head B head_i_epoch 0 batch 0: avg loss -1.926127 avg loss no lamb -1.926127 time 2019-02-25 20:48:45.646389
Model ind 640 epoch 734 head B head_i_epoch 0 batch 100: avg loss -1.768610 avg loss no lamb -1.768610 time 2019-02-25 20:50:39.948546
Model ind 640 epoch 734 head B head_i_epoch 0 batch 200: avg loss -1.865130 avg loss no lamb -1.865130 time 2019-02-25 20:52:31.050779
last batch sz 160
Model ind 640 epoch 734 head B head_i_epoch 1 batch 0: avg loss -1.798469 avg loss no lamb -1.798469 time 2019-02-25 20:53:56.783869
Model ind 640 epoch 734 head B head_i_epoch 1 batch 100: avg loss -1.745388 avg loss no lamb -1.745388 time 2019-02-25 20:55:49.232971
Model ind 640 epoch 734 head B head_i_epoch 1 batch 200: avg loss -1.849875 avg loss no lamb -1.849875 time 2019-02-25 20:57:40.568659
last batch sz 160
Pre: time 2019-02-25 20:59:28.046046: 
 	std: 0.05180428
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5046667, 0.61, 0.6103167, 0.61041665, 0.5043333]
	train_accs: [0.5046667, 0.61, 0.6103167, 0.61041665, 0.5043333]
	best_train_sub_head: 3
	worst: 0.5043333
	avg: 0.5679467
	best: 0.61041665

Starting e_i: 735
Model ind 640 epoch 735 head A head_i_epoch 0 batch 0: avg loss -3.199068 avg loss no lamb -3.199068 time 2019-02-25 20:59:30.911353
Model ind 640 epoch 735 head A head_i_epoch 0 batch 100: avg loss -3.124707 avg loss no lamb -3.124707 time 2019-02-25 21:01:22.220260
Model ind 640 epoch 735 head A head_i_epoch 0 batch 200: avg loss -3.175835 avg loss no lamb -3.175835 time 2019-02-25 21:03:18.744072
last batch sz 160
Model ind 640 epoch 735 head B head_i_epoch 0 batch 0: avg loss -1.858807 avg loss no lamb -1.858807 time 2019-02-25 21:04:41.813267
Model ind 640 epoch 735 head B head_i_epoch 0 batch 100: avg loss -1.725747 avg loss no lamb -1.725747 time 2019-02-25 21:06:33.181904
Model ind 640 epoch 735 head B head_i_epoch 0 batch 200: avg loss -1.888168 avg loss no lamb -1.888168 time 2019-02-25 21:08:30.387813
last batch sz 160
Model ind 640 epoch 735 head B head_i_epoch 1 batch 0: avg loss -1.795224 avg loss no lamb -1.795224 time 2019-02-25 21:09:52.844149
Model ind 640 epoch 735 head B head_i_epoch 1 batch 100: avg loss -1.730266 avg loss no lamb -1.730266 time 2019-02-25 21:11:44.740652
Model ind 640 epoch 735 head B head_i_epoch 1 batch 200: avg loss -1.835203 avg loss no lamb -1.835203 time 2019-02-25 21:13:42.262462
last batch sz 160
Pre: time 2019-02-25 21:15:26.236106: 
 	std: 0.051918317
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50668335, 0.61266667, 0.61255, 0.61261666, 0.50658333]
	train_accs: [0.50668335, 0.61266667, 0.61255, 0.61261666, 0.50658333]
	best_train_sub_head: 1
	worst: 0.50658333
	avg: 0.57022
	best: 0.61266667

Starting e_i: 736
Model ind 640 epoch 736 head A head_i_epoch 0 batch 0: avg loss -3.158041 avg loss no lamb -3.158041 time 2019-02-25 21:15:28.672164
Model ind 640 epoch 736 head A head_i_epoch 0 batch 100: avg loss -3.067001 avg loss no lamb -3.067001 time 2019-02-25 21:17:25.216507
Model ind 640 epoch 736 head A head_i_epoch 0 batch 200: avg loss -3.238499 avg loss no lamb -3.238499 time 2019-02-25 21:19:19.924090
last batch sz 160
Model ind 640 epoch 736 head B head_i_epoch 0 batch 0: avg loss -1.795189 avg loss no lamb -1.795189 time 2019-02-25 21:20:41.263166
Model ind 640 epoch 736 head B head_i_epoch 0 batch 100: avg loss -1.865118 avg loss no lamb -1.865118 time 2019-02-25 21:22:38.251022
Model ind 640 epoch 736 head B head_i_epoch 0 batch 200: avg loss -1.875502 avg loss no lamb -1.875502 time 2019-02-25 21:24:30.878834
last batch sz 160
Model ind 640 epoch 736 head B head_i_epoch 1 batch 0: avg loss -1.854702 avg loss no lamb -1.854702 time 2019-02-25 21:25:54.704967
Model ind 640 epoch 736 head B head_i_epoch 1 batch 100: avg loss -1.810693 avg loss no lamb -1.810693 time 2019-02-25 21:27:51.546967
Model ind 640 epoch 736 head B head_i_epoch 1 batch 200: avg loss -1.837334 avg loss no lamb -1.837334 time 2019-02-25 21:29:43.987498
last batch sz 160
Pre: time 2019-02-25 21:31:31.906717: 
 	std: 0.051032502
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50706667, 0.61105, 0.6111, 0.6110833, 0.50675]
	train_accs: [0.50706667, 0.61105, 0.6111, 0.6110833, 0.50675]
	best_train_sub_head: 2
	worst: 0.50675
	avg: 0.56940997
	best: 0.6111

Starting e_i: 737
Model ind 640 epoch 737 head A head_i_epoch 0 batch 0: avg loss -3.143915 avg loss no lamb -3.143915 time 2019-02-25 21:31:35.120722
Model ind 640 epoch 737 head A head_i_epoch 0 batch 100: avg loss -3.162000 avg loss no lamb -3.162000 time 2019-02-25 21:33:28.893814
Model ind 640 epoch 737 head A head_i_epoch 0 batch 200: avg loss -3.148242 avg loss no lamb -3.148242 time 2019-02-25 21:35:20.378080
last batch sz 160
Model ind 640 epoch 737 head B head_i_epoch 0 batch 0: avg loss -1.863128 avg loss no lamb -1.863128 time 2019-02-25 21:36:46.132588
Model ind 640 epoch 737 head B head_i_epoch 0 batch 100: avg loss -1.739498 avg loss no lamb -1.739498 time 2019-02-25 21:38:38.585526
Model ind 640 epoch 737 head B head_i_epoch 0 batch 200: avg loss -1.868786 avg loss no lamb -1.868786 time 2019-02-25 21:40:32.539061
last batch sz 160
Model ind 640 epoch 737 head B head_i_epoch 1 batch 0: avg loss -1.799819 avg loss no lamb -1.799819 time 2019-02-25 21:41:57.127380
Model ind 640 epoch 737 head B head_i_epoch 1 batch 100: avg loss -1.801600 avg loss no lamb -1.801600 time 2019-02-25 21:43:49.001115
Model ind 640 epoch 737 head B head_i_epoch 1 batch 200: avg loss -1.883041 avg loss no lamb -1.883041 time 2019-02-25 21:45:45.650796
last batch sz 160
Pre: time 2019-02-25 21:47:30.215584: 
 	std: 0.05113843
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51026666, 0.61371666, 0.61395, 0.61385, 0.50865]
	train_accs: [0.51026666, 0.61371666, 0.61395, 0.61385, 0.50865]
	best_train_sub_head: 2
	worst: 0.50865
	avg: 0.5720867
	best: 0.61395

Starting e_i: 738
Model ind 640 epoch 738 head A head_i_epoch 0 batch 0: avg loss -3.124053 avg loss no lamb -3.124053 time 2019-02-25 21:47:32.927233
Model ind 640 epoch 738 head A head_i_epoch 0 batch 100: avg loss -3.057926 avg loss no lamb -3.057926 time 2019-02-25 21:49:25.318986
Model ind 640 epoch 738 head A head_i_epoch 0 batch 200: avg loss -3.183896 avg loss no lamb -3.183896 time 2019-02-25 21:51:23.284866
last batch sz 160
Model ind 640 epoch 738 head B head_i_epoch 0 batch 0: avg loss -1.778003 avg loss no lamb -1.778003 time 2019-02-25 21:52:44.921326
Model ind 640 epoch 738 head B head_i_epoch 0 batch 100: avg loss -1.768996 avg loss no lamb -1.768996 time 2019-02-25 21:54:42.422493
Model ind 640 epoch 738 head B head_i_epoch 0 batch 200: avg loss -1.907037 avg loss no lamb -1.907037 time 2019-02-25 21:56:36.583189
last batch sz 160
Model ind 640 epoch 738 head B head_i_epoch 1 batch 0: avg loss -1.882266 avg loss no lamb -1.882266 time 2019-02-25 21:57:57.915931
Model ind 640 epoch 738 head B head_i_epoch 1 batch 100: avg loss -1.706855 avg loss no lamb -1.706855 time 2019-02-25 21:59:54.747338
Model ind 640 epoch 738 head B head_i_epoch 1 batch 200: avg loss -1.828772 avg loss no lamb -1.828772 time 2019-02-25 22:01:48.201146
last batch sz 160
Pre: time 2019-02-25 22:03:31.171841: 
 	std: 0.05117131
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50953335, 0.6133, 0.61333334, 0.6135167, 0.5083333]
	train_accs: [0.50953335, 0.6133, 0.61333334, 0.6135167, 0.5083333]
	best_train_sub_head: 3
	worst: 0.5083333
	avg: 0.5716033
	best: 0.6135167

Starting e_i: 739
Model ind 640 epoch 739 head A head_i_epoch 0 batch 0: avg loss -3.080979 avg loss no lamb -3.080979 time 2019-02-25 22:03:34.201022
Model ind 640 epoch 739 head A head_i_epoch 0 batch 100: avg loss -3.138612 avg loss no lamb -3.138612 time 2019-02-25 22:05:31.099818
Model ind 640 epoch 739 head A head_i_epoch 0 batch 200: avg loss -3.237404 avg loss no lamb -3.237404 time 2019-02-25 22:07:22.677625
last batch sz 160
Model ind 640 epoch 739 head B head_i_epoch 0 batch 0: avg loss -1.733019 avg loss no lamb -1.733019 time 2019-02-25 22:08:48.352033
Model ind 640 epoch 739 head B head_i_epoch 0 batch 100: avg loss -1.821105 avg loss no lamb -1.821105 time 2019-02-25 22:10:42.232525
Model ind 640 epoch 739 head B head_i_epoch 0 batch 200: avg loss -1.861917 avg loss no lamb -1.861917 time 2019-02-25 22:12:33.495958
last batch sz 160
Model ind 640 epoch 739 head B head_i_epoch 1 batch 0: avg loss -1.721354 avg loss no lamb -1.721354 time 2019-02-25 22:13:59.195947
Model ind 640 epoch 739 head B head_i_epoch 1 batch 100: avg loss -1.653782 avg loss no lamb -1.653782 time 2019-02-25 22:15:52.673644
Model ind 640 epoch 739 head B head_i_epoch 1 batch 200: avg loss -1.866518 avg loss no lamb -1.866518 time 2019-02-25 22:17:45.119962
last batch sz 160
Pre: time 2019-02-25 22:19:32.305746: 
 	std: 0.05118874
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50958335, 0.6135167, 0.61365, 0.6134167, 0.5085]
	train_accs: [0.50958335, 0.6135167, 0.61365, 0.6134167, 0.5085]
	best_train_sub_head: 2
	worst: 0.5085
	avg: 0.57173336
	best: 0.61365

Starting e_i: 740
Model ind 640 epoch 740 head A head_i_epoch 0 batch 0: avg loss -3.143632 avg loss no lamb -3.143632 time 2019-02-25 22:19:35.036807
Model ind 640 epoch 740 head A head_i_epoch 0 batch 100: avg loss -3.109807 avg loss no lamb -3.109807 time 2019-02-25 22:21:27.879920
Model ind 640 epoch 740 head A head_i_epoch 0 batch 200: avg loss -3.161210 avg loss no lamb -3.161210 time 2019-02-25 22:23:24.178232
last batch sz 160
Model ind 640 epoch 740 head B head_i_epoch 0 batch 0: avg loss -1.839289 avg loss no lamb -1.839289 time 2019-02-25 22:24:48.880936
Model ind 640 epoch 740 head B head_i_epoch 0 batch 100: avg loss -1.727930 avg loss no lamb -1.727930 time 2019-02-25 22:26:40.285773
Model ind 640 epoch 740 head B head_i_epoch 0 batch 200: avg loss -1.935315 avg loss no lamb -1.935315 time 2019-02-25 22:28:37.773787
last batch sz 160
Model ind 640 epoch 740 head B head_i_epoch 1 batch 0: avg loss -1.799774 avg loss no lamb -1.799774 time 2019-02-25 22:30:15.749302
Model ind 640 epoch 740 head B head_i_epoch 1 batch 100: avg loss -1.757650 avg loss no lamb -1.757650 time 2019-02-25 22:33:26.626393
Model ind 640 epoch 740 head B head_i_epoch 1 batch 200: avg loss -1.853763 avg loss no lamb -1.853763 time 2019-02-25 22:35:18.103985
last batch sz 160
Pre: time 2019-02-25 22:37:04.347067: 
 	std: 0.051171344
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50958335, 0.6138833, 0.61403334, 0.6137667, 0.5093]
	train_accs: [0.50958335, 0.6138833, 0.61403334, 0.6137667, 0.5093]
	best_train_sub_head: 2
	worst: 0.5093
	avg: 0.5721134
	best: 0.61403334

Starting e_i: 741
Model ind 640 epoch 741 head A head_i_epoch 0 batch 0: avg loss -3.120832 avg loss no lamb -3.120832 time 2019-02-25 22:37:11.146957
Model ind 640 epoch 741 head A head_i_epoch 0 batch 100: avg loss -3.136524 avg loss no lamb -3.136524 time 2019-02-25 22:39:05.111815
Model ind 640 epoch 741 head A head_i_epoch 0 batch 200: avg loss -3.224399 avg loss no lamb -3.224399 time 2019-02-25 22:40:55.984959
last batch sz 160
Model ind 640 epoch 741 head B head_i_epoch 0 batch 0: avg loss -1.768365 avg loss no lamb -1.768365 time 2019-02-25 22:42:21.193300
Model ind 640 epoch 741 head B head_i_epoch 0 batch 100: avg loss -1.761400 avg loss no lamb -1.761400 time 2019-02-25 22:44:14.724601
Model ind 640 epoch 741 head B head_i_epoch 0 batch 200: avg loss -1.904096 avg loss no lamb -1.904096 time 2019-02-25 22:46:28.860256
last batch sz 160
Model ind 640 epoch 741 head B head_i_epoch 1 batch 0: avg loss -1.800884 avg loss no lamb -1.800884 time 2019-02-25 22:48:04.831447
Model ind 640 epoch 741 head B head_i_epoch 1 batch 100: avg loss -1.866285 avg loss no lamb -1.866285 time 2019-02-25 22:49:56.230613
Model ind 640 epoch 741 head B head_i_epoch 1 batch 200: avg loss -1.975032 avg loss no lamb -1.975032 time 2019-02-25 22:51:53.523320
last batch sz 160
Pre: time 2019-02-25 22:53:36.392197: 
 	std: 0.04996159
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51196665, 0.61403334, 0.61368334, 0.6138333, 0.5117667]
	train_accs: [0.51196665, 0.61403334, 0.61368334, 0.6138333, 0.5117667]
	best_train_sub_head: 1
	worst: 0.5117667
	avg: 0.57305664
	best: 0.61403334

Starting e_i: 742
Model ind 640 epoch 742 head A head_i_epoch 0 batch 0: avg loss -3.212200 avg loss no lamb -3.212200 time 2019-02-25 22:53:39.482262
Model ind 640 epoch 742 head A head_i_epoch 0 batch 100: avg loss -3.072728 avg loss no lamb -3.072728 time 2019-02-25 22:55:31.581587
Model ind 640 epoch 742 head A head_i_epoch 0 batch 200: avg loss -3.098832 avg loss no lamb -3.098832 time 2019-02-25 22:57:28.428294
last batch sz 160
Model ind 640 epoch 742 head B head_i_epoch 0 batch 0: avg loss -1.866732 avg loss no lamb -1.866732 time 2019-02-25 22:58:49.712508
Model ind 640 epoch 742 head B head_i_epoch 0 batch 100: avg loss -1.722083 avg loss no lamb -1.722083 time 2019-02-25 23:00:45.176356
Model ind 640 epoch 742 head B head_i_epoch 0 batch 200: avg loss -1.805747 avg loss no lamb -1.805747 time 2019-02-25 23:02:39.047628
last batch sz 160
Model ind 640 epoch 742 head B head_i_epoch 1 batch 0: avg loss -1.839206 avg loss no lamb -1.839206 time 2019-02-25 23:03:59.612964
Model ind 640 epoch 742 head B head_i_epoch 1 batch 100: avg loss -1.743956 avg loss no lamb -1.743956 time 2019-02-25 23:05:56.095481
Model ind 640 epoch 742 head B head_i_epoch 1 batch 200: avg loss -1.898210 avg loss no lamb -1.898210 time 2019-02-25 23:07:49.162967
last batch sz 160
Pre: time 2019-02-25 23:09:31.988120: 
 	std: 0.05184855
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50945, 0.6149, 0.61475, 0.6147, 0.50845]
	train_accs: [0.50945, 0.6149, 0.61475, 0.6147, 0.50845]
	best_train_sub_head: 1
	worst: 0.50845
	avg: 0.57245004
	best: 0.6149

Starting e_i: 743
Model ind 640 epoch 743 head A head_i_epoch 0 batch 0: avg loss -3.116066 avg loss no lamb -3.116066 time 2019-02-25 23:09:34.798712
Model ind 640 epoch 743 head A head_i_epoch 0 batch 100: avg loss -3.071165 avg loss no lamb -3.071165 time 2019-02-25 23:12:24.563032
Model ind 640 epoch 743 head A head_i_epoch 0 batch 200: avg loss -3.131566 avg loss no lamb -3.131566 time 2019-02-25 23:15:03.494846
last batch sz 160
Model ind 640 epoch 743 head B head_i_epoch 0 batch 0: avg loss -1.868259 avg loss no lamb -1.868259 time 2019-02-25 23:16:25.573504
Model ind 640 epoch 743 head B head_i_epoch 0 batch 100: avg loss -1.750591 avg loss no lamb -1.750591 time 2019-02-25 23:18:16.075281
Model ind 640 epoch 743 head B head_i_epoch 0 batch 200: avg loss -1.935039 avg loss no lamb -1.935039 time 2019-02-25 23:20:12.708832
last batch sz 160
Model ind 640 epoch 743 head B head_i_epoch 1 batch 0: avg loss -1.875136 avg loss no lamb -1.875136 time 2019-02-25 23:21:35.210205
Model ind 640 epoch 743 head B head_i_epoch 1 batch 100: avg loss -1.759259 avg loss no lamb -1.759259 time 2019-02-25 23:23:24.610883
Model ind 640 epoch 743 head B head_i_epoch 1 batch 200: avg loss -1.932318 avg loss no lamb -1.932318 time 2019-02-25 23:25:45.038793
last batch sz 160
Pre: time 2019-02-25 23:28:38.218378: 
 	std: 0.051387798
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107833, 0.61543334, 0.6154, 0.6155, 0.51031667]
	train_accs: [0.5107833, 0.61543334, 0.6154, 0.6155, 0.51031667]
	best_train_sub_head: 3
	worst: 0.51031667
	avg: 0.5734867
	best: 0.6155

Starting e_i: 744
Model ind 640 epoch 744 head A head_i_epoch 0 batch 0: avg loss -3.200573 avg loss no lamb -3.200573 time 2019-02-25 23:28:49.540828
Model ind 640 epoch 744 head A head_i_epoch 0 batch 100: avg loss -3.054601 avg loss no lamb -3.054601 time 2019-02-25 23:30:44.790313
Model ind 640 epoch 744 head A head_i_epoch 0 batch 200: avg loss -3.147155 avg loss no lamb -3.147155 time 2019-02-25 23:33:00.122391
last batch sz 160
Model ind 640 epoch 744 head B head_i_epoch 0 batch 0: avg loss -1.853568 avg loss no lamb -1.853568 time 2019-02-25 23:34:37.867550
Model ind 640 epoch 744 head B head_i_epoch 0 batch 100: avg loss -1.741051 avg loss no lamb -1.741051 time 2019-02-25 23:36:29.522050
Model ind 640 epoch 744 head B head_i_epoch 0 batch 200: avg loss -1.923368 avg loss no lamb -1.923368 time 2019-02-25 23:38:26.503256
last batch sz 160
Model ind 640 epoch 744 head B head_i_epoch 1 batch 0: avg loss -1.645820 avg loss no lamb -1.645820 time 2019-02-25 23:40:19.352630
Model ind 640 epoch 744 head B head_i_epoch 1 batch 100: avg loss -1.686700 avg loss no lamb -1.686700 time 2019-02-25 23:42:32.496577
Model ind 640 epoch 744 head B head_i_epoch 1 batch 200: avg loss -1.909546 avg loss no lamb -1.909546 time 2019-02-25 23:44:46.986679
last batch sz 160
Pre: time 2019-02-25 23:47:00.737340: 
 	std: 0.049836498
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115833, 0.61305, 0.61321664, 0.61296666, 0.5111167]
	train_accs: [0.5115833, 0.61305, 0.61321664, 0.61296666, 0.5111167]
	best_train_sub_head: 2
	worst: 0.5111167
	avg: 0.5723867
	best: 0.61321664

Starting e_i: 745
Model ind 640 epoch 745 head A head_i_epoch 0 batch 0: avg loss -3.131184 avg loss no lamb -3.131184 time 2019-02-25 23:47:05.043120
Model ind 640 epoch 745 head A head_i_epoch 0 batch 100: avg loss -3.026953 avg loss no lamb -3.026953 time 2019-02-25 23:49:35.911020
Model ind 640 epoch 745 head A head_i_epoch 0 batch 200: avg loss -3.156944 avg loss no lamb -3.156944 time 2019-02-25 23:51:49.629946
last batch sz 160
Model ind 640 epoch 745 head B head_i_epoch 0 batch 0: avg loss -1.860807 avg loss no lamb -1.860807 time 2019-02-25 23:53:29.921972
Model ind 640 epoch 745 head B head_i_epoch 0 batch 100: avg loss -1.778476 avg loss no lamb -1.778476 time 2019-02-25 23:55:52.700955
Model ind 640 epoch 745 head B head_i_epoch 0 batch 200: avg loss -1.867226 avg loss no lamb -1.867226 time 2019-02-25 23:58:22.014566
last batch sz 160
Model ind 640 epoch 745 head B head_i_epoch 1 batch 0: avg loss -1.910429 avg loss no lamb -1.910429 time 2019-02-25 23:59:51.221274
Model ind 640 epoch 745 head B head_i_epoch 1 batch 100: avg loss -1.813782 avg loss no lamb -1.813782 time 2019-02-26 00:02:19.962118
Model ind 640 epoch 745 head B head_i_epoch 1 batch 200: avg loss -1.808482 avg loss no lamb -1.808482 time 2019-02-26 00:04:41.465650
last batch sz 160
Pre: time 2019-02-26 00:06:51.523034: 
 	std: 0.0510062
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50948334, 0.61331666, 0.6131167, 0.61293334, 0.50853336]
	train_accs: [0.50948334, 0.61331666, 0.6131167, 0.61293334, 0.50853336]
	best_train_sub_head: 1
	worst: 0.50853336
	avg: 0.57147664
	best: 0.61331666

Starting e_i: 746
Model ind 640 epoch 746 head A head_i_epoch 0 batch 0: avg loss -3.116960 avg loss no lamb -3.116960 time 2019-02-26 00:06:55.153997
Model ind 640 epoch 746 head A head_i_epoch 0 batch 100: avg loss -3.150813 avg loss no lamb -3.150813 time 2019-02-26 00:09:02.734492
Model ind 640 epoch 746 head A head_i_epoch 0 batch 200: avg loss -3.187928 avg loss no lamb -3.187928 time 2019-02-26 00:11:30.588212
last batch sz 160
Model ind 640 epoch 746 head B head_i_epoch 0 batch 0: avg loss -1.901150 avg loss no lamb -1.901150 time 2019-02-26 00:13:20.549972
Model ind 640 epoch 746 head B head_i_epoch 0 batch 100: avg loss -1.752993 avg loss no lamb -1.752993 time 2019-02-26 00:15:35.250593
Model ind 640 epoch 746 head B head_i_epoch 0 batch 200: avg loss -1.933108 avg loss no lamb -1.933108 time 2019-02-26 00:17:47.740752
last batch sz 160
Model ind 640 epoch 746 head B head_i_epoch 1 batch 0: avg loss -1.910403 avg loss no lamb -1.910403 time 2019-02-26 00:19:38.560161
Model ind 640 epoch 746 head B head_i_epoch 1 batch 100: avg loss -1.779529 avg loss no lamb -1.779529 time 2019-02-26 00:21:56.421551
Model ind 640 epoch 746 head B head_i_epoch 1 batch 200: avg loss -1.907371 avg loss no lamb -1.907371 time 2019-02-26 00:24:09.905934
last batch sz 160
Pre: time 2019-02-26 00:26:26.746013: 
 	std: 0.05136045
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5093333, 0.61405, 0.61401665, 0.614, 0.5090333]
	train_accs: [0.5093333, 0.61405, 0.61401665, 0.614, 0.5090333]
	best_train_sub_head: 1
	worst: 0.5090333
	avg: 0.57208663
	best: 0.61405

Starting e_i: 747
Model ind 640 epoch 747 head A head_i_epoch 0 batch 0: avg loss -3.168985 avg loss no lamb -3.168985 time 2019-02-26 00:26:29.885716
Model ind 640 epoch 747 head A head_i_epoch 0 batch 100: avg loss -3.032492 avg loss no lamb -3.032492 time 2019-02-26 00:28:53.623166
Model ind 640 epoch 747 head A head_i_epoch 0 batch 200: avg loss -3.156912 avg loss no lamb -3.156912 time 2019-02-26 00:31:23.102365
last batch sz 160
Model ind 640 epoch 747 head B head_i_epoch 0 batch 0: avg loss -1.834664 avg loss no lamb -1.834664 time 2019-02-26 00:32:50.828142
Model ind 640 epoch 747 head B head_i_epoch 0 batch 100: avg loss -1.793105 avg loss no lamb -1.793105 time 2019-02-26 00:35:15.464987
Model ind 640 epoch 747 head B head_i_epoch 0 batch 200: avg loss -1.928765 avg loss no lamb -1.928765 time 2019-02-26 00:37:41.354936
last batch sz 160
Model ind 640 epoch 747 head B head_i_epoch 1 batch 0: avg loss -1.915328 avg loss no lamb -1.915328 time 2019-02-26 00:39:15.237373
Model ind 640 epoch 747 head B head_i_epoch 1 batch 100: avg loss -1.792175 avg loss no lamb -1.792175 time 2019-02-26 00:41:39.472089
Model ind 640 epoch 747 head B head_i_epoch 1 batch 200: avg loss -1.909200 avg loss no lamb -1.909200 time 2019-02-26 00:44:04.983482
last batch sz 160
Pre: time 2019-02-26 00:46:14.016166: 
 	std: 0.050681453
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51126665, 0.61475, 0.61455, 0.61448336, 0.51101667]
	train_accs: [0.51126665, 0.61475, 0.61455, 0.61448336, 0.51101667]
	best_train_sub_head: 1
	worst: 0.51101667
	avg: 0.57321334
	best: 0.61475

Starting e_i: 748
Model ind 640 epoch 748 head A head_i_epoch 0 batch 0: avg loss -3.192411 avg loss no lamb -3.192411 time 2019-02-26 00:46:17.890240
Model ind 640 epoch 748 head A head_i_epoch 0 batch 100: avg loss -3.133477 avg loss no lamb -3.133477 time 2019-02-26 00:48:28.947511
Model ind 640 epoch 748 head A head_i_epoch 0 batch 200: avg loss -3.285945 avg loss no lamb -3.285945 time 2019-02-26 00:50:47.467373
last batch sz 160
Model ind 640 epoch 748 head B head_i_epoch 0 batch 0: avg loss -1.869805 avg loss no lamb -1.869805 time 2019-02-26 00:52:39.317128
Model ind 640 epoch 748 head B head_i_epoch 0 batch 100: avg loss -1.768561 avg loss no lamb -1.768561 time 2019-02-26 00:54:56.530992
Model ind 640 epoch 748 head B head_i_epoch 0 batch 200: avg loss -1.840090 avg loss no lamb -1.840090 time 2019-02-26 00:57:08.471446
last batch sz 160
Model ind 640 epoch 748 head B head_i_epoch 1 batch 0: avg loss -1.802788 avg loss no lamb -1.802788 time 2019-02-26 00:59:02.835806
Model ind 640 epoch 748 head B head_i_epoch 1 batch 100: avg loss -1.741289 avg loss no lamb -1.741289 time 2019-02-26 01:01:26.896612
Model ind 640 epoch 748 head B head_i_epoch 1 batch 200: avg loss -1.917924 avg loss no lamb -1.917924 time 2019-02-26 01:03:42.789879
last batch sz 160
Pre: time 2019-02-26 01:05:46.914918: 
 	std: 0.05098242
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51066667, 0.6145, 0.61465, 0.6143, 0.51016665]
	train_accs: [0.51066667, 0.6145, 0.61465, 0.6143, 0.51016665]
	best_train_sub_head: 2
	worst: 0.51016665
	avg: 0.57285666
	best: 0.61465

Starting e_i: 749
Model ind 640 epoch 749 head A head_i_epoch 0 batch 0: avg loss -3.133818 avg loss no lamb -3.133818 time 2019-02-26 01:05:49.804296
Model ind 640 epoch 749 head A head_i_epoch 0 batch 100: avg loss -3.171487 avg loss no lamb -3.171487 time 2019-02-26 01:08:16.596840
Model ind 640 epoch 749 head A head_i_epoch 0 batch 200: avg loss -3.216208 avg loss no lamb -3.216208 time 2019-02-26 01:10:41.947992
last batch sz 160
Model ind 640 epoch 749 head B head_i_epoch 0 batch 0: avg loss -1.788710 avg loss no lamb -1.788710 time 2019-02-26 01:12:16.579558
Model ind 640 epoch 749 head B head_i_epoch 0 batch 100: avg loss -1.771774 avg loss no lamb -1.771774 time 2019-02-26 01:14:40.245158
Model ind 640 epoch 749 head B head_i_epoch 0 batch 200: avg loss -1.843531 avg loss no lamb -1.843531 time 2019-02-26 01:17:06.203512
last batch sz 160
Model ind 640 epoch 749 head B head_i_epoch 1 batch 0: avg loss -1.813088 avg loss no lamb -1.813088 time 2019-02-26 01:18:43.151351
Model ind 640 epoch 749 head B head_i_epoch 1 batch 100: avg loss -1.731871 avg loss no lamb -1.731871 time 2019-02-26 01:20:58.525381
Model ind 640 epoch 749 head B head_i_epoch 1 batch 200: avg loss -1.847438 avg loss no lamb -1.847438 time 2019-02-26 01:23:27.365008
last batch sz 160
Pre: time 2019-02-26 01:25:39.676578: 
 	std: 0.05060539
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50991666, 0.613, 0.61308336, 0.6128333, 0.5094333]
	train_accs: [0.50991666, 0.613, 0.61308336, 0.6128333, 0.5094333]
	best_train_sub_head: 2
	worst: 0.5094333
	avg: 0.5716533
	best: 0.61308336

Starting e_i: 750
Model ind 640 epoch 750 head A head_i_epoch 0 batch 0: avg loss -3.156005 avg loss no lamb -3.156005 time 2019-02-26 01:25:43.095953
Model ind 640 epoch 750 head A head_i_epoch 0 batch 100: avg loss -3.039175 avg loss no lamb -3.039175 time 2019-02-26 01:28:00.150037
Model ind 640 epoch 750 head A head_i_epoch 0 batch 200: avg loss -3.127464 avg loss no lamb -3.127464 time 2019-02-26 01:30:13.491991
last batch sz 160
Model ind 640 epoch 750 head B head_i_epoch 0 batch 0: avg loss -1.833313 avg loss no lamb -1.833313 time 2019-02-26 01:32:06.256481
Model ind 640 epoch 750 head B head_i_epoch 0 batch 100: avg loss -1.753042 avg loss no lamb -1.753042 time 2019-02-26 01:34:23.276123
Model ind 640 epoch 750 head B head_i_epoch 0 batch 200: avg loss -1.914130 avg loss no lamb -1.914130 time 2019-02-26 01:36:40.076454
last batch sz 160
Model ind 640 epoch 750 head B head_i_epoch 1 batch 0: avg loss -1.735440 avg loss no lamb -1.735440 time 2019-02-26 01:38:23.536263
Model ind 640 epoch 750 head B head_i_epoch 1 batch 100: avg loss -1.690937 avg loss no lamb -1.690937 time 2019-02-26 01:40:50.639829
Model ind 640 epoch 750 head B head_i_epoch 1 batch 200: avg loss -1.939211 avg loss no lamb -1.939211 time 2019-02-26 01:43:03.361236
last batch sz 160
Pre: time 2019-02-26 01:45:10.692980: 
 	std: 0.050873227
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50865, 0.6125, 0.61261666, 0.6124167, 0.5086833]
	train_accs: [0.50865, 0.6125, 0.61261666, 0.6124167, 0.5086833]
	best_train_sub_head: 2
	worst: 0.50865
	avg: 0.5709733
	best: 0.61261666

Starting e_i: 751
Model ind 640 epoch 751 head A head_i_epoch 0 batch 0: avg loss -3.091431 avg loss no lamb -3.091431 time 2019-02-26 01:45:17.092316
Model ind 640 epoch 751 head A head_i_epoch 0 batch 100: avg loss -3.139196 avg loss no lamb -3.139196 time 2019-02-26 01:47:40.949375
Model ind 640 epoch 751 head A head_i_epoch 0 batch 200: avg loss -3.178489 avg loss no lamb -3.178489 time 2019-02-26 01:50:05.969940
last batch sz 160
Model ind 640 epoch 751 head B head_i_epoch 0 batch 0: avg loss -1.824993 avg loss no lamb -1.824993 time 2019-02-26 01:51:34.999376
Model ind 640 epoch 751 head B head_i_epoch 0 batch 100: avg loss -1.717920 avg loss no lamb -1.717920 time 2019-02-26 01:54:00.038453
Model ind 640 epoch 751 head B head_i_epoch 0 batch 200: avg loss -1.901275 avg loss no lamb -1.901275 time 2019-02-26 01:56:27.378700
last batch sz 160
Model ind 640 epoch 751 head B head_i_epoch 1 batch 0: avg loss -1.886669 avg loss no lamb -1.886669 time 2019-02-26 01:58:06.670752
Model ind 640 epoch 751 head B head_i_epoch 1 batch 100: avg loss -1.827689 avg loss no lamb -1.827689 time 2019-02-26 02:00:22.384155
Model ind 640 epoch 751 head B head_i_epoch 1 batch 200: avg loss -1.858078 avg loss no lamb -1.858078 time 2019-02-26 02:02:45.687677
last batch sz 160
Pre: time 2019-02-26 02:05:03.887351: 
 	std: 0.051733486
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5064, 0.61221665, 0.6120667, 0.6117167, 0.5064]
	train_accs: [0.5064, 0.61221665, 0.6120667, 0.6117167, 0.5064]
	best_train_sub_head: 1
	worst: 0.5064
	avg: 0.56976
	best: 0.61221665

Starting e_i: 752
Model ind 640 epoch 752 head A head_i_epoch 0 batch 0: avg loss -3.116179 avg loss no lamb -3.116179 time 2019-02-26 02:05:06.984886
Model ind 640 epoch 752 head A head_i_epoch 0 batch 100: avg loss -3.069330 avg loss no lamb -3.069330 time 2019-02-26 02:07:17.970256
Model ind 640 epoch 752 head A head_i_epoch 0 batch 200: avg loss -3.202617 avg loss no lamb -3.202617 time 2019-02-26 02:09:33.541393
last batch sz 160
Model ind 640 epoch 752 head B head_i_epoch 0 batch 0: avg loss -1.833053 avg loss no lamb -1.833053 time 2019-02-26 02:11:22.954947
Model ind 640 epoch 752 head B head_i_epoch 0 batch 100: avg loss -1.798602 avg loss no lamb -1.798602 time 2019-02-26 02:13:48.206446
Model ind 640 epoch 752 head B head_i_epoch 0 batch 200: avg loss -1.855269 avg loss no lamb -1.855269 time 2019-02-26 02:15:58.298222
last batch sz 160
Model ind 640 epoch 752 head B head_i_epoch 1 batch 0: avg loss -1.845231 avg loss no lamb -1.845231 time 2019-02-26 02:17:42.475331
Model ind 640 epoch 752 head B head_i_epoch 1 batch 100: avg loss -1.797702 avg loss no lamb -1.797702 time 2019-02-26 02:20:09.101309
Model ind 640 epoch 752 head B head_i_epoch 1 batch 200: avg loss -1.857912 avg loss no lamb -1.857912 time 2019-02-26 02:22:23.213153
last batch sz 160
Pre: time 2019-02-26 02:24:30.565060: 
 	std: 0.050603822
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50945, 0.61291665, 0.61285, 0.61291665, 0.50975]
	train_accs: [0.50945, 0.61291665, 0.61285, 0.61291665, 0.50975]
	best_train_sub_head: 1
	worst: 0.50945
	avg: 0.5715767
	best: 0.61291665

Starting e_i: 753
Model ind 640 epoch 753 head A head_i_epoch 0 batch 0: avg loss -3.142161 avg loss no lamb -3.142161 time 2019-02-26 02:24:33.220488
Model ind 640 epoch 753 head A head_i_epoch 0 batch 100: avg loss -3.039506 avg loss no lamb -3.039506 time 2019-02-26 02:26:54.252775
Model ind 640 epoch 753 head A head_i_epoch 0 batch 200: avg loss -3.235493 avg loss no lamb -3.235493 time 2019-02-26 02:29:23.364135
last batch sz 160
Model ind 640 epoch 753 head B head_i_epoch 0 batch 0: avg loss -1.807428 avg loss no lamb -1.807428 time 2019-02-26 02:30:50.521675
Model ind 640 epoch 753 head B head_i_epoch 0 batch 100: avg loss -1.783153 avg loss no lamb -1.783153 time 2019-02-26 02:33:17.545471
Model ind 640 epoch 753 head B head_i_epoch 0 batch 200: avg loss -1.878283 avg loss no lamb -1.878283 time 2019-02-26 02:35:39.709261
last batch sz 160
Model ind 640 epoch 753 head B head_i_epoch 1 batch 0: avg loss -1.850869 avg loss no lamb -1.850869 time 2019-02-26 02:37:26.167039
Model ind 640 epoch 753 head B head_i_epoch 1 batch 100: avg loss -1.772961 avg loss no lamb -1.772961 time 2019-02-26 02:39:39.413913
Model ind 640 epoch 753 head B head_i_epoch 1 batch 200: avg loss -1.875522 avg loss no lamb -1.875522 time 2019-02-26 02:42:06.528280
last batch sz 160
Pre: time 2019-02-26 02:44:18.696521: 
 	std: 0.051205643
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50818336, 0.61295, 0.6131167, 0.613, 0.50881666]
	train_accs: [0.50818336, 0.61295, 0.6131167, 0.613, 0.50881666]
	best_train_sub_head: 2
	worst: 0.50818336
	avg: 0.57121336
	best: 0.6131167

Starting e_i: 754
Model ind 640 epoch 754 head A head_i_epoch 0 batch 0: avg loss -3.117037 avg loss no lamb -3.117037 time 2019-02-26 02:44:21.518476
Model ind 640 epoch 754 head A head_i_epoch 0 batch 100: avg loss -3.094750 avg loss no lamb -3.094750 time 2019-02-26 02:46:34.284372
Model ind 640 epoch 754 head A head_i_epoch 0 batch 200: avg loss -3.155318 avg loss no lamb -3.155318 time 2019-02-26 02:48:47.310199
last batch sz 160
Model ind 640 epoch 754 head B head_i_epoch 0 batch 0: avg loss -1.867257 avg loss no lamb -1.867257 time 2019-02-26 02:50:39.480734
Model ind 640 epoch 754 head B head_i_epoch 0 batch 100: avg loss -1.744346 avg loss no lamb -1.744346 time 2019-02-26 02:53:02.548271
Model ind 640 epoch 754 head B head_i_epoch 0 batch 200: avg loss -1.808060 avg loss no lamb -1.808060 time 2019-02-26 02:55:10.190696
last batch sz 160
Model ind 640 epoch 754 head B head_i_epoch 1 batch 0: avg loss -1.834631 avg loss no lamb -1.834631 time 2019-02-26 02:56:59.913486
Model ind 640 epoch 754 head B head_i_epoch 1 batch 100: avg loss -1.728368 avg loss no lamb -1.728368 time 2019-02-26 02:59:23.612632
Model ind 640 epoch 754 head B head_i_epoch 1 batch 200: avg loss -1.921189 avg loss no lamb -1.921189 time 2019-02-26 03:01:37.942602
last batch sz 160
Pre: time 2019-02-26 03:03:40.773208: 
 	std: 0.051640563
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51023334, 0.6163, 0.61616665, 0.61613333, 0.51135]
	train_accs: [0.51023334, 0.6163, 0.61616665, 0.61613333, 0.51135]
	best_train_sub_head: 1
	worst: 0.51023334
	avg: 0.57403666
	best: 0.6163

Starting e_i: 755
Model ind 640 epoch 755 head A head_i_epoch 0 batch 0: avg loss -3.164918 avg loss no lamb -3.164918 time 2019-02-26 03:03:48.159112
Model ind 640 epoch 755 head A head_i_epoch 0 batch 100: avg loss -3.138582 avg loss no lamb -3.138582 time 2019-02-26 03:06:14.623262
Model ind 640 epoch 755 head A head_i_epoch 0 batch 200: avg loss -3.191065 avg loss no lamb -3.191065 time 2019-02-26 03:08:37.764832
last batch sz 160
Model ind 640 epoch 755 head B head_i_epoch 0 batch 0: avg loss -1.764570 avg loss no lamb -1.764570 time 2019-02-26 03:10:17.629924
Model ind 640 epoch 755 head B head_i_epoch 0 batch 100: avg loss -1.748639 avg loss no lamb -1.748639 time 2019-02-26 03:12:36.194548
Model ind 640 epoch 755 head B head_i_epoch 0 batch 200: avg loss -1.865778 avg loss no lamb -1.865778 time 2019-02-26 03:15:02.640996
last batch sz 160
Model ind 640 epoch 755 head B head_i_epoch 1 batch 0: avg loss -1.806930 avg loss no lamb -1.806930 time 2019-02-26 03:16:44.373177
Model ind 640 epoch 755 head B head_i_epoch 1 batch 100: avg loss -1.733910 avg loss no lamb -1.733910 time 2019-02-26 03:18:55.581553
Model ind 640 epoch 755 head B head_i_epoch 1 batch 200: avg loss -1.869254 avg loss no lamb -1.869254 time 2019-02-26 03:21:22.146938
last batch sz 160
Pre: time 2019-02-26 03:23:36.005393: 
 	std: 0.051450863
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5083167, 0.6134667, 0.6137667, 0.6138833, 0.50905]
	train_accs: [0.5083167, 0.6134667, 0.6137667, 0.6138833, 0.50905]
	best_train_sub_head: 3
	worst: 0.5083167
	avg: 0.57169664
	best: 0.6138833

Starting e_i: 756
Model ind 640 epoch 756 head A head_i_epoch 0 batch 0: avg loss -3.114497 avg loss no lamb -3.114497 time 2019-02-26 03:23:39.070366
Model ind 640 epoch 756 head A head_i_epoch 0 batch 100: avg loss -3.144678 avg loss no lamb -3.144678 time 2019-02-26 03:26:00.911025
Model ind 640 epoch 756 head A head_i_epoch 0 batch 200: avg loss -3.108354 avg loss no lamb -3.108354 time 2019-02-26 03:28:12.090689
last batch sz 160
Model ind 640 epoch 756 head B head_i_epoch 0 batch 0: avg loss -1.861221 avg loss no lamb -1.861221 time 2019-02-26 03:30:01.559818
Model ind 640 epoch 756 head B head_i_epoch 0 batch 100: avg loss -1.738616 avg loss no lamb -1.738616 time 2019-02-26 03:32:26.739973
Model ind 640 epoch 756 head B head_i_epoch 0 batch 200: avg loss -1.883966 avg loss no lamb -1.883966 time 2019-02-26 03:34:41.213771
last batch sz 160
Model ind 640 epoch 756 head B head_i_epoch 1 batch 0: avg loss -1.788364 avg loss no lamb -1.788364 time 2019-02-26 03:36:20.753543
Model ind 640 epoch 756 head B head_i_epoch 1 batch 100: avg loss -1.812355 avg loss no lamb -1.812355 time 2019-02-26 03:38:45.766596
Model ind 640 epoch 756 head B head_i_epoch 1 batch 200: avg loss -1.838084 avg loss no lamb -1.838084 time 2019-02-26 03:41:10.894165
last batch sz 160
Pre: time 2019-02-26 03:43:08.799420: 
 	std: 0.0516995
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50913334, 0.61425, 0.6145167, 0.61465, 0.50875]
	train_accs: [0.50913334, 0.61425, 0.6145167, 0.61465, 0.50875]
	best_train_sub_head: 3
	worst: 0.50875
	avg: 0.57226
	best: 0.61465

Starting e_i: 757
Model ind 640 epoch 757 head A head_i_epoch 0 batch 0: avg loss -3.128778 avg loss no lamb -3.128778 time 2019-02-26 03:43:11.498758
Model ind 640 epoch 757 head A head_i_epoch 0 batch 100: avg loss -3.016427 avg loss no lamb -3.016427 time 2019-02-26 03:45:33.720846
Model ind 640 epoch 757 head A head_i_epoch 0 batch 200: avg loss -3.092709 avg loss no lamb -3.092709 time 2019-02-26 03:47:59.165130
last batch sz 160
Model ind 640 epoch 757 head B head_i_epoch 0 batch 0: avg loss -1.795334 avg loss no lamb -1.795334 time 2019-02-26 03:49:40.641303
Model ind 640 epoch 757 head B head_i_epoch 0 batch 100: avg loss -1.752342 avg loss no lamb -1.752342 time 2019-02-26 03:51:55.171815
Model ind 640 epoch 757 head B head_i_epoch 0 batch 200: avg loss -1.779981 avg loss no lamb -1.779981 time 2019-02-26 03:54:22.132533
last batch sz 160
Model ind 640 epoch 757 head B head_i_epoch 1 batch 0: avg loss -1.806390 avg loss no lamb -1.806390 time 2019-02-26 03:56:06.203020
Model ind 640 epoch 757 head B head_i_epoch 1 batch 100: avg loss -1.726992 avg loss no lamb -1.726992 time 2019-02-26 03:58:22.417329
Model ind 640 epoch 757 head B head_i_epoch 1 batch 200: avg loss -1.929639 avg loss no lamb -1.929639 time 2019-02-26 04:00:40.226896
last batch sz 160
Pre: time 2019-02-26 04:02:57.445763: 
 	std: 0.05147742
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101333, 0.6152, 0.6153, 0.6153333, 0.51026666]
	train_accs: [0.5101333, 0.6152, 0.6153, 0.6153333, 0.51026666]
	best_train_sub_head: 3
	worst: 0.5101333
	avg: 0.57324666
	best: 0.6153333

Starting e_i: 758
Model ind 640 epoch 758 head A head_i_epoch 0 batch 0: avg loss -3.180121 avg loss no lamb -3.180121 time 2019-02-26 04:03:00.001289
Model ind 640 epoch 758 head A head_i_epoch 0 batch 100: avg loss -3.032821 avg loss no lamb -3.032821 time 2019-02-26 04:05:22.796942
Model ind 640 epoch 758 head A head_i_epoch 0 batch 200: avg loss -3.187417 avg loss no lamb -3.187417 time 2019-02-26 04:07:39.884579
last batch sz 160
Model ind 640 epoch 758 head B head_i_epoch 0 batch 0: avg loss -1.827436 avg loss no lamb -1.827436 time 2019-02-26 04:09:17.171259
Model ind 640 epoch 758 head B head_i_epoch 0 batch 100: avg loss -1.750443 avg loss no lamb -1.750443 time 2019-02-26 04:11:46.868915
Model ind 640 epoch 758 head B head_i_epoch 0 batch 200: avg loss -1.901420 avg loss no lamb -1.901420 time 2019-02-26 04:13:58.206567
last batch sz 160
Model ind 640 epoch 758 head B head_i_epoch 1 batch 0: avg loss -1.858024 avg loss no lamb -1.858024 time 2019-02-26 04:15:42.419860
Model ind 640 epoch 758 head B head_i_epoch 1 batch 100: avg loss -1.692244 avg loss no lamb -1.692244 time 2019-02-26 04:18:05.611615
Model ind 640 epoch 758 head B head_i_epoch 1 batch 200: avg loss -1.805719 avg loss no lamb -1.805719 time 2019-02-26 04:20:28.076231
last batch sz 160
Pre: time 2019-02-26 04:22:24.843026: 
 	std: 0.050998483
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104, 0.6143, 0.61435, 0.6144, 0.5101]
	train_accs: [0.5104, 0.6143, 0.61435, 0.6144, 0.5101]
	best_train_sub_head: 3
	worst: 0.5101
	avg: 0.57271004
	best: 0.6144

Starting e_i: 759
Model ind 640 epoch 759 head A head_i_epoch 0 batch 0: avg loss -3.181301 avg loss no lamb -3.181301 time 2019-02-26 04:22:28.174509
Model ind 640 epoch 759 head A head_i_epoch 0 batch 100: avg loss -3.159638 avg loss no lamb -3.159638 time 2019-02-26 04:24:53.652997
Model ind 640 epoch 759 head A head_i_epoch 0 batch 200: avg loss -3.234294 avg loss no lamb -3.234294 time 2019-02-26 04:27:17.850192
last batch sz 160
Model ind 640 epoch 759 head B head_i_epoch 0 batch 0: avg loss -1.726083 avg loss no lamb -1.726083 time 2019-02-26 04:28:57.912085
Model ind 640 epoch 759 head B head_i_epoch 0 batch 100: avg loss -1.776251 avg loss no lamb -1.776251 time 2019-02-26 04:31:13.981318
Model ind 640 epoch 759 head B head_i_epoch 0 batch 200: avg loss -1.850904 avg loss no lamb -1.850904 time 2019-02-26 04:33:37.120269
last batch sz 160
Model ind 640 epoch 759 head B head_i_epoch 1 batch 0: avg loss -1.831879 avg loss no lamb -1.831879 time 2019-02-26 04:35:30.370281
Model ind 640 epoch 759 head B head_i_epoch 1 batch 100: avg loss -1.713381 avg loss no lamb -1.713381 time 2019-02-26 04:37:36.250991
Model ind 640 epoch 759 head B head_i_epoch 1 batch 200: avg loss -1.871315 avg loss no lamb -1.871315 time 2019-02-26 04:39:58.672059
last batch sz 160
Pre: time 2019-02-26 04:41:42.301486: 
 	std: 0.05050314
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51213336, 0.6149833, 0.61518335, 0.6151, 0.5118667]
	train_accs: [0.51213336, 0.6149833, 0.61518335, 0.6151, 0.5118667]
	best_train_sub_head: 2
	worst: 0.5118667
	avg: 0.5738533
	best: 0.61518335

Starting e_i: 760
Model ind 640 epoch 760 head A head_i_epoch 0 batch 0: avg loss -3.133110 avg loss no lamb -3.133110 time 2019-02-26 04:41:44.595286
Model ind 640 epoch 760 head A head_i_epoch 0 batch 100: avg loss -3.096087 avg loss no lamb -3.096087 time 2019-02-26 04:43:39.199906
Model ind 640 epoch 760 head A head_i_epoch 0 batch 200: avg loss -3.234789 avg loss no lamb -3.234789 time 2019-02-26 04:45:33.561954
last batch sz 160
Model ind 640 epoch 760 head B head_i_epoch 0 batch 0: avg loss -1.801268 avg loss no lamb -1.801268 time 2019-02-26 04:46:53.832199
Model ind 640 epoch 760 head B head_i_epoch 0 batch 100: avg loss -1.808787 avg loss no lamb -1.808787 time 2019-02-26 04:48:49.950397
Model ind 640 epoch 760 head B head_i_epoch 0 batch 200: avg loss -1.871149 avg loss no lamb -1.871149 time 2019-02-26 04:50:42.071149
last batch sz 160
Model ind 640 epoch 760 head B head_i_epoch 1 batch 0: avg loss -1.824658 avg loss no lamb -1.824658 time 2019-02-26 04:52:02.620452
Model ind 640 epoch 760 head B head_i_epoch 1 batch 100: avg loss -1.756977 avg loss no lamb -1.756977 time 2019-02-26 04:53:59.852056
Model ind 640 epoch 760 head B head_i_epoch 1 batch 200: avg loss -1.895472 avg loss no lamb -1.895472 time 2019-02-26 04:55:51.174369
last batch sz 160
Pre: time 2019-02-26 04:57:41.311688: 
 	std: 0.050718077
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50965, 0.6132333, 0.6131167, 0.61308336, 0.50958335]
	train_accs: [0.50965, 0.6132333, 0.6131167, 0.61308336, 0.50958335]
	best_train_sub_head: 1
	worst: 0.50958335
	avg: 0.57173336
	best: 0.6132333

Starting e_i: 761
Model ind 640 epoch 761 head A head_i_epoch 0 batch 0: avg loss -3.175520 avg loss no lamb -3.175520 time 2019-02-26 04:57:51.450296
Model ind 640 epoch 761 head A head_i_epoch 0 batch 100: avg loss -3.024091 avg loss no lamb -3.024091 time 2019-02-26 04:59:45.623051
Model ind 640 epoch 761 head A head_i_epoch 0 batch 200: avg loss -3.134634 avg loss no lamb -3.134634 time 2019-02-26 05:01:36.409588
last batch sz 160
Model ind 640 epoch 761 head B head_i_epoch 0 batch 0: avg loss -1.842222 avg loss no lamb -1.842222 time 2019-02-26 05:03:03.044652
Model ind 640 epoch 761 head B head_i_epoch 0 batch 100: avg loss -1.808106 avg loss no lamb -1.808106 time 2019-02-26 05:04:55.907012
Model ind 640 epoch 761 head B head_i_epoch 0 batch 200: avg loss -1.831286 avg loss no lamb -1.831286 time 2019-02-26 05:06:49.984625
last batch sz 160
Model ind 640 epoch 761 head B head_i_epoch 1 batch 0: avg loss -1.880839 avg loss no lamb -1.880839 time 2019-02-26 05:08:14.469389
Model ind 640 epoch 761 head B head_i_epoch 1 batch 100: avg loss -1.748052 avg loss no lamb -1.748052 time 2019-02-26 05:10:06.679556
Model ind 640 epoch 761 head B head_i_epoch 1 batch 200: avg loss -1.939978 avg loss no lamb -1.939978 time 2019-02-26 05:12:02.239222
last batch sz 160
Pre: time 2019-02-26 05:13:46.442509: 
 	std: 0.05072435
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118667, 0.61501664, 0.61505, 0.6148, 0.51096666]
	train_accs: [0.5118667, 0.61501664, 0.61505, 0.6148, 0.51096666]
	best_train_sub_head: 2
	worst: 0.51096666
	avg: 0.57354003
	best: 0.61505

Starting e_i: 762
Model ind 640 epoch 762 head A head_i_epoch 0 batch 0: avg loss -3.211934 avg loss no lamb -3.211934 time 2019-02-26 05:13:49.006957
Model ind 640 epoch 762 head A head_i_epoch 0 batch 100: avg loss -3.076561 avg loss no lamb -3.076561 time 2019-02-26 05:15:40.880401
Model ind 640 epoch 762 head A head_i_epoch 0 batch 200: avg loss -3.279330 avg loss no lamb -3.279330 time 2019-02-26 05:17:38.392667
last batch sz 160
Model ind 640 epoch 762 head B head_i_epoch 0 batch 0: avg loss -1.865048 avg loss no lamb -1.865048 time 2019-02-26 05:19:01.017489
Model ind 640 epoch 762 head B head_i_epoch 0 batch 100: avg loss -1.763978 avg loss no lamb -1.763978 time 2019-02-26 05:20:52.977983
Model ind 640 epoch 762 head B head_i_epoch 0 batch 200: avg loss -1.902598 avg loss no lamb -1.902598 time 2019-02-26 05:22:53.254801
last batch sz 160
Model ind 640 epoch 762 head B head_i_epoch 1 batch 0: avg loss -1.843453 avg loss no lamb -1.843453 time 2019-02-26 05:24:14.427325
Model ind 640 epoch 762 head B head_i_epoch 1 batch 100: avg loss -1.764684 avg loss no lamb -1.764684 time 2019-02-26 05:26:11.359253
Model ind 640 epoch 762 head B head_i_epoch 1 batch 200: avg loss -1.905865 avg loss no lamb -1.905865 time 2019-02-26 05:28:04.768489
last batch sz 160
Pre: time 2019-02-26 05:29:47.139535: 
 	std: 0.05123392
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50983334, 0.61448336, 0.6142667, 0.6142167, 0.50965]
	train_accs: [0.50983334, 0.61448336, 0.6142667, 0.6142167, 0.50965]
	best_train_sub_head: 1
	worst: 0.50965
	avg: 0.57249
	best: 0.61448336

Starting e_i: 763
Model ind 640 epoch 763 head A head_i_epoch 0 batch 0: avg loss -3.113373 avg loss no lamb -3.113373 time 2019-02-26 05:29:50.247654
Model ind 640 epoch 763 head A head_i_epoch 0 batch 100: avg loss -3.072627 avg loss no lamb -3.072627 time 2019-02-26 05:31:47.756109
Model ind 640 epoch 763 head A head_i_epoch 0 batch 200: avg loss -3.111850 avg loss no lamb -3.111850 time 2019-02-26 05:33:39.023857
last batch sz 160
Model ind 640 epoch 763 head B head_i_epoch 0 batch 0: avg loss -1.828627 avg loss no lamb -1.828627 time 2019-02-26 05:35:02.690984
Model ind 640 epoch 763 head B head_i_epoch 0 batch 100: avg loss -1.717699 avg loss no lamb -1.717699 time 2019-02-26 05:36:58.880288
Model ind 640 epoch 763 head B head_i_epoch 0 batch 200: avg loss -1.819478 avg loss no lamb -1.819478 time 2019-02-26 05:38:49.650385
last batch sz 160
Model ind 640 epoch 763 head B head_i_epoch 1 batch 0: avg loss -1.756487 avg loss no lamb -1.756487 time 2019-02-26 05:40:15.514473
Model ind 640 epoch 763 head B head_i_epoch 1 batch 100: avg loss -1.721864 avg loss no lamb -1.721864 time 2019-02-26 05:42:08.822604
Model ind 640 epoch 763 head B head_i_epoch 1 batch 200: avg loss -1.882447 avg loss no lamb -1.882447 time 2019-02-26 05:44:01.032188
last batch sz 160
Pre: time 2019-02-26 05:45:49.822867: 
 	std: 0.050649025
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50988334, 0.61343336, 0.61366665, 0.6135833, 0.5104667]
	train_accs: [0.50988334, 0.61343336, 0.61366665, 0.6135833, 0.5104667]
	best_train_sub_head: 2
	worst: 0.50988334
	avg: 0.5722067
	best: 0.61366665

Starting e_i: 764
Model ind 640 epoch 764 head A head_i_epoch 0 batch 0: avg loss -3.175001 avg loss no lamb -3.175001 time 2019-02-26 05:45:52.669422
Model ind 640 epoch 764 head A head_i_epoch 0 batch 100: avg loss -3.073807 avg loss no lamb -3.073807 time 2019-02-26 05:47:44.934896
Model ind 640 epoch 764 head A head_i_epoch 0 batch 200: avg loss -3.151516 avg loss no lamb -3.151516 time 2019-02-26 05:49:42.258020
last batch sz 160
Model ind 640 epoch 764 head B head_i_epoch 0 batch 0: avg loss -1.747520 avg loss no lamb -1.747520 time 2019-02-26 05:51:06.211211
Model ind 640 epoch 764 head B head_i_epoch 0 batch 100: avg loss -1.761183 avg loss no lamb -1.761183 time 2019-02-26 05:52:56.672209
Model ind 640 epoch 764 head B head_i_epoch 0 batch 200: avg loss -1.850196 avg loss no lamb -1.850196 time 2019-02-26 05:54:54.719345
last batch sz 160
Model ind 640 epoch 764 head B head_i_epoch 1 batch 0: avg loss -1.883796 avg loss no lamb -1.883796 time 2019-02-26 05:56:16.485557
Model ind 640 epoch 764 head B head_i_epoch 1 batch 100: avg loss -1.762943 avg loss no lamb -1.762943 time 2019-02-26 05:58:08.009070
Model ind 640 epoch 764 head B head_i_epoch 1 batch 200: avg loss -1.949211 avg loss no lamb -1.949211 time 2019-02-26 06:00:06.349835
last batch sz 160
Pre: time 2019-02-26 06:01:50.266293: 
 	std: 0.050695147
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51055, 0.61436665, 0.6142333, 0.6141667, 0.511]
	train_accs: [0.51055, 0.61436665, 0.6142333, 0.6141667, 0.511]
	best_train_sub_head: 1
	worst: 0.51055
	avg: 0.57286334
	best: 0.61436665

Starting e_i: 765
Model ind 640 epoch 765 head A head_i_epoch 0 batch 0: avg loss -3.222107 avg loss no lamb -3.222107 time 2019-02-26 06:01:53.347166
Model ind 640 epoch 765 head A head_i_epoch 0 batch 100: avg loss -3.022423 avg loss no lamb -3.022423 time 2019-02-26 06:03:50.186484
Model ind 640 epoch 765 head A head_i_epoch 0 batch 200: avg loss -3.184259 avg loss no lamb -3.184259 time 2019-02-26 06:05:43.407829
last batch sz 160
Model ind 640 epoch 765 head B head_i_epoch 0 batch 0: avg loss -1.781682 avg loss no lamb -1.781682 time 2019-02-26 06:07:04.204763
Model ind 640 epoch 765 head B head_i_epoch 0 batch 100: avg loss -1.708998 avg loss no lamb -1.708998 time 2019-02-26 06:09:01.524674
Model ind 640 epoch 765 head B head_i_epoch 0 batch 200: avg loss -1.818298 avg loss no lamb -1.818298 time 2019-02-26 06:10:53.773224
last batch sz 160
Model ind 640 epoch 765 head B head_i_epoch 1 batch 0: avg loss -1.830274 avg loss no lamb -1.830274 time 2019-02-26 06:12:15.055020
Model ind 640 epoch 765 head B head_i_epoch 1 batch 100: avg loss -1.692233 avg loss no lamb -1.692233 time 2019-02-26 06:14:15.314437
Model ind 640 epoch 765 head B head_i_epoch 1 batch 200: avg loss -1.846883 avg loss no lamb -1.846883 time 2019-02-26 06:16:07.364639
last batch sz 160
Pre: time 2019-02-26 06:17:55.056513: 
 	std: 0.050182253
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50985, 0.61265, 0.61261666, 0.61228335, 0.51031667]
	train_accs: [0.50985, 0.61265, 0.61261666, 0.61228335, 0.51031667]
	best_train_sub_head: 1
	worst: 0.50985
	avg: 0.57154334
	best: 0.61265

Starting e_i: 766
Model ind 640 epoch 766 head A head_i_epoch 0 batch 0: avg loss -3.198730 avg loss no lamb -3.198730 time 2019-02-26 06:17:57.741618
Model ind 640 epoch 766 head A head_i_epoch 0 batch 100: avg loss -3.012330 avg loss no lamb -3.012330 time 2019-02-26 06:19:50.479781
Model ind 640 epoch 766 head A head_i_epoch 0 batch 200: avg loss -3.135647 avg loss no lamb -3.135647 time 2019-02-26 06:21:44.889994
last batch sz 160
Model ind 640 epoch 766 head B head_i_epoch 0 batch 0: avg loss -1.838390 avg loss no lamb -1.838390 time 2019-02-26 06:23:13.003250
Model ind 640 epoch 766 head B head_i_epoch 0 batch 100: avg loss -1.740495 avg loss no lamb -1.740495 time 2019-02-26 06:25:05.525336
Model ind 640 epoch 766 head B head_i_epoch 0 batch 200: avg loss -1.814341 avg loss no lamb -1.814341 time 2019-02-26 06:27:01.294701
last batch sz 160
Model ind 640 epoch 766 head B head_i_epoch 1 batch 0: avg loss -1.842221 avg loss no lamb -1.842221 time 2019-02-26 06:28:24.910577
Model ind 640 epoch 766 head B head_i_epoch 1 batch 100: avg loss -1.765181 avg loss no lamb -1.765181 time 2019-02-26 06:30:15.935738
Model ind 640 epoch 766 head B head_i_epoch 1 batch 200: avg loss -1.865114 avg loss no lamb -1.865114 time 2019-02-26 06:32:13.185916
last batch sz 160
Pre: time 2019-02-26 06:33:57.078153: 
 	std: 0.05071336
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5103833, 0.6142167, 0.6145, 0.6141833, 0.5111833]
	train_accs: [0.5103833, 0.6142167, 0.6145, 0.6141833, 0.5111833]
	best_train_sub_head: 2
	worst: 0.5103833
	avg: 0.5728933
	best: 0.6145

Starting e_i: 767
Model ind 640 epoch 767 head A head_i_epoch 0 batch 0: avg loss -3.134205 avg loss no lamb -3.134205 time 2019-02-26 06:34:00.157098
Model ind 640 epoch 767 head A head_i_epoch 0 batch 100: avg loss -3.059592 avg loss no lamb -3.059592 time 2019-02-26 06:35:54.641525
Model ind 640 epoch 767 head A head_i_epoch 0 batch 200: avg loss -3.213832 avg loss no lamb -3.213832 time 2019-02-26 06:37:50.156771
last batch sz 160
Model ind 640 epoch 767 head B head_i_epoch 0 batch 0: avg loss -1.850648 avg loss no lamb -1.850648 time 2019-02-26 06:39:10.755777
Model ind 640 epoch 767 head B head_i_epoch 0 batch 100: avg loss -1.690578 avg loss no lamb -1.690578 time 2019-02-26 06:41:07.141303
Model ind 640 epoch 767 head B head_i_epoch 0 batch 200: avg loss -1.917942 avg loss no lamb -1.917942 time 2019-02-26 06:43:01.145404
last batch sz 160
Model ind 640 epoch 767 head B head_i_epoch 1 batch 0: avg loss -1.826544 avg loss no lamb -1.826544 time 2019-02-26 06:44:22.264067
Model ind 640 epoch 767 head B head_i_epoch 1 batch 100: avg loss -1.737999 avg loss no lamb -1.737999 time 2019-02-26 06:46:20.523045
Model ind 640 epoch 767 head B head_i_epoch 1 batch 200: avg loss -1.870924 avg loss no lamb -1.870924 time 2019-02-26 06:48:12.991917
last batch sz 160
Pre: time 2019-02-26 06:49:58.521169: 
 	std: 0.05139455
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50851667, 0.6142, 0.61375, 0.61406666, 0.5096833]
	train_accs: [0.50851667, 0.6142, 0.61375, 0.61406666, 0.5096833]
	best_train_sub_head: 1
	worst: 0.50851667
	avg: 0.5720433
	best: 0.6142

Starting e_i: 768
Model ind 640 epoch 768 head A head_i_epoch 0 batch 0: avg loss -3.197261 avg loss no lamb -3.197261 time 2019-02-26 06:50:02.114323
Model ind 640 epoch 768 head A head_i_epoch 0 batch 100: avg loss -3.013172 avg loss no lamb -3.013172 time 2019-02-26 06:51:57.333701
Model ind 640 epoch 768 head A head_i_epoch 0 batch 200: avg loss -3.170325 avg loss no lamb -3.170325 time 2019-02-26 06:53:48.621040
last batch sz 160
Model ind 640 epoch 768 head B head_i_epoch 0 batch 0: avg loss -1.803299 avg loss no lamb -1.803299 time 2019-02-26 06:55:15.978450
Model ind 640 epoch 768 head B head_i_epoch 0 batch 100: avg loss -1.724016 avg loss no lamb -1.724016 time 2019-02-26 06:57:08.859392
Model ind 640 epoch 768 head B head_i_epoch 0 batch 200: avg loss -1.898362 avg loss no lamb -1.898362 time 2019-02-26 06:59:00.685069
last batch sz 160
Model ind 640 epoch 768 head B head_i_epoch 1 batch 0: avg loss -1.786311 avg loss no lamb -1.786311 time 2019-02-26 07:00:26.940832
Model ind 640 epoch 768 head B head_i_epoch 1 batch 100: avg loss -1.841998 avg loss no lamb -1.841998 time 2019-02-26 07:02:19.928576
Model ind 640 epoch 768 head B head_i_epoch 1 batch 200: avg loss -1.889411 avg loss no lamb -1.889411 time 2019-02-26 07:04:14.617296
last batch sz 160
Pre: time 2019-02-26 07:05:59.417920: 
 	std: 0.051152226
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50913334, 0.61375, 0.61355, 0.61366665, 0.50935]
	train_accs: [0.50913334, 0.61375, 0.61355, 0.61366665, 0.50935]
	best_train_sub_head: 1
	worst: 0.50913334
	avg: 0.57189
	best: 0.61375

Starting e_i: 769
Model ind 640 epoch 769 head A head_i_epoch 0 batch 0: avg loss -3.110533 avg loss no lamb -3.110533 time 2019-02-26 07:06:02.390625
Model ind 640 epoch 769 head A head_i_epoch 0 batch 100: avg loss -3.020773 avg loss no lamb -3.020773 time 2019-02-26 07:07:54.206343
Model ind 640 epoch 769 head A head_i_epoch 0 batch 200: avg loss -3.130784 avg loss no lamb -3.130784 time 2019-02-26 07:09:49.947786
last batch sz 160
Model ind 640 epoch 769 head B head_i_epoch 0 batch 0: avg loss -1.819016 avg loss no lamb -1.819016 time 2019-02-26 07:11:11.874130
Model ind 640 epoch 769 head B head_i_epoch 0 batch 100: avg loss -1.716927 avg loss no lamb -1.716927 time 2019-02-26 07:13:03.282730
Model ind 640 epoch 769 head B head_i_epoch 0 batch 200: avg loss -1.893653 avg loss no lamb -1.893653 time 2019-02-26 07:15:00.697076
last batch sz 160
Model ind 640 epoch 769 head B head_i_epoch 1 batch 0: avg loss -1.781965 avg loss no lamb -1.781965 time 2019-02-26 07:16:22.108519
Model ind 640 epoch 769 head B head_i_epoch 1 batch 100: avg loss -1.745157 avg loss no lamb -1.745157 time 2019-02-26 07:18:17.843266
Model ind 640 epoch 769 head B head_i_epoch 1 batch 200: avg loss -1.903205 avg loss no lamb -1.903205 time 2019-02-26 07:20:10.189579
last batch sz 160
Pre: time 2019-02-26 07:21:54.588001: 
 	std: 0.051171273
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.509, 0.61338335, 0.61326665, 0.61333334, 0.50875]
	train_accs: [0.509, 0.61338335, 0.61326665, 0.61333334, 0.50875]
	best_train_sub_head: 1
	worst: 0.50875
	avg: 0.5715467
	best: 0.61338335

Starting e_i: 770
Model ind 640 epoch 770 head A head_i_epoch 0 batch 0: avg loss -3.154339 avg loss no lamb -3.154339 time 2019-02-26 07:21:57.247689
Model ind 640 epoch 770 head A head_i_epoch 0 batch 100: avg loss -3.067629 avg loss no lamb -3.067629 time 2019-02-26 07:23:55.147462
Model ind 640 epoch 770 head A head_i_epoch 0 batch 200: avg loss -3.216755 avg loss no lamb -3.216755 time 2019-02-26 07:25:48.412227
last batch sz 160
Model ind 640 epoch 770 head B head_i_epoch 0 batch 0: avg loss -1.879156 avg loss no lamb -1.879156 time 2019-02-26 07:27:10.973304
Model ind 640 epoch 770 head B head_i_epoch 0 batch 100: avg loss -1.811620 avg loss no lamb -1.811620 time 2019-02-26 07:29:06.950665
Model ind 640 epoch 770 head B head_i_epoch 0 batch 200: avg loss -1.878027 avg loss no lamb -1.878027 time 2019-02-26 07:31:00.248816
last batch sz 160
Model ind 640 epoch 770 head B head_i_epoch 1 batch 0: avg loss -1.772640 avg loss no lamb -1.772640 time 2019-02-26 07:32:28.460229
Model ind 640 epoch 770 head B head_i_epoch 1 batch 100: avg loss -1.769817 avg loss no lamb -1.769817 time 2019-02-26 07:34:22.951831
Model ind 640 epoch 770 head B head_i_epoch 1 batch 200: avg loss -1.884838 avg loss no lamb -1.884838 time 2019-02-26 07:36:15.474752
last batch sz 160
Pre: time 2019-02-26 07:38:02.993924: 
 	std: 0.051339045
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5071333, 0.61225, 0.6121167, 0.6123667, 0.50776666]
	train_accs: [0.5071333, 0.61225, 0.6121167, 0.6123667, 0.50776666]
	best_train_sub_head: 3
	worst: 0.5071333
	avg: 0.5703267
	best: 0.6123667

Starting e_i: 771
Model ind 640 epoch 771 head A head_i_epoch 0 batch 0: avg loss -3.148987 avg loss no lamb -3.148987 time 2019-02-26 07:38:09.761081
Model ind 640 epoch 771 head A head_i_epoch 0 batch 100: avg loss -3.128854 avg loss no lamb -3.128854 time 2019-02-26 07:40:01.956918
Model ind 640 epoch 771 head A head_i_epoch 0 batch 200: avg loss -3.162959 avg loss no lamb -3.162959 time 2019-02-26 07:41:58.199240
last batch sz 160
Model ind 640 epoch 771 head B head_i_epoch 0 batch 0: avg loss -1.781917 avg loss no lamb -1.781917 time 2019-02-26 07:43:20.982083
Model ind 640 epoch 771 head B head_i_epoch 0 batch 100: avg loss -1.753785 avg loss no lamb -1.753785 time 2019-02-26 07:45:12.180984
Model ind 640 epoch 771 head B head_i_epoch 0 batch 200: avg loss -1.906018 avg loss no lamb -1.906018 time 2019-02-26 07:47:10.123109
last batch sz 160
Model ind 640 epoch 771 head B head_i_epoch 1 batch 0: avg loss -1.826150 avg loss no lamb -1.826150 time 2019-02-26 07:48:31.878582
Model ind 640 epoch 771 head B head_i_epoch 1 batch 100: avg loss -1.734262 avg loss no lamb -1.734262 time 2019-02-26 07:50:24.293966
Model ind 640 epoch 771 head B head_i_epoch 1 batch 200: avg loss -1.851455 avg loss no lamb -1.851455 time 2019-02-26 07:52:22.190620
last batch sz 160
Pre: time 2019-02-26 07:54:04.416665: 
 	std: 0.050614774
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51238334, 0.6159833, 0.6156667, 0.61575, 0.5125833]
	train_accs: [0.51238334, 0.6159833, 0.6156667, 0.61575, 0.5125833]
	best_train_sub_head: 1
	worst: 0.51238334
	avg: 0.5744733
	best: 0.6159833

Starting e_i: 772
Model ind 640 epoch 772 head A head_i_epoch 0 batch 0: avg loss -3.188767 avg loss no lamb -3.188767 time 2019-02-26 07:54:07.040166
Model ind 640 epoch 772 head A head_i_epoch 0 batch 100: avg loss -3.089293 avg loss no lamb -3.089293 time 2019-02-26 07:56:04.015041
Model ind 640 epoch 772 head A head_i_epoch 0 batch 200: avg loss -3.156281 avg loss no lamb -3.156281 time 2019-02-26 07:57:57.637705
last batch sz 160
Model ind 640 epoch 772 head B head_i_epoch 0 batch 0: avg loss -1.839547 avg loss no lamb -1.839547 time 2019-02-26 07:59:17.814311
Model ind 640 epoch 772 head B head_i_epoch 0 batch 100: avg loss -1.712181 avg loss no lamb -1.712181 time 2019-02-26 08:01:15.140527
Model ind 640 epoch 772 head B head_i_epoch 0 batch 200: avg loss -1.872766 avg loss no lamb -1.872766 time 2019-02-26 08:03:08.039113
last batch sz 160
Model ind 640 epoch 772 head B head_i_epoch 1 batch 0: avg loss -1.833096 avg loss no lamb -1.833096 time 2019-02-26 08:04:29.626738
Model ind 640 epoch 772 head B head_i_epoch 1 batch 100: avg loss -1.783290 avg loss no lamb -1.783290 time 2019-02-26 08:06:26.415028
Model ind 640 epoch 772 head B head_i_epoch 1 batch 200: avg loss -1.934420 avg loss no lamb -1.934420 time 2019-02-26 08:08:17.962764
last batch sz 160
Pre: time 2019-02-26 08:10:04.851780: 
 	std: 0.050618824
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114667, 0.61476666, 0.6145833, 0.6146, 0.5111833]
	train_accs: [0.5114667, 0.61476666, 0.6145833, 0.6146, 0.5111833]
	best_train_sub_head: 1
	worst: 0.5111833
	avg: 0.57332
	best: 0.61476666

Starting e_i: 773
Model ind 640 epoch 773 head A head_i_epoch 0 batch 0: avg loss -3.153977 avg loss no lamb -3.153977 time 2019-02-26 08:10:08.182379
Model ind 640 epoch 773 head A head_i_epoch 0 batch 100: avg loss -3.132446 avg loss no lamb -3.132446 time 2019-02-26 08:12:01.325376
Model ind 640 epoch 773 head A head_i_epoch 0 batch 200: avg loss -3.224485 avg loss no lamb -3.224485 time 2019-02-26 08:13:53.499391
last batch sz 160
Model ind 640 epoch 773 head B head_i_epoch 0 batch 0: avg loss -1.837874 avg loss no lamb -1.837874 time 2019-02-26 08:15:19.069825
Model ind 640 epoch 773 head B head_i_epoch 0 batch 100: avg loss -1.750603 avg loss no lamb -1.750603 time 2019-02-26 08:17:10.487852
Model ind 640 epoch 773 head B head_i_epoch 0 batch 200: avg loss -1.927050 avg loss no lamb -1.927050 time 2019-02-26 08:19:04.026241
last batch sz 160
Model ind 640 epoch 773 head B head_i_epoch 1 batch 0: avg loss -1.875826 avg loss no lamb -1.875826 time 2019-02-26 08:20:30.865841
Model ind 640 epoch 773 head B head_i_epoch 1 batch 100: avg loss -1.760049 avg loss no lamb -1.760049 time 2019-02-26 08:22:23.731970
Model ind 640 epoch 773 head B head_i_epoch 1 batch 200: avg loss -1.968764 avg loss no lamb -1.968764 time 2019-02-26 08:24:19.928337
last batch sz 160
Pre: time 2019-02-26 08:26:03.335705: 
 	std: 0.051034052
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50906664, 0.6134, 0.6128833, 0.6131833, 0.5089]
	train_accs: [0.50906664, 0.6134, 0.6128833, 0.6131833, 0.5089]
	best_train_sub_head: 1
	worst: 0.5089
	avg: 0.5714866
	best: 0.6134

Starting e_i: 774
Model ind 640 epoch 774 head A head_i_epoch 0 batch 0: avg loss -3.128831 avg loss no lamb -3.128831 time 2019-02-26 08:26:05.715419
Model ind 640 epoch 774 head A head_i_epoch 0 batch 100: avg loss -3.126753 avg loss no lamb -3.126753 time 2019-02-26 08:27:57.480124
Model ind 640 epoch 774 head A head_i_epoch 0 batch 200: avg loss -3.199881 avg loss no lamb -3.199881 time 2019-02-26 08:29:55.156459
last batch sz 160
Model ind 640 epoch 774 head B head_i_epoch 0 batch 0: avg loss -1.752206 avg loss no lamb -1.752206 time 2019-02-26 08:31:17.964001
Model ind 640 epoch 774 head B head_i_epoch 0 batch 100: avg loss -1.625773 avg loss no lamb -1.625773 time 2019-02-26 08:33:11.326061
Model ind 640 epoch 774 head B head_i_epoch 0 batch 200: avg loss -1.943979 avg loss no lamb -1.943979 time 2019-02-26 08:35:08.983113
last batch sz 160
Model ind 640 epoch 774 head B head_i_epoch 1 batch 0: avg loss -1.803022 avg loss no lamb -1.803022 time 2019-02-26 08:36:31.746454
Model ind 640 epoch 774 head B head_i_epoch 1 batch 100: avg loss -1.724818 avg loss no lamb -1.724818 time 2019-02-26 08:38:23.800080
Model ind 640 epoch 774 head B head_i_epoch 1 batch 200: avg loss -1.863021 avg loss no lamb -1.863021 time 2019-02-26 08:52:31.241489
last batch sz 160
Pre: time 2019-02-26 08:54:13.888054: 
 	std: 0.051771346
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50845, 0.61405, 0.6141833, 0.6142, 0.50848335]
	train_accs: [0.50845, 0.61405, 0.6141833, 0.6142, 0.50848335]
	best_train_sub_head: 3
	worst: 0.50845
	avg: 0.5718733
	best: 0.6142

Starting e_i: 775
Model ind 640 epoch 775 head A head_i_epoch 0 batch 0: avg loss -3.224286 avg loss no lamb -3.224286 time 2019-02-26 08:54:16.957494
Model ind 640 epoch 775 head A head_i_epoch 0 batch 100: avg loss -3.113080 avg loss no lamb -3.113080 time 2019-02-26 08:56:12.566887
Model ind 640 epoch 775 head A head_i_epoch 0 batch 200: avg loss -3.170258 avg loss no lamb -3.170258 time 2019-02-26 08:58:06.364515
last batch sz 160
Model ind 640 epoch 775 head B head_i_epoch 0 batch 0: avg loss -1.865290 avg loss no lamb -1.865290 time 2019-02-26 08:59:27.249808
Model ind 640 epoch 775 head B head_i_epoch 0 batch 100: avg loss -1.809516 avg loss no lamb -1.809516 time 2019-02-26 09:01:52.511749
Model ind 640 epoch 775 head B head_i_epoch 0 batch 200: avg loss -1.957202 avg loss no lamb -1.957202 time 2019-02-26 09:04:11.285607
last batch sz 160
Model ind 640 epoch 775 head B head_i_epoch 1 batch 0: avg loss -1.787559 avg loss no lamb -1.787559 time 2019-02-26 09:05:56.773660
Model ind 640 epoch 775 head B head_i_epoch 1 batch 100: avg loss -1.676933 avg loss no lamb -1.676933 time 2019-02-26 09:08:19.295303
Model ind 640 epoch 775 head B head_i_epoch 1 batch 200: avg loss -1.908341 avg loss no lamb -1.908341 time 2019-02-26 09:10:39.399282
last batch sz 160
Pre: time 2019-02-26 09:12:51.810853: 
 	std: 0.050534654
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5085833, 0.6120833, 0.61198336, 0.61196667, 0.50913334]
	train_accs: [0.5085833, 0.6120833, 0.61198336, 0.61196667, 0.50913334]
	best_train_sub_head: 1
	worst: 0.5085833
	avg: 0.57075
	best: 0.6120833

Starting e_i: 776
Model ind 640 epoch 776 head A head_i_epoch 0 batch 0: avg loss -3.127934 avg loss no lamb -3.127934 time 2019-02-26 09:12:55.029206
Model ind 640 epoch 776 head A head_i_epoch 0 batch 100: avg loss -3.168020 avg loss no lamb -3.168020 time 2019-02-26 09:15:07.286941
Model ind 640 epoch 776 head A head_i_epoch 0 batch 200: avg loss -3.179502 avg loss no lamb -3.179502 time 2019-02-26 09:17:37.183314
last batch sz 160
Model ind 640 epoch 776 head B head_i_epoch 0 batch 0: avg loss -1.807868 avg loss no lamb -1.807868 time 2019-02-26 09:19:11.159984
Model ind 640 epoch 776 head B head_i_epoch 0 batch 100: avg loss -1.730341 avg loss no lamb -1.730341 time 2019-02-26 09:21:40.087027
Model ind 640 epoch 776 head B head_i_epoch 0 batch 200: avg loss -1.866538 avg loss no lamb -1.866538 time 2019-02-26 09:23:44.361680
last batch sz 160
Model ind 640 epoch 776 head B head_i_epoch 1 batch 0: avg loss -1.805696 avg loss no lamb -1.805696 time 2019-02-26 09:25:09.743066
Model ind 640 epoch 776 head B head_i_epoch 1 batch 100: avg loss -1.722796 avg loss no lamb -1.722796 time 2019-02-26 09:27:03.894094
Model ind 640 epoch 776 head B head_i_epoch 1 batch 200: avg loss -1.850035 avg loss no lamb -1.850035 time 2019-02-26 09:28:55.047549
last batch sz 160
Pre: time 2019-02-26 09:30:42.814219: 
 	std: 0.050745297
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51206666, 0.6156333, 0.6155667, 0.6155, 0.5119]
	train_accs: [0.51206666, 0.6156333, 0.6155667, 0.6155, 0.5119]
	best_train_sub_head: 1
	worst: 0.5119
	avg: 0.5741333
	best: 0.6156333

Starting e_i: 777
Model ind 640 epoch 777 head A head_i_epoch 0 batch 0: avg loss -3.135024 avg loss no lamb -3.135024 time 2019-02-26 09:30:45.847181
Model ind 640 epoch 777 head A head_i_epoch 0 batch 100: avg loss -3.051444 avg loss no lamb -3.051444 time 2019-02-26 09:32:38.514237
Model ind 640 epoch 777 head A head_i_epoch 0 batch 200: avg loss -3.176409 avg loss no lamb -3.176409 time 2019-02-26 09:34:36.731597
last batch sz 160
Model ind 640 epoch 777 head B head_i_epoch 0 batch 0: avg loss -1.926999 avg loss no lamb -1.926999 time 2019-02-26 09:36:00.116384
Model ind 640 epoch 777 head B head_i_epoch 0 batch 100: avg loss -1.789940 avg loss no lamb -1.789940 time 2019-02-26 09:37:51.963763
Model ind 640 epoch 777 head B head_i_epoch 0 batch 200: avg loss -1.928102 avg loss no lamb -1.928102 time 2019-02-26 09:39:48.156688
last batch sz 160
Model ind 640 epoch 777 head B head_i_epoch 1 batch 0: avg loss -1.793123 avg loss no lamb -1.793123 time 2019-02-26 09:41:11.119946
Model ind 640 epoch 777 head B head_i_epoch 1 batch 100: avg loss -1.702602 avg loss no lamb -1.702602 time 2019-02-26 09:43:02.176993
Model ind 640 epoch 777 head B head_i_epoch 1 batch 200: avg loss -1.995709 avg loss no lamb -1.995709 time 2019-02-26 09:44:59.877475
last batch sz 160
Pre: time 2019-02-26 09:46:42.705122: 
 	std: 0.05101209
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50948334, 0.61365, 0.6134833, 0.6138, 0.50955]
	train_accs: [0.50948334, 0.61365, 0.6134833, 0.6138, 0.50955]
	best_train_sub_head: 3
	worst: 0.50948334
	avg: 0.57199335
	best: 0.6138

Starting e_i: 778
Model ind 640 epoch 778 head A head_i_epoch 0 batch 0: avg loss -3.175224 avg loss no lamb -3.175224 time 2019-02-26 09:46:45.194316
Model ind 640 epoch 778 head A head_i_epoch 0 batch 100: avg loss -3.108975 avg loss no lamb -3.108975 time 2019-02-26 09:48:41.161611
Model ind 640 epoch 778 head A head_i_epoch 0 batch 200: avg loss -3.156496 avg loss no lamb -3.156496 time 2019-02-26 09:50:36.238193
last batch sz 160
Model ind 640 epoch 778 head B head_i_epoch 0 batch 0: avg loss -1.887229 avg loss no lamb -1.887229 time 2019-02-26 09:51:57.536457
Model ind 640 epoch 778 head B head_i_epoch 0 batch 100: avg loss -1.756082 avg loss no lamb -1.756082 time 2019-02-26 09:53:55.514789
Model ind 640 epoch 778 head B head_i_epoch 0 batch 200: avg loss -1.987171 avg loss no lamb -1.987171 time 2019-02-26 09:55:48.862840
last batch sz 160
Model ind 640 epoch 778 head B head_i_epoch 1 batch 0: avg loss -1.829424 avg loss no lamb -1.829424 time 2019-02-26 09:57:09.979674
Model ind 640 epoch 778 head B head_i_epoch 1 batch 100: avg loss -1.742082 avg loss no lamb -1.742082 time 2019-02-26 09:59:07.849813
Model ind 640 epoch 778 head B head_i_epoch 1 batch 200: avg loss -1.915721 avg loss no lamb -1.915721 time 2019-02-26 10:01:00.416604
last batch sz 160
Pre: time 2019-02-26 10:02:46.403050: 
 	std: 0.050967105
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50838333, 0.6123833, 0.61231667, 0.6124833, 0.5083333]
	train_accs: [0.50838333, 0.6123833, 0.61231667, 0.6124833, 0.5083333]
	best_train_sub_head: 3
	worst: 0.5083333
	avg: 0.57078
	best: 0.6124833

Starting e_i: 779
Model ind 640 epoch 779 head A head_i_epoch 0 batch 0: avg loss -3.196927 avg loss no lamb -3.196927 time 2019-02-26 10:02:50.501518
Model ind 640 epoch 779 head A head_i_epoch 0 batch 100: avg loss -3.091211 avg loss no lamb -3.091211 time 2019-02-26 10:04:46.156121
Model ind 640 epoch 779 head A head_i_epoch 0 batch 200: avg loss -3.176970 avg loss no lamb -3.176970 time 2019-02-26 10:06:37.069525
last batch sz 160
Model ind 640 epoch 779 head B head_i_epoch 0 batch 0: avg loss -1.871584 avg loss no lamb -1.871584 time 2019-02-26 10:08:04.130018
Model ind 640 epoch 779 head B head_i_epoch 0 batch 100: avg loss -1.743302 avg loss no lamb -1.743302 time 2019-02-26 10:09:57.736926
Model ind 640 epoch 779 head B head_i_epoch 0 batch 200: avg loss -1.892877 avg loss no lamb -1.892877 time 2019-02-26 10:11:49.416967
last batch sz 160
Model ind 640 epoch 779 head B head_i_epoch 1 batch 0: avg loss -1.840324 avg loss no lamb -1.840324 time 2019-02-26 10:13:14.768951
Model ind 640 epoch 779 head B head_i_epoch 1 batch 100: avg loss -1.796322 avg loss no lamb -1.796322 time 2019-02-26 10:15:06.968718
Model ind 640 epoch 779 head B head_i_epoch 1 batch 200: avg loss -1.837984 avg loss no lamb -1.837984 time 2019-02-26 10:17:01.055023
last batch sz 160
Pre: time 2019-02-26 10:18:47.614302: 
 	std: 0.051078696
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50951666, 0.6138333, 0.61385, 0.61368334, 0.50953335]
	train_accs: [0.50951666, 0.6138333, 0.61385, 0.61368334, 0.50953335]
	best_train_sub_head: 2
	worst: 0.50951666
	avg: 0.57208335
	best: 0.61385

Starting e_i: 780
Model ind 640 epoch 780 head A head_i_epoch 0 batch 0: avg loss -3.149795 avg loss no lamb -3.149795 time 2019-02-26 10:18:50.391654
Model ind 640 epoch 780 head A head_i_epoch 0 batch 100: avg loss -3.158202 avg loss no lamb -3.158202 time 2019-02-26 10:20:42.385852
Model ind 640 epoch 780 head A head_i_epoch 0 batch 200: avg loss -3.153289 avg loss no lamb -3.153289 time 2019-02-26 10:22:40.495486
last batch sz 160
Model ind 640 epoch 780 head B head_i_epoch 0 batch 0: avg loss -1.818464 avg loss no lamb -1.818464 time 2019-02-26 10:24:02.899600
Model ind 640 epoch 780 head B head_i_epoch 0 batch 100: avg loss -1.731222 avg loss no lamb -1.731222 time 2019-02-26 10:25:55.364317
Model ind 640 epoch 780 head B head_i_epoch 0 batch 200: avg loss -1.932246 avg loss no lamb -1.932246 time 2019-02-26 10:27:53.405763
last batch sz 160
Model ind 640 epoch 780 head B head_i_epoch 1 batch 0: avg loss -1.800833 avg loss no lamb -1.800833 time 2019-02-26 10:29:14.222418
Model ind 640 epoch 780 head B head_i_epoch 1 batch 100: avg loss -1.760510 avg loss no lamb -1.760510 time 2019-02-26 10:31:08.585852
Model ind 640 epoch 780 head B head_i_epoch 1 batch 200: avg loss -1.830644 avg loss no lamb -1.830644 time 2019-02-26 10:33:05.106704
last batch sz 160
Pre: time 2019-02-26 10:34:47.406311: 
 	std: 0.05101478
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50963336, 0.61371666, 0.61371666, 0.61396664, 0.5097]
	train_accs: [0.50963336, 0.61371666, 0.61371666, 0.61396664, 0.5097]
	best_train_sub_head: 3
	worst: 0.50963336
	avg: 0.5721467
	best: 0.61396664

Starting e_i: 781
Model ind 640 epoch 781 head A head_i_epoch 0 batch 0: avg loss -3.203655 avg loss no lamb -3.203655 time 2019-02-26 10:34:54.099466
Model ind 640 epoch 781 head A head_i_epoch 0 batch 100: avg loss -3.042711 avg loss no lamb -3.042711 time 2019-02-26 10:36:52.482444
Model ind 640 epoch 781 head A head_i_epoch 0 batch 200: avg loss -3.232447 avg loss no lamb -3.232447 time 2019-02-26 10:38:45.047587
last batch sz 160
Model ind 640 epoch 781 head B head_i_epoch 0 batch 0: avg loss -1.955462 avg loss no lamb -1.955462 time 2019-02-26 10:40:08.295904
Model ind 640 epoch 781 head B head_i_epoch 0 batch 100: avg loss -1.720860 avg loss no lamb -1.720860 time 2019-02-26 10:42:04.154640
Model ind 640 epoch 781 head B head_i_epoch 0 batch 200: avg loss -1.918261 avg loss no lamb -1.918261 time 2019-02-26 10:43:55.154358
last batch sz 160
Model ind 640 epoch 781 head B head_i_epoch 1 batch 0: avg loss -1.830685 avg loss no lamb -1.830685 time 2019-02-26 10:45:21.319800
Model ind 640 epoch 781 head B head_i_epoch 1 batch 100: avg loss -1.785448 avg loss no lamb -1.785448 time 2019-02-26 10:47:15.074503
Model ind 640 epoch 781 head B head_i_epoch 1 batch 200: avg loss -1.914618 avg loss no lamb -1.914618 time 2019-02-26 10:49:05.906627
last batch sz 160
Pre: time 2019-02-26 10:50:53.079703: 
 	std: 0.051439445
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5079, 0.6131167, 0.6130667, 0.6131167, 0.5083]
	train_accs: [0.5079, 0.6131167, 0.6130667, 0.6131167, 0.5083]
	best_train_sub_head: 1
	worst: 0.5079
	avg: 0.57110006
	best: 0.6131167

Starting e_i: 782
Model ind 640 epoch 782 head A head_i_epoch 0 batch 0: avg loss -3.123744 avg loss no lamb -3.123744 time 2019-02-26 10:50:55.787130
Model ind 640 epoch 782 head A head_i_epoch 0 batch 100: avg loss -3.072449 avg loss no lamb -3.072449 time 2019-02-26 10:52:47.965718
Model ind 640 epoch 782 head A head_i_epoch 0 batch 200: avg loss -3.162376 avg loss no lamb -3.162376 time 2019-02-26 10:54:43.631786
last batch sz 160
Model ind 640 epoch 782 head B head_i_epoch 0 batch 0: avg loss -1.879885 avg loss no lamb -1.879885 time 2019-02-26 10:56:06.955629
Model ind 640 epoch 782 head B head_i_epoch 0 batch 100: avg loss -1.757937 avg loss no lamb -1.757937 time 2019-02-26 10:57:58.107125
Model ind 640 epoch 782 head B head_i_epoch 0 batch 200: avg loss -1.894637 avg loss no lamb -1.894637 time 2019-02-26 10:59:56.168848
last batch sz 160
Model ind 640 epoch 782 head B head_i_epoch 1 batch 0: avg loss -1.837350 avg loss no lamb -1.837350 time 2019-02-26 11:01:18.247214
Model ind 640 epoch 782 head B head_i_epoch 1 batch 100: avg loss -1.704767 avg loss no lamb -1.704767 time 2019-02-26 11:03:10.397939
Model ind 640 epoch 782 head B head_i_epoch 1 batch 200: avg loss -1.860720 avg loss no lamb -1.860720 time 2019-02-26 11:05:07.521290
last batch sz 160
Pre: time 2019-02-26 11:06:49.569935: 
 	std: 0.050365746
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51055, 0.6134, 0.6135167, 0.61363333, 0.51086664]
	train_accs: [0.51055, 0.6134, 0.6135167, 0.61363333, 0.51086664]
	best_train_sub_head: 3
	worst: 0.51055
	avg: 0.5723933
	best: 0.61363333

Starting e_i: 783
Model ind 640 epoch 783 head A head_i_epoch 0 batch 0: avg loss -3.195319 avg loss no lamb -3.195319 time 2019-02-26 11:06:52.435951
Model ind 640 epoch 783 head A head_i_epoch 0 batch 100: avg loss -3.117137 avg loss no lamb -3.117137 time 2019-02-26 11:08:49.955794
Model ind 640 epoch 783 head A head_i_epoch 0 batch 200: avg loss -3.261041 avg loss no lamb -3.261041 time 2019-02-26 11:10:42.765382
last batch sz 160
Model ind 640 epoch 783 head B head_i_epoch 0 batch 0: avg loss -1.882562 avg loss no lamb -1.882562 time 2019-02-26 11:12:03.498244
Model ind 640 epoch 783 head B head_i_epoch 0 batch 100: avg loss -1.768306 avg loss no lamb -1.768306 time 2019-02-26 11:14:00.828637
Model ind 640 epoch 783 head B head_i_epoch 0 batch 200: avg loss -1.928577 avg loss no lamb -1.928577 time 2019-02-26 11:15:53.978611
last batch sz 160
Model ind 640 epoch 783 head B head_i_epoch 1 batch 0: avg loss -1.876738 avg loss no lamb -1.876738 time 2019-02-26 11:17:15.901619
Model ind 640 epoch 783 head B head_i_epoch 1 batch 100: avg loss -1.815920 avg loss no lamb -1.815920 time 2019-02-26 11:19:12.144962
Model ind 640 epoch 783 head B head_i_epoch 1 batch 200: avg loss -1.854199 avg loss no lamb -1.854199 time 2019-02-26 11:21:03.806649
last batch sz 160
Pre: time 2019-02-26 11:22:50.611589: 
 	std: 0.050531723
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51093334, 0.6142167, 0.6141833, 0.61431664, 0.51125]
	train_accs: [0.51093334, 0.6142167, 0.6141833, 0.61431664, 0.51125]
	best_train_sub_head: 3
	worst: 0.51093334
	avg: 0.57298
	best: 0.61431664

Starting e_i: 784
Model ind 640 epoch 784 head A head_i_epoch 0 batch 0: avg loss -3.145326 avg loss no lamb -3.145326 time 2019-02-26 11:22:53.691899
Model ind 640 epoch 784 head A head_i_epoch 0 batch 100: avg loss -3.090251 avg loss no lamb -3.090251 time 2019-02-26 11:24:47.894932
Model ind 640 epoch 784 head A head_i_epoch 0 batch 200: avg loss -3.237308 avg loss no lamb -3.237308 time 2019-02-26 11:26:39.513953
last batch sz 160
Model ind 640 epoch 784 head B head_i_epoch 0 batch 0: avg loss -1.855173 avg loss no lamb -1.855173 time 2019-02-26 11:28:07.454466
Model ind 640 epoch 784 head B head_i_epoch 0 batch 100: avg loss -1.817869 avg loss no lamb -1.817869 time 2019-02-26 11:30:00.686161
Model ind 640 epoch 784 head B head_i_epoch 0 batch 200: avg loss -1.900329 avg loss no lamb -1.900329 time 2019-02-26 11:31:59.820299
last batch sz 160
Model ind 640 epoch 784 head B head_i_epoch 1 batch 0: avg loss -1.823687 avg loss no lamb -1.823687 time 2019-02-26 11:33:24.603964
Model ind 640 epoch 784 head B head_i_epoch 1 batch 100: avg loss -1.752952 avg loss no lamb -1.752952 time 2019-02-26 11:35:16.017700
Model ind 640 epoch 784 head B head_i_epoch 1 batch 200: avg loss -1.950707 avg loss no lamb -1.950707 time 2019-02-26 11:37:10.275800
last batch sz 160
Pre: time 2019-02-26 11:38:52.433844: 
 	std: 0.051731907
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5085, 0.6142333, 0.61408335, 0.61405, 0.50855]
	train_accs: [0.5085, 0.6142333, 0.61408335, 0.61405, 0.50855]
	best_train_sub_head: 1
	worst: 0.5085
	avg: 0.5718833
	best: 0.6142333

Starting e_i: 785
Model ind 640 epoch 785 head A head_i_epoch 0 batch 0: avg loss -3.131981 avg loss no lamb -3.131981 time 2019-02-26 11:38:55.418523
Model ind 640 epoch 785 head A head_i_epoch 0 batch 100: avg loss -3.133025 avg loss no lamb -3.133025 time 2019-02-26 11:40:46.299516
Model ind 640 epoch 785 head A head_i_epoch 0 batch 200: avg loss -3.173621 avg loss no lamb -3.173621 time 2019-02-26 11:42:36.197163
last batch sz 160
Model ind 640 epoch 785 head B head_i_epoch 0 batch 0: avg loss -1.897580 avg loss no lamb -1.897580 time 2019-02-26 11:43:56.432369
Model ind 640 epoch 785 head B head_i_epoch 0 batch 100: avg loss -1.797408 avg loss no lamb -1.797408 time 2019-02-26 11:45:46.856365
Model ind 640 epoch 785 head B head_i_epoch 0 batch 200: avg loss -1.965545 avg loss no lamb -1.965545 time 2019-02-26 11:47:37.225043
last batch sz 160
Model ind 640 epoch 785 head B head_i_epoch 1 batch 0: avg loss -1.855743 avg loss no lamb -1.855743 time 2019-02-26 11:48:57.427411
Model ind 640 epoch 785 head B head_i_epoch 1 batch 100: avg loss -1.769259 avg loss no lamb -1.769259 time 2019-02-26 11:50:47.954868
Model ind 640 epoch 785 head B head_i_epoch 1 batch 200: avg loss -1.949697 avg loss no lamb -1.949697 time 2019-02-26 11:52:37.934878
last batch sz 160
Pre: time 2019-02-26 11:54:18.939410: 
 	std: 0.05058079
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51213336, 0.6156333, 0.61525, 0.61553335, 0.51231664]
	train_accs: [0.51213336, 0.6156333, 0.61525, 0.61553335, 0.51231664]
	best_train_sub_head: 1
	worst: 0.51213336
	avg: 0.57417333
	best: 0.6156333

Starting e_i: 786
Model ind 640 epoch 786 head A head_i_epoch 0 batch 0: avg loss -3.188076 avg loss no lamb -3.188076 time 2019-02-26 11:54:21.412233
Model ind 640 epoch 786 head A head_i_epoch 0 batch 100: avg loss -3.073348 avg loss no lamb -3.073348 time 2019-02-26 11:56:12.435955
Model ind 640 epoch 786 head A head_i_epoch 0 batch 200: avg loss -3.158120 avg loss no lamb -3.158120 time 2019-02-26 11:58:03.226630
last batch sz 160
Model ind 640 epoch 786 head B head_i_epoch 0 batch 0: avg loss -1.939885 avg loss no lamb -1.939885 time 2019-02-26 11:59:24.108504
Model ind 640 epoch 786 head B head_i_epoch 0 batch 100: avg loss -1.766942 avg loss no lamb -1.766942 time 2019-02-26 12:01:15.791892
Model ind 640 epoch 786 head B head_i_epoch 0 batch 200: avg loss -1.960810 avg loss no lamb -1.960810 time 2019-02-26 12:03:06.854621
last batch sz 160
Model ind 640 epoch 786 head B head_i_epoch 1 batch 0: avg loss -1.872322 avg loss no lamb -1.872322 time 2019-02-26 12:04:27.587332
Model ind 640 epoch 786 head B head_i_epoch 1 batch 100: avg loss -1.862426 avg loss no lamb -1.862426 time 2019-02-26 12:06:18.748592
Model ind 640 epoch 786 head B head_i_epoch 1 batch 200: avg loss -1.971017 avg loss no lamb -1.971017 time 2019-02-26 12:08:10.290077
last batch sz 160
Pre: time 2019-02-26 12:09:51.845760: 
 	std: 0.0502965
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5108167, 0.61366665, 0.6138833, 0.61345, 0.5111833]
	train_accs: [0.5108167, 0.61366665, 0.6138833, 0.61345, 0.5111833]
	best_train_sub_head: 2
	worst: 0.5108167
	avg: 0.5726
	best: 0.6138833

Starting e_i: 787
Model ind 640 epoch 787 head A head_i_epoch 0 batch 0: avg loss -3.078949 avg loss no lamb -3.078949 time 2019-02-26 12:09:54.766403
Model ind 640 epoch 787 head A head_i_epoch 0 batch 100: avg loss -3.084697 avg loss no lamb -3.084697 time 2019-02-26 12:11:45.681620
Model ind 640 epoch 787 head A head_i_epoch 0 batch 200: avg loss -3.193157 avg loss no lamb -3.193157 time 2019-02-26 12:13:35.883855
last batch sz 160
Model ind 640 epoch 787 head B head_i_epoch 0 batch 0: avg loss -1.832053 avg loss no lamb -1.832053 time 2019-02-26 12:14:56.634031
Model ind 640 epoch 787 head B head_i_epoch 0 batch 100: avg loss -1.774524 avg loss no lamb -1.774524 time 2019-02-26 12:16:47.662228
Model ind 640 epoch 787 head B head_i_epoch 0 batch 200: avg loss -1.834908 avg loss no lamb -1.834908 time 2019-02-26 12:18:38.851941
last batch sz 160
Model ind 640 epoch 787 head B head_i_epoch 1 batch 0: avg loss -1.811530 avg loss no lamb -1.811530 time 2019-02-26 12:19:59.770468
Model ind 640 epoch 787 head B head_i_epoch 1 batch 100: avg loss -1.763860 avg loss no lamb -1.763860 time 2019-02-26 12:21:50.190704
Model ind 640 epoch 787 head B head_i_epoch 1 batch 200: avg loss -1.810497 avg loss no lamb -1.810497 time 2019-02-26 12:23:40.568275
last batch sz 160
Pre: time 2019-02-26 12:25:21.771687: 
 	std: 0.050380155
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101167, 0.6134, 0.6134833, 0.61345, 0.5111]
	train_accs: [0.5101167, 0.6134, 0.6134833, 0.61345, 0.5111]
	best_train_sub_head: 2
	worst: 0.5101167
	avg: 0.57231003
	best: 0.6134833

Starting e_i: 788
Model ind 640 epoch 788 head A head_i_epoch 0 batch 0: avg loss -3.244263 avg loss no lamb -3.244263 time 2019-02-26 12:25:24.173948
Model ind 640 epoch 788 head A head_i_epoch 0 batch 100: avg loss -3.112055 avg loss no lamb -3.112055 time 2019-02-26 12:27:15.659686
Model ind 640 epoch 788 head A head_i_epoch 0 batch 200: avg loss -3.173797 avg loss no lamb -3.173797 time 2019-02-26 12:29:06.534734
last batch sz 160
Model ind 640 epoch 788 head B head_i_epoch 0 batch 0: avg loss -1.858236 avg loss no lamb -1.858236 time 2019-02-26 12:30:25.910974
Model ind 640 epoch 788 head B head_i_epoch 0 batch 100: avg loss -1.758064 avg loss no lamb -1.758064 time 2019-02-26 12:32:19.108389
Model ind 640 epoch 788 head B head_i_epoch 0 batch 200: avg loss -1.899523 avg loss no lamb -1.899523 time 2019-02-26 12:34:10.331179
last batch sz 160
Model ind 640 epoch 788 head B head_i_epoch 1 batch 0: avg loss -1.815125 avg loss no lamb -1.815125 time 2019-02-26 12:35:31.239106
Model ind 640 epoch 788 head B head_i_epoch 1 batch 100: avg loss -1.723367 avg loss no lamb -1.723367 time 2019-02-26 12:37:21.352420
Model ind 640 epoch 788 head B head_i_epoch 1 batch 200: avg loss -1.858958 avg loss no lamb -1.858958 time 2019-02-26 12:39:12.463063
last batch sz 160
Pre: time 2019-02-26 12:40:53.933627: 
 	std: 0.050530422
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50921667, 0.61286664, 0.61291665, 0.6131167, 0.5104333]
	train_accs: [0.50921667, 0.61286664, 0.61291665, 0.6131167, 0.5104333]
	best_train_sub_head: 3
	worst: 0.50921667
	avg: 0.57171
	best: 0.6131167

Starting e_i: 789
Model ind 640 epoch 789 head A head_i_epoch 0 batch 0: avg loss -3.126971 avg loss no lamb -3.126971 time 2019-02-26 12:40:56.972018
Model ind 640 epoch 789 head A head_i_epoch 0 batch 100: avg loss -3.113951 avg loss no lamb -3.113951 time 2019-02-26 12:42:48.348128
Model ind 640 epoch 789 head A head_i_epoch 0 batch 200: avg loss -3.258131 avg loss no lamb -3.258131 time 2019-02-26 12:44:39.629533
last batch sz 160
Model ind 640 epoch 789 head B head_i_epoch 0 batch 0: avg loss -1.918674 avg loss no lamb -1.918674 time 2019-02-26 12:45:59.406605
Model ind 640 epoch 789 head B head_i_epoch 0 batch 100: avg loss -1.786440 avg loss no lamb -1.786440 time 2019-02-26 12:47:50.014873
Model ind 640 epoch 789 head B head_i_epoch 0 batch 200: avg loss -1.904770 avg loss no lamb -1.904770 time 2019-02-26 12:49:41.158461
last batch sz 160
Model ind 640 epoch 789 head B head_i_epoch 1 batch 0: avg loss -1.861238 avg loss no lamb -1.861238 time 2019-02-26 12:51:01.269461
Model ind 640 epoch 789 head B head_i_epoch 1 batch 100: avg loss -1.786352 avg loss no lamb -1.786352 time 2019-02-26 12:52:54.211682
Model ind 640 epoch 789 head B head_i_epoch 1 batch 200: avg loss -1.922183 avg loss no lamb -1.922183 time 2019-02-26 12:54:46.001844
last batch sz 160
Pre: time 2019-02-26 12:56:28.725365: 
 	std: 0.050561804
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51165, 0.61508334, 0.6149667, 0.6152, 0.5121]
	train_accs: [0.51165, 0.61508334, 0.6149667, 0.6152, 0.5121]
	best_train_sub_head: 3
	worst: 0.51165
	avg: 0.57379997
	best: 0.6152

Starting e_i: 790
Model ind 640 epoch 790 head A head_i_epoch 0 batch 0: avg loss -3.182833 avg loss no lamb -3.182833 time 2019-02-26 12:56:31.187099
Model ind 640 epoch 790 head A head_i_epoch 0 batch 100: avg loss -3.123279 avg loss no lamb -3.123279 time 2019-02-26 12:58:21.675774
Model ind 640 epoch 790 head A head_i_epoch 0 batch 200: avg loss -3.211950 avg loss no lamb -3.211950 time 2019-02-26 13:00:12.841096
last batch sz 160
Model ind 640 epoch 790 head B head_i_epoch 0 batch 0: avg loss -1.878864 avg loss no lamb -1.878864 time 2019-02-26 13:01:33.637612
Model ind 640 epoch 790 head B head_i_epoch 0 batch 100: avg loss -1.792335 avg loss no lamb -1.792335 time 2019-02-26 13:03:24.164826
Model ind 640 epoch 790 head B head_i_epoch 0 batch 200: avg loss -1.855378 avg loss no lamb -1.855378 time 2019-02-26 13:05:14.793199
last batch sz 160
Model ind 640 epoch 790 head B head_i_epoch 1 batch 0: avg loss -1.761610 avg loss no lamb -1.761610 time 2019-02-26 13:06:35.131099
Model ind 640 epoch 790 head B head_i_epoch 1 batch 100: avg loss -1.759997 avg loss no lamb -1.759997 time 2019-02-26 13:08:25.453946
Model ind 640 epoch 790 head B head_i_epoch 1 batch 200: avg loss -1.888990 avg loss no lamb -1.888990 time 2019-02-26 13:10:16.091852
last batch sz 160
Pre: time 2019-02-26 13:11:56.833345: 
 	std: 0.051776964
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5085833, 0.61395, 0.6141667, 0.6143, 0.5083167]
	train_accs: [0.5085833, 0.61395, 0.6141667, 0.6143, 0.5083167]
	best_train_sub_head: 3
	worst: 0.5083167
	avg: 0.57186335
	best: 0.6143

Starting e_i: 791
Model ind 640 epoch 791 head A head_i_epoch 0 batch 0: avg loss -3.205348 avg loss no lamb -3.205348 time 2019-02-26 13:12:04.359533
Model ind 640 epoch 791 head A head_i_epoch 0 batch 100: avg loss -3.102412 avg loss no lamb -3.102412 time 2019-02-26 13:13:55.865877
Model ind 640 epoch 791 head A head_i_epoch 0 batch 200: avg loss -3.150194 avg loss no lamb -3.150194 time 2019-02-26 13:15:45.847793
last batch sz 160
Model ind 640 epoch 791 head B head_i_epoch 0 batch 0: avg loss -1.887220 avg loss no lamb -1.887220 time 2019-02-26 13:17:06.466410
Model ind 640 epoch 791 head B head_i_epoch 0 batch 100: avg loss -1.715135 avg loss no lamb -1.715135 time 2019-02-26 13:18:56.856315
Model ind 640 epoch 791 head B head_i_epoch 0 batch 200: avg loss -1.929473 avg loss no lamb -1.929473 time 2019-02-26 13:20:47.368535
last batch sz 160
Model ind 640 epoch 791 head B head_i_epoch 1 batch 0: avg loss -1.850513 avg loss no lamb -1.850513 time 2019-02-26 13:22:08.022947
Model ind 640 epoch 791 head B head_i_epoch 1 batch 100: avg loss -1.702387 avg loss no lamb -1.702387 time 2019-02-26 13:23:57.851063
Model ind 640 epoch 791 head B head_i_epoch 1 batch 200: avg loss -1.844402 avg loss no lamb -1.844402 time 2019-02-26 13:25:48.026556
last batch sz 160
Pre: time 2019-02-26 13:27:29.401231: 
 	std: 0.050564375
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51125, 0.61425, 0.61435, 0.6145167, 0.5110667]
	train_accs: [0.51125, 0.61425, 0.61435, 0.6145167, 0.5110667]
	best_train_sub_head: 3
	worst: 0.5110667
	avg: 0.5730867
	best: 0.6145167

Starting e_i: 792
Model ind 640 epoch 792 head A head_i_epoch 0 batch 0: avg loss -3.139590 avg loss no lamb -3.139590 time 2019-02-26 13:27:32.033573
Model ind 640 epoch 792 head A head_i_epoch 0 batch 100: avg loss -3.046326 avg loss no lamb -3.046326 time 2019-02-26 13:29:22.437786
Model ind 640 epoch 792 head A head_i_epoch 0 batch 200: avg loss -3.171956 avg loss no lamb -3.171956 time 2019-02-26 13:31:15.186106
last batch sz 160
Model ind 640 epoch 792 head B head_i_epoch 0 batch 0: avg loss -1.807129 avg loss no lamb -1.807129 time 2019-02-26 13:32:36.137433
Model ind 640 epoch 792 head B head_i_epoch 0 batch 100: avg loss -1.779562 avg loss no lamb -1.779562 time 2019-02-26 13:34:26.710195
Model ind 640 epoch 792 head B head_i_epoch 0 batch 200: avg loss -1.888165 avg loss no lamb -1.888165 time 2019-02-26 13:36:17.216964
last batch sz 160
Model ind 640 epoch 792 head B head_i_epoch 1 batch 0: avg loss -1.779512 avg loss no lamb -1.779512 time 2019-02-26 13:37:38.002776
Model ind 640 epoch 792 head B head_i_epoch 1 batch 100: avg loss -1.782232 avg loss no lamb -1.782232 time 2019-02-26 13:39:29.432264
Model ind 640 epoch 792 head B head_i_epoch 1 batch 200: avg loss -1.890389 avg loss no lamb -1.890389 time 2019-02-26 13:41:19.783899
last batch sz 160
Pre: time 2019-02-26 13:43:01.831060: 
 	std: 0.05186273
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50703335, 0.61291665, 0.61303335, 0.61326665, 0.50738335]
	train_accs: [0.50703335, 0.61291665, 0.61303335, 0.61326665, 0.50738335]
	best_train_sub_head: 3
	worst: 0.50703335
	avg: 0.5707267
	best: 0.61326665

Starting e_i: 793
Model ind 640 epoch 793 head A head_i_epoch 0 batch 0: avg loss -3.177570 avg loss no lamb -3.177570 time 2019-02-26 13:43:04.881698
Model ind 640 epoch 793 head A head_i_epoch 0 batch 100: avg loss -3.148899 avg loss no lamb -3.148899 time 2019-02-26 13:44:55.573399
Model ind 640 epoch 793 head A head_i_epoch 0 batch 200: avg loss -3.127406 avg loss no lamb -3.127406 time 2019-02-26 13:46:45.980464
last batch sz 160
Model ind 640 epoch 793 head B head_i_epoch 0 batch 0: avg loss -1.807990 avg loss no lamb -1.807990 time 2019-02-26 13:48:06.702691
Model ind 640 epoch 793 head B head_i_epoch 0 batch 100: avg loss -1.807271 avg loss no lamb -1.807271 time 2019-02-26 13:49:56.976983
Model ind 640 epoch 793 head B head_i_epoch 0 batch 200: avg loss -1.850772 avg loss no lamb -1.850772 time 2019-02-26 13:51:47.329469
last batch sz 160
Model ind 640 epoch 793 head B head_i_epoch 1 batch 0: avg loss -1.827186 avg loss no lamb -1.827186 time 2019-02-26 13:53:07.810354
Model ind 640 epoch 793 head B head_i_epoch 1 batch 100: avg loss -1.764159 avg loss no lamb -1.764159 time 2019-02-26 13:54:58.043851
Model ind 640 epoch 793 head B head_i_epoch 1 batch 200: avg loss -1.872143 avg loss no lamb -1.872143 time 2019-02-26 13:56:48.737719
last batch sz 160
Pre: time 2019-02-26 13:58:30.020232: 
 	std: 0.050544642
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50885, 0.6123, 0.61225, 0.61266667, 0.5096167]
	train_accs: [0.50885, 0.6123, 0.61225, 0.61266667, 0.5096167]
	best_train_sub_head: 3
	worst: 0.50885
	avg: 0.57113665
	best: 0.61266667

Starting e_i: 794
Model ind 640 epoch 794 head A head_i_epoch 0 batch 0: avg loss -3.124343 avg loss no lamb -3.124343 time 2019-02-26 13:58:32.389275
Model ind 640 epoch 794 head A head_i_epoch 0 batch 100: avg loss -3.069970 avg loss no lamb -3.069970 time 2019-02-26 14:00:22.940405
Model ind 640 epoch 794 head A head_i_epoch 0 batch 200: avg loss -3.118780 avg loss no lamb -3.118780 time 2019-02-26 14:02:13.637186
last batch sz 160
Model ind 640 epoch 794 head B head_i_epoch 0 batch 0: avg loss -1.839286 avg loss no lamb -1.839286 time 2019-02-26 14:03:33.939342
Model ind 640 epoch 794 head B head_i_epoch 0 batch 100: avg loss -1.814953 avg loss no lamb -1.814953 time 2019-02-26 14:05:24.640450
Model ind 640 epoch 794 head B head_i_epoch 0 batch 200: avg loss -1.912362 avg loss no lamb -1.912362 time 2019-02-26 14:07:15.378986
last batch sz 160
Model ind 640 epoch 794 head B head_i_epoch 1 batch 0: avg loss -1.751554 avg loss no lamb -1.751554 time 2019-02-26 14:08:35.335310
Model ind 640 epoch 794 head B head_i_epoch 1 batch 100: avg loss -1.792276 avg loss no lamb -1.792276 time 2019-02-26 14:10:28.660191
Model ind 640 epoch 794 head B head_i_epoch 1 batch 200: avg loss -1.898962 avg loss no lamb -1.898962 time 2019-02-26 14:12:19.605990
last batch sz 160
Pre: time 2019-02-26 14:14:01.270405: 
 	std: 0.050906107
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50981665, 0.61338335, 0.6135167, 0.61368334, 0.50941664]
	train_accs: [0.50981665, 0.61338335, 0.6135167, 0.61368334, 0.50941664]
	best_train_sub_head: 3
	worst: 0.50941664
	avg: 0.5719633
	best: 0.61368334

Starting e_i: 795
Model ind 640 epoch 795 head A head_i_epoch 0 batch 0: avg loss -3.122398 avg loss no lamb -3.122398 time 2019-02-26 14:14:04.145968
Model ind 640 epoch 795 head A head_i_epoch 0 batch 100: avg loss -3.091500 avg loss no lamb -3.091500 time 2019-02-26 14:15:55.266999
Model ind 640 epoch 795 head A head_i_epoch 0 batch 200: avg loss -3.093580 avg loss no lamb -3.093580 time 2019-02-26 14:17:45.607388
last batch sz 160
Model ind 640 epoch 795 head B head_i_epoch 0 batch 0: avg loss -1.833804 avg loss no lamb -1.833804 time 2019-02-26 14:19:05.486096
Model ind 640 epoch 795 head B head_i_epoch 0 batch 100: avg loss -1.765257 avg loss no lamb -1.765257 time 2019-02-26 14:20:56.049865
Model ind 640 epoch 795 head B head_i_epoch 0 batch 200: avg loss -1.832252 avg loss no lamb -1.832252 time 2019-02-26 14:22:46.406706
last batch sz 160
Model ind 640 epoch 795 head B head_i_epoch 1 batch 0: avg loss -1.835282 avg loss no lamb -1.835282 time 2019-02-26 14:24:06.984690
Model ind 640 epoch 795 head B head_i_epoch 1 batch 100: avg loss -1.755565 avg loss no lamb -1.755565 time 2019-02-26 14:25:57.747799
Model ind 640 epoch 795 head B head_i_epoch 1 batch 200: avg loss -1.839034 avg loss no lamb -1.839034 time 2019-02-26 14:27:48.299856
last batch sz 160
Pre: time 2019-02-26 14:29:27.973444: 
 	std: 0.051445182
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50911665, 0.61443335, 0.6145167, 0.61443335, 0.5097833]
	train_accs: [0.50911665, 0.61443335, 0.6145167, 0.61443335, 0.5097833]
	best_train_sub_head: 2
	worst: 0.50911665
	avg: 0.57245666
	best: 0.6145167

Starting e_i: 796
Model ind 640 epoch 796 head A head_i_epoch 0 batch 0: avg loss -3.157666 avg loss no lamb -3.157666 time 2019-02-26 14:29:30.287293
Model ind 640 epoch 796 head A head_i_epoch 0 batch 100: avg loss -3.051866 avg loss no lamb -3.051866 time 2019-02-26 14:31:24.833947
Model ind 640 epoch 796 head A head_i_epoch 0 batch 200: avg loss -3.253730 avg loss no lamb -3.253730 time 2019-02-26 14:33:15.428890
last batch sz 160
Model ind 640 epoch 796 head B head_i_epoch 0 batch 0: avg loss -1.783660 avg loss no lamb -1.783660 time 2019-02-26 14:34:35.959975
Model ind 640 epoch 796 head B head_i_epoch 0 batch 100: avg loss -1.724338 avg loss no lamb -1.724338 time 2019-02-26 14:36:27.422492
Model ind 640 epoch 796 head B head_i_epoch 0 batch 200: avg loss -1.890108 avg loss no lamb -1.890108 time 2019-02-26 14:38:18.373420
last batch sz 160
Model ind 640 epoch 796 head B head_i_epoch 1 batch 0: avg loss -1.786796 avg loss no lamb -1.786796 time 2019-02-26 14:39:39.515574
Model ind 640 epoch 796 head B head_i_epoch 1 batch 100: avg loss -1.829497 avg loss no lamb -1.829497 time 2019-02-26 14:41:31.149183
Model ind 640 epoch 796 head B head_i_epoch 1 batch 200: avg loss -1.861693 avg loss no lamb -1.861693 time 2019-02-26 14:43:21.598638
last batch sz 160
Pre: time 2019-02-26 14:45:03.327898: 
 	std: 0.050921086
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50806665, 0.61231667, 0.61226666, 0.61221665, 0.5085833]
	train_accs: [0.50806665, 0.61231667, 0.61226666, 0.61221665, 0.5085833]
	best_train_sub_head: 1
	worst: 0.50806665
	avg: 0.57069
	best: 0.61231667

Starting e_i: 797
Model ind 640 epoch 797 head A head_i_epoch 0 batch 0: avg loss -3.223512 avg loss no lamb -3.223512 time 2019-02-26 14:45:06.289700
Model ind 640 epoch 797 head A head_i_epoch 0 batch 100: avg loss -3.090308 avg loss no lamb -3.090308 time 2019-02-26 14:46:56.639528
Model ind 640 epoch 797 head A head_i_epoch 0 batch 200: avg loss -3.215118 avg loss no lamb -3.215118 time 2019-02-26 14:48:47.068288
last batch sz 160
Model ind 640 epoch 797 head B head_i_epoch 0 batch 0: avg loss -1.799675 avg loss no lamb -1.799675 time 2019-02-26 14:50:07.537372
Model ind 640 epoch 797 head B head_i_epoch 0 batch 100: avg loss -1.834201 avg loss no lamb -1.834201 time 2019-02-26 14:51:57.987573
Model ind 640 epoch 797 head B head_i_epoch 0 batch 200: avg loss -1.838755 avg loss no lamb -1.838755 time 2019-02-26 14:53:48.771429
last batch sz 160
Model ind 640 epoch 797 head B head_i_epoch 1 batch 0: avg loss -1.859795 avg loss no lamb -1.859795 time 2019-02-26 14:55:08.893307
Model ind 640 epoch 797 head B head_i_epoch 1 batch 100: avg loss -1.724200 avg loss no lamb -1.724200 time 2019-02-26 14:56:59.555404
Model ind 640 epoch 797 head B head_i_epoch 1 batch 200: avg loss -1.873550 avg loss no lamb -1.873550 time 2019-02-26 14:58:49.563952
last batch sz 160
Pre: time 2019-02-26 15:00:31.080297: 
 	std: 0.051773444
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50703335, 0.6131833, 0.6128167, 0.61326665, 0.50778335]
	train_accs: [0.50703335, 0.6131833, 0.6128167, 0.61326665, 0.50778335]
	best_train_sub_head: 3
	worst: 0.50703335
	avg: 0.57081664
	best: 0.61326665

Starting e_i: 798
Model ind 640 epoch 798 head A head_i_epoch 0 batch 0: avg loss -3.105151 avg loss no lamb -3.105151 time 2019-02-26 15:00:33.678001
Model ind 640 epoch 798 head A head_i_epoch 0 batch 100: avg loss -3.094444 avg loss no lamb -3.094444 time 2019-02-26 15:02:24.881125
Model ind 640 epoch 798 head A head_i_epoch 0 batch 200: avg loss -3.142500 avg loss no lamb -3.142500 time 2019-02-26 15:04:15.661623
last batch sz 160
Model ind 640 epoch 798 head B head_i_epoch 0 batch 0: avg loss -1.864400 avg loss no lamb -1.864400 time 2019-02-26 15:05:36.252226
Model ind 640 epoch 798 head B head_i_epoch 0 batch 100: avg loss -1.828583 avg loss no lamb -1.828583 time 2019-02-26 15:07:23.600008
Model ind 640 epoch 798 head B head_i_epoch 0 batch 200: avg loss -1.887443 avg loss no lamb -1.887443 time 2019-02-26 15:09:26.242412
last batch sz 160
Model ind 640 epoch 798 head B head_i_epoch 1 batch 0: avg loss -1.770007 avg loss no lamb -1.770007 time 2019-02-26 15:10:54.751814
Model ind 640 epoch 798 head B head_i_epoch 1 batch 100: avg loss -1.801834 avg loss no lamb -1.801834 time 2019-02-26 15:12:53.302535
Model ind 640 epoch 798 head B head_i_epoch 1 batch 200: avg loss -1.882108 avg loss no lamb -1.882108 time 2019-02-26 15:14:46.505888
last batch sz 160
Pre: time 2019-02-26 15:16:40.390954: 
 	std: 0.050401047
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50963336, 0.6126, 0.6124333, 0.61268336, 0.50975]
	train_accs: [0.50963336, 0.6126, 0.6124333, 0.61268336, 0.50975]
	best_train_sub_head: 3
	worst: 0.50963336
	avg: 0.57142
	best: 0.61268336

Starting e_i: 799
Model ind 640 epoch 799 head A head_i_epoch 0 batch 0: avg loss -3.172582 avg loss no lamb -3.172582 time 2019-02-26 15:16:43.512729
Model ind 640 epoch 799 head A head_i_epoch 0 batch 100: avg loss -3.030159 avg loss no lamb -3.030159 time 2019-02-26 15:18:39.606214
Model ind 640 epoch 799 head A head_i_epoch 0 batch 200: avg loss -3.204143 avg loss no lamb -3.204143 time 2019-02-26 15:20:38.849113
last batch sz 160
Model ind 640 epoch 799 head B head_i_epoch 0 batch 0: avg loss -1.795043 avg loss no lamb -1.795043 time 2019-02-26 15:22:04.187065
Model ind 640 epoch 799 head B head_i_epoch 0 batch 100: avg loss -1.770633 avg loss no lamb -1.770633 time 2019-02-26 15:23:57.884650
Model ind 640 epoch 799 head B head_i_epoch 0 batch 200: avg loss -1.828932 avg loss no lamb -1.828932 time 2019-02-26 15:25:55.874585
last batch sz 160
Model ind 640 epoch 799 head B head_i_epoch 1 batch 0: avg loss -1.806282 avg loss no lamb -1.806282 time 2019-02-26 15:27:23.711261
Model ind 640 epoch 799 head B head_i_epoch 1 batch 100: avg loss -1.887555 avg loss no lamb -1.887555 time 2019-02-26 15:29:24.307275
Model ind 640 epoch 799 head B head_i_epoch 1 batch 200: avg loss -1.873014 avg loss no lamb -1.873014 time 2019-02-26 15:31:22.636059
last batch sz 160
Pre: time 2019-02-26 15:33:12.274930: 
 	std: 0.050557524
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5117, 0.61475, 0.6148, 0.6148, 0.5114667]
	train_accs: [0.5117, 0.61475, 0.6148, 0.6148, 0.5114667]
	best_train_sub_head: 2
	worst: 0.5114667
	avg: 0.5735034
	best: 0.6148

Starting e_i: 800
Model ind 640 epoch 800 head A head_i_epoch 0 batch 0: avg loss -3.154121 avg loss no lamb -3.154121 time 2019-02-26 15:33:14.997863
Model ind 640 epoch 800 head A head_i_epoch 0 batch 100: avg loss -3.077397 avg loss no lamb -3.077397 time 2019-02-26 15:35:44.364088
Model ind 640 epoch 800 head A head_i_epoch 0 batch 200: avg loss -3.174194 avg loss no lamb -3.174194 time 2019-02-26 15:39:39.567855
last batch sz 160
Model ind 640 epoch 800 head B head_i_epoch 0 batch 0: avg loss -1.804397 avg loss no lamb -1.804397 time 2019-02-26 15:42:31.974078
Model ind 640 epoch 800 head B head_i_epoch 0 batch 100: avg loss -1.754926 avg loss no lamb -1.754926 time 2019-02-26 15:46:35.024735
Model ind 640 epoch 800 head B head_i_epoch 0 batch 200: avg loss -1.911569 avg loss no lamb -1.911569 time 2019-02-26 15:49:31.510121
last batch sz 160
Model ind 640 epoch 800 head B head_i_epoch 1 batch 0: avg loss -1.726796 avg loss no lamb -1.726796 time 2019-02-26 15:52:22.826923
Model ind 640 epoch 800 head B head_i_epoch 1 batch 100: avg loss -1.725968 avg loss no lamb -1.725968 time 2019-02-26 15:56:24.561380
Model ind 640 epoch 800 head B head_i_epoch 1 batch 200: avg loss -1.904096 avg loss no lamb -1.904096 time 2019-02-26 16:00:23.325993
last batch sz 160
Pre: time 2019-02-26 16:04:04.226002: 
 	std: 0.050466347
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5097167, 0.61285, 0.61266667, 0.61285, 0.50983334]
	train_accs: [0.5097167, 0.61285, 0.61266667, 0.61285, 0.50983334]
	best_train_sub_head: 1
	worst: 0.5097167
	avg: 0.57158333
	best: 0.61285

Starting e_i: 801
Model ind 640 epoch 801 head A head_i_epoch 0 batch 0: avg loss -3.147215 avg loss no lamb -3.147215 time 2019-02-26 16:04:12.182945
Model ind 640 epoch 801 head A head_i_epoch 0 batch 100: avg loss -3.123871 avg loss no lamb -3.123871 time 2019-02-26 16:08:13.031859
Model ind 640 epoch 801 head A head_i_epoch 0 batch 200: avg loss -3.256958 avg loss no lamb -3.256958 time 2019-02-26 16:12:18.147939
last batch sz 160
Model ind 640 epoch 801 head B head_i_epoch 0 batch 0: avg loss -1.864599 avg loss no lamb -1.864599 time 2019-02-26 16:15:09.428151
Model ind 640 epoch 801 head B head_i_epoch 0 batch 100: avg loss -1.782454 avg loss no lamb -1.782454 time 2019-02-26 16:18:51.685356
Model ind 640 epoch 801 head B head_i_epoch 0 batch 200: avg loss -1.834509 avg loss no lamb -1.834509 time 2019-02-26 16:21:37.706719
last batch sz 160
Model ind 640 epoch 801 head B head_i_epoch 1 batch 0: avg loss -1.817010 avg loss no lamb -1.817010 time 2019-02-26 16:24:34.814193
Model ind 640 epoch 801 head B head_i_epoch 1 batch 100: avg loss -1.798293 avg loss no lamb -1.798293 time 2019-02-26 16:28:31.249926
Model ind 640 epoch 801 head B head_i_epoch 1 batch 200: avg loss -1.870017 avg loss no lamb -1.870017 time 2019-02-26 16:32:28.606900
last batch sz 160
Pre: time 2019-02-26 16:36:09.281264: 
 	std: 0.05064209
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51061666, 0.61401665, 0.61361665, 0.6138333, 0.51028335]
	train_accs: [0.51061666, 0.61401665, 0.61361665, 0.6138333, 0.51028335]
	best_train_sub_head: 1
	worst: 0.51028335
	avg: 0.57247335
	best: 0.61401665

Starting e_i: 802
Model ind 640 epoch 802 head A head_i_epoch 0 batch 0: avg loss -3.193672 avg loss no lamb -3.193672 time 2019-02-26 16:36:13.059125
Model ind 640 epoch 802 head A head_i_epoch 0 batch 100: avg loss -3.068143 avg loss no lamb -3.068143 time 2019-02-26 16:40:13.784046
Model ind 640 epoch 802 head A head_i_epoch 0 batch 200: avg loss -3.176499 avg loss no lamb -3.176499 time 2019-02-26 16:44:14.215823
last batch sz 160
Model ind 640 epoch 802 head B head_i_epoch 0 batch 0: avg loss -1.856483 avg loss no lamb -1.856483 time 2019-02-26 16:47:07.273002
Model ind 640 epoch 802 head B head_i_epoch 0 batch 100: avg loss -1.763355 avg loss no lamb -1.763355 time 2019-02-26 16:50:49.555438
Model ind 640 epoch 802 head B head_i_epoch 0 batch 200: avg loss -1.873546 avg loss no lamb -1.873546 time 2019-02-26 16:54:28.954017
last batch sz 160
Model ind 640 epoch 802 head B head_i_epoch 1 batch 0: avg loss -1.844634 avg loss no lamb -1.844634 time 2019-02-26 16:57:19.214786
Model ind 640 epoch 802 head B head_i_epoch 1 batch 100: avg loss -1.741833 avg loss no lamb -1.741833 time 2019-02-26 17:01:22.486894
Model ind 640 epoch 802 head B head_i_epoch 1 batch 200: avg loss -1.842337 avg loss no lamb -1.842337 time 2019-02-26 17:05:25.160310
last batch sz 160
Pre: time 2019-02-26 17:08:59.399359: 
 	std: 0.05052239
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51055, 0.61336666, 0.6134, 0.6134167, 0.50998336]
	train_accs: [0.51055, 0.61336666, 0.6134, 0.6134167, 0.50998336]
	best_train_sub_head: 3
	worst: 0.50998336
	avg: 0.5721433
	best: 0.6134167

Starting e_i: 803
Model ind 640 epoch 803 head A head_i_epoch 0 batch 0: avg loss -3.133548 avg loss no lamb -3.133548 time 2019-02-26 17:09:03.712417
Model ind 640 epoch 803 head A head_i_epoch 0 batch 100: avg loss -3.101500 avg loss no lamb -3.101500 time 2019-02-26 17:12:58.389506
Model ind 640 epoch 803 head A head_i_epoch 0 batch 200: avg loss -3.201874 avg loss no lamb -3.201874 time 2019-02-26 17:17:06.705413
last batch sz 160
Model ind 640 epoch 803 head B head_i_epoch 0 batch 0: avg loss -1.803811 avg loss no lamb -1.803811 time 2019-02-26 17:20:08.440571
Model ind 640 epoch 803 head B head_i_epoch 0 batch 100: avg loss -1.738170 avg loss no lamb -1.738170 time 2019-02-26 17:23:26.978925
Model ind 640 epoch 803 head B head_i_epoch 0 batch 200: avg loss -1.844011 avg loss no lamb -1.844011 time 2019-02-26 17:27:26.920210
last batch sz 160
Model ind 640 epoch 803 head B head_i_epoch 1 batch 0: avg loss -1.810256 avg loss no lamb -1.810256 time 2019-02-26 17:30:23.981528
Model ind 640 epoch 803 head B head_i_epoch 1 batch 100: avg loss -1.851390 avg loss no lamb -1.851390 time 2019-02-26 17:34:31.804646
Model ind 640 epoch 803 head B head_i_epoch 1 batch 200: avg loss -1.867021 avg loss no lamb -1.867021 time 2019-02-26 17:38:27.092696
last batch sz 160
Pre: time 2019-02-26 17:42:03.031564: 
 	std: 0.050953176
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101333, 0.6135, 0.6138667, 0.6136, 0.50916666]
	train_accs: [0.5101333, 0.6135, 0.6138667, 0.6136, 0.50916666]
	best_train_sub_head: 2
	worst: 0.50916666
	avg: 0.5720533
	best: 0.6138667

Starting e_i: 804
Model ind 640 epoch 804 head A head_i_epoch 0 batch 0: avg loss -3.147301 avg loss no lamb -3.147301 time 2019-02-26 17:42:07.316075
Model ind 640 epoch 804 head A head_i_epoch 0 batch 100: avg loss -3.134628 avg loss no lamb -3.134628 time 2019-02-26 17:46:09.551051
Model ind 640 epoch 804 head A head_i_epoch 0 batch 200: avg loss -3.246721 avg loss no lamb -3.246721 time 2019-02-26 17:50:08.957432
last batch sz 160
Model ind 640 epoch 804 head B head_i_epoch 0 batch 0: avg loss -1.884206 avg loss no lamb -1.884206 time 2019-02-26 17:52:49.217294
Model ind 640 epoch 804 head B head_i_epoch 0 batch 100: avg loss -1.738571 avg loss no lamb -1.738571 time 2019-02-26 17:55:21.437711
Model ind 640 epoch 804 head B head_i_epoch 0 batch 200: avg loss -1.910087 avg loss no lamb -1.910087 time 2019-02-26 17:57:51.639570
last batch sz 160
Model ind 640 epoch 804 head B head_i_epoch 1 batch 0: avg loss -1.838704 avg loss no lamb -1.838704 time 2019-02-26 17:59:14.443033
Model ind 640 epoch 804 head B head_i_epoch 1 batch 100: avg loss -1.783838 avg loss no lamb -1.783838 time 2019-02-26 18:01:14.512076
Model ind 640 epoch 804 head B head_i_epoch 1 batch 200: avg loss -1.868924 avg loss no lamb -1.868924 time 2019-02-26 18:03:17.470519
last batch sz 160
Pre: time 2019-02-26 18:05:01.330638: 
 	std: 0.051385187
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5071167, 0.61163336, 0.61186665, 0.6117167, 0.50658333]
	train_accs: [0.5071167, 0.61163336, 0.61186665, 0.6117167, 0.50658333]
	best_train_sub_head: 2
	worst: 0.50658333
	avg: 0.5697834
	best: 0.61186665

Starting e_i: 805
Model ind 640 epoch 805 head A head_i_epoch 0 batch 0: avg loss -3.125793 avg loss no lamb -3.125793 time 2019-02-26 18:05:04.414527
Model ind 640 epoch 805 head A head_i_epoch 0 batch 100: avg loss -3.063995 avg loss no lamb -3.063995 time 2019-02-26 18:07:11.067938
Model ind 640 epoch 805 head A head_i_epoch 0 batch 200: avg loss -3.163189 avg loss no lamb -3.163189 time 2019-02-26 18:09:09.548930
last batch sz 160
Model ind 640 epoch 805 head B head_i_epoch 0 batch 0: avg loss -1.757917 avg loss no lamb -1.757917 time 2019-02-26 18:10:33.770417
Model ind 640 epoch 805 head B head_i_epoch 0 batch 100: avg loss -1.768758 avg loss no lamb -1.768758 time 2019-02-26 18:12:41.659365
Model ind 640 epoch 805 head B head_i_epoch 0 batch 200: avg loss -1.855834 avg loss no lamb -1.855834 time 2019-02-26 18:14:36.585464
last batch sz 160
Model ind 640 epoch 805 head B head_i_epoch 1 batch 0: avg loss -1.781173 avg loss no lamb -1.781173 time 2019-02-26 18:16:04.833264
Model ind 640 epoch 805 head B head_i_epoch 1 batch 100: avg loss -1.749134 avg loss no lamb -1.749134 time 2019-02-26 18:18:02.134541
Model ind 640 epoch 805 head B head_i_epoch 1 batch 200: avg loss -1.877015 avg loss no lamb -1.877015 time 2019-02-26 18:20:05.634876
last batch sz 160
Pre: time 2019-02-26 18:21:54.818965: 
 	std: 0.05108162
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50963336, 0.61355, 0.61385, 0.61373335, 0.50925]
	train_accs: [0.50963336, 0.61355, 0.61385, 0.61373335, 0.50925]
	best_train_sub_head: 2
	worst: 0.50925
	avg: 0.5720033
	best: 0.61385

Starting e_i: 806
Model ind 640 epoch 806 head A head_i_epoch 0 batch 0: avg loss -3.148088 avg loss no lamb -3.148088 time 2019-02-26 18:21:57.654252
Model ind 640 epoch 806 head A head_i_epoch 0 batch 100: avg loss -3.132022 avg loss no lamb -3.132022 time 2019-02-26 18:24:00.537728
Model ind 640 epoch 806 head A head_i_epoch 0 batch 200: avg loss -3.170312 avg loss no lamb -3.170312 time 2019-02-26 18:26:00.155100
last batch sz 160
Model ind 640 epoch 806 head B head_i_epoch 0 batch 0: avg loss -1.861663 avg loss no lamb -1.861663 time 2019-02-26 18:27:22.434415
Model ind 640 epoch 806 head B head_i_epoch 0 batch 100: avg loss -1.835055 avg loss no lamb -1.835055 time 2019-02-26 18:29:31.864957
Model ind 640 epoch 806 head B head_i_epoch 0 batch 200: avg loss -1.852262 avg loss no lamb -1.852262 time 2019-02-26 18:31:29.376245
last batch sz 160
Model ind 640 epoch 806 head B head_i_epoch 1 batch 0: avg loss -1.840798 avg loss no lamb -1.840798 time 2019-02-26 18:32:54.509001
Model ind 640 epoch 806 head B head_i_epoch 1 batch 100: avg loss -1.749842 avg loss no lamb -1.749842 time 2019-02-26 18:34:58.465896
Model ind 640 epoch 806 head B head_i_epoch 1 batch 200: avg loss -1.840577 avg loss no lamb -1.840577 time 2019-02-26 18:36:51.396471
last batch sz 160
Pre: time 2019-02-26 18:38:48.536557: 
 	std: 0.050124794
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115167, 0.61365, 0.61378336, 0.6138167, 0.51135]
	train_accs: [0.5115167, 0.61365, 0.61378336, 0.6138167, 0.51135]
	best_train_sub_head: 3
	worst: 0.51135
	avg: 0.57282335
	best: 0.6138167

Starting e_i: 807
Model ind 640 epoch 807 head A head_i_epoch 0 batch 0: avg loss -3.123283 avg loss no lamb -3.123283 time 2019-02-26 18:38:51.606475
Model ind 640 epoch 807 head A head_i_epoch 0 batch 100: avg loss -3.177871 avg loss no lamb -3.177871 time 2019-02-26 18:40:48.512781
Model ind 640 epoch 807 head A head_i_epoch 0 batch 200: avg loss -3.166640 avg loss no lamb -3.166640 time 2019-02-26 18:42:41.730618
last batch sz 160
Model ind 640 epoch 807 head B head_i_epoch 0 batch 0: avg loss -1.841706 avg loss no lamb -1.841706 time 2019-02-26 18:44:19.062300
Model ind 640 epoch 807 head B head_i_epoch 0 batch 100: avg loss -1.743695 avg loss no lamb -1.743695 time 2019-02-26 18:46:14.717080
Model ind 640 epoch 807 head B head_i_epoch 0 batch 200: avg loss -1.866114 avg loss no lamb -1.866114 time 2019-02-26 18:48:14.253643
last batch sz 160
Model ind 640 epoch 807 head B head_i_epoch 1 batch 0: avg loss -1.871241 avg loss no lamb -1.871241 time 2019-02-26 18:49:41.883485
Model ind 640 epoch 807 head B head_i_epoch 1 batch 100: avg loss -1.813507 avg loss no lamb -1.813507 time 2019-02-26 18:51:42.313417
Model ind 640 epoch 807 head B head_i_epoch 1 batch 200: avg loss -1.835670 avg loss no lamb -1.835670 time 2019-02-26 18:53:40.941268
last batch sz 160
Pre: time 2019-02-26 18:55:34.553836: 
 	std: 0.05042959
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50946665, 0.6124, 0.61245, 0.61226666, 0.5094]
	train_accs: [0.50946665, 0.6124, 0.61245, 0.61226666, 0.5094]
	best_train_sub_head: 2
	worst: 0.5094
	avg: 0.5711967
	best: 0.61245

Starting e_i: 808
Model ind 640 epoch 808 head A head_i_epoch 0 batch 0: avg loss -3.235971 avg loss no lamb -3.235971 time 2019-02-26 18:55:37.314307
Model ind 640 epoch 808 head A head_i_epoch 0 batch 100: avg loss -3.112478 avg loss no lamb -3.112478 time 2019-02-26 18:57:37.701478
Model ind 640 epoch 808 head A head_i_epoch 0 batch 200: avg loss -3.083244 avg loss no lamb -3.083244 time 2019-02-26 18:59:42.753265
last batch sz 160
Model ind 640 epoch 808 head B head_i_epoch 0 batch 0: avg loss -1.761802 avg loss no lamb -1.761802 time 2019-02-26 19:01:11.066246
Model ind 640 epoch 808 head B head_i_epoch 0 batch 100: avg loss -1.812916 avg loss no lamb -1.812916 time 2019-02-26 19:03:07.750242
Model ind 640 epoch 808 head B head_i_epoch 0 batch 200: avg loss -1.842174 avg loss no lamb -1.842174 time 2019-02-26 19:05:06.830230
last batch sz 160
Model ind 640 epoch 808 head B head_i_epoch 1 batch 0: avg loss -1.826028 avg loss no lamb -1.826028 time 2019-02-26 19:06:36.339655
Model ind 640 epoch 808 head B head_i_epoch 1 batch 100: avg loss -1.815262 avg loss no lamb -1.815262 time 2019-02-26 19:08:31.662015
Model ind 640 epoch 808 head B head_i_epoch 1 batch 200: avg loss -1.947858 avg loss no lamb -1.947858 time 2019-02-26 19:10:39.137450
last batch sz 160
Pre: time 2019-02-26 19:12:27.163302: 
 	std: 0.051032614
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50811666, 0.61228335, 0.61258334, 0.61251664, 0.50846666]
	train_accs: [0.50811666, 0.61228335, 0.61258334, 0.61251664, 0.50846666]
	best_train_sub_head: 2
	worst: 0.50811666
	avg: 0.57079333
	best: 0.61258334

Starting e_i: 809
Model ind 640 epoch 809 head A head_i_epoch 0 batch 0: avg loss -3.124709 avg loss no lamb -3.124709 time 2019-02-26 19:12:31.255311
Model ind 640 epoch 809 head A head_i_epoch 0 batch 100: avg loss -3.089770 avg loss no lamb -3.089770 time 2019-02-26 19:14:26.593311
Model ind 640 epoch 809 head A head_i_epoch 0 batch 200: avg loss -3.133807 avg loss no lamb -3.133807 time 2019-02-26 19:16:34.680137
last batch sz 160
Model ind 640 epoch 809 head B head_i_epoch 0 batch 0: avg loss -1.850034 avg loss no lamb -1.850034 time 2019-02-26 19:17:57.329876
Model ind 640 epoch 809 head B head_i_epoch 0 batch 100: avg loss -1.812162 avg loss no lamb -1.812162 time 2019-02-26 19:19:58.429442
Model ind 640 epoch 809 head B head_i_epoch 0 batch 200: avg loss -1.832113 avg loss no lamb -1.832113 time 2019-02-26 19:22:02.844552
last batch sz 160
Model ind 640 epoch 809 head B head_i_epoch 1 batch 0: avg loss -1.828104 avg loss no lamb -1.828104 time 2019-02-26 19:23:27.531574
Model ind 640 epoch 809 head B head_i_epoch 1 batch 100: avg loss -1.848770 avg loss no lamb -1.848770 time 2019-02-26 19:25:32.509322
Model ind 640 epoch 809 head B head_i_epoch 1 batch 200: avg loss -1.821201 avg loss no lamb -1.821201 time 2019-02-26 19:27:31.246725
last batch sz 160
Pre: time 2019-02-26 19:29:17.785332: 
 	std: 0.050232608
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51056665, 0.6135333, 0.6133, 0.6133, 0.5111167]
	train_accs: [0.51056665, 0.6135333, 0.6133, 0.6133, 0.5111167]
	best_train_sub_head: 1
	worst: 0.51056665
	avg: 0.5723634
	best: 0.6135333

Starting e_i: 810
Model ind 640 epoch 810 head A head_i_epoch 0 batch 0: avg loss -3.173960 avg loss no lamb -3.173960 time 2019-02-26 19:29:21.479602
Model ind 640 epoch 810 head A head_i_epoch 0 batch 100: avg loss -3.109425 avg loss no lamb -3.109425 time 2019-02-26 19:31:25.201090
Model ind 640 epoch 810 head A head_i_epoch 0 batch 200: avg loss -3.230060 avg loss no lamb -3.230060 time 2019-02-26 19:33:25.113401
last batch sz 160
Model ind 640 epoch 810 head B head_i_epoch 0 batch 0: avg loss -1.851412 avg loss no lamb -1.851412 time 2019-02-26 19:34:51.313629
Model ind 640 epoch 810 head B head_i_epoch 0 batch 100: avg loss -1.759392 avg loss no lamb -1.759392 time 2019-02-26 19:36:52.859964
Model ind 640 epoch 810 head B head_i_epoch 0 batch 200: avg loss -1.840416 avg loss no lamb -1.840416 time 2019-02-26 19:38:53.948482
last batch sz 160
Model ind 640 epoch 810 head B head_i_epoch 1 batch 0: avg loss -1.869425 avg loss no lamb -1.869425 time 2019-02-26 19:40:16.151256
Model ind 640 epoch 810 head B head_i_epoch 1 batch 100: avg loss -1.728705 avg loss no lamb -1.728705 time 2019-02-26 19:42:27.474085
Model ind 640 epoch 810 head B head_i_epoch 1 batch 200: avg loss -1.909428 avg loss no lamb -1.909428 time 2019-02-26 19:44:25.401824
last batch sz 160
Pre: time 2019-02-26 19:46:11.538710: 
 	std: 0.050541706
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5061833, 0.60966665, 0.6098833, 0.60945, 0.5068167]
	train_accs: [0.5061833, 0.60966665, 0.6098833, 0.60945, 0.5068167]
	best_train_sub_head: 2
	worst: 0.5061833
	avg: 0.56839997
	best: 0.6098833

Starting e_i: 811
Model ind 640 epoch 811 head A head_i_epoch 0 batch 0: avg loss -3.110217 avg loss no lamb -3.110217 time 2019-02-26 19:46:20.098649
Model ind 640 epoch 811 head A head_i_epoch 0 batch 100: avg loss -3.145402 avg loss no lamb -3.145402 time 2019-02-26 19:48:24.738139
Model ind 640 epoch 811 head A head_i_epoch 0 batch 200: avg loss -3.255853 avg loss no lamb -3.255853 time 2019-02-26 19:50:18.228907
last batch sz 160
Model ind 640 epoch 811 head B head_i_epoch 0 batch 0: avg loss -1.837694 avg loss no lamb -1.837694 time 2019-02-26 19:51:53.963446
Model ind 640 epoch 811 head B head_i_epoch 0 batch 100: avg loss -1.831772 avg loss no lamb -1.831772 time 2019-02-26 19:53:50.390244
Model ind 640 epoch 811 head B head_i_epoch 0 batch 200: avg loss -1.905172 avg loss no lamb -1.905172 time 2019-02-26 19:55:46.894472
last batch sz 160
Model ind 640 epoch 811 head B head_i_epoch 1 batch 0: avg loss -1.894700 avg loss no lamb -1.894700 time 2019-02-26 19:57:25.317825
Model ind 640 epoch 811 head B head_i_epoch 1 batch 100: avg loss -1.770693 avg loss no lamb -1.770693 time 2019-02-26 19:59:22.170378
Model ind 640 epoch 811 head B head_i_epoch 1 batch 200: avg loss -1.766817 avg loss no lamb -1.766817 time 2019-02-26 20:01:21.527833
last batch sz 160
Pre: time 2019-02-26 20:03:13.139503: 
 	std: 0.0507623
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50948334, 0.61365, 0.61338335, 0.6134667, 0.51028335]
	train_accs: [0.50948334, 0.61365, 0.61338335, 0.6134667, 0.51028335]
	best_train_sub_head: 1
	worst: 0.50948334
	avg: 0.5720533
	best: 0.61365

Starting e_i: 812
Model ind 640 epoch 812 head A head_i_epoch 0 batch 0: avg loss -3.144296 avg loss no lamb -3.144296 time 2019-02-26 20:03:16.795230
Model ind 640 epoch 812 head A head_i_epoch 0 batch 100: avg loss -3.142588 avg loss no lamb -3.142588 time 2019-02-26 20:05:15.279157
Model ind 640 epoch 812 head A head_i_epoch 0 batch 200: avg loss -3.201494 avg loss no lamb -3.201494 time 2019-02-26 20:07:18.829800
last batch sz 160
Model ind 640 epoch 812 head B head_i_epoch 0 batch 0: avg loss -1.753842 avg loss no lamb -1.753842 time 2019-02-26 20:08:45.256310
Model ind 640 epoch 812 head B head_i_epoch 0 batch 100: avg loss -1.755362 avg loss no lamb -1.755362 time 2019-02-26 20:10:44.399837
Model ind 640 epoch 812 head B head_i_epoch 0 batch 200: avg loss -1.921352 avg loss no lamb -1.921352 time 2019-02-26 20:12:43.379956
last batch sz 160
Model ind 640 epoch 812 head B head_i_epoch 1 batch 0: avg loss -1.893001 avg loss no lamb -1.893001 time 2019-02-26 20:14:14.559099
Model ind 640 epoch 812 head B head_i_epoch 1 batch 100: avg loss -1.787342 avg loss no lamb -1.787342 time 2019-02-26 20:16:07.732754
Model ind 640 epoch 812 head B head_i_epoch 1 batch 200: avg loss -1.925869 avg loss no lamb -1.925869 time 2019-02-26 20:18:11.324170
last batch sz 160
Pre: time 2019-02-26 20:20:02.716942: 
 	std: 0.050828457
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108333, 0.61485, 0.61478335, 0.61475, 0.51125]
	train_accs: [0.5108333, 0.61485, 0.61478335, 0.61475, 0.51125]
	best_train_sub_head: 1
	worst: 0.5108333
	avg: 0.5732933
	best: 0.61485

Starting e_i: 813
Model ind 640 epoch 813 head A head_i_epoch 0 batch 0: avg loss -3.201601 avg loss no lamb -3.201601 time 2019-02-26 20:20:06.352254
Model ind 640 epoch 813 head A head_i_epoch 0 batch 100: avg loss -3.112468 avg loss no lamb -3.112468 time 2019-02-26 20:21:59.585060
Model ind 640 epoch 813 head A head_i_epoch 0 batch 200: avg loss -3.204377 avg loss no lamb -3.204377 time 2019-02-26 20:24:10.012257
last batch sz 160
Model ind 640 epoch 813 head B head_i_epoch 0 batch 0: avg loss -1.812906 avg loss no lamb -1.812906 time 2019-02-26 20:25:33.312162
Model ind 640 epoch 813 head B head_i_epoch 0 batch 100: avg loss -1.771302 avg loss no lamb -1.771302 time 2019-02-26 20:27:32.642513
Model ind 640 epoch 813 head B head_i_epoch 0 batch 200: avg loss -1.862471 avg loss no lamb -1.862471 time 2019-02-26 20:29:38.352563
last batch sz 160
Model ind 640 epoch 813 head B head_i_epoch 1 batch 0: avg loss -1.758931 avg loss no lamb -1.758931 time 2019-02-26 20:31:01.091711
Model ind 640 epoch 813 head B head_i_epoch 1 batch 100: avg loss -1.776209 avg loss no lamb -1.776209 time 2019-02-26 20:33:00.286278
Model ind 640 epoch 813 head B head_i_epoch 1 batch 200: avg loss -1.801709 avg loss no lamb -1.801709 time 2019-02-26 20:35:01.904454
last batch sz 160
Pre: time 2019-02-26 20:36:50.104193: 
 	std: 0.05049093
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118167, 0.61516666, 0.6149667, 0.6149333, 0.5121]
	train_accs: [0.5118167, 0.61516666, 0.6149667, 0.6149333, 0.5121]
	best_train_sub_head: 1
	worst: 0.5118167
	avg: 0.57379663
	best: 0.61516666

Starting e_i: 814
Model ind 640 epoch 814 head A head_i_epoch 0 batch 0: avg loss -3.190420 avg loss no lamb -3.190420 time 2019-02-26 20:36:52.734897
Model ind 640 epoch 814 head A head_i_epoch 0 batch 100: avg loss -3.173761 avg loss no lamb -3.173761 time 2019-02-26 20:39:01.365719
Model ind 640 epoch 814 head A head_i_epoch 0 batch 200: avg loss -3.180904 avg loss no lamb -3.180904 time 2019-02-26 20:40:54.185048
last batch sz 160
Model ind 640 epoch 814 head B head_i_epoch 0 batch 0: avg loss -1.869510 avg loss no lamb -1.869510 time 2019-02-26 20:42:23.489158
Model ind 640 epoch 814 head B head_i_epoch 0 batch 100: avg loss -1.766044 avg loss no lamb -1.766044 time 2019-02-26 20:44:23.427232
Model ind 640 epoch 814 head B head_i_epoch 0 batch 200: avg loss -1.906008 avg loss no lamb -1.906008 time 2019-02-26 20:46:23.936358
last batch sz 160
Model ind 640 epoch 814 head B head_i_epoch 1 batch 0: avg loss -1.833597 avg loss no lamb -1.833597 time 2019-02-26 20:47:47.690901
Model ind 640 epoch 814 head B head_i_epoch 1 batch 100: avg loss -1.787940 avg loss no lamb -1.787940 time 2019-02-26 20:49:52.129492
Model ind 640 epoch 814 head B head_i_epoch 1 batch 200: avg loss -1.813175 avg loss no lamb -1.813175 time 2019-02-26 20:51:51.831404
last batch sz 160
Pre: time 2019-02-26 20:53:36.486618: 
 	std: 0.050899148
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50976664, 0.6138333, 0.6138167, 0.61356664, 0.50991666]
	train_accs: [0.50976664, 0.6138333, 0.6138167, 0.61356664, 0.50991666]
	best_train_sub_head: 1
	worst: 0.50976664
	avg: 0.57218
	best: 0.6138333

Starting e_i: 815
Model ind 640 epoch 815 head A head_i_epoch 0 batch 0: avg loss -3.219484 avg loss no lamb -3.219484 time 2019-02-26 20:53:40.099017
Model ind 640 epoch 815 head A head_i_epoch 0 batch 100: avg loss -3.069125 avg loss no lamb -3.069125 time 2019-02-26 20:55:51.989077
Model ind 640 epoch 815 head A head_i_epoch 0 batch 200: avg loss -3.241461 avg loss no lamb -3.241461 time 2019-02-26 20:57:49.657626
last batch sz 160
Model ind 640 epoch 815 head B head_i_epoch 0 batch 0: avg loss -1.824312 avg loss no lamb -1.824312 time 2019-02-26 20:59:15.172571
Model ind 640 epoch 815 head B head_i_epoch 0 batch 100: avg loss -1.811581 avg loss no lamb -1.811581 time 2019-02-26 21:01:20.189055
Model ind 640 epoch 815 head B head_i_epoch 0 batch 200: avg loss -1.874625 avg loss no lamb -1.874625 time 2019-02-26 21:03:14.027123
last batch sz 160
Model ind 640 epoch 815 head B head_i_epoch 1 batch 0: avg loss -1.811893 avg loss no lamb -1.811893 time 2019-02-26 21:04:46.587296
Model ind 640 epoch 815 head B head_i_epoch 1 batch 100: avg loss -1.910759 avg loss no lamb -1.910759 time 2019-02-26 21:06:46.052574
Model ind 640 epoch 815 head B head_i_epoch 1 batch 200: avg loss -1.873674 avg loss no lamb -1.873674 time 2019-02-26 21:08:43.532805
last batch sz 160
Pre: time 2019-02-26 21:10:41.718413: 
 	std: 0.050637852
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51016665, 0.61356664, 0.61331666, 0.6135833, 0.5100833]
	train_accs: [0.51016665, 0.61356664, 0.61331666, 0.6135833, 0.5100833]
	best_train_sub_head: 3
	worst: 0.5100833
	avg: 0.57214326
	best: 0.6135833

Starting e_i: 816
Model ind 640 epoch 816 head A head_i_epoch 0 batch 0: avg loss -3.213402 avg loss no lamb -3.213402 time 2019-02-26 21:10:45.057847
Model ind 640 epoch 816 head A head_i_epoch 0 batch 100: avg loss -3.122320 avg loss no lamb -3.122320 time 2019-02-26 21:12:40.403457
Model ind 640 epoch 816 head A head_i_epoch 0 batch 200: avg loss -3.220849 avg loss no lamb -3.220849 time 2019-02-26 21:14:39.811540
last batch sz 160
Model ind 640 epoch 816 head B head_i_epoch 0 batch 0: avg loss -1.815330 avg loss no lamb -1.815330 time 2019-02-26 21:16:08.106586
Model ind 640 epoch 816 head B head_i_epoch 0 batch 100: avg loss -1.794723 avg loss no lamb -1.794723 time 2019-02-26 21:18:08.675787
Model ind 640 epoch 816 head B head_i_epoch 0 batch 200: avg loss -1.806862 avg loss no lamb -1.806862 time 2019-02-26 21:20:08.883496
last batch sz 160
Model ind 640 epoch 816 head B head_i_epoch 1 batch 0: avg loss -1.711248 avg loss no lamb -1.711248 time 2019-02-26 21:21:37.509776
Model ind 640 epoch 816 head B head_i_epoch 1 batch 100: avg loss -1.760745 avg loss no lamb -1.760745 time 2019-02-26 21:23:36.296727
Model ind 640 epoch 816 head B head_i_epoch 1 batch 200: avg loss -1.896064 avg loss no lamb -1.896064 time 2019-02-26 21:25:34.587856
last batch sz 160
Pre: time 2019-02-26 21:27:22.618545: 
 	std: 0.051255737
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103, 0.6150333, 0.61511666, 0.6152, 0.51068336]
	train_accs: [0.5103, 0.6150333, 0.61511666, 0.6152, 0.51068336]
	best_train_sub_head: 3
	worst: 0.5103
	avg: 0.5732666
	best: 0.6152

Starting e_i: 817
Model ind 640 epoch 817 head A head_i_epoch 0 batch 0: avg loss -3.220556 avg loss no lamb -3.220556 time 2019-02-26 21:27:25.697043
Model ind 640 epoch 817 head A head_i_epoch 0 batch 100: avg loss -3.088982 avg loss no lamb -3.088982 time 2019-02-26 21:29:29.973671
Model ind 640 epoch 817 head A head_i_epoch 0 batch 200: avg loss -3.232748 avg loss no lamb -3.232748 time 2019-02-26 21:31:28.836894
last batch sz 160
Model ind 640 epoch 817 head B head_i_epoch 0 batch 0: avg loss -1.848951 avg loss no lamb -1.848951 time 2019-02-26 21:32:56.989914
Model ind 640 epoch 817 head B head_i_epoch 0 batch 100: avg loss -1.795558 avg loss no lamb -1.795558 time 2019-02-26 21:34:56.258418
Model ind 640 epoch 817 head B head_i_epoch 0 batch 200: avg loss -1.813684 avg loss no lamb -1.813684 time 2019-02-26 21:36:57.932096
last batch sz 160
Model ind 640 epoch 817 head B head_i_epoch 1 batch 0: avg loss -1.808651 avg loss no lamb -1.808651 time 2019-02-26 21:38:23.240759
Model ind 640 epoch 817 head B head_i_epoch 1 batch 100: avg loss -1.767936 avg loss no lamb -1.767936 time 2019-02-26 21:40:25.154047
Model ind 640 epoch 817 head B head_i_epoch 1 batch 200: avg loss -1.991790 avg loss no lamb -1.991790 time 2019-02-26 21:42:24.102217
last batch sz 160
Pre: time 2019-02-26 21:44:13.373634: 
 	std: 0.050539963
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50988334, 0.6130667, 0.61326665, 0.6128333, 0.5099]
	train_accs: [0.50988334, 0.6130667, 0.61326665, 0.6128333, 0.5099]
	best_train_sub_head: 2
	worst: 0.50988334
	avg: 0.57179004
	best: 0.61326665

Starting e_i: 818
Model ind 640 epoch 818 head A head_i_epoch 0 batch 0: avg loss -3.095554 avg loss no lamb -3.095554 time 2019-02-26 21:44:16.304359
Model ind 640 epoch 818 head A head_i_epoch 0 batch 100: avg loss -3.058093 avg loss no lamb -3.058093 time 2019-02-26 21:46:19.983288
Model ind 640 epoch 818 head A head_i_epoch 0 batch 200: avg loss -3.204383 avg loss no lamb -3.204383 time 2019-02-26 21:48:18.487204
last batch sz 160
Model ind 640 epoch 818 head B head_i_epoch 0 batch 0: avg loss -1.802568 avg loss no lamb -1.802568 time 2019-02-26 21:49:48.293069
Model ind 640 epoch 818 head B head_i_epoch 0 batch 100: avg loss -1.800602 avg loss no lamb -1.800602 time 2019-02-26 21:51:43.782154
Model ind 640 epoch 818 head B head_i_epoch 0 batch 200: avg loss -1.919460 avg loss no lamb -1.919460 time 2019-02-26 21:53:45.373842
last batch sz 160
Model ind 640 epoch 818 head B head_i_epoch 1 batch 0: avg loss -1.739488 avg loss no lamb -1.739488 time 2019-02-26 21:55:17.559697
Model ind 640 epoch 818 head B head_i_epoch 1 batch 100: avg loss -1.784160 avg loss no lamb -1.784160 time 2019-02-26 21:57:12.134628
Model ind 640 epoch 818 head B head_i_epoch 1 batch 200: avg loss -1.856037 avg loss no lamb -1.856037 time 2019-02-26 21:59:16.174137
last batch sz 160
Pre: time 2019-02-26 22:01:08.521639: 
 	std: 0.05069538
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51075, 0.6138667, 0.61385, 0.6143, 0.5103]
	train_accs: [0.51075, 0.6138667, 0.61385, 0.6143, 0.5103]
	best_train_sub_head: 3
	worst: 0.5103
	avg: 0.5726133
	best: 0.6143

Starting e_i: 819
Model ind 640 epoch 819 head A head_i_epoch 0 batch 0: avg loss -3.182767 avg loss no lamb -3.182767 time 2019-02-26 22:01:13.418400
Model ind 640 epoch 819 head A head_i_epoch 0 batch 100: avg loss -3.129920 avg loss no lamb -3.129920 time 2019-02-26 22:03:09.238042
Model ind 640 epoch 819 head A head_i_epoch 0 batch 200: avg loss -3.204255 avg loss no lamb -3.204255 time 2019-02-26 22:05:14.199045
last batch sz 160
Model ind 640 epoch 819 head B head_i_epoch 0 batch 0: avg loss -1.893010 avg loss no lamb -1.893010 time 2019-02-26 22:06:37.075691
Model ind 640 epoch 819 head B head_i_epoch 0 batch 100: avg loss -1.784720 avg loss no lamb -1.784720 time 2019-02-26 22:08:41.313453
Model ind 640 epoch 819 head B head_i_epoch 0 batch 200: avg loss -1.865015 avg loss no lamb -1.865015 time 2019-02-26 22:10:44.155587
last batch sz 160
Model ind 640 epoch 819 head B head_i_epoch 1 batch 0: avg loss -1.861321 avg loss no lamb -1.861321 time 2019-02-26 22:12:07.286182
Model ind 640 epoch 819 head B head_i_epoch 1 batch 100: avg loss -1.824981 avg loss no lamb -1.824981 time 2019-02-26 22:14:13.514516
Model ind 640 epoch 819 head B head_i_epoch 1 batch 200: avg loss -1.908248 avg loss no lamb -1.908248 time 2019-02-26 22:16:05.097778
last batch sz 160
Pre: time 2019-02-26 22:18:05.853650: 
 	std: 0.05098778
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107667, 0.6145167, 0.61466664, 0.61455, 0.51023334]
	train_accs: [0.5107667, 0.6145167, 0.61466664, 0.61455, 0.51023334]
	best_train_sub_head: 2
	worst: 0.51023334
	avg: 0.57294667
	best: 0.61466664

Starting e_i: 820
Model ind 640 epoch 820 head A head_i_epoch 0 batch 0: avg loss -3.179142 avg loss no lamb -3.179142 time 2019-02-26 22:18:08.473519
Model ind 640 epoch 820 head A head_i_epoch 0 batch 100: avg loss -3.095782 avg loss no lamb -3.095782 time 2019-02-26 22:20:15.452366
Model ind 640 epoch 820 head A head_i_epoch 0 batch 200: avg loss -3.246974 avg loss no lamb -3.246974 time 2019-02-26 22:22:09.555370
last batch sz 160
Model ind 640 epoch 820 head B head_i_epoch 0 batch 0: avg loss -1.816553 avg loss no lamb -1.816553 time 2019-02-26 22:23:46.617028
Model ind 640 epoch 820 head B head_i_epoch 0 batch 100: avg loss -1.728994 avg loss no lamb -1.728994 time 2019-02-26 22:25:41.178157
Model ind 640 epoch 820 head B head_i_epoch 0 batch 200: avg loss -1.812005 avg loss no lamb -1.812005 time 2019-02-26 22:27:50.944263
last batch sz 160
Model ind 640 epoch 820 head B head_i_epoch 1 batch 0: avg loss -1.840579 avg loss no lamb -1.840579 time 2019-02-26 22:29:22.360888
Model ind 640 epoch 820 head B head_i_epoch 1 batch 100: avg loss -1.706723 avg loss no lamb -1.706723 time 2019-02-26 22:31:22.430700
Model ind 640 epoch 820 head B head_i_epoch 1 batch 200: avg loss -1.849483 avg loss no lamb -1.849483 time 2019-02-26 22:33:28.859555
last batch sz 160
Pre: time 2019-02-26 22:35:15.195153: 
 	std: 0.050585333
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51085, 0.61371666, 0.61356664, 0.61398333, 0.51015]
	train_accs: [0.51085, 0.61371666, 0.61356664, 0.61398333, 0.51015]
	best_train_sub_head: 3
	worst: 0.51015
	avg: 0.5724533
	best: 0.61398333

Starting e_i: 821
Model ind 640 epoch 821 head A head_i_epoch 0 batch 0: avg loss -3.127896 avg loss no lamb -3.127896 time 2019-02-26 22:35:25.104255
Model ind 640 epoch 821 head A head_i_epoch 0 batch 100: avg loss -3.173678 avg loss no lamb -3.173678 time 2019-02-26 22:37:26.357264
Model ind 640 epoch 821 head A head_i_epoch 0 batch 200: avg loss -3.246381 avg loss no lamb -3.246381 time 2019-02-26 22:39:27.558466
last batch sz 160
Model ind 640 epoch 821 head B head_i_epoch 0 batch 0: avg loss -1.884418 avg loss no lamb -1.884418 time 2019-02-26 22:40:52.075310
Model ind 640 epoch 821 head B head_i_epoch 0 batch 100: avg loss -1.799902 avg loss no lamb -1.799902 time 2019-02-26 22:42:55.542303
Model ind 640 epoch 821 head B head_i_epoch 0 batch 200: avg loss -1.834193 avg loss no lamb -1.834193 time 2019-02-26 22:44:54.348195
last batch sz 160
Model ind 640 epoch 821 head B head_i_epoch 1 batch 0: avg loss -1.830643 avg loss no lamb -1.830643 time 2019-02-26 22:46:23.325457
Model ind 640 epoch 821 head B head_i_epoch 1 batch 100: avg loss -1.785192 avg loss no lamb -1.785192 time 2019-02-26 22:48:22.393374
Model ind 640 epoch 821 head B head_i_epoch 1 batch 200: avg loss -1.870361 avg loss no lamb -1.870361 time 2019-02-26 22:50:23.674452
last batch sz 160
Pre: time 2019-02-26 22:52:12.094780: 
 	std: 0.05100335
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50958335, 0.61315, 0.61326665, 0.6132333, 0.5086333]
	train_accs: [0.50958335, 0.61315, 0.61326665, 0.6132333, 0.5086333]
	best_train_sub_head: 2
	worst: 0.5086333
	avg: 0.5715734
	best: 0.61326665

Starting e_i: 822
Model ind 640 epoch 822 head A head_i_epoch 0 batch 0: avg loss -3.171214 avg loss no lamb -3.171214 time 2019-02-26 22:52:14.988178
Model ind 640 epoch 822 head A head_i_epoch 0 batch 100: avg loss -3.067037 avg loss no lamb -3.067037 time 2019-02-26 22:54:13.308730
Model ind 640 epoch 822 head A head_i_epoch 0 batch 200: avg loss -3.179587 avg loss no lamb -3.179587 time 2019-02-26 22:56:13.964475
last batch sz 160
Model ind 640 epoch 822 head B head_i_epoch 0 batch 0: avg loss -1.844034 avg loss no lamb -1.844034 time 2019-02-26 22:57:39.123191
Model ind 640 epoch 822 head B head_i_epoch 0 batch 100: avg loss -1.903194 avg loss no lamb -1.903194 time 2019-02-26 22:59:40.987406
Model ind 640 epoch 822 head B head_i_epoch 0 batch 200: avg loss -1.964401 avg loss no lamb -1.964401 time 2019-02-26 23:01:39.193011
last batch sz 160
Model ind 640 epoch 822 head B head_i_epoch 1 batch 0: avg loss -1.810354 avg loss no lamb -1.810354 time 2019-02-26 23:03:04.417044
Model ind 640 epoch 822 head B head_i_epoch 1 batch 100: avg loss -1.841600 avg loss no lamb -1.841600 time 2019-02-26 23:05:05.611951
Model ind 640 epoch 822 head B head_i_epoch 1 batch 200: avg loss -1.981042 avg loss no lamb -1.981042 time 2019-02-26 23:07:05.071714
last batch sz 160
Pre: time 2019-02-26 23:08:57.638364: 
 	std: 0.050442774
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5125167, 0.61475, 0.61518335, 0.6153833, 0.5117667]
	train_accs: [0.5125167, 0.61475, 0.61518335, 0.6153833, 0.5117667]
	best_train_sub_head: 3
	worst: 0.5117667
	avg: 0.57392
	best: 0.6153833

Starting e_i: 823
Model ind 640 epoch 823 head A head_i_epoch 0 batch 0: avg loss -3.209635 avg loss no lamb -3.209635 time 2019-02-26 23:09:01.190240
Model ind 640 epoch 823 head A head_i_epoch 0 batch 100: avg loss -3.170686 avg loss no lamb -3.170686 time 2019-02-26 23:11:00.439070
Model ind 640 epoch 823 head A head_i_epoch 0 batch 200: avg loss -3.179374 avg loss no lamb -3.179374 time 2019-02-26 23:13:00.674820
last batch sz 160
Model ind 640 epoch 823 head B head_i_epoch 0 batch 0: avg loss -1.880390 avg loss no lamb -1.880390 time 2019-02-26 23:14:22.260034
Model ind 640 epoch 823 head B head_i_epoch 0 batch 100: avg loss -1.810287 avg loss no lamb -1.810287 time 2019-02-26 23:16:15.588289
Model ind 640 epoch 823 head B head_i_epoch 0 batch 200: avg loss -1.830012 avg loss no lamb -1.830012 time 2019-02-26 23:18:09.547979
last batch sz 160
Model ind 640 epoch 823 head B head_i_epoch 1 batch 0: avg loss -1.854496 avg loss no lamb -1.854496 time 2019-02-26 23:19:29.720746
Model ind 640 epoch 823 head B head_i_epoch 1 batch 100: avg loss -1.775321 avg loss no lamb -1.775321 time 2019-02-26 23:21:24.691931
Model ind 640 epoch 823 head B head_i_epoch 1 batch 200: avg loss -1.895024 avg loss no lamb -1.895024 time 2019-02-26 23:23:18.332252
last batch sz 160
Pre: time 2019-02-26 23:25:10.595290: 
 	std: 0.050404735
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51105, 0.61325, 0.6136, 0.6135333, 0.5101]
	train_accs: [0.51105, 0.61325, 0.6136, 0.6135333, 0.5101]
	best_train_sub_head: 2
	worst: 0.5101
	avg: 0.57230663
	best: 0.6136

Starting e_i: 824
Model ind 640 epoch 824 head A head_i_epoch 0 batch 0: avg loss -3.127317 avg loss no lamb -3.127317 time 2019-02-26 23:25:13.444259
Model ind 640 epoch 824 head A head_i_epoch 0 batch 100: avg loss -3.111785 avg loss no lamb -3.111785 time 2019-02-26 23:27:09.102303
Model ind 640 epoch 824 head A head_i_epoch 0 batch 200: avg loss -3.203493 avg loss no lamb -3.203493 time 2019-02-26 23:29:10.587707
last batch sz 160
Model ind 640 epoch 824 head B head_i_epoch 0 batch 0: avg loss -1.845639 avg loss no lamb -1.845639 time 2019-02-26 23:30:37.495403
Model ind 640 epoch 824 head B head_i_epoch 0 batch 100: avg loss -1.682279 avg loss no lamb -1.682279 time 2019-02-26 23:32:38.526364
Model ind 640 epoch 824 head B head_i_epoch 0 batch 200: avg loss -1.894582 avg loss no lamb -1.894582 time 2019-02-26 23:34:36.037699
last batch sz 160
Model ind 640 epoch 824 head B head_i_epoch 1 batch 0: avg loss -1.836766 avg loss no lamb -1.836766 time 2019-02-26 23:36:03.920496
Model ind 640 epoch 824 head B head_i_epoch 1 batch 100: avg loss -1.760831 avg loss no lamb -1.760831 time 2019-02-26 23:38:01.324897
Model ind 640 epoch 824 head B head_i_epoch 1 batch 200: avg loss -1.850502 avg loss no lamb -1.850502 time 2019-02-26 23:40:00.329431
last batch sz 160
Pre: time 2019-02-26 23:41:55.524725: 
 	std: 0.05110207
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5093, 0.61326665, 0.6134833, 0.61373335, 0.50906664]
	train_accs: [0.5093, 0.61326665, 0.6134833, 0.61373335, 0.50906664]
	best_train_sub_head: 3
	worst: 0.50906664
	avg: 0.57177
	best: 0.61373335

Starting e_i: 825
Model ind 640 epoch 825 head A head_i_epoch 0 batch 0: avg loss -3.222867 avg loss no lamb -3.222867 time 2019-02-26 23:41:58.604087
Model ind 640 epoch 825 head A head_i_epoch 0 batch 100: avg loss -3.165874 avg loss no lamb -3.165874 time 2019-02-26 23:43:53.016610
Model ind 640 epoch 825 head A head_i_epoch 0 batch 200: avg loss -3.218094 avg loss no lamb -3.218094 time 2019-02-26 23:46:00.146044
last batch sz 160
Model ind 640 epoch 825 head B head_i_epoch 0 batch 0: avg loss -1.853702 avg loss no lamb -1.853702 time 2019-02-26 23:47:26.090014
Model ind 640 epoch 825 head B head_i_epoch 0 batch 100: avg loss -1.785785 avg loss no lamb -1.785785 time 2019-02-26 23:49:28.530559
Model ind 640 epoch 825 head B head_i_epoch 0 batch 200: avg loss -1.860683 avg loss no lamb -1.860683 time 2019-02-26 23:51:36.221483
last batch sz 160
Model ind 640 epoch 825 head B head_i_epoch 1 batch 0: avg loss -1.862162 avg loss no lamb -1.862162 time 2019-02-26 23:53:00.001289
Model ind 640 epoch 825 head B head_i_epoch 1 batch 100: avg loss -1.818867 avg loss no lamb -1.818867 time 2019-02-26 23:55:05.205025
Model ind 640 epoch 825 head B head_i_epoch 1 batch 200: avg loss -1.915920 avg loss no lamb -1.915920 time 2019-02-26 23:57:01.749279
last batch sz 160
Pre: time 2019-02-26 23:58:52.150166: 
 	std: 0.050695002
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5112333, 0.61471665, 0.6145, 0.6146, 0.51101667]
	train_accs: [0.5112333, 0.61471665, 0.6145, 0.6146, 0.51101667]
	best_train_sub_head: 1
	worst: 0.51101667
	avg: 0.5732133
	best: 0.61471665

Starting e_i: 826
Model ind 640 epoch 826 head A head_i_epoch 0 batch 0: avg loss -3.197919 avg loss no lamb -3.197919 time 2019-02-26 23:58:54.899042
Model ind 640 epoch 826 head A head_i_epoch 0 batch 100: avg loss -3.115496 avg loss no lamb -3.115496 time 2019-02-27 00:00:52.503127
Model ind 640 epoch 826 head A head_i_epoch 0 batch 200: avg loss -3.186791 avg loss no lamb -3.186791 time 2019-02-27 00:02:53.710151
last batch sz 160
Model ind 640 epoch 826 head B head_i_epoch 0 batch 0: avg loss -1.804098 avg loss no lamb -1.804098 time 2019-02-27 00:04:18.210659
Model ind 640 epoch 826 head B head_i_epoch 0 batch 100: avg loss -1.710295 avg loss no lamb -1.710295 time 2019-02-27 00:06:20.573622
Model ind 640 epoch 826 head B head_i_epoch 0 batch 200: avg loss -1.883434 avg loss no lamb -1.883434 time 2019-02-27 00:08:17.596622
last batch sz 160
Model ind 640 epoch 826 head B head_i_epoch 1 batch 0: avg loss -1.911646 avg loss no lamb -1.911646 time 2019-02-27 00:09:47.843203
Model ind 640 epoch 826 head B head_i_epoch 1 batch 100: avg loss -1.728110 avg loss no lamb -1.728110 time 2019-02-27 00:11:46.921295
Model ind 640 epoch 826 head B head_i_epoch 1 batch 200: avg loss -1.903603 avg loss no lamb -1.903603 time 2019-02-27 00:13:41.216028
last batch sz 160
Pre: time 2019-02-27 00:15:40.517401: 
 	std: 0.050440684
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51086664, 0.6135167, 0.61363333, 0.61363333, 0.5104]
	train_accs: [0.51086664, 0.6135167, 0.61363333, 0.61363333, 0.5104]
	best_train_sub_head: 2
	worst: 0.5104
	avg: 0.57241
	best: 0.61363333

Starting e_i: 827
Model ind 640 epoch 827 head A head_i_epoch 0 batch 0: avg loss -3.179884 avg loss no lamb -3.179884 time 2019-02-27 00:15:43.452950
Model ind 640 epoch 827 head A head_i_epoch 0 batch 100: avg loss -3.214931 avg loss no lamb -3.214931 time 2019-02-27 00:17:39.474390
Model ind 640 epoch 827 head A head_i_epoch 0 batch 200: avg loss -3.131299 avg loss no lamb -3.131299 time 2019-02-27 00:19:44.792270
last batch sz 160
Model ind 640 epoch 827 head B head_i_epoch 0 batch 0: avg loss -1.804039 avg loss no lamb -1.804039 time 2019-02-27 00:21:16.072072
Model ind 640 epoch 827 head B head_i_epoch 0 batch 100: avg loss -1.769203 avg loss no lamb -1.769203 time 2019-02-27 00:23:13.148767
Model ind 640 epoch 827 head B head_i_epoch 0 batch 200: avg loss -1.907616 avg loss no lamb -1.907616 time 2019-02-27 00:25:17.171640
last batch sz 160
Model ind 640 epoch 827 head B head_i_epoch 1 batch 0: avg loss -1.903165 avg loss no lamb -1.903165 time 2019-02-27 00:26:39.324579
Model ind 640 epoch 827 head B head_i_epoch 1 batch 100: avg loss -1.888380 avg loss no lamb -1.888380 time 2019-02-27 00:28:32.503358
Model ind 640 epoch 827 head B head_i_epoch 1 batch 200: avg loss -1.888549 avg loss no lamb -1.888549 time 2019-02-27 00:30:22.820340
last batch sz 160
Pre: time 2019-02-27 00:32:09.013127: 
 	std: 0.050949547
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5091, 0.61293334, 0.6128167, 0.613, 0.50873333]
	train_accs: [0.5091, 0.61293334, 0.6128167, 0.613, 0.50873333]
	best_train_sub_head: 3
	worst: 0.50873333
	avg: 0.57131666
	best: 0.613

Starting e_i: 828
Model ind 640 epoch 828 head A head_i_epoch 0 batch 0: avg loss -3.196771 avg loss no lamb -3.196771 time 2019-02-27 00:32:11.575436
Model ind 640 epoch 828 head A head_i_epoch 0 batch 100: avg loss -3.123869 avg loss no lamb -3.123869 time 2019-02-27 00:34:01.073163
Model ind 640 epoch 828 head A head_i_epoch 0 batch 200: avg loss -3.243777 avg loss no lamb -3.243777 time 2019-02-27 00:35:55.042528
last batch sz 160
Model ind 640 epoch 828 head B head_i_epoch 0 batch 0: avg loss -1.814842 avg loss no lamb -1.814842 time 2019-02-27 00:37:11.324683
Model ind 640 epoch 828 head B head_i_epoch 0 batch 100: avg loss -1.746478 avg loss no lamb -1.746478 time 2019-02-27 00:39:06.802822
Model ind 640 epoch 828 head B head_i_epoch 0 batch 200: avg loss -1.922950 avg loss no lamb -1.922950 time 2019-02-27 00:40:57.684133
last batch sz 160
Model ind 640 epoch 828 head B head_i_epoch 1 batch 0: avg loss -1.828656 avg loss no lamb -1.828656 time 2019-02-27 00:42:20.256784
Model ind 640 epoch 828 head B head_i_epoch 1 batch 100: avg loss -1.820542 avg loss no lamb -1.820542 time 2019-02-27 00:44:10.959208
Model ind 640 epoch 828 head B head_i_epoch 1 batch 200: avg loss -1.860242 avg loss no lamb -1.860242 time 2019-02-27 00:46:03.386804
last batch sz 160
Pre: time 2019-02-27 00:47:47.536221: 
 	std: 0.05079656
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51203334, 0.61505, 0.61536664, 0.61536664, 0.5111167]
	train_accs: [0.51203334, 0.61505, 0.61536664, 0.61536664, 0.5111167]
	best_train_sub_head: 2
	worst: 0.5111167
	avg: 0.5737867
	best: 0.61536664

Starting e_i: 829
Model ind 640 epoch 829 head A head_i_epoch 0 batch 0: avg loss -3.209612 avg loss no lamb -3.209612 time 2019-02-27 00:47:50.107722
Model ind 640 epoch 829 head A head_i_epoch 0 batch 100: avg loss -3.120167 avg loss no lamb -3.120167 time 2019-02-27 00:49:41.383862
Model ind 640 epoch 829 head A head_i_epoch 0 batch 200: avg loss -3.192771 avg loss no lamb -3.192771 time 2019-02-27 00:51:35.745294
last batch sz 160
Model ind 640 epoch 829 head B head_i_epoch 0 batch 0: avg loss -1.868039 avg loss no lamb -1.868039 time 2019-02-27 00:52:56.204071
Model ind 640 epoch 829 head B head_i_epoch 0 batch 100: avg loss -1.696343 avg loss no lamb -1.696343 time 2019-02-27 00:54:49.765422
Model ind 640 epoch 829 head B head_i_epoch 0 batch 200: avg loss -1.834165 avg loss no lamb -1.834165 time 2019-02-27 00:56:40.333623
last batch sz 160
Model ind 640 epoch 829 head B head_i_epoch 1 batch 0: avg loss -1.813685 avg loss no lamb -1.813685 time 2019-02-27 00:58:04.259666
Model ind 640 epoch 829 head B head_i_epoch 1 batch 100: avg loss -1.763312 avg loss no lamb -1.763312 time 2019-02-27 00:59:53.674322
Model ind 640 epoch 829 head B head_i_epoch 1 batch 200: avg loss -1.940030 avg loss no lamb -1.940030 time 2019-02-27 01:01:46.034403
last batch sz 160
Pre: time 2019-02-27 01:03:27.561092: 
 	std: 0.050982412
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51101667, 0.61468333, 0.6148, 0.6149167, 0.51045]
	train_accs: [0.51101667, 0.61468333, 0.6148, 0.6149167, 0.51045]
	best_train_sub_head: 3
	worst: 0.51045
	avg: 0.57317334
	best: 0.6149167

Starting e_i: 830
Model ind 640 epoch 830 head A head_i_epoch 0 batch 0: avg loss -3.207862 avg loss no lamb -3.207862 time 2019-02-27 01:03:30.226333
Model ind 640 epoch 830 head A head_i_epoch 0 batch 100: avg loss -3.122125 avg loss no lamb -3.122125 time 2019-02-27 01:05:24.248410
Model ind 640 epoch 830 head A head_i_epoch 0 batch 200: avg loss -3.131113 avg loss no lamb -3.131113 time 2019-02-27 01:07:14.578611
last batch sz 160
Model ind 640 epoch 830 head B head_i_epoch 0 batch 0: avg loss -1.900539 avg loss no lamb -1.900539 time 2019-02-27 01:08:37.358636
Model ind 640 epoch 830 head B head_i_epoch 0 batch 100: avg loss -1.678707 avg loss no lamb -1.678707 time 2019-02-27 01:10:27.627390
Model ind 640 epoch 830 head B head_i_epoch 0 batch 200: avg loss -1.967219 avg loss no lamb -1.967219 time 2019-02-27 01:12:20.474300
last batch sz 160
Model ind 640 epoch 830 head B head_i_epoch 1 batch 0: avg loss -1.876863 avg loss no lamb -1.876863 time 2019-02-27 01:13:41.495468
Model ind 640 epoch 830 head B head_i_epoch 1 batch 100: avg loss -1.681985 avg loss no lamb -1.681985 time 2019-02-27 01:15:32.699207
Model ind 640 epoch 830 head B head_i_epoch 1 batch 200: avg loss -1.939473 avg loss no lamb -1.939473 time 2019-02-27 01:17:26.029158
last batch sz 160
Pre: time 2019-02-27 01:19:07.315774: 
 	std: 0.050373845
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108333, 0.6134833, 0.6135167, 0.6137, 0.51065]
	train_accs: [0.5108333, 0.6134833, 0.6135167, 0.6137, 0.51065]
	best_train_sub_head: 3
	worst: 0.51065
	avg: 0.5724367
	best: 0.6137

Starting e_i: 831
Model ind 640 epoch 831 head A head_i_epoch 0 batch 0: avg loss -3.153993 avg loss no lamb -3.153993 time 2019-02-27 01:19:13.129860
Model ind 640 epoch 831 head A head_i_epoch 0 batch 100: avg loss -3.126827 avg loss no lamb -3.126827 time 2019-02-27 01:21:08.021703
Model ind 640 epoch 831 head A head_i_epoch 0 batch 200: avg loss -3.238263 avg loss no lamb -3.238263 time 2019-02-27 01:22:57.998935
last batch sz 160
Model ind 640 epoch 831 head B head_i_epoch 0 batch 0: avg loss -1.786324 avg loss no lamb -1.786324 time 2019-02-27 01:24:19.713941
Model ind 640 epoch 831 head B head_i_epoch 0 batch 100: avg loss -1.756497 avg loss no lamb -1.756497 time 2019-02-27 01:26:10.548993
Model ind 640 epoch 831 head B head_i_epoch 0 batch 200: avg loss -1.888238 avg loss no lamb -1.888238 time 2019-02-27 01:28:05.134636
last batch sz 160
Model ind 640 epoch 831 head B head_i_epoch 1 batch 0: avg loss -1.854217 avg loss no lamb -1.854217 time 2019-02-27 01:29:24.627614
Model ind 640 epoch 831 head B head_i_epoch 1 batch 100: avg loss -1.787186 avg loss no lamb -1.787186 time 2019-02-27 01:31:19.043449
Model ind 640 epoch 831 head B head_i_epoch 1 batch 200: avg loss -1.891982 avg loss no lamb -1.891982 time 2019-02-27 01:33:17.436950
last batch sz 160
Pre: time 2019-02-27 01:35:07.147576: 
 	std: 0.050153542
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115, 0.61356664, 0.6137667, 0.61361665, 0.51105]
	train_accs: [0.5115, 0.61356664, 0.6137667, 0.61361665, 0.51105]
	best_train_sub_head: 2
	worst: 0.51105
	avg: 0.57269996
	best: 0.6137667

Starting e_i: 832
Model ind 640 epoch 832 head A head_i_epoch 0 batch 0: avg loss -3.180793 avg loss no lamb -3.180793 time 2019-02-27 01:35:09.626066
Model ind 640 epoch 832 head A head_i_epoch 0 batch 100: avg loss -3.067504 avg loss no lamb -3.067504 time 2019-02-27 01:37:14.490069
Model ind 640 epoch 832 head A head_i_epoch 0 batch 200: avg loss -3.215642 avg loss no lamb -3.215642 time 2019-02-27 01:39:08.726682
last batch sz 160
Model ind 640 epoch 832 head B head_i_epoch 0 batch 0: avg loss -1.851391 avg loss no lamb -1.851391 time 2019-02-27 01:40:41.130466
Model ind 640 epoch 832 head B head_i_epoch 0 batch 100: avg loss -1.827235 avg loss no lamb -1.827235 time 2019-02-27 01:42:38.352385
Model ind 640 epoch 832 head B head_i_epoch 0 batch 200: avg loss -1.862463 avg loss no lamb -1.862463 time 2019-02-27 01:44:37.486063
last batch sz 160
Model ind 640 epoch 832 head B head_i_epoch 1 batch 0: avg loss -1.883131 avg loss no lamb -1.883131 time 2019-02-27 01:46:04.283342
Model ind 640 epoch 832 head B head_i_epoch 1 batch 100: avg loss -1.825637 avg loss no lamb -1.825637 time 2019-02-27 01:48:01.345536
Model ind 640 epoch 832 head B head_i_epoch 1 batch 200: avg loss -1.860337 avg loss no lamb -1.860337 time 2019-02-27 01:50:01.079621
last batch sz 160
Pre: time 2019-02-27 01:51:53.265354: 
 	std: 0.051175334
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107, 0.61506665, 0.61501664, 0.6152, 0.51056665]
	train_accs: [0.5107, 0.61506665, 0.61501664, 0.6152, 0.51056665]
	best_train_sub_head: 3
	worst: 0.51056665
	avg: 0.57331
	best: 0.6152

Starting e_i: 833
Model ind 640 epoch 833 head A head_i_epoch 0 batch 0: avg loss -3.200778 avg loss no lamb -3.200778 time 2019-02-27 01:51:56.195181
Model ind 640 epoch 833 head A head_i_epoch 0 batch 100: avg loss -3.197608 avg loss no lamb -3.197608 time 2019-02-27 01:53:53.351061
Model ind 640 epoch 833 head A head_i_epoch 0 batch 200: avg loss -3.221334 avg loss no lamb -3.221334 time 2019-02-27 01:55:53.547263
last batch sz 160
Model ind 640 epoch 833 head B head_i_epoch 0 batch 0: avg loss -1.773417 avg loss no lamb -1.773417 time 2019-02-27 01:57:26.949057
Model ind 640 epoch 833 head B head_i_epoch 0 batch 100: avg loss -1.813084 avg loss no lamb -1.813084 time 2019-02-27 01:59:22.496506
Model ind 640 epoch 833 head B head_i_epoch 0 batch 200: avg loss -1.843834 avg loss no lamb -1.843834 time 2019-02-27 02:01:30.433024
last batch sz 160
Model ind 640 epoch 833 head B head_i_epoch 1 batch 0: avg loss -1.814911 avg loss no lamb -1.814911 time 2019-02-27 02:02:51.246942
Model ind 640 epoch 833 head B head_i_epoch 1 batch 100: avg loss -1.795277 avg loss no lamb -1.795277 time 2019-02-27 02:05:01.159853
Model ind 640 epoch 833 head B head_i_epoch 1 batch 200: avg loss -1.896060 avg loss no lamb -1.896060 time 2019-02-27 02:07:04.523814
last batch sz 160
Pre: time 2019-02-27 02:08:50.853455: 
 	std: 0.050516717
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50953335, 0.61268336, 0.61245, 0.61261666, 0.5094]
	train_accs: [0.50953335, 0.61268336, 0.61245, 0.61261666, 0.5094]
	best_train_sub_head: 1
	worst: 0.5094
	avg: 0.5713366
	best: 0.61268336

Starting e_i: 834
Model ind 640 epoch 834 head A head_i_epoch 0 batch 0: avg loss -3.163118 avg loss no lamb -3.163118 time 2019-02-27 02:08:53.985554
Model ind 640 epoch 834 head A head_i_epoch 0 batch 100: avg loss -3.156512 avg loss no lamb -3.156512 time 2019-02-27 02:10:57.352910
Model ind 640 epoch 834 head A head_i_epoch 0 batch 200: avg loss -3.230320 avg loss no lamb -3.230320 time 2019-02-27 02:12:55.948998
last batch sz 160
Model ind 640 epoch 834 head B head_i_epoch 0 batch 0: avg loss -1.881002 avg loss no lamb -1.881002 time 2019-02-27 02:14:25.604303
Model ind 640 epoch 834 head B head_i_epoch 0 batch 100: avg loss -1.774840 avg loss no lamb -1.774840 time 2019-02-27 02:16:24.894009
Model ind 640 epoch 834 head B head_i_epoch 0 batch 200: avg loss -1.911739 avg loss no lamb -1.911739 time 2019-02-27 02:18:26.753978
last batch sz 160
Model ind 640 epoch 834 head B head_i_epoch 1 batch 0: avg loss -1.800848 avg loss no lamb -1.800848 time 2019-02-27 02:19:51.148634
Model ind 640 epoch 834 head B head_i_epoch 1 batch 100: avg loss -1.801882 avg loss no lamb -1.801882 time 2019-02-27 02:21:53.129454
Model ind 640 epoch 834 head B head_i_epoch 1 batch 200: avg loss -1.829392 avg loss no lamb -1.829392 time 2019-02-27 02:23:50.037228
last batch sz 160
Pre: time 2019-02-27 02:25:42.009230: 
 	std: 0.05092235
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50946665, 0.61361665, 0.61343336, 0.61368334, 0.5098]
	train_accs: [0.50946665, 0.61361665, 0.61343336, 0.61368334, 0.5098]
	best_train_sub_head: 3
	worst: 0.50946665
	avg: 0.572
	best: 0.61368334

Starting e_i: 835
Model ind 640 epoch 835 head A head_i_epoch 0 batch 0: avg loss -3.235507 avg loss no lamb -3.235507 time 2019-02-27 02:25:44.523808
Model ind 640 epoch 835 head A head_i_epoch 0 batch 100: avg loss -3.136069 avg loss no lamb -3.136069 time 2019-02-27 02:27:48.346579
Model ind 640 epoch 835 head A head_i_epoch 0 batch 200: avg loss -3.227584 avg loss no lamb -3.227584 time 2019-02-27 02:29:42.067236
last batch sz 160
Model ind 640 epoch 835 head B head_i_epoch 0 batch 0: avg loss -1.848109 avg loss no lamb -1.848109 time 2019-02-27 02:31:16.248899
Model ind 640 epoch 835 head B head_i_epoch 0 batch 100: avg loss -1.803081 avg loss no lamb -1.803081 time 2019-02-27 02:33:09.223048
Model ind 640 epoch 835 head B head_i_epoch 0 batch 200: avg loss -1.927559 avg loss no lamb -1.927559 time 2019-02-27 02:35:13.761277
last batch sz 160
Model ind 640 epoch 835 head B head_i_epoch 1 batch 0: avg loss -1.835630 avg loss no lamb -1.835630 time 2019-02-27 02:36:38.579261
Model ind 640 epoch 835 head B head_i_epoch 1 batch 100: avg loss -1.825346 avg loss no lamb -1.825346 time 2019-02-27 02:38:43.718472
Model ind 640 epoch 835 head B head_i_epoch 1 batch 200: avg loss -1.887781 avg loss no lamb -1.887781 time 2019-02-27 02:40:46.766004
last batch sz 160
Pre: time 2019-02-27 02:42:30.786303: 
 	std: 0.05070055
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50975, 0.61296666, 0.6131, 0.61303335, 0.5093333]
	train_accs: [0.50975, 0.61296666, 0.6131, 0.61303335, 0.5093333]
	best_train_sub_head: 2
	worst: 0.5093333
	avg: 0.5716367
	best: 0.6131

Starting e_i: 836
Model ind 640 epoch 836 head A head_i_epoch 0 batch 0: avg loss -3.205505 avg loss no lamb -3.205505 time 2019-02-27 02:42:34.762695
Model ind 640 epoch 836 head A head_i_epoch 0 batch 100: avg loss -3.057680 avg loss no lamb -3.057680 time 2019-02-27 02:44:37.829489
Model ind 640 epoch 836 head A head_i_epoch 0 batch 200: avg loss -3.247974 avg loss no lamb -3.247974 time 2019-02-27 02:46:35.394041
last batch sz 160
Model ind 640 epoch 836 head B head_i_epoch 0 batch 0: avg loss -1.853182 avg loss no lamb -1.853182 time 2019-02-27 02:48:02.592129
Model ind 640 epoch 836 head B head_i_epoch 0 batch 100: avg loss -1.727998 avg loss no lamb -1.727998 time 2019-02-27 02:50:01.022565
Model ind 640 epoch 836 head B head_i_epoch 0 batch 200: avg loss -1.891779 avg loss no lamb -1.891779 time 2019-02-27 02:52:02.668519
last batch sz 160
Model ind 640 epoch 836 head B head_i_epoch 1 batch 0: avg loss -1.850809 avg loss no lamb -1.850809 time 2019-02-27 02:53:26.352748
Model ind 640 epoch 836 head B head_i_epoch 1 batch 100: avg loss -1.958075 avg loss no lamb -1.958075 time 2019-02-27 02:55:30.490432
Model ind 640 epoch 836 head B head_i_epoch 1 batch 200: avg loss -1.955283 avg loss no lamb -1.955283 time 2019-02-27 02:57:28.776503
last batch sz 160
Pre: time 2019-02-27 02:59:15.672175: 
 	std: 0.051473577
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50801665, 0.61286664, 0.6128, 0.6128167, 0.5075]
	train_accs: [0.50801665, 0.61286664, 0.6128, 0.6128167, 0.5075]
	best_train_sub_head: 1
	worst: 0.5075
	avg: 0.57079995
	best: 0.61286664

Starting e_i: 837
Model ind 640 epoch 837 head A head_i_epoch 0 batch 0: avg loss -3.151292 avg loss no lamb -3.151292 time 2019-02-27 02:59:18.064804
Model ind 640 epoch 837 head A head_i_epoch 0 batch 100: avg loss -3.093226 avg loss no lamb -3.093226 time 2019-02-27 03:01:25.386853
Model ind 640 epoch 837 head A head_i_epoch 0 batch 200: avg loss -3.262505 avg loss no lamb -3.262505 time 2019-02-27 03:03:18.179174
last batch sz 160
Model ind 640 epoch 837 head B head_i_epoch 0 batch 0: avg loss -1.893137 avg loss no lamb -1.893137 time 2019-02-27 03:04:56.024069
Model ind 640 epoch 837 head B head_i_epoch 0 batch 100: avg loss -1.769506 avg loss no lamb -1.769506 time 2019-02-27 03:06:49.667271
Model ind 640 epoch 837 head B head_i_epoch 0 batch 200: avg loss -1.867151 avg loss no lamb -1.867151 time 2019-02-27 03:08:53.486843
last batch sz 160
Model ind 640 epoch 837 head B head_i_epoch 1 batch 0: avg loss -1.800603 avg loss no lamb -1.800603 time 2019-02-27 03:10:19.657810
Model ind 640 epoch 837 head B head_i_epoch 1 batch 100: avg loss -1.733272 avg loss no lamb -1.733272 time 2019-02-27 03:12:20.088228
Model ind 640 epoch 837 head B head_i_epoch 1 batch 200: avg loss -1.915137 avg loss no lamb -1.915137 time 2019-02-27 03:14:23.354035
last batch sz 160
Pre: time 2019-02-27 03:16:10.817110: 
 	std: 0.05134577
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51018333, 0.61466664, 0.6146333, 0.61475, 0.50956666]
	train_accs: [0.51018333, 0.61466664, 0.6146333, 0.61475, 0.50956666]
	best_train_sub_head: 3
	worst: 0.50956666
	avg: 0.57276
	best: 0.61475

Starting e_i: 838
Model ind 640 epoch 838 head A head_i_epoch 0 batch 0: avg loss -3.162255 avg loss no lamb -3.162255 time 2019-02-27 03:16:13.703883
Model ind 640 epoch 838 head A head_i_epoch 0 batch 100: avg loss -3.192585 avg loss no lamb -3.192585 time 2019-02-27 03:18:14.092791
Model ind 640 epoch 838 head A head_i_epoch 0 batch 200: avg loss -3.198339 avg loss no lamb -3.198339 time 2019-02-27 03:20:13.746744
last batch sz 160
Model ind 640 epoch 838 head B head_i_epoch 0 batch 0: avg loss -1.841342 avg loss no lamb -1.841342 time 2019-02-27 03:21:40.124029
Model ind 640 epoch 838 head B head_i_epoch 0 batch 100: avg loss -1.828512 avg loss no lamb -1.828512 time 2019-02-27 03:23:38.029250
Model ind 640 epoch 838 head B head_i_epoch 0 batch 200: avg loss -1.918791 avg loss no lamb -1.918791 time 2019-02-27 03:25:41.854583
last batch sz 160
Model ind 640 epoch 838 head B head_i_epoch 1 batch 0: avg loss -1.756937 avg loss no lamb -1.756937 time 2019-02-27 03:27:04.801747
Model ind 640 epoch 838 head B head_i_epoch 1 batch 100: avg loss -1.718580 avg loss no lamb -1.718580 time 2019-02-27 03:29:08.931300
Model ind 640 epoch 838 head B head_i_epoch 1 batch 200: avg loss -1.831823 avg loss no lamb -1.831823 time 2019-02-27 03:31:07.553253
last batch sz 160
Pre: time 2019-02-27 03:32:58.905392: 
 	std: 0.050652746
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51068336, 0.61415, 0.6141667, 0.6141667, 0.51085]
	train_accs: [0.51068336, 0.61415, 0.6141667, 0.6141667, 0.51085]
	best_train_sub_head: 2
	worst: 0.51068336
	avg: 0.5728034
	best: 0.6141667

Starting e_i: 839
Model ind 640 epoch 839 head A head_i_epoch 0 batch 0: avg loss -3.136713 avg loss no lamb -3.136713 time 2019-02-27 03:33:01.371331
Model ind 640 epoch 839 head A head_i_epoch 0 batch 100: avg loss -3.127552 avg loss no lamb -3.127552 time 2019-02-27 03:35:09.073832
Model ind 640 epoch 839 head A head_i_epoch 0 batch 200: avg loss -3.139016 avg loss no lamb -3.139016 time 2019-02-27 03:37:03.329106
last batch sz 160
Model ind 640 epoch 839 head B head_i_epoch 0 batch 0: avg loss -1.804179 avg loss no lamb -1.804179 time 2019-02-27 03:38:42.049328
Model ind 640 epoch 839 head B head_i_epoch 0 batch 100: avg loss -1.828060 avg loss no lamb -1.828060 time 2019-02-27 03:40:39.833486
Model ind 640 epoch 839 head B head_i_epoch 0 batch 200: avg loss -1.888059 avg loss no lamb -1.888059 time 2019-02-27 03:42:41.759475
last batch sz 160
Model ind 640 epoch 839 head B head_i_epoch 1 batch 0: avg loss -1.839469 avg loss no lamb -1.839469 time 2019-02-27 03:44:09.595895
Model ind 640 epoch 839 head B head_i_epoch 1 batch 100: avg loss -1.819503 avg loss no lamb -1.819503 time 2019-02-27 03:46:06.668316
Model ind 640 epoch 839 head B head_i_epoch 1 batch 200: avg loss -1.942490 avg loss no lamb -1.942490 time 2019-02-27 03:48:10.877825
last batch sz 160
Pre: time 2019-02-27 03:49:58.662514: 
 	std: 0.0505933
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50906664, 0.61245, 0.6123667, 0.61285, 0.5095]
	train_accs: [0.50906664, 0.61245, 0.6123667, 0.61285, 0.5095]
	best_train_sub_head: 3
	worst: 0.50906664
	avg: 0.5712467
	best: 0.61285

Starting e_i: 840
Model ind 640 epoch 840 head A head_i_epoch 0 batch 0: avg loss -3.131093 avg loss no lamb -3.131093 time 2019-02-27 03:50:01.345263
Model ind 640 epoch 840 head A head_i_epoch 0 batch 100: avg loss -3.133677 avg loss no lamb -3.133677 time 2019-02-27 03:52:00.792494
Model ind 640 epoch 840 head A head_i_epoch 0 batch 200: avg loss -3.210428 avg loss no lamb -3.210428 time 2019-02-27 03:53:59.935833
last batch sz 160
Model ind 640 epoch 840 head B head_i_epoch 0 batch 0: avg loss -1.856847 avg loss no lamb -1.856847 time 2019-02-27 03:55:28.076086
Model ind 640 epoch 840 head B head_i_epoch 0 batch 100: avg loss -1.818682 avg loss no lamb -1.818682 time 2019-02-27 03:57:27.043655
Model ind 640 epoch 840 head B head_i_epoch 0 batch 200: avg loss -1.911227 avg loss no lamb -1.911227 time 2019-02-27 03:59:29.743550
last batch sz 160
Model ind 640 epoch 840 head B head_i_epoch 1 batch 0: avg loss -1.899723 avg loss no lamb -1.899723 time 2019-02-27 04:00:55.947193
Model ind 640 epoch 840 head B head_i_epoch 1 batch 100: avg loss -1.827276 avg loss no lamb -1.827276 time 2019-02-27 04:02:57.235606
Model ind 640 epoch 840 head B head_i_epoch 1 batch 200: avg loss -1.883229 avg loss no lamb -1.883229 time 2019-02-27 04:05:02.123369
last batch sz 160
Pre: time 2019-02-27 04:06:46.273052: 
 	std: 0.050118864
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51, 0.61258334, 0.6126, 0.613, 0.51085]
	train_accs: [0.51, 0.61258334, 0.6126, 0.613, 0.51085]
	best_train_sub_head: 3
	worst: 0.51
	avg: 0.57180667
	best: 0.613

Starting e_i: 841
Model ind 640 epoch 841 head A head_i_epoch 0 batch 0: avg loss -3.208324 avg loss no lamb -3.208324 time 2019-02-27 04:06:55.292197
Model ind 640 epoch 841 head A head_i_epoch 0 batch 100: avg loss -3.095479 avg loss no lamb -3.095479 time 2019-02-27 04:09:00.640130
Model ind 640 epoch 841 head A head_i_epoch 0 batch 200: avg loss -3.116807 avg loss no lamb -3.116807 time 2019-02-27 04:11:07.250262
last batch sz 160
Model ind 640 epoch 841 head B head_i_epoch 0 batch 0: avg loss -1.853835 avg loss no lamb -1.853835 time 2019-02-27 04:12:32.152320
Model ind 640 epoch 841 head B head_i_epoch 0 batch 100: avg loss -1.830559 avg loss no lamb -1.830559 time 2019-02-27 04:14:34.702666
Model ind 640 epoch 841 head B head_i_epoch 0 batch 200: avg loss -1.919679 avg loss no lamb -1.919679 time 2019-02-27 04:16:29.010672
last batch sz 160
Model ind 640 epoch 841 head B head_i_epoch 1 batch 0: avg loss -1.815258 avg loss no lamb -1.815258 time 2019-02-27 04:18:01.169447
Model ind 640 epoch 841 head B head_i_epoch 1 batch 100: avg loss -1.756756 avg loss no lamb -1.756756 time 2019-02-27 04:19:58.096928
Model ind 640 epoch 841 head B head_i_epoch 1 batch 200: avg loss -1.877927 avg loss no lamb -1.877927 time 2019-02-27 04:21:57.625951
last batch sz 160
Pre: time 2019-02-27 04:23:46.046973: 
 	std: 0.050566256
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101, 0.6137, 0.61373335, 0.61371666, 0.5109]
	train_accs: [0.5101, 0.6137, 0.61373335, 0.61371666, 0.5109]
	best_train_sub_head: 2
	worst: 0.5101
	avg: 0.57243
	best: 0.61373335

Starting e_i: 842
Model ind 640 epoch 842 head A head_i_epoch 0 batch 0: avg loss -3.187808 avg loss no lamb -3.187808 time 2019-02-27 04:23:49.011832
Model ind 640 epoch 842 head A head_i_epoch 0 batch 100: avg loss -3.149043 avg loss no lamb -3.149043 time 2019-02-27 04:25:49.989721
Model ind 640 epoch 842 head A head_i_epoch 0 batch 200: avg loss -3.200267 avg loss no lamb -3.200267 time 2019-02-27 04:27:47.886337
last batch sz 160
Model ind 640 epoch 842 head B head_i_epoch 0 batch 0: avg loss -1.805637 avg loss no lamb -1.805637 time 2019-02-27 04:29:16.143912
Model ind 640 epoch 842 head B head_i_epoch 0 batch 100: avg loss -1.798572 avg loss no lamb -1.798572 time 2019-02-27 04:31:13.749968
Model ind 640 epoch 842 head B head_i_epoch 0 batch 200: avg loss -1.899449 avg loss no lamb -1.899449 time 2019-02-27 04:33:10.297696
last batch sz 160
Model ind 640 epoch 842 head B head_i_epoch 1 batch 0: avg loss -1.800406 avg loss no lamb -1.800406 time 2019-02-27 04:34:41.793939
Model ind 640 epoch 842 head B head_i_epoch 1 batch 100: avg loss -1.726547 avg loss no lamb -1.726547 time 2019-02-27 04:36:41.124692
Model ind 640 epoch 842 head B head_i_epoch 1 batch 200: avg loss -1.943997 avg loss no lamb -1.943997 time 2019-02-27 04:38:45.100396
last batch sz 160
Pre: time 2019-02-27 04:40:29.032805: 
 	std: 0.05043099
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51028335, 0.61336666, 0.61335, 0.61338335, 0.51056665]
	train_accs: [0.51028335, 0.61336666, 0.61335, 0.61338335, 0.51056665]
	best_train_sub_head: 3
	worst: 0.51028335
	avg: 0.57219
	best: 0.61338335

Starting e_i: 843
Model ind 640 epoch 843 head A head_i_epoch 0 batch 0: avg loss -3.185725 avg loss no lamb -3.185725 time 2019-02-27 04:40:31.765221
Model ind 640 epoch 843 head A head_i_epoch 0 batch 100: avg loss -3.092088 avg loss no lamb -3.092088 time 2019-02-27 04:42:38.822850
Model ind 640 epoch 843 head A head_i_epoch 0 batch 200: avg loss -3.205913 avg loss no lamb -3.205913 time 2019-02-27 04:44:42.376274
last batch sz 160
Model ind 640 epoch 843 head B head_i_epoch 0 batch 0: avg loss -1.887703 avg loss no lamb -1.887703 time 2019-02-27 04:46:06.659907
Model ind 640 epoch 843 head B head_i_epoch 0 batch 100: avg loss -1.829991 avg loss no lamb -1.829991 time 2019-02-27 04:48:11.505051
Model ind 640 epoch 843 head B head_i_epoch 0 batch 200: avg loss -1.910312 avg loss no lamb -1.910312 time 2019-02-27 04:50:06.743926
last batch sz 160
Model ind 640 epoch 843 head B head_i_epoch 1 batch 0: avg loss -1.780107 avg loss no lamb -1.780107 time 2019-02-27 04:51:36.187063
Model ind 640 epoch 843 head B head_i_epoch 1 batch 100: avg loss -1.745574 avg loss no lamb -1.745574 time 2019-02-27 04:53:33.619219
Model ind 640 epoch 843 head B head_i_epoch 1 batch 200: avg loss -1.850551 avg loss no lamb -1.850551 time 2019-02-27 04:55:33.454087
last batch sz 160
Pre: time 2019-02-27 04:57:18.622730: 
 	std: 0.05152914
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50741667, 0.61255, 0.6124667, 0.61253333, 0.50725]
	train_accs: [0.50741667, 0.61255, 0.6124667, 0.61253333, 0.50725]
	best_train_sub_head: 1
	worst: 0.50725
	avg: 0.5704434
	best: 0.61255

Starting e_i: 844
Model ind 640 epoch 844 head A head_i_epoch 0 batch 0: avg loss -3.227714 avg loss no lamb -3.227714 time 2019-02-27 04:57:21.278556
Model ind 640 epoch 844 head A head_i_epoch 0 batch 100: avg loss -3.131725 avg loss no lamb -3.131725 time 2019-02-27 04:59:25.860289
Model ind 640 epoch 844 head A head_i_epoch 0 batch 200: avg loss -3.238804 avg loss no lamb -3.238804 time 2019-02-27 05:01:23.214176
last batch sz 160
Model ind 640 epoch 844 head B head_i_epoch 0 batch 0: avg loss -1.811692 avg loss no lamb -1.811692 time 2019-02-27 05:02:52.844105
Model ind 640 epoch 844 head B head_i_epoch 0 batch 100: avg loss -1.777967 avg loss no lamb -1.777967 time 2019-02-27 05:04:51.914768
Model ind 640 epoch 844 head B head_i_epoch 0 batch 200: avg loss -1.905636 avg loss no lamb -1.905636 time 2019-02-27 05:06:52.186815
last batch sz 160
Model ind 640 epoch 844 head B head_i_epoch 1 batch 0: avg loss -1.821188 avg loss no lamb -1.821188 time 2019-02-27 05:08:24.575691
Model ind 640 epoch 844 head B head_i_epoch 1 batch 100: avg loss -1.839816 avg loss no lamb -1.839816 time 2019-02-27 05:10:19.085450
Model ind 640 epoch 844 head B head_i_epoch 1 batch 200: avg loss -1.850040 avg loss no lamb -1.850040 time 2019-02-27 05:12:24.548039
last batch sz 160
Pre: time 2019-02-27 05:14:13.763126: 
 	std: 0.050519582
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103667, 0.61325, 0.6132, 0.6134667, 0.51]
	train_accs: [0.5103667, 0.61325, 0.6132, 0.6134667, 0.51]
	best_train_sub_head: 3
	worst: 0.51
	avg: 0.57205665
	best: 0.6134667

Starting e_i: 845
Model ind 640 epoch 845 head A head_i_epoch 0 batch 0: avg loss -3.097274 avg loss no lamb -3.097274 time 2019-02-27 05:14:19.407717
Model ind 640 epoch 845 head A head_i_epoch 0 batch 100: avg loss -3.114583 avg loss no lamb -3.114583 time 2019-02-27 05:16:18.807916
Model ind 640 epoch 845 head A head_i_epoch 0 batch 200: avg loss -3.156052 avg loss no lamb -3.156052 time 2019-02-27 05:18:26.925901
last batch sz 160
Model ind 640 epoch 845 head B head_i_epoch 0 batch 0: avg loss -1.882962 avg loss no lamb -1.882962 time 2019-02-27 05:19:47.909730
Model ind 640 epoch 845 head B head_i_epoch 0 batch 100: avg loss -1.774228 avg loss no lamb -1.774228 time 2019-02-27 05:21:53.534771
Model ind 640 epoch 845 head B head_i_epoch 0 batch 200: avg loss -1.879538 avg loss no lamb -1.879538 time 2019-02-27 05:23:50.925685
last batch sz 160
Model ind 640 epoch 845 head B head_i_epoch 1 batch 0: avg loss -1.880172 avg loss no lamb -1.880172 time 2019-02-27 05:25:20.403164
Model ind 640 epoch 845 head B head_i_epoch 1 batch 100: avg loss -1.859989 avg loss no lamb -1.859989 time 2019-02-27 05:27:17.829506
Model ind 640 epoch 845 head B head_i_epoch 1 batch 200: avg loss -1.850086 avg loss no lamb -1.850086 time 2019-02-27 05:29:16.233194
last batch sz 160
Pre: time 2019-02-27 05:31:05.415449: 
 	std: 0.050767276
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101167, 0.6135333, 0.6134, 0.61365, 0.5096833]
	train_accs: [0.5101167, 0.6135333, 0.6134, 0.61365, 0.5096833]
	best_train_sub_head: 3
	worst: 0.5096833
	avg: 0.5720767
	best: 0.61365

Starting e_i: 846
Model ind 640 epoch 846 head A head_i_epoch 0 batch 0: avg loss -3.171113 avg loss no lamb -3.171113 time 2019-02-27 05:31:08.584485
Model ind 640 epoch 846 head A head_i_epoch 0 batch 100: avg loss -3.133600 avg loss no lamb -3.133600 time 2019-02-27 05:33:08.394218
Model ind 640 epoch 846 head A head_i_epoch 0 batch 200: avg loss -3.216817 avg loss no lamb -3.216817 time 2019-02-27 05:35:06.394482
last batch sz 160
Model ind 640 epoch 846 head B head_i_epoch 0 batch 0: avg loss -1.935367 avg loss no lamb -1.935367 time 2019-02-27 05:36:35.468046
Model ind 640 epoch 846 head B head_i_epoch 0 batch 100: avg loss -1.796112 avg loss no lamb -1.796112 time 2019-02-27 05:38:36.099502
Model ind 640 epoch 846 head B head_i_epoch 0 batch 200: avg loss -1.887819 avg loss no lamb -1.887819 time 2019-02-27 05:40:32.726715
last batch sz 160
Model ind 640 epoch 846 head B head_i_epoch 1 batch 0: avg loss -1.815802 avg loss no lamb -1.815802 time 2019-02-27 05:42:05.980024
Model ind 640 epoch 846 head B head_i_epoch 1 batch 100: avg loss -1.737559 avg loss no lamb -1.737559 time 2019-02-27 05:43:55.982775
Model ind 640 epoch 846 head B head_i_epoch 1 batch 200: avg loss -1.862686 avg loss no lamb -1.862686 time 2019-02-27 05:46:03.845929
last batch sz 160
Pre: time 2019-02-27 05:47:56.218303: 
 	std: 0.051476337
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5076333, 0.6124667, 0.61233336, 0.6126, 0.50715]
	train_accs: [0.5076333, 0.6124667, 0.61233336, 0.6126, 0.50715]
	best_train_sub_head: 3
	worst: 0.50715
	avg: 0.57043666
	best: 0.6126

Starting e_i: 847
Model ind 640 epoch 847 head A head_i_epoch 0 batch 0: avg loss -3.243571 avg loss no lamb -3.243571 time 2019-02-27 05:48:03.587034
Model ind 640 epoch 847 head A head_i_epoch 0 batch 100: avg loss -3.099902 avg loss no lamb -3.099902 time 2019-02-27 05:50:00.187451
Model ind 640 epoch 847 head A head_i_epoch 0 batch 200: avg loss -3.167261 avg loss no lamb -3.167261 time 2019-02-27 05:52:06.192154
last batch sz 160
Model ind 640 epoch 847 head B head_i_epoch 0 batch 0: avg loss -1.891000 avg loss no lamb -1.891000 time 2019-02-27 05:53:28.083421
Model ind 640 epoch 847 head B head_i_epoch 0 batch 100: avg loss -1.736819 avg loss no lamb -1.736819 time 2019-02-27 05:55:32.572363
Model ind 640 epoch 847 head B head_i_epoch 0 batch 200: avg loss -1.914181 avg loss no lamb -1.914181 time 2019-02-27 05:57:30.039683
last batch sz 160
Model ind 640 epoch 847 head B head_i_epoch 1 batch 0: avg loss -1.870002 avg loss no lamb -1.870002 time 2019-02-27 05:58:58.901180
Model ind 640 epoch 847 head B head_i_epoch 1 batch 100: avg loss -1.716569 avg loss no lamb -1.716569 time 2019-02-27 06:00:56.957552
Model ind 640 epoch 847 head B head_i_epoch 1 batch 200: avg loss -1.905035 avg loss no lamb -1.905035 time 2019-02-27 06:02:57.556284
last batch sz 160
Pre: time 2019-02-27 06:04:46.969561: 
 	std: 0.05030458
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5100667, 0.61301666, 0.6127167, 0.61301666, 0.5104]
	train_accs: [0.5100667, 0.61301666, 0.6127167, 0.61301666, 0.5104]
	best_train_sub_head: 1
	worst: 0.5100667
	avg: 0.5718433
	best: 0.61301666

Starting e_i: 848
Model ind 640 epoch 848 head A head_i_epoch 0 batch 0: avg loss -3.219350 avg loss no lamb -3.219350 time 2019-02-27 06:04:49.802101
Model ind 640 epoch 848 head A head_i_epoch 0 batch 100: avg loss -3.071856 avg loss no lamb -3.071856 time 2019-02-27 06:06:48.712386
Model ind 640 epoch 848 head A head_i_epoch 0 batch 200: avg loss -3.260539 avg loss no lamb -3.260539 time 2019-02-27 06:08:46.250861
last batch sz 160
Model ind 640 epoch 848 head B head_i_epoch 0 batch 0: avg loss -1.884454 avg loss no lamb -1.884454 time 2019-02-27 06:10:14.201533
Model ind 640 epoch 848 head B head_i_epoch 0 batch 100: avg loss -1.729367 avg loss no lamb -1.729367 time 2019-02-27 06:12:17.137134
Model ind 640 epoch 848 head B head_i_epoch 0 batch 200: avg loss -1.891000 avg loss no lamb -1.891000 time 2019-02-27 06:14:11.261894
last batch sz 160
Model ind 640 epoch 848 head B head_i_epoch 1 batch 0: avg loss -1.766570 avg loss no lamb -1.766570 time 2019-02-27 06:15:44.804335
Model ind 640 epoch 848 head B head_i_epoch 1 batch 100: avg loss -1.769049 avg loss no lamb -1.769049 time 2019-02-27 06:17:40.356596
Model ind 640 epoch 848 head B head_i_epoch 1 batch 200: avg loss -1.878857 avg loss no lamb -1.878857 time 2019-02-27 06:19:42.142171
last batch sz 160
Pre: time 2019-02-27 06:21:40.340380: 
 	std: 0.050780706
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50958335, 0.61321664, 0.61308336, 0.6131167, 0.5093833]
	train_accs: [0.50958335, 0.61321664, 0.61308336, 0.6131167, 0.5093833]
	best_train_sub_head: 1
	worst: 0.5093833
	avg: 0.5716766
	best: 0.61321664

Starting e_i: 849
Model ind 640 epoch 849 head A head_i_epoch 0 batch 0: avg loss -3.239328 avg loss no lamb -3.239328 time 2019-02-27 06:21:43.845718
Model ind 640 epoch 849 head A head_i_epoch 0 batch 100: avg loss -3.164438 avg loss no lamb -3.164438 time 2019-02-27 06:23:38.008280
Model ind 640 epoch 849 head A head_i_epoch 0 batch 200: avg loss -3.211660 avg loss no lamb -3.211660 time 2019-02-27 06:25:43.985153
last batch sz 160
Model ind 640 epoch 849 head B head_i_epoch 0 batch 0: avg loss -1.885874 avg loss no lamb -1.885874 time 2019-02-27 06:27:10.215209
Model ind 640 epoch 849 head B head_i_epoch 0 batch 100: avg loss -1.840655 avg loss no lamb -1.840655 time 2019-02-27 06:29:11.354744
Model ind 640 epoch 849 head B head_i_epoch 0 batch 200: avg loss -1.915684 avg loss no lamb -1.915684 time 2019-02-27 06:31:09.828345
last batch sz 160
Model ind 640 epoch 849 head B head_i_epoch 1 batch 0: avg loss -1.867605 avg loss no lamb -1.867605 time 2019-02-27 06:32:34.593521
Model ind 640 epoch 849 head B head_i_epoch 1 batch 100: avg loss -1.840844 avg loss no lamb -1.840844 time 2019-02-27 06:34:38.361512
Model ind 640 epoch 849 head B head_i_epoch 1 batch 200: avg loss -1.894795 avg loss no lamb -1.894795 time 2019-02-27 06:36:38.192509
last batch sz 160
Pre: time 2019-02-27 06:38:26.382914: 
 	std: 0.050896447
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50958335, 0.6135833, 0.61361665, 0.61375, 0.50993335]
	train_accs: [0.50958335, 0.6135833, 0.61361665, 0.61375, 0.50993335]
	best_train_sub_head: 3
	worst: 0.50958335
	avg: 0.5720933
	best: 0.61375

Starting e_i: 850
Model ind 640 epoch 850 head A head_i_epoch 0 batch 0: avg loss -3.287704 avg loss no lamb -3.287704 time 2019-02-27 06:38:29.572580
Model ind 640 epoch 850 head A head_i_epoch 0 batch 100: avg loss -3.082678 avg loss no lamb -3.082678 time 2019-02-27 06:40:29.058578
Model ind 640 epoch 850 head A head_i_epoch 0 batch 200: avg loss -3.291543 avg loss no lamb -3.291543 time 2019-02-27 06:42:33.276782
last batch sz 160
Model ind 640 epoch 850 head B head_i_epoch 0 batch 0: avg loss -1.899876 avg loss no lamb -1.899876 time 2019-02-27 06:43:56.103859
Model ind 640 epoch 850 head B head_i_epoch 0 batch 100: avg loss -1.814441 avg loss no lamb -1.814441 time 2019-02-27 06:46:02.600356
Model ind 640 epoch 850 head B head_i_epoch 0 batch 200: avg loss -1.861958 avg loss no lamb -1.861958 time 2019-02-27 06:48:03.698762
last batch sz 160
Model ind 640 epoch 850 head B head_i_epoch 1 batch 0: avg loss -1.804238 avg loss no lamb -1.804238 time 2019-02-27 06:49:31.085955
Model ind 640 epoch 850 head B head_i_epoch 1 batch 100: avg loss -1.818143 avg loss no lamb -1.818143 time 2019-02-27 06:51:36.870339
Model ind 640 epoch 850 head B head_i_epoch 1 batch 200: avg loss -1.810097 avg loss no lamb -1.810097 time 2019-02-27 06:53:32.025973
last batch sz 160
Pre: time 2019-02-27 06:55:23.095582: 
 	std: 0.050780658
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51098335, 0.61466664, 0.6145667, 0.6145833, 0.51091665]
	train_accs: [0.51098335, 0.61466664, 0.6145667, 0.6145833, 0.51091665]
	best_train_sub_head: 1
	worst: 0.51091665
	avg: 0.57314336
	best: 0.61466664

Starting e_i: 851
Model ind 640 epoch 851 head A head_i_epoch 0 batch 0: avg loss -3.216500 avg loss no lamb -3.216500 time 2019-02-27 06:55:29.961881
Model ind 640 epoch 851 head A head_i_epoch 0 batch 100: avg loss -3.152872 avg loss no lamb -3.152872 time 2019-02-27 06:57:28.754715
Model ind 640 epoch 851 head A head_i_epoch 0 batch 200: avg loss -3.153873 avg loss no lamb -3.153873 time 2019-02-27 06:59:30.169858
last batch sz 160
Model ind 640 epoch 851 head B head_i_epoch 0 batch 0: avg loss -1.825965 avg loss no lamb -1.825965 time 2019-02-27 07:00:57.233283
Model ind 640 epoch 851 head B head_i_epoch 0 batch 100: avg loss -1.746550 avg loss no lamb -1.746550 time 2019-02-27 07:02:57.848972
Model ind 640 epoch 851 head B head_i_epoch 0 batch 200: avg loss -1.885779 avg loss no lamb -1.885779 time 2019-02-27 07:04:59.086550
last batch sz 160
Model ind 640 epoch 851 head B head_i_epoch 1 batch 0: avg loss -1.816911 avg loss no lamb -1.816911 time 2019-02-27 07:06:26.986995
Model ind 640 epoch 851 head B head_i_epoch 1 batch 100: avg loss -1.775496 avg loss no lamb -1.775496 time 2019-02-27 07:08:24.168277
Model ind 640 epoch 851 head B head_i_epoch 1 batch 200: avg loss -1.888364 avg loss no lamb -1.888364 time 2019-02-27 07:10:25.384645
last batch sz 160
Pre: time 2019-02-27 07:12:19.581563: 
 	std: 0.050915446
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50955, 0.6135833, 0.6135167, 0.61366665, 0.50976664]
	train_accs: [0.50955, 0.6135833, 0.6135167, 0.61366665, 0.50976664]
	best_train_sub_head: 3
	worst: 0.50955
	avg: 0.5720166
	best: 0.61366665

Starting e_i: 852
Model ind 640 epoch 852 head A head_i_epoch 0 batch 0: avg loss -3.138836 avg loss no lamb -3.138836 time 2019-02-27 07:12:22.824038
Model ind 640 epoch 852 head A head_i_epoch 0 batch 100: avg loss -3.140271 avg loss no lamb -3.140271 time 2019-02-27 07:14:16.231841
Model ind 640 epoch 852 head A head_i_epoch 0 batch 200: avg loss -3.177842 avg loss no lamb -3.177842 time 2019-02-27 07:16:20.909864
last batch sz 160
Model ind 640 epoch 852 head B head_i_epoch 0 batch 0: avg loss -1.865892 avg loss no lamb -1.865892 time 2019-02-27 07:17:43.313078
Model ind 640 epoch 852 head B head_i_epoch 0 batch 100: avg loss -1.713244 avg loss no lamb -1.713244 time 2019-02-27 07:19:46.699509
Model ind 640 epoch 852 head B head_i_epoch 0 batch 200: avg loss -1.887128 avg loss no lamb -1.887128 time 2019-02-27 07:21:51.317506
last batch sz 160
Model ind 640 epoch 852 head B head_i_epoch 1 batch 0: avg loss -1.835368 avg loss no lamb -1.835368 time 2019-02-27 07:23:14.000152
Model ind 640 epoch 852 head B head_i_epoch 1 batch 100: avg loss -1.824768 avg loss no lamb -1.824768 time 2019-02-27 07:25:17.163696
Model ind 640 epoch 852 head B head_i_epoch 1 batch 200: avg loss -1.869344 avg loss no lamb -1.869344 time 2019-02-27 07:27:13.252660
last batch sz 160
Pre: time 2019-02-27 07:29:03.936466: 
 	std: 0.0509606
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50965, 0.61396664, 0.6137, 0.61405, 0.5101167]
	train_accs: [0.50965, 0.61396664, 0.6137, 0.61405, 0.5101167]
	best_train_sub_head: 3
	worst: 0.50965
	avg: 0.5722966
	best: 0.61405

Starting e_i: 853
Model ind 640 epoch 853 head A head_i_epoch 0 batch 0: avg loss -3.180160 avg loss no lamb -3.180160 time 2019-02-27 07:29:06.857652
Model ind 640 epoch 853 head A head_i_epoch 0 batch 100: avg loss -3.118447 avg loss no lamb -3.118447 time 2019-02-27 07:31:05.559846
Model ind 640 epoch 853 head A head_i_epoch 0 batch 200: avg loss -3.221769 avg loss no lamb -3.221769 time 2019-02-27 07:33:05.128504
last batch sz 160
Model ind 640 epoch 853 head B head_i_epoch 0 batch 0: avg loss -1.894228 avg loss no lamb -1.894228 time 2019-02-27 07:34:30.889968
Model ind 640 epoch 853 head B head_i_epoch 0 batch 100: avg loss -1.795942 avg loss no lamb -1.795942 time 2019-02-27 07:36:32.278418
Model ind 640 epoch 853 head B head_i_epoch 0 batch 200: avg loss -1.860212 avg loss no lamb -1.860212 time 2019-02-27 07:38:31.341793
last batch sz 160
Model ind 640 epoch 853 head B head_i_epoch 1 batch 0: avg loss -1.829984 avg loss no lamb -1.829984 time 2019-02-27 07:40:01.532203
Model ind 640 epoch 853 head B head_i_epoch 1 batch 100: avg loss -1.783381 avg loss no lamb -1.783381 time 2019-02-27 07:41:59.467036
Model ind 640 epoch 853 head B head_i_epoch 1 batch 200: avg loss -1.837060 avg loss no lamb -1.837060 time 2019-02-27 07:43:57.286595
last batch sz 160
Pre: time 2019-02-27 07:45:55.113422: 
 	std: 0.05119826
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50698334, 0.61195, 0.6120667, 0.6121, 0.50808334]
	train_accs: [0.50698334, 0.61195, 0.6120667, 0.6121, 0.50808334]
	best_train_sub_head: 3
	worst: 0.50698334
	avg: 0.5702367
	best: 0.6121

Starting e_i: 854
Model ind 640 epoch 854 head A head_i_epoch 0 batch 0: avg loss -3.176747 avg loss no lamb -3.176747 time 2019-02-27 07:45:58.012707
Model ind 640 epoch 854 head A head_i_epoch 0 batch 100: avg loss -3.177340 avg loss no lamb -3.177340 time 2019-02-27 07:47:52.846956
Model ind 640 epoch 854 head A head_i_epoch 0 batch 200: avg loss -3.243134 avg loss no lamb -3.243134 time 2019-02-27 07:49:58.477040
last batch sz 160
Model ind 640 epoch 854 head B head_i_epoch 0 batch 0: avg loss -1.873566 avg loss no lamb -1.873566 time 2019-02-27 07:51:27.375664
Model ind 640 epoch 854 head B head_i_epoch 0 batch 100: avg loss -1.786425 avg loss no lamb -1.786425 time 2019-02-27 07:53:26.002660
Model ind 640 epoch 854 head B head_i_epoch 0 batch 200: avg loss -1.800151 avg loss no lamb -1.800151 time 2019-02-27 07:55:30.640063
last batch sz 160
Model ind 640 epoch 854 head B head_i_epoch 1 batch 0: avg loss -1.881821 avg loss no lamb -1.881821 time 2019-02-27 07:56:53.296680
Model ind 640 epoch 854 head B head_i_epoch 1 batch 100: avg loss -1.900124 avg loss no lamb -1.900124 time 2019-02-27 07:58:56.684709
Model ind 640 epoch 854 head B head_i_epoch 1 batch 200: avg loss -1.872599 avg loss no lamb -1.872599 time 2019-02-27 08:00:53.604852
last batch sz 160
Pre: time 2019-02-27 08:02:42.892718: 
 	std: 0.050476562
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5112, 0.6148, 0.61445, 0.61465, 0.512]
	train_accs: [0.5112, 0.6148, 0.61445, 0.61465, 0.512]
	best_train_sub_head: 1
	worst: 0.5112
	avg: 0.57342
	best: 0.6148

Starting e_i: 855
Model ind 640 epoch 855 head A head_i_epoch 0 batch 0: avg loss -3.229607 avg loss no lamb -3.229607 time 2019-02-27 08:02:45.784615
Model ind 640 epoch 855 head A head_i_epoch 0 batch 100: avg loss -3.122081 avg loss no lamb -3.122081 time 2019-02-27 08:04:43.832801
Model ind 640 epoch 855 head A head_i_epoch 0 batch 200: avg loss -3.272060 avg loss no lamb -3.272060 time 2019-02-27 08:06:43.596629
last batch sz 160
Model ind 640 epoch 855 head B head_i_epoch 0 batch 0: avg loss -1.894689 avg loss no lamb -1.894689 time 2019-02-27 08:08:10.663075
Model ind 640 epoch 855 head B head_i_epoch 0 batch 100: avg loss -1.776474 avg loss no lamb -1.776474 time 2019-02-27 08:10:11.877387
Model ind 640 epoch 855 head B head_i_epoch 0 batch 200: avg loss -1.906731 avg loss no lamb -1.906731 time 2019-02-27 08:12:11.257169
last batch sz 160
Model ind 640 epoch 855 head B head_i_epoch 1 batch 0: avg loss -1.895439 avg loss no lamb -1.895439 time 2019-02-27 08:13:37.654325
Model ind 640 epoch 855 head B head_i_epoch 1 batch 100: avg loss -1.861616 avg loss no lamb -1.861616 time 2019-02-27 08:15:41.066516
Model ind 640 epoch 855 head B head_i_epoch 1 batch 200: avg loss -1.935452 avg loss no lamb -1.935452 time 2019-02-27 08:17:34.987168
last batch sz 160
Pre: time 2019-02-27 08:19:31.736004: 
 	std: 0.05080661
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111167, 0.61506665, 0.61476666, 0.6148667, 0.51126665]
	train_accs: [0.5111167, 0.61506665, 0.61476666, 0.6148667, 0.51126665]
	best_train_sub_head: 1
	worst: 0.5111167
	avg: 0.57341665
	best: 0.61506665

Starting e_i: 856
Model ind 640 epoch 856 head A head_i_epoch 0 batch 0: avg loss -3.221759 avg loss no lamb -3.221759 time 2019-02-27 08:19:34.310353
Model ind 640 epoch 856 head A head_i_epoch 0 batch 100: avg loss -3.121994 avg loss no lamb -3.121994 time 2019-02-27 08:21:36.336103
Model ind 640 epoch 856 head A head_i_epoch 0 batch 200: avg loss -3.214893 avg loss no lamb -3.214893 time 2019-02-27 08:23:34.484911
last batch sz 160
Model ind 640 epoch 856 head B head_i_epoch 0 batch 0: avg loss -1.749971 avg loss no lamb -1.749971 time 2019-02-27 08:25:07.358611
Model ind 640 epoch 856 head B head_i_epoch 0 batch 100: avg loss -1.750708 avg loss no lamb -1.750708 time 2019-02-27 08:27:02.433245
Model ind 640 epoch 856 head B head_i_epoch 0 batch 200: avg loss -1.871808 avg loss no lamb -1.871808 time 2019-02-27 08:29:04.967524
last batch sz 160
Model ind 640 epoch 856 head B head_i_epoch 1 batch 0: avg loss -1.940645 avg loss no lamb -1.940645 time 2019-02-27 08:30:26.731670
Model ind 640 epoch 856 head B head_i_epoch 1 batch 100: avg loss -1.844193 avg loss no lamb -1.844193 time 2019-02-27 08:32:31.954102
Model ind 640 epoch 856 head B head_i_epoch 1 batch 200: avg loss -1.897779 avg loss no lamb -1.897779 time 2019-02-27 08:34:28.408166
last batch sz 160
Pre: time 2019-02-27 08:36:17.562164: 
 	std: 0.050645962
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51133335, 0.61478335, 0.61473334, 0.61485, 0.5114833]
	train_accs: [0.51133335, 0.61478335, 0.61473334, 0.61485, 0.5114833]
	best_train_sub_head: 3
	worst: 0.51133335
	avg: 0.5734366
	best: 0.61485

Starting e_i: 857
Model ind 640 epoch 857 head A head_i_epoch 0 batch 0: avg loss -3.195046 avg loss no lamb -3.195046 time 2019-02-27 08:36:20.507801
Model ind 640 epoch 857 head A head_i_epoch 0 batch 100: avg loss -3.171994 avg loss no lamb -3.171994 time 2019-02-27 08:38:22.304035
Model ind 640 epoch 857 head A head_i_epoch 0 batch 200: avg loss -3.232145 avg loss no lamb -3.232145 time 2019-02-27 08:40:22.157235
last batch sz 160
Model ind 640 epoch 857 head B head_i_epoch 0 batch 0: avg loss -1.912223 avg loss no lamb -1.912223 time 2019-02-27 08:41:47.914130
Model ind 640 epoch 857 head B head_i_epoch 0 batch 100: avg loss -1.825581 avg loss no lamb -1.825581 time 2019-02-27 08:43:49.657204
Model ind 640 epoch 857 head B head_i_epoch 0 batch 200: avg loss -1.882980 avg loss no lamb -1.882980 time 2019-02-27 08:45:52.772986
last batch sz 160
Model ind 640 epoch 857 head B head_i_epoch 1 batch 0: avg loss -1.906394 avg loss no lamb -1.906394 time 2019-02-27 08:47:16.181080
Model ind 640 epoch 857 head B head_i_epoch 1 batch 100: avg loss -1.730959 avg loss no lamb -1.730959 time 2019-02-27 08:49:20.538737
Model ind 640 epoch 857 head B head_i_epoch 1 batch 200: avg loss -1.897707 avg loss no lamb -1.897707 time 2019-02-27 08:51:14.152591
last batch sz 160
Pre: time 2019-02-27 08:53:09.762933: 
 	std: 0.0507916
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108167, 0.61431664, 0.61443335, 0.6145833, 0.5107167]
	train_accs: [0.5108167, 0.61431664, 0.61443335, 0.6145833, 0.5107167]
	best_train_sub_head: 3
	worst: 0.5107167
	avg: 0.5729733
	best: 0.6145833

Starting e_i: 858
Model ind 640 epoch 858 head A head_i_epoch 0 batch 0: avg loss -3.227045 avg loss no lamb -3.227045 time 2019-02-27 08:53:12.778285
Model ind 640 epoch 858 head A head_i_epoch 0 batch 100: avg loss -3.211734 avg loss no lamb -3.211734 time 2019-02-27 08:55:18.571406
Model ind 640 epoch 858 head A head_i_epoch 0 batch 200: avg loss -3.278450 avg loss no lamb -3.278450 time 2019-02-27 08:57:13.638722
last batch sz 160
Model ind 640 epoch 858 head B head_i_epoch 0 batch 0: avg loss -1.807814 avg loss no lamb -1.807814 time 2019-02-27 08:58:48.697940
Model ind 640 epoch 858 head B head_i_epoch 0 batch 100: avg loss -1.794417 avg loss no lamb -1.794417 time 2019-02-27 09:00:44.629073
Model ind 640 epoch 858 head B head_i_epoch 0 batch 200: avg loss -1.959954 avg loss no lamb -1.959954 time 2019-02-27 09:02:46.756678
last batch sz 160
Model ind 640 epoch 858 head B head_i_epoch 1 batch 0: avg loss -1.827244 avg loss no lamb -1.827244 time 2019-02-27 09:04:12.227331
Model ind 640 epoch 858 head B head_i_epoch 1 batch 100: avg loss -1.673340 avg loss no lamb -1.673340 time 2019-02-27 09:06:10.775375
Model ind 640 epoch 858 head B head_i_epoch 1 batch 200: avg loss -1.854798 avg loss no lamb -1.854798 time 2019-02-27 09:08:08.481268
last batch sz 160
Pre: time 2019-02-27 09:10:00.084197: 
 	std: 0.050232455
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5098, 0.6125, 0.6124667, 0.61266667, 0.51021665]
	train_accs: [0.5098, 0.6125, 0.6124667, 0.61266667, 0.51021665]
	best_train_sub_head: 3
	worst: 0.5098
	avg: 0.57153
	best: 0.61266667

Starting e_i: 859
Model ind 640 epoch 859 head A head_i_epoch 0 batch 0: avg loss -3.193788 avg loss no lamb -3.193788 time 2019-02-27 09:10:02.902676
Model ind 640 epoch 859 head A head_i_epoch 0 batch 100: avg loss -3.121201 avg loss no lamb -3.121201 time 2019-02-27 09:12:01.359061
Model ind 640 epoch 859 head A head_i_epoch 0 batch 200: avg loss -3.171977 avg loss no lamb -3.171977 time 2019-02-27 09:14:01.442798
last batch sz 160
Model ind 640 epoch 859 head B head_i_epoch 0 batch 0: avg loss -1.882384 avg loss no lamb -1.882384 time 2019-02-27 09:15:27.298689
Model ind 640 epoch 859 head B head_i_epoch 0 batch 100: avg loss -1.781774 avg loss no lamb -1.781774 time 2019-02-27 09:17:25.170699
Model ind 640 epoch 859 head B head_i_epoch 0 batch 200: avg loss -1.865990 avg loss no lamb -1.865990 time 2019-02-27 09:19:31.835500
last batch sz 160
Model ind 640 epoch 859 head B head_i_epoch 1 batch 0: avg loss -1.878802 avg loss no lamb -1.878802 time 2019-02-27 09:20:54.295701
Model ind 640 epoch 859 head B head_i_epoch 1 batch 100: avg loss -1.849077 avg loss no lamb -1.849077 time 2019-02-27 09:23:02.084390
Model ind 640 epoch 859 head B head_i_epoch 1 batch 200: avg loss -1.858431 avg loss no lamb -1.858431 time 2019-02-27 09:25:02.005713
last batch sz 160
Pre: time 2019-02-27 09:26:53.868525: 
 	std: 0.050956506
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50776666, 0.612, 0.61183333, 0.61218333, 0.5082167]
	train_accs: [0.50776666, 0.612, 0.61183333, 0.61218333, 0.5082167]
	best_train_sub_head: 3
	worst: 0.50776666
	avg: 0.5704
	best: 0.61218333

Starting e_i: 860
Model ind 640 epoch 860 head A head_i_epoch 0 batch 0: avg loss -3.161201 avg loss no lamb -3.161201 time 2019-02-27 09:26:56.975033
Model ind 640 epoch 860 head A head_i_epoch 0 batch 100: avg loss -3.069969 avg loss no lamb -3.069969 time 2019-02-27 09:29:01.945265
Model ind 640 epoch 860 head A head_i_epoch 0 batch 200: avg loss -3.214930 avg loss no lamb -3.214930 time 2019-02-27 09:30:58.152809
last batch sz 160
Model ind 640 epoch 860 head B head_i_epoch 0 batch 0: avg loss -1.853548 avg loss no lamb -1.853548 time 2019-02-27 09:32:29.294750
Model ind 640 epoch 860 head B head_i_epoch 0 batch 100: avg loss -1.814260 avg loss no lamb -1.814260 time 2019-02-27 09:34:27.289671
Model ind 640 epoch 860 head B head_i_epoch 0 batch 200: avg loss -1.883360 avg loss no lamb -1.883360 time 2019-02-27 09:36:26.323076
last batch sz 160
Model ind 640 epoch 860 head B head_i_epoch 1 batch 0: avg loss -1.760480 avg loss no lamb -1.760480 time 2019-02-27 09:37:52.792594
Model ind 640 epoch 860 head B head_i_epoch 1 batch 100: avg loss -1.813252 avg loss no lamb -1.813252 time 2019-02-27 09:39:51.238912
Model ind 640 epoch 860 head B head_i_epoch 1 batch 200: avg loss -1.868197 avg loss no lamb -1.868197 time 2019-02-27 09:41:50.618704
last batch sz 160
Pre: time 2019-02-27 09:43:42.170974: 
 	std: 0.05075507
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5107333, 0.61446667, 0.614, 0.61441666, 0.51065]
	train_accs: [0.5107333, 0.61446667, 0.614, 0.61441666, 0.51065]
	best_train_sub_head: 1
	worst: 0.51065
	avg: 0.5728533
	best: 0.61446667

Starting e_i: 861
Model ind 640 epoch 861 head A head_i_epoch 0 batch 0: avg loss -3.212119 avg loss no lamb -3.212119 time 2019-02-27 09:43:50.112034
Model ind 640 epoch 861 head A head_i_epoch 0 batch 100: avg loss -3.040378 avg loss no lamb -3.040378 time 2019-02-27 09:45:52.078048
Model ind 640 epoch 861 head A head_i_epoch 0 batch 200: avg loss -3.246876 avg loss no lamb -3.246876 time 2019-02-27 09:47:48.612486
last batch sz 160
Model ind 640 epoch 861 head B head_i_epoch 0 batch 0: avg loss -1.776236 avg loss no lamb -1.776236 time 2019-02-27 09:49:22.854291
Model ind 640 epoch 861 head B head_i_epoch 0 batch 100: avg loss -1.784122 avg loss no lamb -1.784122 time 2019-02-27 09:51:16.544782
Model ind 640 epoch 861 head B head_i_epoch 0 batch 200: avg loss -1.872527 avg loss no lamb -1.872527 time 2019-02-27 09:53:21.244311
last batch sz 160
Model ind 640 epoch 861 head B head_i_epoch 1 batch 0: avg loss -1.812127 avg loss no lamb -1.812127 time 2019-02-27 09:54:47.553392
Model ind 640 epoch 861 head B head_i_epoch 1 batch 100: avg loss -1.773635 avg loss no lamb -1.773635 time 2019-02-27 09:56:51.150513
Model ind 640 epoch 861 head B head_i_epoch 1 batch 200: avg loss -1.939431 avg loss no lamb -1.939431 time 2019-02-27 09:58:57.456267
last batch sz 160
Pre: time 2019-02-27 10:00:41.982548: 
 	std: 0.050101697
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51526666, 0.6168, 0.6172, 0.6169, 0.51413333]
	train_accs: [0.51526666, 0.6168, 0.6172, 0.6169, 0.51413333]
	best_train_sub_head: 2
	worst: 0.51413333
	avg: 0.57606
	best: 0.6172

Starting e_i: 862
Model ind 640 epoch 862 head A head_i_epoch 0 batch 0: avg loss -3.161909 avg loss no lamb -3.161909 time 2019-02-27 10:00:49.730135
Model ind 640 epoch 862 head A head_i_epoch 0 batch 100: avg loss -3.136428 avg loss no lamb -3.136428 time 2019-02-27 10:02:55.827332
Model ind 640 epoch 862 head A head_i_epoch 0 batch 200: avg loss -3.182783 avg loss no lamb -3.182783 time 2019-02-27 10:04:53.605918
last batch sz 160
Model ind 640 epoch 862 head B head_i_epoch 0 batch 0: avg loss -1.869102 avg loss no lamb -1.869102 time 2019-02-27 10:06:23.519465
Model ind 640 epoch 862 head B head_i_epoch 0 batch 100: avg loss -1.873849 avg loss no lamb -1.873849 time 2019-02-27 10:08:22.117940
Model ind 640 epoch 862 head B head_i_epoch 0 batch 200: avg loss -1.852886 avg loss no lamb -1.852886 time 2019-02-27 10:10:23.077106
last batch sz 160
Model ind 640 epoch 862 head B head_i_epoch 1 batch 0: avg loss -1.847501 avg loss no lamb -1.847501 time 2019-02-27 10:11:50.092366
Model ind 640 epoch 862 head B head_i_epoch 1 batch 100: avg loss -1.815206 avg loss no lamb -1.815206 time 2019-02-27 10:13:51.429729
Model ind 640 epoch 862 head B head_i_epoch 1 batch 200: avg loss -1.921507 avg loss no lamb -1.921507 time 2019-02-27 10:15:50.243732
last batch sz 160
Pre: time 2019-02-27 10:17:40.767623: 
 	std: 0.050865255
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51091665, 0.61401665, 0.61403334, 0.61435, 0.5097]
	train_accs: [0.51091665, 0.61401665, 0.61403334, 0.61435, 0.5097]
	best_train_sub_head: 3
	worst: 0.5097
	avg: 0.57260334
	best: 0.61435

Starting e_i: 863
Model ind 640 epoch 863 head A head_i_epoch 0 batch 0: avg loss -3.279364 avg loss no lamb -3.279364 time 2019-02-27 10:17:43.493646
Model ind 640 epoch 863 head A head_i_epoch 0 batch 100: avg loss -3.173491 avg loss no lamb -3.173491 time 2019-02-27 10:19:48.652068
Model ind 640 epoch 863 head A head_i_epoch 0 batch 200: avg loss -3.224959 avg loss no lamb -3.224959 time 2019-02-27 10:21:47.214600
last batch sz 160
Model ind 640 epoch 863 head B head_i_epoch 0 batch 0: avg loss -1.967774 avg loss no lamb -1.967774 time 2019-02-27 10:23:17.955734
Model ind 640 epoch 863 head B head_i_epoch 0 batch 100: avg loss -1.765301 avg loss no lamb -1.765301 time 2019-02-27 10:25:21.885394
Model ind 640 epoch 863 head B head_i_epoch 0 batch 200: avg loss -1.896116 avg loss no lamb -1.896116 time 2019-02-27 10:27:14.347275
last batch sz 160
Model ind 640 epoch 863 head B head_i_epoch 1 batch 0: avg loss -1.778983 avg loss no lamb -1.778983 time 2019-02-27 10:28:50.920274
Model ind 640 epoch 863 head B head_i_epoch 1 batch 100: avg loss -1.822618 avg loss no lamb -1.822618 time 2019-02-27 10:30:45.456734
Model ind 640 epoch 863 head B head_i_epoch 1 batch 200: avg loss -1.911290 avg loss no lamb -1.911290 time 2019-02-27 10:32:43.636386
last batch sz 160
Pre: time 2019-02-27 10:34:26.833804: 
 	std: 0.050646823
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5115167, 0.61448336, 0.61441666, 0.61436665, 0.51056665]
	train_accs: [0.5115167, 0.61448336, 0.61441666, 0.61436665, 0.51056665]
	best_train_sub_head: 1
	worst: 0.51056665
	avg: 0.57307005
	best: 0.61448336

Starting e_i: 864
Model ind 640 epoch 864 head A head_i_epoch 0 batch 0: avg loss -3.107612 avg loss no lamb -3.107612 time 2019-02-27 10:34:29.681295
Model ind 640 epoch 864 head A head_i_epoch 0 batch 100: avg loss -3.140352 avg loss no lamb -3.140352 time 2019-02-27 10:36:21.614353
Model ind 640 epoch 864 head A head_i_epoch 0 batch 200: avg loss -3.217686 avg loss no lamb -3.217686 time 2019-02-27 10:38:16.539078
last batch sz 160
Model ind 640 epoch 864 head B head_i_epoch 0 batch 0: avg loss -1.890338 avg loss no lamb -1.890338 time 2019-02-27 10:39:36.722594
Model ind 640 epoch 864 head B head_i_epoch 0 batch 100: avg loss -1.866886 avg loss no lamb -1.866886 time 2019-02-27 10:41:31.322263
Model ind 640 epoch 864 head B head_i_epoch 0 batch 200: avg loss -1.924626 avg loss no lamb -1.924626 time 2019-02-27 10:43:25.117785
last batch sz 160
Model ind 640 epoch 864 head B head_i_epoch 1 batch 0: avg loss -1.822101 avg loss no lamb -1.822101 time 2019-02-27 10:44:46.040230
Model ind 640 epoch 864 head B head_i_epoch 1 batch 100: avg loss -1.738665 avg loss no lamb -1.738665 time 2019-02-27 10:46:40.598069
Model ind 640 epoch 864 head B head_i_epoch 1 batch 200: avg loss -1.876828 avg loss no lamb -1.876828 time 2019-02-27 10:48:30.899667
last batch sz 160
Pre: time 2019-02-27 10:50:15.249372: 
 	std: 0.050893966
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5092833, 0.6128333, 0.6128333, 0.61291665, 0.5086667]
	train_accs: [0.5092833, 0.6128333, 0.6128333, 0.61291665, 0.5086667]
	best_train_sub_head: 3
	worst: 0.5086667
	avg: 0.57130665
	best: 0.61291665

Starting e_i: 865
Model ind 640 epoch 865 head A head_i_epoch 0 batch 0: avg loss -3.171709 avg loss no lamb -3.171709 time 2019-02-27 10:50:17.535168
Model ind 640 epoch 865 head A head_i_epoch 0 batch 100: avg loss -3.130787 avg loss no lamb -3.130787 time 2019-02-27 10:52:12.790714
Model ind 640 epoch 865 head A head_i_epoch 0 batch 200: avg loss -3.152947 avg loss no lamb -3.152947 time 2019-02-27 10:54:13.168169
last batch sz 160
Model ind 640 epoch 865 head B head_i_epoch 0 batch 0: avg loss -1.835794 avg loss no lamb -1.835794 time 2019-02-27 10:55:40.452766
Model ind 640 epoch 865 head B head_i_epoch 0 batch 100: avg loss -1.824783 avg loss no lamb -1.824783 time 2019-02-27 10:57:40.056399
Model ind 640 epoch 865 head B head_i_epoch 0 batch 200: avg loss -1.835604 avg loss no lamb -1.835604 time 2019-02-27 10:59:42.205134
last batch sz 160
Model ind 640 epoch 865 head B head_i_epoch 1 batch 0: avg loss -1.883916 avg loss no lamb -1.883916 time 2019-02-27 11:01:08.873609
Model ind 640 epoch 865 head B head_i_epoch 1 batch 100: avg loss -1.723114 avg loss no lamb -1.723114 time 2019-02-27 11:03:08.098329
Model ind 640 epoch 865 head B head_i_epoch 1 batch 200: avg loss -1.915597 avg loss no lamb -1.915597 time 2019-02-27 11:05:07.550204
last batch sz 160
Pre: time 2019-02-27 11:06:58.381753: 
 	std: 0.050725516
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5098, 0.61296666, 0.61293334, 0.6129, 0.5089833]
	train_accs: [0.5098, 0.61296666, 0.61293334, 0.6129, 0.5089833]
	best_train_sub_head: 1
	worst: 0.5089833
	avg: 0.5715167
	best: 0.61296666

Starting e_i: 866
Model ind 640 epoch 866 head A head_i_epoch 0 batch 0: avg loss -3.120209 avg loss no lamb -3.120209 time 2019-02-27 11:07:01.312799
Model ind 640 epoch 866 head A head_i_epoch 0 batch 100: avg loss -3.099898 avg loss no lamb -3.099898 time 2019-02-27 11:08:59.341893
Model ind 640 epoch 866 head A head_i_epoch 0 batch 200: avg loss -3.296783 avg loss no lamb -3.296783 time 2019-02-27 11:10:59.199226
last batch sz 160
Model ind 640 epoch 866 head B head_i_epoch 0 batch 0: avg loss -1.902903 avg loss no lamb -1.902903 time 2019-02-27 11:12:27.452853
Model ind 640 epoch 866 head B head_i_epoch 0 batch 100: avg loss -1.772181 avg loss no lamb -1.772181 time 2019-02-27 11:14:23.867486
Model ind 640 epoch 866 head B head_i_epoch 0 batch 200: avg loss -1.897588 avg loss no lamb -1.897588 time 2019-02-27 11:16:28.422558
last batch sz 160
Model ind 640 epoch 866 head B head_i_epoch 1 batch 0: avg loss -1.883556 avg loss no lamb -1.883556 time 2019-02-27 11:17:54.878872
Model ind 640 epoch 866 head B head_i_epoch 1 batch 100: avg loss -1.836663 avg loss no lamb -1.836663 time 2019-02-27 11:19:55.593333
Model ind 640 epoch 866 head B head_i_epoch 1 batch 200: avg loss -1.949301 avg loss no lamb -1.949301 time 2019-02-27 11:21:59.755620
last batch sz 160
Pre: time 2019-02-27 11:23:48.111290: 
 	std: 0.050922703
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5107667, 0.61448336, 0.61425, 0.61435, 0.5100667]
	train_accs: [0.5107667, 0.61448336, 0.61425, 0.61435, 0.5100667]
	best_train_sub_head: 1
	worst: 0.5100667
	avg: 0.57278335
	best: 0.61448336

Starting e_i: 867
Model ind 640 epoch 867 head A head_i_epoch 0 batch 0: avg loss -3.268900 avg loss no lamb -3.268900 time 2019-02-27 11:23:51.873997
Model ind 640 epoch 867 head A head_i_epoch 0 batch 100: avg loss -3.026484 avg loss no lamb -3.026484 time 2019-02-27 11:25:54.057191
Model ind 640 epoch 867 head A head_i_epoch 0 batch 200: avg loss -3.175596 avg loss no lamb -3.175596 time 2019-02-27 11:27:57.892259
last batch sz 160
Model ind 640 epoch 867 head B head_i_epoch 0 batch 0: avg loss -1.934058 avg loss no lamb -1.934058 time 2019-02-27 11:29:20.482132
Model ind 640 epoch 867 head B head_i_epoch 0 batch 100: avg loss -1.780978 avg loss no lamb -1.780978 time 2019-02-27 11:31:25.074653
Model ind 640 epoch 867 head B head_i_epoch 0 batch 200: avg loss -1.924380 avg loss no lamb -1.924380 time 2019-02-27 11:33:22.033341
last batch sz 160
Model ind 640 epoch 867 head B head_i_epoch 1 batch 0: avg loss -1.855716 avg loss no lamb -1.855716 time 2019-02-27 11:34:51.630398
Model ind 640 epoch 867 head B head_i_epoch 1 batch 100: avg loss -1.863659 avg loss no lamb -1.863659 time 2019-02-27 11:36:49.094359
Model ind 640 epoch 867 head B head_i_epoch 1 batch 200: avg loss -1.899468 avg loss no lamb -1.899468 time 2019-02-27 11:38:35.305622
last batch sz 160
Pre: time 2019-02-27 11:40:23.097411: 
 	std: 0.051151313
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50953335, 0.6135333, 0.6136, 0.6136, 0.5088]
	train_accs: [0.50953335, 0.6135333, 0.6136, 0.6136, 0.5088]
	best_train_sub_head: 2
	worst: 0.5088
	avg: 0.57181334
	best: 0.6136

Starting e_i: 868
Model ind 640 epoch 868 head A head_i_epoch 0 batch 0: avg loss -3.297056 avg loss no lamb -3.297056 time 2019-02-27 11:40:25.545941
Model ind 640 epoch 868 head A head_i_epoch 0 batch 100: avg loss -3.117866 avg loss no lamb -3.117866 time 2019-02-27 11:42:17.440777
Model ind 640 epoch 868 head A head_i_epoch 0 batch 200: avg loss -3.168135 avg loss no lamb -3.168135 time 2019-02-27 11:44:10.583029
last batch sz 160
Model ind 640 epoch 868 head B head_i_epoch 0 batch 0: avg loss -1.827647 avg loss no lamb -1.827647 time 2019-02-27 11:45:33.730909
Model ind 640 epoch 868 head B head_i_epoch 0 batch 100: avg loss -1.850804 avg loss no lamb -1.850804 time 2019-02-27 11:47:25.458833
Model ind 640 epoch 868 head B head_i_epoch 0 batch 200: avg loss -1.840561 avg loss no lamb -1.840561 time 2019-02-27 11:49:19.391406
last batch sz 160
Model ind 640 epoch 868 head B head_i_epoch 1 batch 0: avg loss -1.852514 avg loss no lamb -1.852514 time 2019-02-27 11:50:39.822761
Model ind 640 epoch 868 head B head_i_epoch 1 batch 100: avg loss -1.824647 avg loss no lamb -1.824647 time 2019-02-27 11:52:33.911526
Model ind 640 epoch 868 head B head_i_epoch 1 batch 200: avg loss -1.835718 avg loss no lamb -1.835718 time 2019-02-27 11:54:23.444624
last batch sz 160
Pre: time 2019-02-27 11:56:08.469412: 
 	std: 0.05074402
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50835, 0.6117, 0.61205, 0.61191666, 0.5082667]
	train_accs: [0.50835, 0.6117, 0.61205, 0.61191666, 0.5082667]
	best_train_sub_head: 2
	worst: 0.5082667
	avg: 0.5704566
	best: 0.61205

Starting e_i: 869
Model ind 640 epoch 869 head A head_i_epoch 0 batch 0: avg loss -3.285280 avg loss no lamb -3.285280 time 2019-02-27 11:56:10.789248
Model ind 640 epoch 869 head A head_i_epoch 0 batch 100: avg loss -3.182703 avg loss no lamb -3.182703 time 2019-02-27 11:58:03.964601
Model ind 640 epoch 869 head A head_i_epoch 0 batch 200: avg loss -3.260604 avg loss no lamb -3.260604 time 2019-02-27 11:59:55.138020
last batch sz 160
Model ind 640 epoch 869 head B head_i_epoch 0 batch 0: avg loss -1.813649 avg loss no lamb -1.813649 time 2019-02-27 12:01:16.766569
Model ind 640 epoch 869 head B head_i_epoch 0 batch 100: avg loss -1.869031 avg loss no lamb -1.869031 time 2019-02-27 12:03:07.855820
Model ind 640 epoch 869 head B head_i_epoch 0 batch 200: avg loss -1.903221 avg loss no lamb -1.903221 time 2019-02-27 12:05:02.608190
last batch sz 160
Model ind 640 epoch 869 head B head_i_epoch 1 batch 0: avg loss -1.884399 avg loss no lamb -1.884399 time 2019-02-27 12:06:22.142241
Model ind 640 epoch 869 head B head_i_epoch 1 batch 100: avg loss -1.854075 avg loss no lamb -1.854075 time 2019-02-27 12:08:16.195381
Model ind 640 epoch 869 head B head_i_epoch 1 batch 200: avg loss -1.900207 avg loss no lamb -1.900207 time 2019-02-27 12:10:08.551411
last batch sz 160
Pre: time 2019-02-27 12:11:50.467658: 
 	std: 0.051006537
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5093167, 0.61345, 0.6134833, 0.6134167, 0.50935]
	train_accs: [0.5093167, 0.61345, 0.6134833, 0.6134167, 0.50935]
	best_train_sub_head: 2
	worst: 0.5093167
	avg: 0.57180333
	best: 0.6134833

Starting e_i: 870
Model ind 640 epoch 870 head A head_i_epoch 0 batch 0: avg loss -3.168273 avg loss no lamb -3.168273 time 2019-02-27 12:11:52.852946
Model ind 640 epoch 870 head A head_i_epoch 0 batch 100: avg loss -3.198864 avg loss no lamb -3.198864 time 2019-02-27 12:13:47.567760
Model ind 640 epoch 870 head A head_i_epoch 0 batch 200: avg loss -3.245248 avg loss no lamb -3.245248 time 2019-02-27 12:15:39.069777
last batch sz 160
Model ind 640 epoch 870 head B head_i_epoch 0 batch 0: avg loss -1.829285 avg loss no lamb -1.829285 time 2019-02-27 12:17:03.868963
Model ind 640 epoch 870 head B head_i_epoch 0 batch 100: avg loss -1.816625 avg loss no lamb -1.816625 time 2019-02-27 12:18:54.953073
Model ind 640 epoch 870 head B head_i_epoch 0 batch 200: avg loss -1.932757 avg loss no lamb -1.932757 time 2019-02-27 12:20:49.679027
last batch sz 160
Model ind 640 epoch 870 head B head_i_epoch 1 batch 0: avg loss -1.869774 avg loss no lamb -1.869774 time 2019-02-27 12:22:11.229332
Model ind 640 epoch 870 head B head_i_epoch 1 batch 100: avg loss -1.834917 avg loss no lamb -1.834917 time 2019-02-27 12:24:03.777676
Model ind 640 epoch 870 head B head_i_epoch 1 batch 200: avg loss -1.914496 avg loss no lamb -1.914496 time 2019-02-27 12:25:56.562324
last batch sz 160
Pre: time 2019-02-27 12:27:39.309547: 
 	std: 0.05056569
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51021665, 0.6133, 0.61331666, 0.61333334, 0.50998336]
	train_accs: [0.51021665, 0.6133, 0.61331666, 0.61333334, 0.50998336]
	best_train_sub_head: 3
	worst: 0.50998336
	avg: 0.57203
	best: 0.61333334

Starting e_i: 871
Model ind 640 epoch 871 head A head_i_epoch 0 batch 0: avg loss -3.242973 avg loss no lamb -3.242973 time 2019-02-27 12:27:45.656513
Model ind 640 epoch 871 head A head_i_epoch 0 batch 100: avg loss -3.186051 avg loss no lamb -3.186051 time 2019-02-27 12:29:40.956140
Model ind 640 epoch 871 head A head_i_epoch 0 batch 200: avg loss -3.222985 avg loss no lamb -3.222985 time 2019-02-27 12:31:32.467225
last batch sz 160
Model ind 640 epoch 871 head B head_i_epoch 0 batch 0: avg loss -1.877922 avg loss no lamb -1.877922 time 2019-02-27 12:32:54.255123
Model ind 640 epoch 871 head B head_i_epoch 0 batch 100: avg loss -1.863848 avg loss no lamb -1.863848 time 2019-02-27 12:34:47.648588
Model ind 640 epoch 871 head B head_i_epoch 0 batch 200: avg loss -1.935163 avg loss no lamb -1.935163 time 2019-02-27 12:36:39.325961
last batch sz 160
Model ind 640 epoch 871 head B head_i_epoch 1 batch 0: avg loss -1.958119 avg loss no lamb -1.958119 time 2019-02-27 12:38:03.432584
Model ind 640 epoch 871 head B head_i_epoch 1 batch 100: avg loss -1.781064 avg loss no lamb -1.781064 time 2019-02-27 12:39:53.960622
Model ind 640 epoch 871 head B head_i_epoch 1 batch 200: avg loss -1.870599 avg loss no lamb -1.870599 time 2019-02-27 12:41:48.479370
last batch sz 160
Pre: time 2019-02-27 12:43:30.344956: 
 	std: 0.050406683
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5117667, 0.6145833, 0.6142833, 0.61448336, 0.51135]
	train_accs: [0.5117667, 0.6145833, 0.6142833, 0.61448336, 0.51135]
	best_train_sub_head: 1
	worst: 0.51135
	avg: 0.5732933
	best: 0.6145833

Starting e_i: 872
Model ind 640 epoch 872 head A head_i_epoch 0 batch 0: avg loss -3.245524 avg loss no lamb -3.245524 time 2019-02-27 12:43:33.342883
Model ind 640 epoch 872 head A head_i_epoch 0 batch 100: avg loss -3.113395 avg loss no lamb -3.113395 time 2019-02-27 12:45:26.929965
Model ind 640 epoch 872 head A head_i_epoch 0 batch 200: avg loss -3.220086 avg loss no lamb -3.220086 time 2019-02-27 12:47:21.003093
last batch sz 160
Model ind 640 epoch 872 head B head_i_epoch 0 batch 0: avg loss -1.896623 avg loss no lamb -1.896623 time 2019-02-27 12:48:38.898773
Model ind 640 epoch 872 head B head_i_epoch 0 batch 100: avg loss -1.837478 avg loss no lamb -1.837478 time 2019-02-27 12:50:34.448933
Model ind 640 epoch 872 head B head_i_epoch 0 batch 200: avg loss -1.894473 avg loss no lamb -1.894473 time 2019-02-27 12:52:25.941076
last batch sz 160
Model ind 640 epoch 872 head B head_i_epoch 1 batch 0: avg loss -1.871992 avg loss no lamb -1.871992 time 2019-02-27 12:53:51.258803
Model ind 640 epoch 872 head B head_i_epoch 1 batch 100: avg loss -1.847389 avg loss no lamb -1.847389 time 2019-02-27 12:55:41.939016
Model ind 640 epoch 872 head B head_i_epoch 1 batch 200: avg loss -1.872529 avg loss no lamb -1.872529 time 2019-02-27 12:57:36.666140
last batch sz 160
Pre: time 2019-02-27 12:59:20.502502: 
 	std: 0.050803944
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5103667, 0.61396664, 0.61375, 0.61396664, 0.5100167]
	train_accs: [0.5103667, 0.61396664, 0.61375, 0.61396664, 0.5100167]
	best_train_sub_head: 1
	worst: 0.5100167
	avg: 0.5724133
	best: 0.61396664

Starting e_i: 873
Model ind 640 epoch 873 head A head_i_epoch 0 batch 0: avg loss -3.129557 avg loss no lamb -3.129557 time 2019-02-27 12:59:23.827151
Model ind 640 epoch 873 head A head_i_epoch 0 batch 100: avg loss -3.157102 avg loss no lamb -3.157102 time 2019-02-27 13:01:14.630654
Model ind 640 epoch 873 head A head_i_epoch 0 batch 200: avg loss -3.246754 avg loss no lamb -3.246754 time 2019-02-27 13:03:09.901281
last batch sz 160
Model ind 640 epoch 873 head B head_i_epoch 0 batch 0: avg loss -1.824430 avg loss no lamb -1.824430 time 2019-02-27 13:04:29.558889
Model ind 640 epoch 873 head B head_i_epoch 0 batch 100: avg loss -1.764551 avg loss no lamb -1.764551 time 2019-02-27 13:06:24.830554
Model ind 640 epoch 873 head B head_i_epoch 0 batch 200: avg loss -1.994542 avg loss no lamb -1.994542 time 2019-02-27 13:08:17.742329
last batch sz 160
Model ind 640 epoch 873 head B head_i_epoch 1 batch 0: avg loss -1.877586 avg loss no lamb -1.877586 time 2019-02-27 13:09:40.018267
Model ind 640 epoch 873 head B head_i_epoch 1 batch 100: avg loss -1.846328 avg loss no lamb -1.846328 time 2019-02-27 13:11:31.204407
Model ind 640 epoch 873 head B head_i_epoch 1 batch 200: avg loss -1.909800 avg loss no lamb -1.909800 time 2019-02-27 13:13:23.100323
last batch sz 160
Pre: time 2019-02-27 13:15:08.191580: 
 	std: 0.051352285
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50908333, 0.61406666, 0.61401665, 0.61378336, 0.50918335]
	train_accs: [0.50908333, 0.61406666, 0.61401665, 0.61378336, 0.50918335]
	best_train_sub_head: 1
	worst: 0.50908333
	avg: 0.57202667
	best: 0.61406666

Starting e_i: 874
Model ind 640 epoch 874 head A head_i_epoch 0 batch 0: avg loss -3.225616 avg loss no lamb -3.225616 time 2019-02-27 13:15:10.886266
Model ind 640 epoch 874 head A head_i_epoch 0 batch 100: avg loss -3.129039 avg loss no lamb -3.129039 time 2019-02-27 13:17:01.331064
Model ind 640 epoch 874 head A head_i_epoch 0 batch 200: avg loss -3.173540 avg loss no lamb -3.173540 time 2019-02-27 13:18:54.842554
last batch sz 160
Model ind 640 epoch 874 head B head_i_epoch 0 batch 0: avg loss -1.789920 avg loss no lamb -1.789920 time 2019-02-27 13:20:15.479515
Model ind 640 epoch 874 head B head_i_epoch 0 batch 100: avg loss -1.767635 avg loss no lamb -1.767635 time 2019-02-27 13:22:09.693943
Model ind 640 epoch 874 head B head_i_epoch 0 batch 200: avg loss -1.850150 avg loss no lamb -1.850150 time 2019-02-27 13:24:03.353069
last batch sz 160
Model ind 640 epoch 874 head B head_i_epoch 1 batch 0: avg loss -1.845382 avg loss no lamb -1.845382 time 2019-02-27 13:25:24.010650
Model ind 640 epoch 874 head B head_i_epoch 1 batch 100: avg loss -1.812300 avg loss no lamb -1.812300 time 2019-02-27 13:27:19.189632
Model ind 640 epoch 874 head B head_i_epoch 1 batch 200: avg loss -1.938872 avg loss no lamb -1.938872 time 2019-02-27 13:29:10.088357
last batch sz 160
Pre: time 2019-02-27 13:30:54.762761: 
 	std: 0.05075349
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5108167, 0.6142333, 0.6145, 0.61436665, 0.5107167]
	train_accs: [0.5108167, 0.6142333, 0.6145, 0.61436665, 0.5107167]
	best_train_sub_head: 2
	worst: 0.5107167
	avg: 0.57292664
	best: 0.6145

Starting e_i: 875
Model ind 640 epoch 875 head A head_i_epoch 0 batch 0: avg loss -3.227212 avg loss no lamb -3.227212 time 2019-02-27 13:30:57.728554
Model ind 640 epoch 875 head A head_i_epoch 0 batch 100: avg loss -3.152930 avg loss no lamb -3.152930 time 2019-02-27 13:32:50.310388
Model ind 640 epoch 875 head A head_i_epoch 0 batch 200: avg loss -3.140852 avg loss no lamb -3.140852 time 2019-02-27 13:34:41.816964
last batch sz 160
Model ind 640 epoch 875 head B head_i_epoch 0 batch 0: avg loss -1.787544 avg loss no lamb -1.787544 time 2019-02-27 13:36:04.054882
Model ind 640 epoch 875 head B head_i_epoch 0 batch 100: avg loss -1.772282 avg loss no lamb -1.772282 time 2019-02-27 13:37:55.943911
Model ind 640 epoch 875 head B head_i_epoch 0 batch 200: avg loss -1.879701 avg loss no lamb -1.879701 time 2019-02-27 13:39:50.889640
last batch sz 160
Model ind 640 epoch 875 head B head_i_epoch 1 batch 0: avg loss -1.899347 avg loss no lamb -1.899347 time 2019-02-27 13:41:11.321967
Model ind 640 epoch 875 head B head_i_epoch 1 batch 100: avg loss -1.847471 avg loss no lamb -1.847471 time 2019-02-27 13:43:06.892956
Model ind 640 epoch 875 head B head_i_epoch 1 batch 200: avg loss -1.980466 avg loss no lamb -1.980466 time 2019-02-27 13:44:58.527957
last batch sz 160
Pre: time 2019-02-27 13:46:42.776692: 
 	std: 0.050049085
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5100333, 0.61188334, 0.61195, 0.6117, 0.5093333]
	train_accs: [0.5100333, 0.61188334, 0.61195, 0.6117, 0.5093333]
	best_train_sub_head: 2
	worst: 0.5093333
	avg: 0.57098
	best: 0.61195

Starting e_i: 876
Model ind 640 epoch 876 head A head_i_epoch 0 batch 0: avg loss -3.191027 avg loss no lamb -3.191027 time 2019-02-27 13:46:45.136256
Model ind 640 epoch 876 head A head_i_epoch 0 batch 100: avg loss -3.150787 avg loss no lamb -3.150787 time 2019-02-27 13:48:38.456479
Model ind 640 epoch 876 head A head_i_epoch 0 batch 200: avg loss -3.231773 avg loss no lamb -3.231773 time 2019-02-27 13:50:28.623503
last batch sz 160
Model ind 640 epoch 876 head B head_i_epoch 0 batch 0: avg loss -1.849751 avg loss no lamb -1.849751 time 2019-02-27 13:51:52.470857
Model ind 640 epoch 876 head B head_i_epoch 0 batch 100: avg loss -1.771328 avg loss no lamb -1.771328 time 2019-02-27 13:53:43.028436
Model ind 640 epoch 876 head B head_i_epoch 0 batch 200: avg loss -1.893868 avg loss no lamb -1.893868 time 2019-02-27 13:55:37.324686
last batch sz 160
Model ind 640 epoch 876 head B head_i_epoch 1 batch 0: avg loss -1.854780 avg loss no lamb -1.854780 time 2019-02-27 13:56:57.636482
Model ind 640 epoch 876 head B head_i_epoch 1 batch 100: avg loss -1.787552 avg loss no lamb -1.787552 time 2019-02-27 13:58:58.880335
Model ind 640 epoch 876 head B head_i_epoch 1 batch 200: avg loss -1.873177 avg loss no lamb -1.873177 time 2019-02-27 14:01:01.206878
last batch sz 160
Pre: time 2019-02-27 14:02:48.715544: 
 	std: 0.050786205
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50913334, 0.61261666, 0.6127167, 0.61256665, 0.5088]
	train_accs: [0.50913334, 0.61261666, 0.6127167, 0.61256665, 0.5088]
	best_train_sub_head: 2
	worst: 0.5088
	avg: 0.57116663
	best: 0.6127167

Starting e_i: 877
Model ind 640 epoch 877 head A head_i_epoch 0 batch 0: avg loss -3.198283 avg loss no lamb -3.198283 time 2019-02-27 14:02:51.461828
Model ind 640 epoch 877 head A head_i_epoch 0 batch 100: avg loss -3.045599 avg loss no lamb -3.045599 time 2019-02-27 14:04:55.567961
Model ind 640 epoch 877 head A head_i_epoch 0 batch 200: avg loss -3.254192 avg loss no lamb -3.254192 time 2019-02-27 14:06:52.498740
last batch sz 160
Model ind 640 epoch 877 head B head_i_epoch 0 batch 0: avg loss -1.906294 avg loss no lamb -1.906294 time 2019-02-27 14:08:22.085950
Model ind 640 epoch 877 head B head_i_epoch 0 batch 100: avg loss -1.795095 avg loss no lamb -1.795095 time 2019-02-27 14:10:24.435836
Model ind 640 epoch 877 head B head_i_epoch 0 batch 200: avg loss -1.933268 avg loss no lamb -1.933268 time 2019-02-27 14:12:19.712494
last batch sz 160
Model ind 640 epoch 877 head B head_i_epoch 1 batch 0: avg loss -1.757082 avg loss no lamb -1.757082 time 2019-02-27 14:13:53.645376
Model ind 640 epoch 877 head B head_i_epoch 1 batch 100: avg loss -1.796580 avg loss no lamb -1.796580 time 2019-02-27 14:15:50.639899
Model ind 640 epoch 877 head B head_i_epoch 1 batch 200: avg loss -1.905612 avg loss no lamb -1.905612 time 2019-02-27 14:17:54.626639
last batch sz 160
Pre: time 2019-02-27 14:19:49.711592: 
 	std: 0.05086283
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5093167, 0.6127, 0.6132, 0.6127667, 0.50881666]
	train_accs: [0.5093167, 0.6127, 0.6132, 0.6127667, 0.50881666]
	best_train_sub_head: 2
	worst: 0.50881666
	avg: 0.57136
	best: 0.6132

Starting e_i: 878
Model ind 640 epoch 878 head A head_i_epoch 0 batch 0: avg loss -3.170639 avg loss no lamb -3.170639 time 2019-02-27 14:19:52.783684
Model ind 640 epoch 878 head A head_i_epoch 0 batch 100: avg loss -3.160961 avg loss no lamb -3.160961 time 2019-02-27 14:21:44.936824
Model ind 640 epoch 878 head A head_i_epoch 0 batch 200: avg loss -3.194559 avg loss no lamb -3.194559 time 2019-02-27 14:23:50.931296
last batch sz 160
Model ind 640 epoch 878 head B head_i_epoch 0 batch 0: avg loss -1.861941 avg loss no lamb -1.861941 time 2019-02-27 14:25:16.793827
Model ind 640 epoch 878 head B head_i_epoch 0 batch 100: avg loss -1.826933 avg loss no lamb -1.826933 time 2019-02-27 14:27:19.599998
Model ind 640 epoch 878 head B head_i_epoch 0 batch 200: avg loss -1.851507 avg loss no lamb -1.851507 time 2019-02-27 14:29:23.190536
last batch sz 160
Model ind 640 epoch 878 head B head_i_epoch 1 batch 0: avg loss -1.856514 avg loss no lamb -1.856514 time 2019-02-27 14:30:45.602594
Model ind 640 epoch 878 head B head_i_epoch 1 batch 100: avg loss -1.919551 avg loss no lamb -1.919551 time 2019-02-27 14:32:52.632660
Model ind 640 epoch 878 head B head_i_epoch 1 batch 200: avg loss -1.904981 avg loss no lamb -1.904981 time 2019-02-27 14:34:50.760405
last batch sz 160
Pre: time 2019-02-27 14:36:41.530891: 
 	std: 0.05006505
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111333, 0.6131, 0.61315, 0.61308336, 0.5107]
	train_accs: [0.5111333, 0.6131, 0.61315, 0.61308336, 0.5107]
	best_train_sub_head: 2
	worst: 0.5107
	avg: 0.5722333
	best: 0.61315

Starting e_i: 879
Model ind 640 epoch 879 head A head_i_epoch 0 batch 0: avg loss -3.188493 avg loss no lamb -3.188493 time 2019-02-27 14:36:44.162140
Model ind 640 epoch 879 head A head_i_epoch 0 batch 100: avg loss -3.182985 avg loss no lamb -3.182985 time 2019-02-27 14:38:50.990938
Model ind 640 epoch 879 head A head_i_epoch 0 batch 200: avg loss -3.250510 avg loss no lamb -3.250510 time 2019-02-27 14:40:45.829945
last batch sz 160
Model ind 640 epoch 879 head B head_i_epoch 0 batch 0: avg loss -1.842122 avg loss no lamb -1.842122 time 2019-02-27 14:42:23.666798
Model ind 640 epoch 879 head B head_i_epoch 0 batch 100: avg loss -1.801644 avg loss no lamb -1.801644 time 2019-02-27 14:44:26.206971
Model ind 640 epoch 879 head B head_i_epoch 0 batch 200: avg loss -1.941980 avg loss no lamb -1.941980 time 2019-02-27 14:46:25.121594
last batch sz 160
Model ind 640 epoch 879 head B head_i_epoch 1 batch 0: avg loss -1.826014 avg loss no lamb -1.826014 time 2019-02-27 14:48:01.886752
Model ind 640 epoch 879 head B head_i_epoch 1 batch 100: avg loss -1.786160 avg loss no lamb -1.786160 time 2019-02-27 14:49:57.114071
Model ind 640 epoch 879 head B head_i_epoch 1 batch 200: avg loss -1.840987 avg loss no lamb -1.840987 time 2019-02-27 14:52:04.594104
last batch sz 160
Pre: time 2019-02-27 14:53:54.533577: 
 	std: 0.050872244
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50951666, 0.613, 0.61298335, 0.6131667, 0.5089]
	train_accs: [0.50951666, 0.613, 0.61298335, 0.6131667, 0.5089]
	best_train_sub_head: 3
	worst: 0.5089
	avg: 0.5715133
	best: 0.6131667

Starting e_i: 880
Model ind 640 epoch 880 head A head_i_epoch 0 batch 0: avg loss -3.228488 avg loss no lamb -3.228488 time 2019-02-27 14:53:58.989517
Model ind 640 epoch 880 head A head_i_epoch 0 batch 100: avg loss -3.117192 avg loss no lamb -3.117192 time 2019-02-27 14:55:57.678239
Model ind 640 epoch 880 head A head_i_epoch 0 batch 200: avg loss -3.147272 avg loss no lamb -3.147272 time 2019-02-27 14:58:02.241780
last batch sz 160
Model ind 640 epoch 880 head B head_i_epoch 0 batch 0: avg loss -1.928009 avg loss no lamb -1.928009 time 2019-02-27 14:59:24.434512
Model ind 640 epoch 880 head B head_i_epoch 0 batch 100: avg loss -1.750276 avg loss no lamb -1.750276 time 2019-02-27 15:01:27.942844
Model ind 640 epoch 880 head B head_i_epoch 0 batch 200: avg loss -1.833471 avg loss no lamb -1.833471 time 2019-02-27 15:03:25.547194
last batch sz 160
Model ind 640 epoch 880 head B head_i_epoch 1 batch 0: avg loss -1.756473 avg loss no lamb -1.756473 time 2019-02-27 15:04:52.546435
Model ind 640 epoch 880 head B head_i_epoch 1 batch 100: avg loss -1.899004 avg loss no lamb -1.899004 time 2019-02-27 15:06:54.154528
Model ind 640 epoch 880 head B head_i_epoch 1 batch 200: avg loss -1.878125 avg loss no lamb -1.878125 time 2019-02-27 15:08:51.849476
last batch sz 160
Pre: time 2019-02-27 15:10:41.706556: 
 	std: 0.050794765
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50995, 0.6131833, 0.61335, 0.61331666, 0.50925]
	train_accs: [0.50995, 0.6131833, 0.61335, 0.61331666, 0.50925]
	best_train_sub_head: 2
	worst: 0.50925
	avg: 0.57181
	best: 0.61335

Starting e_i: 881
Model ind 640 epoch 881 head A head_i_epoch 0 batch 0: avg loss -3.249562 avg loss no lamb -3.249562 time 2019-02-27 15:10:51.926832
Model ind 640 epoch 881 head A head_i_epoch 0 batch 100: avg loss -3.069695 avg loss no lamb -3.069695 time 2019-02-27 15:12:51.627053
Model ind 640 epoch 881 head A head_i_epoch 0 batch 200: avg loss -3.171684 avg loss no lamb -3.171684 time 2019-02-27 15:14:52.508810
last batch sz 160
Model ind 640 epoch 881 head B head_i_epoch 0 batch 0: avg loss -1.946421 avg loss no lamb -1.946421 time 2019-02-27 15:16:19.622645
Model ind 640 epoch 881 head B head_i_epoch 0 batch 100: avg loss -1.826074 avg loss no lamb -1.826074 time 2019-02-27 15:18:19.408538
Model ind 640 epoch 881 head B head_i_epoch 0 batch 200: avg loss -1.864123 avg loss no lamb -1.864123 time 2019-02-27 15:20:18.595266
last batch sz 160
Model ind 640 epoch 881 head B head_i_epoch 1 batch 0: avg loss -1.852373 avg loss no lamb -1.852373 time 2019-02-27 15:21:46.557894
Model ind 640 epoch 881 head B head_i_epoch 1 batch 100: avg loss -1.851861 avg loss no lamb -1.851861 time 2019-02-27 15:23:48.697449
Model ind 640 epoch 881 head B head_i_epoch 1 batch 200: avg loss -1.936422 avg loss no lamb -1.936422 time 2019-02-27 15:25:47.058204
last batch sz 160
Pre: time 2019-02-27 15:27:38.584911: 
 	std: 0.05071954
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51085, 0.61413336, 0.61438334, 0.61425, 0.5106]
	train_accs: [0.51085, 0.61413336, 0.61438334, 0.61425, 0.5106]
	best_train_sub_head: 2
	worst: 0.5106
	avg: 0.5728434
	best: 0.61438334

Starting e_i: 882
Model ind 640 epoch 882 head A head_i_epoch 0 batch 0: avg loss -3.166805 avg loss no lamb -3.166805 time 2019-02-27 15:27:41.239784
Model ind 640 epoch 882 head A head_i_epoch 0 batch 100: avg loss -3.072084 avg loss no lamb -3.072084 time 2019-02-27 15:29:42.755965
Model ind 640 epoch 882 head A head_i_epoch 0 batch 200: avg loss -3.247719 avg loss no lamb -3.247719 time 2019-02-27 15:31:39.953495
last batch sz 160
Model ind 640 epoch 882 head B head_i_epoch 0 batch 0: avg loss -1.860608 avg loss no lamb -1.860608 time 2019-02-27 15:33:06.761810
Model ind 640 epoch 882 head B head_i_epoch 0 batch 100: avg loss -1.836332 avg loss no lamb -1.836332 time 2019-02-27 15:35:06.220669
Model ind 640 epoch 882 head B head_i_epoch 0 batch 200: avg loss -1.904122 avg loss no lamb -1.904122 time 2019-02-27 15:37:06.744680
last batch sz 160
Model ind 640 epoch 882 head B head_i_epoch 1 batch 0: avg loss -1.896541 avg loss no lamb -1.896541 time 2019-02-27 15:38:35.931446
Model ind 640 epoch 882 head B head_i_epoch 1 batch 100: avg loss -1.812907 avg loss no lamb -1.812907 time 2019-02-27 15:40:34.131150
Model ind 640 epoch 882 head B head_i_epoch 1 batch 200: avg loss -1.836133 avg loss no lamb -1.836133 time 2019-02-27 15:42:38.122196
last batch sz 160
Pre: time 2019-02-27 15:44:25.904007: 
 	std: 0.050369795
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115833, 0.61415, 0.6143, 0.6143, 0.51128334]
	train_accs: [0.5115833, 0.61415, 0.6143, 0.6143, 0.51128334]
	best_train_sub_head: 2
	worst: 0.51128334
	avg: 0.57312334
	best: 0.6143

Starting e_i: 883
Model ind 640 epoch 883 head A head_i_epoch 0 batch 0: avg loss -3.264193 avg loss no lamb -3.264193 time 2019-02-27 15:44:28.907699
Model ind 640 epoch 883 head A head_i_epoch 0 batch 100: avg loss -3.090256 avg loss no lamb -3.090256 time 2019-02-27 15:46:29.656062
Model ind 640 epoch 883 head A head_i_epoch 0 batch 200: avg loss -3.210889 avg loss no lamb -3.210889 time 2019-02-27 15:48:34.816576
last batch sz 160
Model ind 640 epoch 883 head B head_i_epoch 0 batch 0: avg loss -1.866332 avg loss no lamb -1.866332 time 2019-02-27 15:49:57.915904
Model ind 640 epoch 883 head B head_i_epoch 0 batch 100: avg loss -1.755724 avg loss no lamb -1.755724 time 2019-02-27 15:52:03.103086
Model ind 640 epoch 883 head B head_i_epoch 0 batch 200: avg loss -1.887401 avg loss no lamb -1.887401 time 2019-02-27 15:54:05.715648
last batch sz 160
Model ind 640 epoch 883 head B head_i_epoch 1 batch 0: avg loss -1.838467 avg loss no lamb -1.838467 time 2019-02-27 15:55:32.167760
Model ind 640 epoch 883 head B head_i_epoch 1 batch 100: avg loss -1.802191 avg loss no lamb -1.802191 time 2019-02-27 15:57:37.261092
Model ind 640 epoch 883 head B head_i_epoch 1 batch 200: avg loss -1.921085 avg loss no lamb -1.921085 time 2019-02-27 15:59:37.012319
last batch sz 160
Pre: time 2019-02-27 16:01:31.717387: 
 	std: 0.0506733
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5097833, 0.613, 0.6131, 0.61298335, 0.5094]
	train_accs: [0.5097833, 0.613, 0.6131, 0.61298335, 0.5094]
	best_train_sub_head: 2
	worst: 0.5094
	avg: 0.57165325
	best: 0.6131

Starting e_i: 884
Model ind 640 epoch 884 head A head_i_epoch 0 batch 0: avg loss -3.164134 avg loss no lamb -3.164134 time 2019-02-27 16:01:34.975111
Model ind 640 epoch 884 head A head_i_epoch 0 batch 100: avg loss -3.154632 avg loss no lamb -3.154632 time 2019-02-27 16:03:39.459848
Model ind 640 epoch 884 head A head_i_epoch 0 batch 200: avg loss -3.158260 avg loss no lamb -3.158260 time 2019-02-27 16:05:35.503278
last batch sz 160
Model ind 640 epoch 884 head B head_i_epoch 0 batch 0: avg loss -1.925233 avg loss no lamb -1.925233 time 2019-02-27 16:07:12.161171
Model ind 640 epoch 884 head B head_i_epoch 0 batch 100: avg loss -1.799071 avg loss no lamb -1.799071 time 2019-02-27 16:09:08.713069
Model ind 640 epoch 884 head B head_i_epoch 0 batch 200: avg loss -1.901144 avg loss no lamb -1.901144 time 2019-02-27 16:11:11.983875
last batch sz 160
Model ind 640 epoch 884 head B head_i_epoch 1 batch 0: avg loss -1.840051 avg loss no lamb -1.840051 time 2019-02-27 16:12:44.312249
Model ind 640 epoch 884 head B head_i_epoch 1 batch 100: avg loss -1.740238 avg loss no lamb -1.740238 time 2019-02-27 16:14:41.222343
Model ind 640 epoch 884 head B head_i_epoch 1 batch 200: avg loss -1.911241 avg loss no lamb -1.911241 time 2019-02-27 16:16:48.378751
last batch sz 160
Pre: time 2019-02-27 16:18:37.016699: 
 	std: 0.051066574
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5110667, 0.6150333, 0.6152667, 0.61541665, 0.51093334]
	train_accs: [0.5110667, 0.6150333, 0.6152667, 0.61541665, 0.51093334]
	best_train_sub_head: 3
	worst: 0.51093334
	avg: 0.57354337
	best: 0.61541665

Starting e_i: 885
Model ind 640 epoch 885 head A head_i_epoch 0 batch 0: avg loss -3.217317 avg loss no lamb -3.217317 time 2019-02-27 16:18:40.812089
Model ind 640 epoch 885 head A head_i_epoch 0 batch 100: avg loss -3.104537 avg loss no lamb -3.104537 time 2019-02-27 16:20:40.274949
Model ind 640 epoch 885 head A head_i_epoch 0 batch 200: avg loss -3.127070 avg loss no lamb -3.127070 time 2019-02-27 16:22:49.500451
last batch sz 160
Model ind 640 epoch 885 head B head_i_epoch 0 batch 0: avg loss -1.823667 avg loss no lamb -1.823667 time 2019-02-27 16:24:12.373977
Model ind 640 epoch 885 head B head_i_epoch 0 batch 100: avg loss -1.827219 avg loss no lamb -1.827219 time 2019-02-27 16:26:18.631157
Model ind 640 epoch 885 head B head_i_epoch 0 batch 200: avg loss -1.935099 avg loss no lamb -1.935099 time 2019-02-27 16:28:21.562437
last batch sz 160
Model ind 640 epoch 885 head B head_i_epoch 1 batch 0: avg loss -1.922402 avg loss no lamb -1.922402 time 2019-02-27 16:29:47.691832
Model ind 640 epoch 885 head B head_i_epoch 1 batch 100: avg loss -1.774480 avg loss no lamb -1.774480 time 2019-02-27 16:31:54.518024
Model ind 640 epoch 885 head B head_i_epoch 1 batch 200: avg loss -1.929506 avg loss no lamb -1.929506 time 2019-02-27 16:33:48.676216
last batch sz 160
Pre: time 2019-02-27 16:35:46.441856: 
 	std: 0.051169995
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50953335, 0.61368334, 0.61398333, 0.6138833, 0.5092667]
	train_accs: [0.50953335, 0.61368334, 0.61398333, 0.6138833, 0.5092667]
	best_train_sub_head: 2
	worst: 0.5092667
	avg: 0.57207
	best: 0.61398333

Starting e_i: 886
Model ind 640 epoch 886 head A head_i_epoch 0 batch 0: avg loss -3.173213 avg loss no lamb -3.173213 time 2019-02-27 16:35:49.735089
Model ind 640 epoch 886 head A head_i_epoch 0 batch 100: avg loss -3.072641 avg loss no lamb -3.072641 time 2019-02-27 16:37:56.046383
Model ind 640 epoch 886 head A head_i_epoch 0 batch 200: avg loss -3.228273 avg loss no lamb -3.228273 time 2019-02-27 16:39:51.103783
last batch sz 160
Model ind 640 epoch 886 head B head_i_epoch 0 batch 0: avg loss -1.828001 avg loss no lamb -1.828001 time 2019-02-27 16:41:29.628180
Model ind 640 epoch 886 head B head_i_epoch 0 batch 100: avg loss -1.783557 avg loss no lamb -1.783557 time 2019-02-27 16:43:21.969883
Model ind 640 epoch 886 head B head_i_epoch 0 batch 200: avg loss -1.939266 avg loss no lamb -1.939266 time 2019-02-27 16:45:29.592760
last batch sz 160
Model ind 640 epoch 886 head B head_i_epoch 1 batch 0: avg loss -1.861615 avg loss no lamb -1.861615 time 2019-02-27 16:47:00.521131
Model ind 640 epoch 886 head B head_i_epoch 1 batch 100: avg loss -1.725026 avg loss no lamb -1.725026 time 2019-02-27 16:49:02.436095
Model ind 640 epoch 886 head B head_i_epoch 1 batch 200: avg loss -1.865207 avg loss no lamb -1.865207 time 2019-02-27 16:51:11.237796
last batch sz 160
Pre: time 2019-02-27 16:52:58.290710: 
 	std: 0.051161792
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50881666, 0.61331666, 0.61345, 0.61343336, 0.50911665]
	train_accs: [0.50881666, 0.61331666, 0.61345, 0.61343336, 0.50911665]
	best_train_sub_head: 2
	worst: 0.50881666
	avg: 0.57162666
	best: 0.61345

Starting e_i: 887
Model ind 640 epoch 887 head A head_i_epoch 0 batch 0: avg loss -3.263202 avg loss no lamb -3.263202 time 2019-02-27 16:53:01.908194
Model ind 640 epoch 887 head A head_i_epoch 0 batch 100: avg loss -3.113929 avg loss no lamb -3.113929 time 2019-02-27 16:55:05.110640
Model ind 640 epoch 887 head A head_i_epoch 0 batch 200: avg loss -3.183785 avg loss no lamb -3.183785 time 2019-02-27 16:57:09.803333
last batch sz 160
Model ind 640 epoch 887 head B head_i_epoch 0 batch 0: avg loss -1.951447 avg loss no lamb -1.951447 time 2019-02-27 16:58:32.204582
Model ind 640 epoch 887 head B head_i_epoch 0 batch 100: avg loss -1.777148 avg loss no lamb -1.777148 time 2019-02-27 17:00:38.595365
Model ind 640 epoch 887 head B head_i_epoch 0 batch 200: avg loss -1.893821 avg loss no lamb -1.893821 time 2019-02-27 17:02:36.611809
last batch sz 160
Model ind 640 epoch 887 head B head_i_epoch 1 batch 0: avg loss -1.824555 avg loss no lamb -1.824555 time 2019-02-27 17:04:05.860612
Model ind 640 epoch 887 head B head_i_epoch 1 batch 100: avg loss -1.749811 avg loss no lamb -1.749811 time 2019-02-27 17:06:06.538783
Model ind 640 epoch 887 head B head_i_epoch 1 batch 200: avg loss -1.873673 avg loss no lamb -1.873673 time 2019-02-27 17:08:04.866053
last batch sz 160
Pre: time 2019-02-27 17:09:58.270374: 
 	std: 0.05002405
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5119, 0.6141, 0.61401665, 0.6139167, 0.5119]
	train_accs: [0.5119, 0.6141, 0.61401665, 0.6139167, 0.5119]
	best_train_sub_head: 1
	worst: 0.5119
	avg: 0.57316667
	best: 0.6141

Starting e_i: 888
Model ind 640 epoch 888 head A head_i_epoch 0 batch 0: avg loss -3.222722 avg loss no lamb -3.222722 time 2019-02-27 17:10:01.151328
Model ind 640 epoch 888 head A head_i_epoch 0 batch 100: avg loss -3.125146 avg loss no lamb -3.125146 time 2019-02-27 17:12:01.320359
Model ind 640 epoch 888 head A head_i_epoch 0 batch 200: avg loss -3.258294 avg loss no lamb -3.258294 time 2019-02-27 17:14:02.884338
last batch sz 160
Model ind 640 epoch 888 head B head_i_epoch 0 batch 0: avg loss -1.937408 avg loss no lamb -1.937408 time 2019-02-27 17:15:29.801158
Model ind 640 epoch 888 head B head_i_epoch 0 batch 100: avg loss -1.809913 avg loss no lamb -1.809913 time 2019-02-27 17:17:29.385789
Model ind 640 epoch 888 head B head_i_epoch 0 batch 200: avg loss -1.922791 avg loss no lamb -1.922791 time 2019-02-27 17:19:28.447048
last batch sz 160
Model ind 640 epoch 888 head B head_i_epoch 1 batch 0: avg loss -1.888526 avg loss no lamb -1.888526 time 2019-02-27 17:20:56.485336
Model ind 640 epoch 888 head B head_i_epoch 1 batch 100: avg loss -1.793642 avg loss no lamb -1.793642 time 2019-02-27 17:22:58.306142
Model ind 640 epoch 888 head B head_i_epoch 1 batch 200: avg loss -1.891663 avg loss no lamb -1.891663 time 2019-02-27 17:24:58.710224
last batch sz 160
Pre: time 2019-02-27 17:26:49.599772: 
 	std: 0.05059977
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107833, 0.6138667, 0.6138833, 0.61398333, 0.5104667]
	train_accs: [0.5107833, 0.6138667, 0.6138833, 0.61398333, 0.5104667]
	best_train_sub_head: 3
	worst: 0.5104667
	avg: 0.57259667
	best: 0.61398333

Starting e_i: 889
Model ind 640 epoch 889 head A head_i_epoch 0 batch 0: avg loss -3.239640 avg loss no lamb -3.239640 time 2019-02-27 17:26:52.414356
Model ind 640 epoch 889 head A head_i_epoch 0 batch 100: avg loss -3.114346 avg loss no lamb -3.114346 time 2019-02-27 17:28:54.964404
Model ind 640 epoch 889 head A head_i_epoch 0 batch 200: avg loss -3.174130 avg loss no lamb -3.174130 time 2019-02-27 17:30:49.792375
last batch sz 160
Model ind 640 epoch 889 head B head_i_epoch 0 batch 0: avg loss -1.890145 avg loss no lamb -1.890145 time 2019-02-27 17:32:22.966898
Model ind 640 epoch 889 head B head_i_epoch 0 batch 100: avg loss -1.777492 avg loss no lamb -1.777492 time 2019-02-27 17:34:19.706862
Model ind 640 epoch 889 head B head_i_epoch 0 batch 200: avg loss -1.850422 avg loss no lamb -1.850422 time 2019-02-27 17:36:19.515969
last batch sz 160
Model ind 640 epoch 889 head B head_i_epoch 1 batch 0: avg loss -1.915768 avg loss no lamb -1.915768 time 2019-02-27 17:37:46.275617
Model ind 640 epoch 889 head B head_i_epoch 1 batch 100: avg loss -1.802161 avg loss no lamb -1.802161 time 2019-02-27 17:39:46.272919
Model ind 640 epoch 889 head B head_i_epoch 1 batch 200: avg loss -1.958272 avg loss no lamb -1.958272 time 2019-02-27 17:41:50.638089
last batch sz 160
Pre: time 2019-02-27 17:43:38.469385: 
 	std: 0.05070205
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5113, 0.6144, 0.6145333, 0.61465, 0.5107667]
	train_accs: [0.5113, 0.6144, 0.6145333, 0.61465, 0.5107667]
	best_train_sub_head: 3
	worst: 0.5107667
	avg: 0.57313
	best: 0.61465

Starting e_i: 890
Model ind 640 epoch 890 head A head_i_epoch 0 batch 0: avg loss -3.204382 avg loss no lamb -3.204382 time 2019-02-27 17:43:41.549727
Model ind 640 epoch 890 head A head_i_epoch 0 batch 100: avg loss -3.139462 avg loss no lamb -3.139462 time 2019-02-27 17:45:44.745722
Model ind 640 epoch 890 head A head_i_epoch 0 batch 200: avg loss -3.178848 avg loss no lamb -3.178848 time 2019-02-27 17:47:50.328679
last batch sz 160
Model ind 640 epoch 890 head B head_i_epoch 0 batch 0: avg loss -1.887571 avg loss no lamb -1.887571 time 2019-02-27 17:49:14.123338
Model ind 640 epoch 890 head B head_i_epoch 0 batch 100: avg loss -1.818476 avg loss no lamb -1.818476 time 2019-02-27 17:51:20.307763
Model ind 640 epoch 890 head B head_i_epoch 0 batch 200: avg loss -1.893426 avg loss no lamb -1.893426 time 2019-02-27 17:53:21.618090
last batch sz 160
Model ind 640 epoch 890 head B head_i_epoch 1 batch 0: avg loss -1.895756 avg loss no lamb -1.895756 time 2019-02-27 17:54:46.942636
Model ind 640 epoch 890 head B head_i_epoch 1 batch 100: avg loss -1.846141 avg loss no lamb -1.846141 time 2019-02-27 17:56:55.461355
Model ind 640 epoch 890 head B head_i_epoch 1 batch 200: avg loss -1.870616 avg loss no lamb -1.870616 time 2019-02-27 17:58:50.088591
last batch sz 160
Pre: time 2019-02-27 18:00:46.581326: 
 	std: 0.050579295
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51126665, 0.61466664, 0.6145333, 0.61443335, 0.51133335]
	train_accs: [0.51126665, 0.61466664, 0.6145333, 0.61443335, 0.51133335]
	best_train_sub_head: 1
	worst: 0.51126665
	avg: 0.57324666
	best: 0.61466664

Starting e_i: 891
Model ind 640 epoch 891 head A head_i_epoch 0 batch 0: avg loss -3.136124 avg loss no lamb -3.136124 time 2019-02-27 18:00:54.682923
Model ind 640 epoch 891 head A head_i_epoch 0 batch 100: avg loss -3.129079 avg loss no lamb -3.129079 time 2019-02-27 18:02:57.506904
Model ind 640 epoch 891 head A head_i_epoch 0 batch 200: avg loss -3.230022 avg loss no lamb -3.230022 time 2019-02-27 18:04:52.137148
last batch sz 160
Model ind 640 epoch 891 head B head_i_epoch 0 batch 0: avg loss -1.858391 avg loss no lamb -1.858391 time 2019-02-27 18:06:26.939165
Model ind 640 epoch 891 head B head_i_epoch 0 batch 100: avg loss -1.754709 avg loss no lamb -1.754709 time 2019-02-27 18:08:21.962230
Model ind 640 epoch 891 head B head_i_epoch 0 batch 200: avg loss -1.889132 avg loss no lamb -1.889132 time 2019-02-27 18:10:26.483164
last batch sz 160
Model ind 640 epoch 891 head B head_i_epoch 1 batch 0: avg loss -1.851839 avg loss no lamb -1.851839 time 2019-02-27 18:11:56.030243
Model ind 640 epoch 891 head B head_i_epoch 1 batch 100: avg loss -1.818439 avg loss no lamb -1.818439 time 2019-02-27 18:13:53.973493
Model ind 640 epoch 891 head B head_i_epoch 1 batch 200: avg loss -1.858444 avg loss no lamb -1.858444 time 2019-02-27 18:16:00.953233
last batch sz 160
Pre: time 2019-02-27 18:17:44.769018: 
 	std: 0.050269283
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5110833, 0.6139167, 0.61385, 0.61406666, 0.5115833]
	train_accs: [0.5110833, 0.6139167, 0.61385, 0.61406666, 0.5115833]
	best_train_sub_head: 3
	worst: 0.5110833
	avg: 0.5729
	best: 0.61406666

Starting e_i: 892
Model ind 640 epoch 892 head A head_i_epoch 0 batch 0: avg loss -3.289895 avg loss no lamb -3.289895 time 2019-02-27 18:17:48.123678
Model ind 640 epoch 892 head A head_i_epoch 0 batch 100: avg loss -3.221464 avg loss no lamb -3.221464 time 2019-02-27 18:19:50.723652
Model ind 640 epoch 892 head A head_i_epoch 0 batch 200: avg loss -3.210247 avg loss no lamb -3.210247 time 2019-02-27 18:21:57.240353
last batch sz 160
Model ind 640 epoch 892 head B head_i_epoch 0 batch 0: avg loss -1.913723 avg loss no lamb -1.913723 time 2019-02-27 18:23:20.032677
Model ind 640 epoch 892 head B head_i_epoch 0 batch 100: avg loss -1.791706 avg loss no lamb -1.791706 time 2019-02-27 18:25:28.508000
Model ind 640 epoch 892 head B head_i_epoch 0 batch 200: avg loss -1.910701 avg loss no lamb -1.910701 time 2019-02-27 18:27:28.404345
last batch sz 160
Model ind 640 epoch 892 head B head_i_epoch 1 batch 0: avg loss -1.851759 avg loss no lamb -1.851759 time 2019-02-27 18:28:59.912746
Model ind 640 epoch 892 head B head_i_epoch 1 batch 100: avg loss -1.807798 avg loss no lamb -1.807798 time 2019-02-27 18:31:06.799055
Model ind 640 epoch 892 head B head_i_epoch 1 batch 200: avg loss -1.866843 avg loss no lamb -1.866843 time 2019-02-27 18:33:00.367968
last batch sz 160
Pre: time 2019-02-27 18:35:01.248136: 
 	std: 0.050337713
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108, 0.614, 0.61395, 0.61395, 0.51163334]
	train_accs: [0.5108, 0.614, 0.61395, 0.61395, 0.51163334]
	best_train_sub_head: 1
	worst: 0.5108
	avg: 0.5728667
	best: 0.614

Starting e_i: 893
Model ind 640 epoch 893 head A head_i_epoch 0 batch 0: avg loss -3.255877 avg loss no lamb -3.255877 time 2019-02-27 18:35:04.599834
Model ind 640 epoch 893 head A head_i_epoch 0 batch 100: avg loss -3.156522 avg loss no lamb -3.156522 time 2019-02-27 18:37:07.166333
Model ind 640 epoch 893 head A head_i_epoch 0 batch 200: avg loss -3.221987 avg loss no lamb -3.221987 time 2019-02-27 18:39:06.919234
last batch sz 160
Model ind 640 epoch 893 head B head_i_epoch 0 batch 0: avg loss -1.813088 avg loss no lamb -1.813088 time 2019-02-27 18:40:42.332799
Model ind 640 epoch 893 head B head_i_epoch 0 batch 100: avg loss -1.769399 avg loss no lamb -1.769399 time 2019-02-27 18:42:33.919581
Model ind 640 epoch 893 head B head_i_epoch 0 batch 200: avg loss -1.891099 avg loss no lamb -1.891099 time 2019-02-27 18:44:44.122639
last batch sz 160
Model ind 640 epoch 893 head B head_i_epoch 1 batch 0: avg loss -1.819788 avg loss no lamb -1.819788 time 2019-02-27 18:46:09.767788
Model ind 640 epoch 893 head B head_i_epoch 1 batch 100: avg loss -1.864540 avg loss no lamb -1.864540 time 2019-02-27 18:48:10.473839
Model ind 640 epoch 893 head B head_i_epoch 1 batch 200: avg loss -1.884827 avg loss no lamb -1.884827 time 2019-02-27 18:50:15.418603
last batch sz 160
Pre: time 2019-02-27 18:52:04.567085: 
 	std: 0.05082176
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51091665, 0.6148833, 0.61475, 0.6150333, 0.51138335]
	train_accs: [0.51091665, 0.6148833, 0.61475, 0.6150333, 0.51138335]
	best_train_sub_head: 3
	worst: 0.51091665
	avg: 0.5733933
	best: 0.6150333

Starting e_i: 894
Model ind 640 epoch 894 head A head_i_epoch 0 batch 0: avg loss -3.200154 avg loss no lamb -3.200154 time 2019-02-27 18:52:07.527941
Model ind 640 epoch 894 head A head_i_epoch 0 batch 100: avg loss -3.160610 avg loss no lamb -3.160610 time 2019-02-27 18:54:08.589848
Model ind 640 epoch 894 head A head_i_epoch 0 batch 200: avg loss -3.238074 avg loss no lamb -3.238074 time 2019-02-27 18:56:09.164804
last batch sz 160
Model ind 640 epoch 894 head B head_i_epoch 0 batch 0: avg loss -1.874122 avg loss no lamb -1.874122 time 2019-02-27 18:57:34.319970
Model ind 640 epoch 894 head B head_i_epoch 0 batch 100: avg loss -1.800675 avg loss no lamb -1.800675 time 2019-02-27 18:59:36.396359
Model ind 640 epoch 894 head B head_i_epoch 0 batch 200: avg loss -1.858070 avg loss no lamb -1.858070 time 2019-02-27 19:01:35.311351
last batch sz 160
Model ind 640 epoch 894 head B head_i_epoch 1 batch 0: avg loss -1.810142 avg loss no lamb -1.810142 time 2019-02-27 19:03:01.974021
Model ind 640 epoch 894 head B head_i_epoch 1 batch 100: avg loss -1.842427 avg loss no lamb -1.842427 time 2019-02-27 19:04:59.941972
Model ind 640 epoch 894 head B head_i_epoch 1 batch 200: avg loss -1.917815 avg loss no lamb -1.917815 time 2019-02-27 19:07:01.216804
last batch sz 160
Pre: time 2019-02-27 19:08:51.282078: 
 	std: 0.050745524
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50988334, 0.61315, 0.6132333, 0.6135167, 0.50955]
	train_accs: [0.50988334, 0.61315, 0.6132333, 0.6135167, 0.50955]
	best_train_sub_head: 3
	worst: 0.50955
	avg: 0.5718667
	best: 0.6135167

Starting e_i: 895
Model ind 640 epoch 895 head A head_i_epoch 0 batch 0: avg loss -3.216704 avg loss no lamb -3.216704 time 2019-02-27 19:08:54.910808
Model ind 640 epoch 895 head A head_i_epoch 0 batch 100: avg loss -3.143324 avg loss no lamb -3.143324 time 2019-02-27 19:10:54.945125
Model ind 640 epoch 895 head A head_i_epoch 0 batch 200: avg loss -3.229908 avg loss no lamb -3.229908 time 2019-02-27 19:12:57.068191
last batch sz 160
Model ind 640 epoch 895 head B head_i_epoch 0 batch 0: avg loss -1.914406 avg loss no lamb -1.914406 time 2019-02-27 19:14:21.889408
Model ind 640 epoch 895 head B head_i_epoch 0 batch 100: avg loss -1.823848 avg loss no lamb -1.823848 time 2019-02-27 19:16:23.270374
Model ind 640 epoch 895 head B head_i_epoch 0 batch 200: avg loss -1.993674 avg loss no lamb -1.993674 time 2019-02-27 19:18:21.775668
last batch sz 160
Model ind 640 epoch 895 head B head_i_epoch 1 batch 0: avg loss -1.888335 avg loss no lamb -1.888335 time 2019-02-27 19:19:51.863685
Model ind 640 epoch 895 head B head_i_epoch 1 batch 100: avg loss -1.828424 avg loss no lamb -1.828424 time 2019-02-27 19:21:52.331537
Model ind 640 epoch 895 head B head_i_epoch 1 batch 200: avg loss -1.892752 avg loss no lamb -1.892752 time 2019-02-27 19:23:51.154787
last batch sz 160
Pre: time 2019-02-27 19:25:44.790490: 
 	std: 0.050470553
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51135, 0.61411667, 0.6145, 0.61445, 0.51131666]
	train_accs: [0.51135, 0.61411667, 0.6145, 0.61445, 0.51131666]
	best_train_sub_head: 2
	worst: 0.51131666
	avg: 0.57314664
	best: 0.6145

Starting e_i: 896
Model ind 640 epoch 896 head A head_i_epoch 0 batch 0: avg loss -3.158658 avg loss no lamb -3.158658 time 2019-02-27 19:25:47.693283
Model ind 640 epoch 896 head A head_i_epoch 0 batch 100: avg loss -3.181215 avg loss no lamb -3.181215 time 2019-02-27 19:27:46.847779
Model ind 640 epoch 896 head A head_i_epoch 0 batch 200: avg loss -3.242745 avg loss no lamb -3.242745 time 2019-02-27 19:29:44.034215
last batch sz 160
Model ind 640 epoch 896 head B head_i_epoch 0 batch 0: avg loss -1.952825 avg loss no lamb -1.952825 time 2019-02-27 19:31:19.905420
Model ind 640 epoch 896 head B head_i_epoch 0 batch 100: avg loss -1.808262 avg loss no lamb -1.808262 time 2019-02-27 19:33:16.802093
Model ind 640 epoch 896 head B head_i_epoch 0 batch 200: avg loss -1.890345 avg loss no lamb -1.890345 time 2019-02-27 19:35:17.716580
last batch sz 160
Model ind 640 epoch 896 head B head_i_epoch 1 batch 0: avg loss -1.865887 avg loss no lamb -1.865887 time 2019-02-27 19:36:43.566677
Model ind 640 epoch 896 head B head_i_epoch 1 batch 100: avg loss -1.813634 avg loss no lamb -1.813634 time 2019-02-27 19:38:43.184468
Model ind 640 epoch 896 head B head_i_epoch 1 batch 200: avg loss -1.863938 avg loss no lamb -1.863938 time 2019-02-27 19:40:48.705904
last batch sz 160
Pre: time 2019-02-27 19:42:36.757498: 
 	std: 0.050237816
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114833, 0.6138667, 0.6141833, 0.6141667, 0.51156664]
	train_accs: [0.5114833, 0.6138667, 0.6141833, 0.6141667, 0.51156664]
	best_train_sub_head: 2
	worst: 0.5114833
	avg: 0.5730533
	best: 0.6141833

Starting e_i: 897
Model ind 640 epoch 897 head A head_i_epoch 0 batch 0: avg loss -3.196166 avg loss no lamb -3.196166 time 2019-02-27 19:42:40.031603
Model ind 640 epoch 897 head A head_i_epoch 0 batch 100: avg loss -3.089518 avg loss no lamb -3.089518 time 2019-02-27 19:44:41.995063
Model ind 640 epoch 897 head A head_i_epoch 0 batch 200: avg loss -3.234739 avg loss no lamb -3.234739 time 2019-02-27 19:46:48.309187
last batch sz 160
Model ind 640 epoch 897 head B head_i_epoch 0 batch 0: avg loss -1.815391 avg loss no lamb -1.815391 time 2019-02-27 19:48:11.702445
Model ind 640 epoch 897 head B head_i_epoch 0 batch 100: avg loss -1.870131 avg loss no lamb -1.870131 time 2019-02-27 19:50:17.636728
Model ind 640 epoch 897 head B head_i_epoch 0 batch 200: avg loss -1.916563 avg loss no lamb -1.916563 time 2019-02-27 19:52:16.981841
last batch sz 160
Model ind 640 epoch 897 head B head_i_epoch 1 batch 0: avg loss -1.842943 avg loss no lamb -1.842943 time 2019-02-27 19:53:43.064964
Model ind 640 epoch 897 head B head_i_epoch 1 batch 100: avg loss -1.803635 avg loss no lamb -1.803635 time 2019-02-27 19:55:49.980047
Model ind 640 epoch 897 head B head_i_epoch 1 batch 200: avg loss -1.817351 avg loss no lamb -1.817351 time 2019-02-27 19:57:45.615114
last batch sz 160
Pre: time 2019-02-27 19:59:42.759355: 
 	std: 0.05009109
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5117, 0.61413336, 0.6142667, 0.61431664, 0.5122833]
	train_accs: [0.5117, 0.61413336, 0.6142667, 0.61431664, 0.5122833]
	best_train_sub_head: 3
	worst: 0.5117
	avg: 0.57334
	best: 0.61431664

Starting e_i: 898
Model ind 640 epoch 898 head A head_i_epoch 0 batch 0: avg loss -3.209941 avg loss no lamb -3.209941 time 2019-02-27 19:59:45.338639
Model ind 640 epoch 898 head A head_i_epoch 0 batch 100: avg loss -3.103510 avg loss no lamb -3.103510 time 2019-02-27 20:01:47.326420
Model ind 640 epoch 898 head A head_i_epoch 0 batch 200: avg loss -3.269924 avg loss no lamb -3.269924 time 2019-02-27 20:03:44.039844
last batch sz 160
Model ind 640 epoch 898 head B head_i_epoch 0 batch 0: avg loss -1.810337 avg loss no lamb -1.810337 time 2019-02-27 20:05:19.053799
Model ind 640 epoch 898 head B head_i_epoch 0 batch 100: avg loss -1.856357 avg loss no lamb -1.856357 time 2019-02-27 20:07:14.107849
Model ind 640 epoch 898 head B head_i_epoch 0 batch 200: avg loss -1.995985 avg loss no lamb -1.995985 time 2019-02-27 20:09:20.207826
last batch sz 160
Model ind 640 epoch 898 head B head_i_epoch 1 batch 0: avg loss -1.838001 avg loss no lamb -1.838001 time 2019-02-27 20:10:49.344404
Model ind 640 epoch 898 head B head_i_epoch 1 batch 100: avg loss -1.765431 avg loss no lamb -1.765431 time 2019-02-27 20:12:48.499314
Model ind 640 epoch 898 head B head_i_epoch 1 batch 200: avg loss -1.885504 avg loss no lamb -1.885504 time 2019-02-27 20:14:54.422407
last batch sz 160
Pre: time 2019-02-27 20:16:38.750816: 
 	std: 0.050881375
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50988334, 0.61378336, 0.6138833, 0.6137667, 0.5100167]
	train_accs: [0.50988334, 0.61378336, 0.6138833, 0.6137667, 0.5100167]
	best_train_sub_head: 2
	worst: 0.50988334
	avg: 0.5722667
	best: 0.6138833

Starting e_i: 899
Model ind 640 epoch 899 head A head_i_epoch 0 batch 0: avg loss -3.251102 avg loss no lamb -3.251102 time 2019-02-27 20:16:42.086800
Model ind 640 epoch 899 head A head_i_epoch 0 batch 100: avg loss -3.222789 avg loss no lamb -3.222789 time 2019-02-27 20:18:45.402942
Model ind 640 epoch 899 head A head_i_epoch 0 batch 200: avg loss -3.203632 avg loss no lamb -3.203632 time 2019-02-27 20:20:50.946851
last batch sz 160
Model ind 640 epoch 899 head B head_i_epoch 0 batch 0: avg loss -1.811630 avg loss no lamb -1.811630 time 2019-02-27 20:22:13.742398
Model ind 640 epoch 899 head B head_i_epoch 0 batch 100: avg loss -1.877301 avg loss no lamb -1.877301 time 2019-02-27 20:24:19.586639
Model ind 640 epoch 899 head B head_i_epoch 0 batch 200: avg loss -1.892531 avg loss no lamb -1.892531 time 2019-02-27 20:26:17.861370
last batch sz 160
Model ind 640 epoch 899 head B head_i_epoch 1 batch 0: avg loss -1.796609 avg loss no lamb -1.796609 time 2019-02-27 20:27:46.944379
Model ind 640 epoch 899 head B head_i_epoch 1 batch 100: avg loss -1.762036 avg loss no lamb -1.762036 time 2019-02-27 20:29:49.442989
Model ind 640 epoch 899 head B head_i_epoch 1 batch 200: avg loss -1.882048 avg loss no lamb -1.882048 time 2019-02-27 20:31:45.128929
last batch sz 160
Pre: time 2019-02-27 20:33:43.420319: 
 	std: 0.051277455
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50846666, 0.61308336, 0.6133, 0.6133, 0.50865]
	train_accs: [0.50846666, 0.61308336, 0.6133, 0.6133, 0.50865]
	best_train_sub_head: 2
	worst: 0.50846666
	avg: 0.57136005
	best: 0.6133

Starting e_i: 900
Model ind 640 epoch 900 head A head_i_epoch 0 batch 0: avg loss -3.296878 avg loss no lamb -3.296878 time 2019-02-27 20:33:46.193118
Model ind 640 epoch 900 head A head_i_epoch 0 batch 100: avg loss -3.127203 avg loss no lamb -3.127203 time 2019-02-27 20:35:46.591081
Model ind 640 epoch 900 head A head_i_epoch 0 batch 200: avg loss -3.211417 avg loss no lamb -3.211417 time 2019-02-27 20:37:45.446305
last batch sz 160
Model ind 640 epoch 900 head B head_i_epoch 0 batch 0: avg loss -1.832438 avg loss no lamb -1.832438 time 2019-02-27 20:39:18.878063
Model ind 640 epoch 900 head B head_i_epoch 0 batch 100: avg loss -1.757173 avg loss no lamb -1.757173 time 2019-02-27 20:41:11.309759
Model ind 640 epoch 900 head B head_i_epoch 0 batch 200: avg loss -1.927208 avg loss no lamb -1.927208 time 2019-02-27 20:43:22.295699
last batch sz 160
Model ind 640 epoch 900 head B head_i_epoch 1 batch 0: avg loss -1.852079 avg loss no lamb -1.852079 time 2019-02-27 20:44:43.332343
Model ind 640 epoch 900 head B head_i_epoch 1 batch 100: avg loss -1.842983 avg loss no lamb -1.842983 time 2019-02-27 20:46:50.336660
Model ind 640 epoch 900 head B head_i_epoch 1 batch 200: avg loss -1.976101 avg loss no lamb -1.976101 time 2019-02-27 20:48:56.440788
last batch sz 160
Pre: time 2019-02-27 20:50:41.218598: 
 	std: 0.050401177
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5113, 0.61436665, 0.6144, 0.61445, 0.51175]
	train_accs: [0.5113, 0.61436665, 0.6144, 0.61445, 0.51175]
	best_train_sub_head: 3
	worst: 0.5113
	avg: 0.5732533
	best: 0.61445

Starting e_i: 901
Model ind 640 epoch 901 head A head_i_epoch 0 batch 0: avg loss -3.139121 avg loss no lamb -3.139121 time 2019-02-27 20:50:47.698416
Model ind 640 epoch 901 head A head_i_epoch 0 batch 100: avg loss -3.161142 avg loss no lamb -3.161142 time 2019-02-27 20:52:57.565247
Model ind 640 epoch 901 head A head_i_epoch 0 batch 200: avg loss -3.189616 avg loss no lamb -3.189616 time 2019-02-27 20:54:58.647330
last batch sz 160
Model ind 640 epoch 901 head B head_i_epoch 0 batch 0: avg loss -1.871585 avg loss no lamb -1.871585 time 2019-02-27 20:56:27.346946
Model ind 640 epoch 901 head B head_i_epoch 0 batch 100: avg loss -1.797852 avg loss no lamb -1.797852 time 2019-02-27 20:58:32.175515
Model ind 640 epoch 901 head B head_i_epoch 0 batch 200: avg loss -1.901013 avg loss no lamb -1.901013 time 2019-02-27 21:00:28.348699
last batch sz 160
Model ind 640 epoch 901 head B head_i_epoch 1 batch 0: avg loss -1.835244 avg loss no lamb -1.835244 time 2019-02-27 21:01:59.602808
Model ind 640 epoch 901 head B head_i_epoch 1 batch 100: avg loss -1.911823 avg loss no lamb -1.911823 time 2019-02-27 21:03:55.736137
Model ind 640 epoch 901 head B head_i_epoch 1 batch 200: avg loss -1.947739 avg loss no lamb -1.947739 time 2019-02-27 21:05:58.012821
last batch sz 160
Pre: time 2019-02-27 21:07:53.351520: 
 	std: 0.051089726
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108333, 0.61481667, 0.6149833, 0.61508334, 0.51051664]
	train_accs: [0.5108333, 0.61481667, 0.6149833, 0.61508334, 0.51051664]
	best_train_sub_head: 3
	worst: 0.51051664
	avg: 0.57324666
	best: 0.61508334

Starting e_i: 902
Model ind 640 epoch 902 head A head_i_epoch 0 batch 0: avg loss -3.153534 avg loss no lamb -3.153534 time 2019-02-27 21:07:56.361715
Model ind 640 epoch 902 head A head_i_epoch 0 batch 100: avg loss -3.126579 avg loss no lamb -3.126579 time 2019-02-27 21:09:55.599628
Model ind 640 epoch 902 head A head_i_epoch 0 batch 200: avg loss -3.219541 avg loss no lamb -3.219541 time 2019-02-27 21:11:54.764983
last batch sz 160
Model ind 640 epoch 902 head B head_i_epoch 0 batch 0: avg loss -1.923149 avg loss no lamb -1.923149 time 2019-02-27 21:13:21.960223
Model ind 640 epoch 902 head B head_i_epoch 0 batch 100: avg loss -1.711259 avg loss no lamb -1.711259 time 2019-02-27 21:15:21.638814
Model ind 640 epoch 902 head B head_i_epoch 0 batch 200: avg loss -2.002518 avg loss no lamb -2.002518 time 2019-02-27 21:17:16.518546
last batch sz 160
Model ind 640 epoch 902 head B head_i_epoch 1 batch 0: avg loss -1.849673 avg loss no lamb -1.849673 time 2019-02-27 21:18:39.319064
Model ind 640 epoch 902 head B head_i_epoch 1 batch 100: avg loss -1.837649 avg loss no lamb -1.837649 time 2019-02-27 21:20:30.521581
Model ind 640 epoch 902 head B head_i_epoch 1 batch 200: avg loss -1.894354 avg loss no lamb -1.894354 time 2019-02-27 21:22:25.980991
last batch sz 160
Pre: time 2019-02-27 21:24:07.794483: 
 	std: 0.050618757
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51016665, 0.6134, 0.61361665, 0.6135333, 0.51021665]
	train_accs: [0.51016665, 0.6134, 0.61361665, 0.6135333, 0.51021665]
	best_train_sub_head: 2
	worst: 0.51016665
	avg: 0.57218665
	best: 0.61361665

Starting e_i: 903
Model ind 640 epoch 903 head A head_i_epoch 0 batch 0: avg loss -3.228235 avg loss no lamb -3.228235 time 2019-02-27 21:24:10.770727
Model ind 640 epoch 903 head A head_i_epoch 0 batch 100: avg loss -3.103442 avg loss no lamb -3.103442 time 2019-02-27 21:26:04.689373
Model ind 640 epoch 903 head A head_i_epoch 0 batch 200: avg loss -3.227627 avg loss no lamb -3.227627 time 2019-02-27 21:27:57.408645
last batch sz 160
Model ind 640 epoch 903 head B head_i_epoch 0 batch 0: avg loss -1.921875 avg loss no lamb -1.921875 time 2019-02-27 21:29:19.338107
Model ind 640 epoch 903 head B head_i_epoch 0 batch 100: avg loss -1.790974 avg loss no lamb -1.790974 time 2019-02-27 21:31:14.058684
Model ind 640 epoch 903 head B head_i_epoch 0 batch 200: avg loss -1.886663 avg loss no lamb -1.886663 time 2019-02-27 21:33:05.519249
last batch sz 160
Model ind 640 epoch 903 head B head_i_epoch 1 batch 0: avg loss -1.874601 avg loss no lamb -1.874601 time 2019-02-27 21:34:28.866142
Model ind 640 epoch 903 head B head_i_epoch 1 batch 100: avg loss -1.793316 avg loss no lamb -1.793316 time 2019-02-27 21:36:21.775350
Model ind 640 epoch 903 head B head_i_epoch 1 batch 200: avg loss -1.902450 avg loss no lamb -1.902450 time 2019-02-27 21:38:12.806836
last batch sz 160
Pre: time 2019-02-27 21:39:58.544349: 
 	std: 0.05101215
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104667, 0.61473334, 0.61465, 0.6149, 0.5108]
	train_accs: [0.5104667, 0.61473334, 0.61465, 0.6149, 0.5108]
	best_train_sub_head: 3
	worst: 0.5104667
	avg: 0.57311
	best: 0.6149

Starting e_i: 904
Model ind 640 epoch 904 head A head_i_epoch 0 batch 0: avg loss -3.188639 avg loss no lamb -3.188639 time 2019-02-27 21:40:01.873326
Model ind 640 epoch 904 head A head_i_epoch 0 batch 100: avg loss -3.172190 avg loss no lamb -3.172190 time 2019-02-27 21:41:53.367794
Model ind 640 epoch 904 head A head_i_epoch 0 batch 200: avg loss -3.234600 avg loss no lamb -3.234600 time 2019-02-27 21:43:47.741816
last batch sz 160
Model ind 640 epoch 904 head B head_i_epoch 0 batch 0: avg loss -1.960114 avg loss no lamb -1.960114 time 2019-02-27 21:45:11.725998
Model ind 640 epoch 904 head B head_i_epoch 0 batch 100: avg loss -1.788538 avg loss no lamb -1.788538 time 2019-02-27 21:47:04.060792
Model ind 640 epoch 904 head B head_i_epoch 0 batch 200: avg loss -1.941067 avg loss no lamb -1.941067 time 2019-02-27 21:49:01.657455
last batch sz 160
Model ind 640 epoch 904 head B head_i_epoch 1 batch 0: avg loss -1.924740 avg loss no lamb -1.924740 time 2019-02-27 21:50:26.161975
Model ind 640 epoch 904 head B head_i_epoch 1 batch 100: avg loss -1.715084 avg loss no lamb -1.715084 time 2019-02-27 21:52:19.946853
Model ind 640 epoch 904 head B head_i_epoch 1 batch 200: avg loss -1.964463 avg loss no lamb -1.964463 time 2019-02-27 21:54:14.146115
last batch sz 160
Pre: time 2019-02-27 21:56:00.899111: 
 	std: 0.05050725
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51035, 0.61338335, 0.61355, 0.61368334, 0.51053333]
	train_accs: [0.51035, 0.61338335, 0.61355, 0.61368334, 0.51053333]
	best_train_sub_head: 3
	worst: 0.51035
	avg: 0.57229996
	best: 0.61368334

Starting e_i: 905
Model ind 640 epoch 905 head A head_i_epoch 0 batch 0: avg loss -3.203687 avg loss no lamb -3.203687 time 2019-02-27 21:56:03.413633
Model ind 640 epoch 905 head A head_i_epoch 0 batch 100: avg loss -3.186088 avg loss no lamb -3.186088 time 2019-02-27 21:57:58.196544
Model ind 640 epoch 905 head A head_i_epoch 0 batch 200: avg loss -3.224328 avg loss no lamb -3.224328 time 2019-02-27 21:59:51.669371
last batch sz 160
Model ind 640 epoch 905 head B head_i_epoch 0 batch 0: avg loss -1.888746 avg loss no lamb -1.888746 time 2019-02-27 22:01:16.665097
Model ind 640 epoch 905 head B head_i_epoch 0 batch 100: avg loss -1.847260 avg loss no lamb -1.847260 time 2019-02-27 22:03:10.561580
Model ind 640 epoch 905 head B head_i_epoch 0 batch 200: avg loss -1.876487 avg loss no lamb -1.876487 time 2019-02-27 22:05:00.453939
last batch sz 160
Model ind 640 epoch 905 head B head_i_epoch 1 batch 0: avg loss -1.890463 avg loss no lamb -1.890463 time 2019-02-27 22:06:25.073265
Model ind 640 epoch 905 head B head_i_epoch 1 batch 100: avg loss -1.824120 avg loss no lamb -1.824120 time 2019-02-27 22:08:15.141328
Model ind 640 epoch 905 head B head_i_epoch 1 batch 200: avg loss -1.833623 avg loss no lamb -1.833623 time 2019-02-27 22:10:08.692394
last batch sz 160
Pre: time 2019-02-27 22:11:52.730025: 
 	std: 0.050788864
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50935, 0.61308336, 0.61291665, 0.6131167, 0.5093833]
	train_accs: [0.50935, 0.61308336, 0.61291665, 0.6131167, 0.5093833]
	best_train_sub_head: 3
	worst: 0.50935
	avg: 0.57157004
	best: 0.6131167

Starting e_i: 906
Model ind 640 epoch 906 head A head_i_epoch 0 batch 0: avg loss -3.197289 avg loss no lamb -3.197289 time 2019-02-27 22:11:56.418592
Model ind 640 epoch 906 head A head_i_epoch 0 batch 100: avg loss -3.125704 avg loss no lamb -3.125704 time 2019-02-27 22:13:46.346831
Model ind 640 epoch 906 head A head_i_epoch 0 batch 200: avg loss -3.279792 avg loss no lamb -3.279792 time 2019-02-27 22:15:40.871257
last batch sz 160
Model ind 640 epoch 906 head B head_i_epoch 0 batch 0: avg loss -1.856288 avg loss no lamb -1.856288 time 2019-02-27 22:17:02.602145
Model ind 640 epoch 906 head B head_i_epoch 0 batch 100: avg loss -1.764733 avg loss no lamb -1.764733 time 2019-02-27 22:18:58.055204
Model ind 640 epoch 906 head B head_i_epoch 0 batch 200: avg loss -1.966447 avg loss no lamb -1.966447 time 2019-02-27 22:20:51.699781
last batch sz 160
Model ind 640 epoch 906 head B head_i_epoch 1 batch 0: avg loss -1.901603 avg loss no lamb -1.901603 time 2019-02-27 22:22:12.526706
Model ind 640 epoch 906 head B head_i_epoch 1 batch 100: avg loss -1.828205 avg loss no lamb -1.828205 time 2019-02-27 22:24:07.643077
Model ind 640 epoch 906 head B head_i_epoch 1 batch 200: avg loss -1.889721 avg loss no lamb -1.889721 time 2019-02-27 22:25:58.172754
last batch sz 160
Pre: time 2019-02-27 22:27:43.174755: 
 	std: 0.0502719
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5122833, 0.6151, 0.6152, 0.615, 0.51268333]
	train_accs: [0.5122833, 0.6151, 0.6152, 0.615, 0.51268333]
	best_train_sub_head: 2
	worst: 0.5122833
	avg: 0.57405335
	best: 0.6152

Starting e_i: 907
Model ind 640 epoch 907 head A head_i_epoch 0 batch 0: avg loss -3.153420 avg loss no lamb -3.153420 time 2019-02-27 22:27:45.599272
Model ind 640 epoch 907 head A head_i_epoch 0 batch 100: avg loss -3.201884 avg loss no lamb -3.201884 time 2019-02-27 22:29:39.191039
Model ind 640 epoch 907 head A head_i_epoch 0 batch 200: avg loss -3.220829 avg loss no lamb -3.220829 time 2019-02-27 22:31:29.690315
last batch sz 160
Model ind 640 epoch 907 head B head_i_epoch 0 batch 0: avg loss -1.816963 avg loss no lamb -1.816963 time 2019-02-27 22:32:53.782974
Model ind 640 epoch 907 head B head_i_epoch 0 batch 100: avg loss -1.820110 avg loss no lamb -1.820110 time 2019-02-27 22:34:44.062825
Model ind 640 epoch 907 head B head_i_epoch 0 batch 200: avg loss -1.913129 avg loss no lamb -1.913129 time 2019-02-27 22:36:37.396396
last batch sz 160
Model ind 640 epoch 907 head B head_i_epoch 1 batch 0: avg loss -1.825555 avg loss no lamb -1.825555 time 2019-02-27 22:37:57.458336
Model ind 640 epoch 907 head B head_i_epoch 1 batch 100: avg loss -1.823179 avg loss no lamb -1.823179 time 2019-02-27 22:39:49.782766
Model ind 640 epoch 907 head B head_i_epoch 1 batch 200: avg loss -1.901597 avg loss no lamb -1.901597 time 2019-02-27 22:41:44.757346
last batch sz 160
Pre: time 2019-02-27 22:43:25.536384: 
 	std: 0.050678864
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5109, 0.61401665, 0.61411667, 0.61413336, 0.5103833]
	train_accs: [0.5109, 0.61401665, 0.61411667, 0.61413336, 0.5103833]
	best_train_sub_head: 3
	worst: 0.5103833
	avg: 0.57271
	best: 0.61413336

Starting e_i: 908
Model ind 640 epoch 908 head A head_i_epoch 0 batch 0: avg loss -3.219368 avg loss no lamb -3.219368 time 2019-02-27 22:43:28.687690
Model ind 640 epoch 908 head A head_i_epoch 0 batch 100: avg loss -3.124011 avg loss no lamb -3.124011 time 2019-02-27 22:45:22.504504
Model ind 640 epoch 908 head A head_i_epoch 0 batch 200: avg loss -3.298019 avg loss no lamb -3.298019 time 2019-02-27 22:47:17.294208
last batch sz 160
Model ind 640 epoch 908 head B head_i_epoch 0 batch 0: avg loss -1.888202 avg loss no lamb -1.888202 time 2019-02-27 22:48:37.427751
Model ind 640 epoch 908 head B head_i_epoch 0 batch 100: avg loss -1.774346 avg loss no lamb -1.774346 time 2019-02-27 22:50:33.095309
Model ind 640 epoch 908 head B head_i_epoch 0 batch 200: avg loss -1.842091 avg loss no lamb -1.842091 time 2019-02-27 22:52:24.908325
last batch sz 160
Model ind 640 epoch 908 head B head_i_epoch 1 batch 0: avg loss -1.789181 avg loss no lamb -1.789181 time 2019-02-27 22:53:48.681614
Model ind 640 epoch 908 head B head_i_epoch 1 batch 100: avg loss -1.760281 avg loss no lamb -1.760281 time 2019-02-27 22:55:41.848504
Model ind 640 epoch 908 head B head_i_epoch 1 batch 200: avg loss -1.924620 avg loss no lamb -1.924620 time 2019-02-27 22:57:33.438189
last batch sz 160
Pre: time 2019-02-27 22:59:20.105234: 
 	std: 0.050889537
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51033336, 0.61411667, 0.61411667, 0.61425, 0.51023334]
	train_accs: [0.51033336, 0.61411667, 0.61411667, 0.61425, 0.51023334]
	best_train_sub_head: 3
	worst: 0.51023334
	avg: 0.57261
	best: 0.61425

Starting e_i: 909
Model ind 640 epoch 909 head A head_i_epoch 0 batch 0: avg loss -3.229506 avg loss no lamb -3.229506 time 2019-02-27 22:59:22.721529
Model ind 640 epoch 909 head A head_i_epoch 0 batch 100: avg loss -3.132702 avg loss no lamb -3.132702 time 2019-02-27 23:01:11.804642
Model ind 640 epoch 909 head A head_i_epoch 0 batch 200: avg loss -3.225995 avg loss no lamb -3.225995 time 2019-02-27 23:03:05.290843
last batch sz 160
Model ind 640 epoch 909 head B head_i_epoch 0 batch 0: avg loss -1.864546 avg loss no lamb -1.864546 time 2019-02-27 23:04:28.573526
Model ind 640 epoch 909 head B head_i_epoch 0 batch 100: avg loss -1.774126 avg loss no lamb -1.774126 time 2019-02-27 23:06:19.391166
Model ind 640 epoch 909 head B head_i_epoch 0 batch 200: avg loss -1.857346 avg loss no lamb -1.857346 time 2019-02-27 23:08:14.106263
last batch sz 160
Model ind 640 epoch 909 head B head_i_epoch 1 batch 0: avg loss -1.944695 avg loss no lamb -1.944695 time 2019-02-27 23:09:34.231012
Model ind 640 epoch 909 head B head_i_epoch 1 batch 100: avg loss -1.800596 avg loss no lamb -1.800596 time 2019-02-27 23:11:28.431835
Model ind 640 epoch 909 head B head_i_epoch 1 batch 200: avg loss -1.895974 avg loss no lamb -1.895974 time 2019-02-27 23:13:21.256686
last batch sz 160
Pre: time 2019-02-27 23:15:03.167935: 
 	std: 0.05106651
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.61558336, 0.61558336, 0.6156, 0.5115]
	train_accs: [0.5112, 0.61558336, 0.61558336, 0.6156, 0.5115]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.57389337
	best: 0.6156

Starting e_i: 910
Model ind 640 epoch 910 head A head_i_epoch 0 batch 0: avg loss -3.225908 avg loss no lamb -3.225908 time 2019-02-27 23:15:05.724484
Model ind 640 epoch 910 head A head_i_epoch 0 batch 100: avg loss -3.204703 avg loss no lamb -3.204703 time 2019-02-27 23:17:00.593421
Model ind 640 epoch 910 head A head_i_epoch 0 batch 200: avg loss -3.275101 avg loss no lamb -3.275101 time 2019-02-27 23:18:51.379813
last batch sz 160
Model ind 640 epoch 910 head B head_i_epoch 0 batch 0: avg loss -1.912680 avg loss no lamb -1.912680 time 2019-02-27 23:20:13.910306
Model ind 640 epoch 910 head B head_i_epoch 0 batch 100: avg loss -1.816507 avg loss no lamb -1.816507 time 2019-02-27 23:22:06.390554
Model ind 640 epoch 910 head B head_i_epoch 0 batch 200: avg loss -1.809848 avg loss no lamb -1.809848 time 2019-02-27 23:23:57.090288
last batch sz 160
Model ind 640 epoch 910 head B head_i_epoch 1 batch 0: avg loss -1.891369 avg loss no lamb -1.891369 time 2019-02-27 23:25:18.896023
Model ind 640 epoch 910 head B head_i_epoch 1 batch 100: avg loss -1.749892 avg loss no lamb -1.749892 time 2019-02-27 23:27:10.035583
Model ind 640 epoch 910 head B head_i_epoch 1 batch 200: avg loss -1.868941 avg loss no lamb -1.868941 time 2019-02-27 23:29:04.849185
last batch sz 160
Pre: time 2019-02-27 23:30:49.729262: 
 	std: 0.050711278
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51088333, 0.6142833, 0.61443335, 0.6144, 0.5108333]
	train_accs: [0.51088333, 0.6142833, 0.61443335, 0.6144, 0.5108333]
	best_train_sub_head: 2
	worst: 0.5108333
	avg: 0.5729667
	best: 0.61443335

Starting e_i: 911
Model ind 640 epoch 911 head A head_i_epoch 0 batch 0: avg loss -3.230738 avg loss no lamb -3.230738 time 2019-02-27 23:30:57.082924
Model ind 640 epoch 911 head A head_i_epoch 0 batch 100: avg loss -3.187260 avg loss no lamb -3.187260 time 2019-02-27 23:32:47.492224
Model ind 640 epoch 911 head A head_i_epoch 0 batch 200: avg loss -3.262170 avg loss no lamb -3.262170 time 2019-02-27 23:34:42.280029
last batch sz 160
Model ind 640 epoch 911 head B head_i_epoch 0 batch 0: avg loss -1.912366 avg loss no lamb -1.912366 time 2019-02-27 23:36:01.829928
Model ind 640 epoch 911 head B head_i_epoch 0 batch 100: avg loss -1.796979 avg loss no lamb -1.796979 time 2019-02-27 23:37:55.706916
Model ind 640 epoch 911 head B head_i_epoch 0 batch 200: avg loss -1.942100 avg loss no lamb -1.942100 time 2019-02-27 23:39:48.632776
last batch sz 160
Model ind 640 epoch 911 head B head_i_epoch 1 batch 0: avg loss -1.882566 avg loss no lamb -1.882566 time 2019-02-27 23:41:09.226674
Model ind 640 epoch 911 head B head_i_epoch 1 batch 100: avg loss -1.815444 avg loss no lamb -1.815444 time 2019-02-27 23:43:03.422751
Model ind 640 epoch 911 head B head_i_epoch 1 batch 200: avg loss -1.852483 avg loss no lamb -1.852483 time 2019-02-27 23:44:53.558026
last batch sz 160
Pre: time 2019-02-27 23:46:38.254339: 
 	std: 0.05057135
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5100167, 0.61321664, 0.61295, 0.6134667, 0.50995]
	train_accs: [0.5100167, 0.61321664, 0.61295, 0.6134667, 0.50995]
	best_train_sub_head: 3
	worst: 0.50995
	avg: 0.57192004
	best: 0.6134667

Starting e_i: 912
Model ind 640 epoch 912 head A head_i_epoch 0 batch 0: avg loss -3.227539 avg loss no lamb -3.227539 time 2019-02-27 23:46:40.729485
Model ind 640 epoch 912 head A head_i_epoch 0 batch 100: avg loss -3.191718 avg loss no lamb -3.191718 time 2019-02-27 23:48:32.253299
Model ind 640 epoch 912 head A head_i_epoch 0 batch 200: avg loss -3.239590 avg loss no lamb -3.239590 time 2019-02-27 23:50:23.712628
last batch sz 160
Model ind 640 epoch 912 head B head_i_epoch 0 batch 0: avg loss -1.884807 avg loss no lamb -1.884807 time 2019-02-27 23:51:47.751267
Model ind 640 epoch 912 head B head_i_epoch 0 batch 100: avg loss -1.780098 avg loss no lamb -1.780098 time 2019-02-27 23:53:37.739479
Model ind 640 epoch 912 head B head_i_epoch 0 batch 200: avg loss -1.976075 avg loss no lamb -1.976075 time 2019-02-27 23:55:31.757768
last batch sz 160
Model ind 640 epoch 912 head B head_i_epoch 1 batch 0: avg loss -1.867039 avg loss no lamb -1.867039 time 2019-02-27 23:56:53.827234
Model ind 640 epoch 912 head B head_i_epoch 1 batch 100: avg loss -1.836480 avg loss no lamb -1.836480 time 2019-02-27 23:58:44.797385
Model ind 640 epoch 912 head B head_i_epoch 1 batch 200: avg loss -1.895366 avg loss no lamb -1.895366 time 2019-02-28 00:00:39.319172
last batch sz 160
Pre: time 2019-02-28 00:02:21.815088: 
 	std: 0.051024526
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50998336, 0.61438334, 0.6142333, 0.6145167, 0.5104667]
	train_accs: [0.50998336, 0.61438334, 0.6142333, 0.6145167, 0.5104667]
	best_train_sub_head: 3
	worst: 0.50998336
	avg: 0.5727167
	best: 0.6145167

Starting e_i: 913
Model ind 640 epoch 913 head A head_i_epoch 0 batch 0: avg loss -3.181085 avg loss no lamb -3.181085 time 2019-02-28 00:02:24.267297
Model ind 640 epoch 913 head A head_i_epoch 0 batch 100: avg loss -3.217223 avg loss no lamb -3.217223 time 2019-02-28 00:04:12.266872
Model ind 640 epoch 913 head A head_i_epoch 0 batch 200: avg loss -3.181710 avg loss no lamb -3.181710 time 2019-02-28 00:06:00.269992
last batch sz 160
Model ind 640 epoch 913 head B head_i_epoch 0 batch 0: avg loss -1.886081 avg loss no lamb -1.886081 time 2019-02-28 00:07:18.776436
Model ind 640 epoch 913 head B head_i_epoch 0 batch 100: avg loss -1.752774 avg loss no lamb -1.752774 time 2019-02-28 00:09:07.826802
Model ind 640 epoch 913 head B head_i_epoch 0 batch 200: avg loss -1.961658 avg loss no lamb -1.961658 time 2019-02-28 00:10:55.041838
last batch sz 160
Model ind 640 epoch 913 head B head_i_epoch 1 batch 0: avg loss -1.823130 avg loss no lamb -1.823130 time 2019-02-28 00:12:12.647118
Model ind 640 epoch 913 head B head_i_epoch 1 batch 100: avg loss -1.810585 avg loss no lamb -1.810585 time 2019-02-28 00:14:06.070114
Model ind 640 epoch 913 head B head_i_epoch 1 batch 200: avg loss -1.991233 avg loss no lamb -1.991233 time 2019-02-28 00:15:55.723933
last batch sz 160
Pre: time 2019-02-28 00:17:33.968065: 
 	std: 0.050290916
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103833, 0.6131667, 0.6131833, 0.61331666, 0.51075]
	train_accs: [0.5103833, 0.6131667, 0.6131833, 0.61331666, 0.51075]
	best_train_sub_head: 3
	worst: 0.5103833
	avg: 0.57216
	best: 0.61331666

Starting e_i: 914
Model ind 640 epoch 914 head A head_i_epoch 0 batch 0: avg loss -3.182091 avg loss no lamb -3.182091 time 2019-02-28 00:17:36.379215
Model ind 640 epoch 914 head A head_i_epoch 0 batch 100: avg loss -3.208186 avg loss no lamb -3.208186 time 2019-02-28 00:19:24.049213
Model ind 640 epoch 914 head A head_i_epoch 0 batch 200: avg loss -3.264606 avg loss no lamb -3.264606 time 2019-02-28 00:21:13.686602
last batch sz 160
Model ind 640 epoch 914 head B head_i_epoch 0 batch 0: avg loss -1.935326 avg loss no lamb -1.935326 time 2019-02-28 00:22:33.201528
Model ind 640 epoch 914 head B head_i_epoch 0 batch 100: avg loss -1.908234 avg loss no lamb -1.908234 time 2019-02-28 00:24:21.935571
Model ind 640 epoch 914 head B head_i_epoch 0 batch 200: avg loss -1.889086 avg loss no lamb -1.889086 time 2019-02-28 00:26:14.370730
last batch sz 160
Model ind 640 epoch 914 head B head_i_epoch 1 batch 0: avg loss -1.921458 avg loss no lamb -1.921458 time 2019-02-28 00:27:33.902071
Model ind 640 epoch 914 head B head_i_epoch 1 batch 100: avg loss -1.818287 avg loss no lamb -1.818287 time 2019-02-28 00:29:29.691668
Model ind 640 epoch 914 head B head_i_epoch 1 batch 200: avg loss -1.917098 avg loss no lamb -1.917098 time 2019-02-28 00:31:58.018960
last batch sz 160
Pre: time 2019-02-28 00:34:26.268218: 
 	std: 0.050447423
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51156664, 0.61425, 0.6144, 0.61445, 0.51121664]
	train_accs: [0.51156664, 0.61425, 0.6144, 0.61445, 0.51121664]
	best_train_sub_head: 3
	worst: 0.51121664
	avg: 0.5731767
	best: 0.61445

Starting e_i: 915
Model ind 640 epoch 915 head A head_i_epoch 0 batch 0: avg loss -3.162572 avg loss no lamb -3.162572 time 2019-02-28 00:34:29.967090
Model ind 640 epoch 915 head A head_i_epoch 0 batch 100: avg loss -3.173296 avg loss no lamb -3.173296 time 2019-02-28 00:37:09.409117
Model ind 640 epoch 915 head A head_i_epoch 0 batch 200: avg loss -3.205938 avg loss no lamb -3.205938 time 2019-02-28 00:39:51.983623
last batch sz 160
Model ind 640 epoch 915 head B head_i_epoch 0 batch 0: avg loss -1.862298 avg loss no lamb -1.862298 time 2019-02-28 00:41:55.659296
Model ind 640 epoch 915 head B head_i_epoch 0 batch 100: avg loss -1.768075 avg loss no lamb -1.768075 time 2019-02-28 00:44:36.808640
Model ind 640 epoch 915 head B head_i_epoch 0 batch 200: avg loss -1.869817 avg loss no lamb -1.869817 time 2019-02-28 00:47:16.084871
last batch sz 160
Model ind 640 epoch 915 head B head_i_epoch 1 batch 0: avg loss -1.891616 avg loss no lamb -1.891616 time 2019-02-28 00:49:20.197770
Model ind 640 epoch 915 head B head_i_epoch 1 batch 100: avg loss -1.839322 avg loss no lamb -1.839322 time 2019-02-28 00:52:02.213886
Model ind 640 epoch 915 head B head_i_epoch 1 batch 200: avg loss -1.883961 avg loss no lamb -1.883961 time 2019-02-28 00:54:41.775036
last batch sz 160
Pre: time 2019-02-28 00:57:10.128281: 
 	std: 0.05036573
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104833, 0.61301666, 0.6131667, 0.61326665, 0.5102]
	train_accs: [0.5104833, 0.61301666, 0.6131667, 0.61326665, 0.5102]
	best_train_sub_head: 3
	worst: 0.5102
	avg: 0.57202667
	best: 0.61326665

Starting e_i: 916
Model ind 640 epoch 916 head A head_i_epoch 0 batch 0: avg loss -3.214587 avg loss no lamb -3.214587 time 2019-02-28 00:57:13.541902
Model ind 640 epoch 916 head A head_i_epoch 0 batch 100: avg loss -3.126700 avg loss no lamb -3.126700 time 2019-02-28 00:59:45.958105
Model ind 640 epoch 916 head A head_i_epoch 0 batch 200: avg loss -3.297153 avg loss no lamb -3.297153 time 2019-02-28 01:02:23.461819
last batch sz 160
Model ind 640 epoch 916 head B head_i_epoch 0 batch 0: avg loss -1.857555 avg loss no lamb -1.857555 time 2019-02-28 01:04:21.280240
Model ind 640 epoch 916 head B head_i_epoch 0 batch 100: avg loss -1.897025 avg loss no lamb -1.897025 time 2019-02-28 01:06:56.397061
Model ind 640 epoch 916 head B head_i_epoch 0 batch 200: avg loss -1.894788 avg loss no lamb -1.894788 time 2019-02-28 01:09:28.599236
last batch sz 160
Model ind 640 epoch 916 head B head_i_epoch 1 batch 0: avg loss -1.941432 avg loss no lamb -1.941432 time 2019-02-28 01:11:28.287717
Model ind 640 epoch 916 head B head_i_epoch 1 batch 100: avg loss -1.860238 avg loss no lamb -1.860238 time 2019-02-28 01:14:09.770909
Model ind 640 epoch 916 head B head_i_epoch 1 batch 200: avg loss -1.887994 avg loss no lamb -1.887994 time 2019-02-28 01:16:59.347367
last batch sz 160
Pre: time 2019-02-28 01:19:36.099370: 
 	std: 0.050768692
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108, 0.61405, 0.6142167, 0.61445, 0.5104167]
	train_accs: [0.5108, 0.61405, 0.6142167, 0.61445, 0.5104167]
	best_train_sub_head: 3
	worst: 0.5104167
	avg: 0.5727867
	best: 0.61445

Starting e_i: 917
Model ind 640 epoch 917 head A head_i_epoch 0 batch 0: avg loss -3.223094 avg loss no lamb -3.223094 time 2019-02-28 01:19:41.278670
Model ind 640 epoch 917 head A head_i_epoch 0 batch 100: avg loss -3.209532 avg loss no lamb -3.209532 time 2019-02-28 01:22:30.721009
Model ind 640 epoch 917 head A head_i_epoch 0 batch 200: avg loss -3.230982 avg loss no lamb -3.230982 time 2019-02-28 01:25:27.602343
last batch sz 160
Model ind 640 epoch 917 head B head_i_epoch 0 batch 0: avg loss -1.856950 avg loss no lamb -1.856950 time 2019-02-28 01:27:36.926925
Model ind 640 epoch 917 head B head_i_epoch 0 batch 100: avg loss -1.831427 avg loss no lamb -1.831427 time 2019-02-28 01:30:33.305650
Model ind 640 epoch 917 head B head_i_epoch 0 batch 200: avg loss -1.882531 avg loss no lamb -1.882531 time 2019-02-28 01:33:28.331536
last batch sz 160
Model ind 640 epoch 917 head B head_i_epoch 1 batch 0: avg loss -1.878122 avg loss no lamb -1.878122 time 2019-02-28 01:35:29.956613
Model ind 640 epoch 917 head B head_i_epoch 1 batch 100: avg loss -1.820513 avg loss no lamb -1.820513 time 2019-02-28 01:38:26.722024
Model ind 640 epoch 917 head B head_i_epoch 1 batch 200: avg loss -1.957644 avg loss no lamb -1.957644 time 2019-02-28 01:41:26.930591
last batch sz 160
Pre: time 2019-02-28 01:44:07.557840: 
 	std: 0.050931793
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51135, 0.6151, 0.6152, 0.6152167, 0.5110667]
	train_accs: [0.51135, 0.6151, 0.6152, 0.6152167, 0.5110667]
	best_train_sub_head: 3
	worst: 0.5110667
	avg: 0.5735867
	best: 0.6152167

Starting e_i: 918
Model ind 640 epoch 918 head A head_i_epoch 0 batch 0: avg loss -3.234421 avg loss no lamb -3.234421 time 2019-02-28 01:44:12.563189
Model ind 640 epoch 918 head A head_i_epoch 0 batch 100: avg loss -3.170008 avg loss no lamb -3.170008 time 2019-02-28 01:47:06.007557
Model ind 640 epoch 918 head A head_i_epoch 0 batch 200: avg loss -3.222619 avg loss no lamb -3.222619 time 2019-02-28 01:50:06.455795
last batch sz 160
Model ind 640 epoch 918 head B head_i_epoch 0 batch 0: avg loss -1.894588 avg loss no lamb -1.894588 time 2019-02-28 01:52:15.785789
Model ind 640 epoch 918 head B head_i_epoch 0 batch 100: avg loss -1.810935 avg loss no lamb -1.810935 time 2019-02-28 01:55:12.974213
Model ind 640 epoch 918 head B head_i_epoch 0 batch 200: avg loss -1.966504 avg loss no lamb -1.966504 time 2019-02-28 01:58:07.812198
last batch sz 160
Model ind 640 epoch 918 head B head_i_epoch 1 batch 0: avg loss -1.928766 avg loss no lamb -1.928766 time 2019-02-28 02:00:12.470447
Model ind 640 epoch 918 head B head_i_epoch 1 batch 100: avg loss -1.794138 avg loss no lamb -1.794138 time 2019-02-28 02:03:09.153078
Model ind 640 epoch 918 head B head_i_epoch 1 batch 200: avg loss -1.904404 avg loss no lamb -1.904404 time 2019-02-28 02:06:04.156508
last batch sz 160
Pre: time 2019-02-28 02:08:50.973704: 
 	std: 0.050573904
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5127, 0.6156833, 0.61595, 0.61586666, 0.5125]
	train_accs: [0.5127, 0.6156833, 0.61595, 0.61586666, 0.5125]
	best_train_sub_head: 2
	worst: 0.5125
	avg: 0.57454
	best: 0.61595

Starting e_i: 919
Model ind 640 epoch 919 head A head_i_epoch 0 batch 0: avg loss -3.239417 avg loss no lamb -3.239417 time 2019-02-28 02:08:56.359570
Model ind 640 epoch 919 head A head_i_epoch 0 batch 100: avg loss -3.140432 avg loss no lamb -3.140432 time 2019-02-28 02:11:51.211764
Model ind 640 epoch 919 head A head_i_epoch 0 batch 200: avg loss -3.259845 avg loss no lamb -3.259845 time 2019-02-28 02:14:51.027190
last batch sz 160
Model ind 640 epoch 919 head B head_i_epoch 0 batch 0: avg loss -1.969324 avg loss no lamb -1.969324 time 2019-02-28 02:17:06.686777
Model ind 640 epoch 919 head B head_i_epoch 0 batch 100: avg loss -1.868510 avg loss no lamb -1.868510 time 2019-02-28 02:20:02.355931
Model ind 640 epoch 919 head B head_i_epoch 0 batch 200: avg loss -1.928505 avg loss no lamb -1.928505 time 2019-02-28 02:22:57.670651
last batch sz 160
Model ind 640 epoch 919 head B head_i_epoch 1 batch 0: avg loss -1.927219 avg loss no lamb -1.927219 time 2019-02-28 02:25:08.513227
Model ind 640 epoch 919 head B head_i_epoch 1 batch 100: avg loss -1.774957 avg loss no lamb -1.774957 time 2019-02-28 02:28:05.625109
Model ind 640 epoch 919 head B head_i_epoch 1 batch 200: avg loss -1.845109 avg loss no lamb -1.845109 time 2019-02-28 02:31:00.353775
last batch sz 160
Pre: time 2019-02-28 02:33:42.587542: 
 	std: 0.050901964
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118833, 0.6156167, 0.6155, 0.6155667, 0.51143336]
	train_accs: [0.5118833, 0.6156167, 0.6155, 0.6155667, 0.51143336]
	best_train_sub_head: 1
	worst: 0.51143336
	avg: 0.574
	best: 0.6156167

Starting e_i: 920
Model ind 640 epoch 920 head A head_i_epoch 0 batch 0: avg loss -3.257229 avg loss no lamb -3.257229 time 2019-02-28 02:33:47.901556
Model ind 640 epoch 920 head A head_i_epoch 0 batch 100: avg loss -3.144653 avg loss no lamb -3.144653 time 2019-02-28 02:36:38.042164
Model ind 640 epoch 920 head A head_i_epoch 0 batch 200: avg loss -3.220598 avg loss no lamb -3.220598 time 2019-02-28 02:39:34.578347
last batch sz 160
Model ind 640 epoch 920 head B head_i_epoch 0 batch 0: avg loss -1.881200 avg loss no lamb -1.881200 time 2019-02-28 02:41:42.659339
Model ind 640 epoch 920 head B head_i_epoch 0 batch 100: avg loss -1.766839 avg loss no lamb -1.766839 time 2019-02-28 02:44:33.460837
Model ind 640 epoch 920 head B head_i_epoch 0 batch 200: avg loss -1.977265 avg loss no lamb -1.977265 time 2019-02-28 02:47:23.934141
last batch sz 160
Model ind 640 epoch 920 head B head_i_epoch 1 batch 0: avg loss -1.849566 avg loss no lamb -1.849566 time 2019-02-28 02:49:33.767513
Model ind 640 epoch 920 head B head_i_epoch 1 batch 100: avg loss -1.748793 avg loss no lamb -1.748793 time 2019-02-28 02:52:21.694063
Model ind 640 epoch 920 head B head_i_epoch 1 batch 200: avg loss -1.924873 avg loss no lamb -1.924873 time 2019-02-28 02:55:16.189115
last batch sz 160
Pre: time 2019-02-28 02:57:48.275539: 
 	std: 0.05088277
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50983334, 0.6135, 0.61361665, 0.6137, 0.50965]
	train_accs: [0.50983334, 0.6135, 0.61361665, 0.6137, 0.50965]
	best_train_sub_head: 3
	worst: 0.50965
	avg: 0.57206
	best: 0.6137

Starting e_i: 921
Model ind 640 epoch 921 head A head_i_epoch 0 batch 0: avg loss -3.260862 avg loss no lamb -3.260862 time 2019-02-28 02:57:58.341680
Model ind 640 epoch 921 head A head_i_epoch 0 batch 100: avg loss -3.179567 avg loss no lamb -3.179567 time 2019-02-28 03:00:46.865059
Model ind 640 epoch 921 head A head_i_epoch 0 batch 200: avg loss -3.234197 avg loss no lamb -3.234197 time 2019-02-28 03:03:43.165695
last batch sz 160
Model ind 640 epoch 921 head B head_i_epoch 0 batch 0: avg loss -1.932593 avg loss no lamb -1.932593 time 2019-02-28 03:05:43.136294
Model ind 640 epoch 921 head B head_i_epoch 0 batch 100: avg loss -1.807819 avg loss no lamb -1.807819 time 2019-02-28 03:08:34.899837
Model ind 640 epoch 921 head B head_i_epoch 0 batch 200: avg loss -1.950199 avg loss no lamb -1.950199 time 2019-02-28 03:11:25.938553
last batch sz 160
Model ind 640 epoch 921 head B head_i_epoch 1 batch 0: avg loss -1.861860 avg loss no lamb -1.861860 time 2019-02-28 03:13:27.065390
Model ind 640 epoch 921 head B head_i_epoch 1 batch 100: avg loss -1.855671 avg loss no lamb -1.855671 time 2019-02-28 03:16:21.175286
Model ind 640 epoch 921 head B head_i_epoch 1 batch 200: avg loss -1.926035 avg loss no lamb -1.926035 time 2019-02-28 03:19:15.018488
last batch sz 160
Pre: time 2019-02-28 03:21:57.644066: 
 	std: 0.050934687
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50963336, 0.6138833, 0.61361665, 0.6139333, 0.51005]
	train_accs: [0.50963336, 0.6138833, 0.61361665, 0.6139333, 0.51005]
	best_train_sub_head: 3
	worst: 0.50963336
	avg: 0.5722233
	best: 0.6139333

Starting e_i: 922
Model ind 640 epoch 922 head A head_i_epoch 0 batch 0: avg loss -3.176929 avg loss no lamb -3.176929 time 2019-02-28 03:22:03.647484
Model ind 640 epoch 922 head A head_i_epoch 0 batch 100: avg loss -3.147893 avg loss no lamb -3.147893 time 2019-02-28 03:24:57.816596
Model ind 640 epoch 922 head A head_i_epoch 0 batch 200: avg loss -3.166892 avg loss no lamb -3.166892 time 2019-02-28 03:27:56.709820
last batch sz 160
Model ind 640 epoch 922 head B head_i_epoch 0 batch 0: avg loss -1.892353 avg loss no lamb -1.892353 time 2019-02-28 03:30:08.217835
Model ind 640 epoch 922 head B head_i_epoch 0 batch 100: avg loss -1.758544 avg loss no lamb -1.758544 time 2019-02-28 03:33:03.733635
Model ind 640 epoch 922 head B head_i_epoch 0 batch 200: avg loss -1.825751 avg loss no lamb -1.825751 time 2019-02-28 03:35:59.548333
last batch sz 160
Model ind 640 epoch 922 head B head_i_epoch 1 batch 0: avg loss -1.871011 avg loss no lamb -1.871011 time 2019-02-28 03:38:14.142113
Model ind 640 epoch 922 head B head_i_epoch 1 batch 100: avg loss -1.716389 avg loss no lamb -1.716389 time 2019-02-28 03:41:12.041490
Model ind 640 epoch 922 head B head_i_epoch 1 batch 200: avg loss -1.881973 avg loss no lamb -1.881973 time 2019-02-28 03:44:10.643263
last batch sz 160
Pre: time 2019-02-28 03:46:50.513643: 
 	std: 0.051266603
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5095, 0.61396664, 0.61395, 0.61395, 0.50911665]
	train_accs: [0.5095, 0.61396664, 0.61395, 0.61395, 0.50911665]
	best_train_sub_head: 1
	worst: 0.50911665
	avg: 0.5720967
	best: 0.61396664

Starting e_i: 923
Model ind 640 epoch 923 head A head_i_epoch 0 batch 0: avg loss -3.227369 avg loss no lamb -3.227369 time 2019-02-28 03:46:54.245588
Model ind 640 epoch 923 head A head_i_epoch 0 batch 100: avg loss -3.184942 avg loss no lamb -3.184942 time 2019-02-28 03:49:46.611904
Model ind 640 epoch 923 head A head_i_epoch 0 batch 200: avg loss -3.313556 avg loss no lamb -3.313556 time 2019-02-28 03:52:46.946922
last batch sz 160
Model ind 640 epoch 923 head B head_i_epoch 0 batch 0: avg loss -1.909292 avg loss no lamb -1.909292 time 2019-02-28 03:54:59.264117
Model ind 640 epoch 923 head B head_i_epoch 0 batch 100: avg loss -1.847750 avg loss no lamb -1.847750 time 2019-02-28 03:57:59.861136
Model ind 640 epoch 923 head B head_i_epoch 0 batch 200: avg loss -1.892917 avg loss no lamb -1.892917 time 2019-02-28 04:00:59.929439
last batch sz 160
Model ind 640 epoch 923 head B head_i_epoch 1 batch 0: avg loss -1.853749 avg loss no lamb -1.853749 time 2019-02-28 04:03:09.437738
Model ind 640 epoch 923 head B head_i_epoch 1 batch 100: avg loss -1.808004 avg loss no lamb -1.808004 time 2019-02-28 04:06:08.068513
Model ind 640 epoch 923 head B head_i_epoch 1 batch 200: avg loss -1.876284 avg loss no lamb -1.876284 time 2019-02-28 04:09:06.844871
last batch sz 160
Pre: time 2019-02-28 04:11:46.064066: 
 	std: 0.051660046
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51045, 0.6156167, 0.6155667, 0.6157167, 0.50991666]
	train_accs: [0.51045, 0.6156167, 0.6155667, 0.6157167, 0.50991666]
	best_train_sub_head: 3
	worst: 0.50991666
	avg: 0.5734533
	best: 0.6157167

Starting e_i: 924
Model ind 640 epoch 924 head A head_i_epoch 0 batch 0: avg loss -3.207597 avg loss no lamb -3.207597 time 2019-02-28 04:11:51.519477
Model ind 640 epoch 924 head A head_i_epoch 0 batch 100: avg loss -3.103654 avg loss no lamb -3.103654 time 2019-02-28 04:14:53.097025
Model ind 640 epoch 924 head A head_i_epoch 0 batch 200: avg loss -3.319064 avg loss no lamb -3.319064 time 2019-02-28 04:17:54.287917
last batch sz 160
Model ind 640 epoch 924 head B head_i_epoch 0 batch 0: avg loss -1.853872 avg loss no lamb -1.853872 time 2019-02-28 04:20:00.361705
Model ind 640 epoch 924 head B head_i_epoch 0 batch 100: avg loss -1.780887 avg loss no lamb -1.780887 time 2019-02-28 04:23:00.638029
Model ind 640 epoch 924 head B head_i_epoch 0 batch 200: avg loss -1.925535 avg loss no lamb -1.925535 time 2019-02-28 04:25:56.412594
last batch sz 160
Model ind 640 epoch 924 head B head_i_epoch 1 batch 0: avg loss -1.866081 avg loss no lamb -1.866081 time 2019-02-28 04:28:03.196536
Model ind 640 epoch 924 head B head_i_epoch 1 batch 100: avg loss -1.754539 avg loss no lamb -1.754539 time 2019-02-28 04:31:01.545332
Model ind 640 epoch 924 head B head_i_epoch 1 batch 200: avg loss -1.962628 avg loss no lamb -1.962628 time 2019-02-28 04:33:58.921658
last batch sz 160
Pre: time 2019-02-28 04:36:40.328350: 
 	std: 0.05039735
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118, 0.6143, 0.6145, 0.6142667, 0.5111667]
	train_accs: [0.5118, 0.6143, 0.6145, 0.6142667, 0.5111667]
	best_train_sub_head: 2
	worst: 0.5111667
	avg: 0.5732067
	best: 0.6145

Starting e_i: 925
Model ind 640 epoch 925 head A head_i_epoch 0 batch 0: avg loss -3.299309 avg loss no lamb -3.299309 time 2019-02-28 04:36:45.140949
Model ind 640 epoch 925 head A head_i_epoch 0 batch 100: avg loss -3.145662 avg loss no lamb -3.145662 time 2019-02-28 04:39:40.373383
Model ind 640 epoch 925 head A head_i_epoch 0 batch 200: avg loss -3.330497 avg loss no lamb -3.330497 time 2019-02-28 04:42:37.849180
last batch sz 160
Model ind 640 epoch 925 head B head_i_epoch 0 batch 0: avg loss -1.931433 avg loss no lamb -1.931433 time 2019-02-28 04:44:44.091278
Model ind 640 epoch 925 head B head_i_epoch 0 batch 100: avg loss -1.788160 avg loss no lamb -1.788160 time 2019-02-28 04:47:36.899908
Model ind 640 epoch 925 head B head_i_epoch 0 batch 200: avg loss -1.902746 avg loss no lamb -1.902746 time 2019-02-28 04:50:31.273491
last batch sz 160
Model ind 640 epoch 925 head B head_i_epoch 1 batch 0: avg loss -1.863861 avg loss no lamb -1.863861 time 2019-02-28 04:52:44.592228
Model ind 640 epoch 925 head B head_i_epoch 1 batch 100: avg loss -1.837562 avg loss no lamb -1.837562 time 2019-02-28 04:55:36.004484
Model ind 640 epoch 925 head B head_i_epoch 1 batch 200: avg loss -1.867612 avg loss no lamb -1.867612 time 2019-02-28 04:58:29.544476
last batch sz 160
Pre: time 2019-02-28 05:01:07.267981: 
 	std: 0.05066371
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5097, 0.6132333, 0.61331666, 0.61325, 0.51]
	train_accs: [0.5097, 0.6132333, 0.61331666, 0.61325, 0.51]
	best_train_sub_head: 2
	worst: 0.5097
	avg: 0.5719
	best: 0.61331666

Starting e_i: 926
Model ind 640 epoch 926 head A head_i_epoch 0 batch 0: avg loss -3.262724 avg loss no lamb -3.262724 time 2019-02-28 05:01:10.443533
Model ind 640 epoch 926 head A head_i_epoch 0 batch 100: avg loss -3.180503 avg loss no lamb -3.180503 time 2019-02-28 05:04:06.050712
Model ind 640 epoch 926 head A head_i_epoch 0 batch 200: avg loss -3.228421 avg loss no lamb -3.228421 time 2019-02-28 05:07:02.460374
last batch sz 160
Model ind 640 epoch 926 head B head_i_epoch 0 batch 0: avg loss -1.887922 avg loss no lamb -1.887922 time 2019-02-28 05:09:12.225560
Model ind 640 epoch 926 head B head_i_epoch 0 batch 100: avg loss -1.827500 avg loss no lamb -1.827500 time 2019-02-28 05:12:09.196333
Model ind 640 epoch 926 head B head_i_epoch 0 batch 200: avg loss -1.897881 avg loss no lamb -1.897881 time 2019-02-28 05:15:05.767606
last batch sz 160
Model ind 640 epoch 926 head B head_i_epoch 1 batch 0: avg loss -1.881578 avg loss no lamb -1.881578 time 2019-02-28 05:17:10.558202
Model ind 640 epoch 926 head B head_i_epoch 1 batch 100: avg loss -1.835997 avg loss no lamb -1.835997 time 2019-02-28 05:20:08.746462
Model ind 640 epoch 926 head B head_i_epoch 1 batch 200: avg loss -1.980090 avg loss no lamb -1.980090 time 2019-02-28 05:23:09.211917
last batch sz 160
Pre: time 2019-02-28 05:25:42.122942: 
 	std: 0.050488178
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5099, 0.61308336, 0.6131333, 0.6131333, 0.51021665]
	train_accs: [0.5099, 0.61308336, 0.6131333, 0.6131333, 0.51021665]
	best_train_sub_head: 2
	worst: 0.5099
	avg: 0.57189333
	best: 0.6131333

Starting e_i: 927
Model ind 640 epoch 927 head A head_i_epoch 0 batch 0: avg loss -3.214944 avg loss no lamb -3.214944 time 2019-02-28 05:25:47.714328
Model ind 640 epoch 927 head A head_i_epoch 0 batch 100: avg loss -3.092625 avg loss no lamb -3.092625 time 2019-02-28 05:28:46.900110
Model ind 640 epoch 927 head A head_i_epoch 0 batch 200: avg loss -3.253843 avg loss no lamb -3.253843 time 2019-02-28 05:31:44.758356
last batch sz 160
Model ind 640 epoch 927 head B head_i_epoch 0 batch 0: avg loss -1.935398 avg loss no lamb -1.935398 time 2019-02-28 05:33:50.753339
Model ind 640 epoch 927 head B head_i_epoch 0 batch 100: avg loss -1.687547 avg loss no lamb -1.687547 time 2019-02-28 05:36:47.246271
Model ind 640 epoch 927 head B head_i_epoch 0 batch 200: avg loss -1.892408 avg loss no lamb -1.892408 time 2019-02-28 05:39:46.571792
last batch sz 160
Model ind 640 epoch 927 head B head_i_epoch 1 batch 0: avg loss -1.969339 avg loss no lamb -1.969339 time 2019-02-28 05:42:00.078738
Model ind 640 epoch 927 head B head_i_epoch 1 batch 100: avg loss -1.869972 avg loss no lamb -1.869972 time 2019-02-28 05:44:56.185132
Model ind 640 epoch 927 head B head_i_epoch 1 batch 200: avg loss -1.941286 avg loss no lamb -1.941286 time 2019-02-28 05:47:53.794207
last batch sz 160
Pre: time 2019-02-28 05:50:42.675025: 
 	std: 0.050601058
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115, 0.61483335, 0.6149167, 0.6148667, 0.51166666]
	train_accs: [0.5115, 0.61483335, 0.6149167, 0.6148667, 0.51166666]
	best_train_sub_head: 2
	worst: 0.5115
	avg: 0.5735567
	best: 0.6149167

Starting e_i: 928
Model ind 640 epoch 928 head A head_i_epoch 0 batch 0: avg loss -3.250430 avg loss no lamb -3.250430 time 2019-02-28 05:50:47.066364
Model ind 640 epoch 928 head A head_i_epoch 0 batch 100: avg loss -3.145268 avg loss no lamb -3.145268 time 2019-02-28 05:53:51.902048
Model ind 640 epoch 928 head A head_i_epoch 0 batch 200: avg loss -3.255476 avg loss no lamb -3.255476 time 2019-02-28 05:56:54.415040
last batch sz 160
Model ind 640 epoch 928 head B head_i_epoch 0 batch 0: avg loss -1.962697 avg loss no lamb -1.962697 time 2019-02-28 05:59:11.086730
Model ind 640 epoch 928 head B head_i_epoch 0 batch 100: avg loss -1.766855 avg loss no lamb -1.766855 time 2019-02-28 06:02:10.212292
Model ind 640 epoch 928 head B head_i_epoch 0 batch 200: avg loss -1.960816 avg loss no lamb -1.960816 time 2019-02-28 06:05:10.210875
last batch sz 160
Model ind 640 epoch 928 head B head_i_epoch 1 batch 0: avg loss -1.885939 avg loss no lamb -1.885939 time 2019-02-28 06:07:25.985716
Model ind 640 epoch 928 head B head_i_epoch 1 batch 100: avg loss -1.786217 avg loss no lamb -1.786217 time 2019-02-28 06:10:25.072223
Model ind 640 epoch 928 head B head_i_epoch 1 batch 200: avg loss -1.934844 avg loss no lamb -1.934844 time 2019-02-28 06:13:21.957230
last batch sz 160
Pre: time 2019-02-28 06:15:58.363466: 
 	std: 0.050685514
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114, 0.61475, 0.61481667, 0.6146167, 0.5111333]
	train_accs: [0.5114, 0.61475, 0.61481667, 0.6146167, 0.5111333]
	best_train_sub_head: 2
	worst: 0.5111333
	avg: 0.5733434
	best: 0.61481667

Starting e_i: 929
Model ind 640 epoch 929 head A head_i_epoch 0 batch 0: avg loss -3.283789 avg loss no lamb -3.283789 time 2019-02-28 06:16:03.430031
Model ind 640 epoch 929 head A head_i_epoch 0 batch 100: avg loss -3.167222 avg loss no lamb -3.167222 time 2019-02-28 06:19:05.434979
Model ind 640 epoch 929 head A head_i_epoch 0 batch 200: avg loss -3.264865 avg loss no lamb -3.264865 time 2019-02-28 06:22:02.323258
last batch sz 160
Model ind 640 epoch 929 head B head_i_epoch 0 batch 0: avg loss -1.828519 avg loss no lamb -1.828519 time 2019-02-28 06:24:08.572656
Model ind 640 epoch 929 head B head_i_epoch 0 batch 100: avg loss -1.795731 avg loss no lamb -1.795731 time 2019-02-28 06:27:05.116994
Model ind 640 epoch 929 head B head_i_epoch 0 batch 200: avg loss -1.908674 avg loss no lamb -1.908674 time 2019-02-28 06:30:01.861406
last batch sz 160
Model ind 640 epoch 929 head B head_i_epoch 1 batch 0: avg loss -1.866130 avg loss no lamb -1.866130 time 2019-02-28 06:32:06.961299
Model ind 640 epoch 929 head B head_i_epoch 1 batch 100: avg loss -1.785814 avg loss no lamb -1.785814 time 2019-02-28 06:35:03.270810
Model ind 640 epoch 929 head B head_i_epoch 1 batch 200: avg loss -1.871473 avg loss no lamb -1.871473 time 2019-02-28 06:38:05.233062
last batch sz 160
Pre: time 2019-02-28 06:40:38.883619: 
 	std: 0.051138587
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50905, 0.61333334, 0.6133, 0.6134, 0.50886667]
	train_accs: [0.50905, 0.61333334, 0.6133, 0.6134, 0.50886667]
	best_train_sub_head: 3
	worst: 0.50886667
	avg: 0.57159007
	best: 0.6134

Starting e_i: 930
Model ind 640 epoch 930 head A head_i_epoch 0 batch 0: avg loss -3.185315 avg loss no lamb -3.185315 time 2019-02-28 06:40:43.696096
Model ind 640 epoch 930 head A head_i_epoch 0 batch 100: avg loss -3.165329 avg loss no lamb -3.165329 time 2019-02-28 06:43:40.651195
Model ind 640 epoch 930 head A head_i_epoch 0 batch 200: avg loss -3.247337 avg loss no lamb -3.247337 time 2019-02-28 06:46:36.883503
last batch sz 160
Model ind 640 epoch 930 head B head_i_epoch 0 batch 0: avg loss -1.973047 avg loss no lamb -1.973047 time 2019-02-28 06:48:36.733041
Model ind 640 epoch 930 head B head_i_epoch 0 batch 100: avg loss -1.778707 avg loss no lamb -1.778707 time 2019-02-28 06:51:29.335242
Model ind 640 epoch 930 head B head_i_epoch 0 batch 200: avg loss -1.906654 avg loss no lamb -1.906654 time 2019-02-28 06:54:22.135943
last batch sz 160
Model ind 640 epoch 930 head B head_i_epoch 1 batch 0: avg loss -1.851611 avg loss no lamb -1.851611 time 2019-02-28 06:56:38.447656
Model ind 640 epoch 930 head B head_i_epoch 1 batch 100: avg loss -1.868164 avg loss no lamb -1.868164 time 2019-02-28 06:59:29.260000
Model ind 640 epoch 930 head B head_i_epoch 1 batch 200: avg loss -1.863495 avg loss no lamb -1.863495 time 2019-02-28 07:02:19.550608
last batch sz 160
Pre: time 2019-02-28 07:04:59.583134: 
 	std: 0.05066659
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51131666, 0.6144, 0.61455, 0.6145667, 0.51085]
	train_accs: [0.51131666, 0.6144, 0.61455, 0.6145667, 0.51085]
	best_train_sub_head: 3
	worst: 0.51085
	avg: 0.5731367
	best: 0.6145667

Starting e_i: 931
Model ind 640 epoch 931 head A head_i_epoch 0 batch 0: avg loss -3.246890 avg loss no lamb -3.246890 time 2019-02-28 07:05:07.382909
Model ind 640 epoch 931 head A head_i_epoch 0 batch 100: avg loss -3.155569 avg loss no lamb -3.155569 time 2019-02-28 07:08:02.042132
Model ind 640 epoch 931 head A head_i_epoch 0 batch 200: avg loss -3.269887 avg loss no lamb -3.269887 time 2019-02-28 07:10:59.708399
last batch sz 160
Model ind 640 epoch 931 head B head_i_epoch 0 batch 0: avg loss -1.840141 avg loss no lamb -1.840141 time 2019-02-28 07:13:13.166392
Model ind 640 epoch 931 head B head_i_epoch 0 batch 100: avg loss -1.840659 avg loss no lamb -1.840659 time 2019-02-28 07:16:05.444631
Model ind 640 epoch 931 head B head_i_epoch 0 batch 200: avg loss -1.920688 avg loss no lamb -1.920688 time 2019-02-28 07:18:58.471722
last batch sz 160
Model ind 640 epoch 931 head B head_i_epoch 1 batch 0: avg loss -1.793244 avg loss no lamb -1.793244 time 2019-02-28 07:21:01.151148
Model ind 640 epoch 931 head B head_i_epoch 1 batch 100: avg loss -1.765934 avg loss no lamb -1.765934 time 2019-02-28 07:23:52.752973
Model ind 640 epoch 931 head B head_i_epoch 1 batch 200: avg loss -1.909985 avg loss no lamb -1.909985 time 2019-02-28 07:26:49.131002
last batch sz 160
Pre: time 2019-02-28 07:29:28.223046: 
 	std: 0.0505358
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125667, 0.6155, 0.6158, 0.61576664, 0.5125]
	train_accs: [0.5125667, 0.6155, 0.6158, 0.61576664, 0.5125]
	best_train_sub_head: 2
	worst: 0.5125
	avg: 0.5744267
	best: 0.6158

Starting e_i: 932
Model ind 640 epoch 932 head A head_i_epoch 0 batch 0: avg loss -3.243126 avg loss no lamb -3.243126 time 2019-02-28 07:29:32.620649
Model ind 640 epoch 932 head A head_i_epoch 0 batch 100: avg loss -3.135209 avg loss no lamb -3.135209 time 2019-02-28 07:32:31.704446
Model ind 640 epoch 932 head A head_i_epoch 0 batch 200: avg loss -3.180969 avg loss no lamb -3.180969 time 2019-02-28 07:35:30.774393
last batch sz 160
Model ind 640 epoch 932 head B head_i_epoch 0 batch 0: avg loss -1.909320 avg loss no lamb -1.909320 time 2019-02-28 07:37:37.625911
Model ind 640 epoch 932 head B head_i_epoch 0 batch 100: avg loss -1.738060 avg loss no lamb -1.738060 time 2019-02-28 07:40:39.849665
Model ind 640 epoch 932 head B head_i_epoch 0 batch 200: avg loss -1.918633 avg loss no lamb -1.918633 time 2019-02-28 07:43:37.538181
last batch sz 160
Model ind 640 epoch 932 head B head_i_epoch 1 batch 0: avg loss -1.869922 avg loss no lamb -1.869922 time 2019-02-28 07:45:49.465311
Model ind 640 epoch 932 head B head_i_epoch 1 batch 100: avg loss -1.791593 avg loss no lamb -1.791593 time 2019-02-28 07:48:53.639294
Model ind 640 epoch 932 head B head_i_epoch 1 batch 200: avg loss -1.828478 avg loss no lamb -1.828478 time 2019-02-28 07:51:51.360053
last batch sz 160
Pre: time 2019-02-28 07:54:44.463343: 
 	std: 0.050817423
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.509, 0.61266667, 0.61268336, 0.6128167, 0.5089833]
	train_accs: [0.509, 0.61266667, 0.61268336, 0.6128167, 0.5089833]
	best_train_sub_head: 3
	worst: 0.5089833
	avg: 0.57123
	best: 0.6128167

Starting e_i: 933
Model ind 640 epoch 933 head A head_i_epoch 0 batch 0: avg loss -3.258819 avg loss no lamb -3.258819 time 2019-02-28 07:54:49.298687
Model ind 640 epoch 933 head A head_i_epoch 0 batch 100: avg loss -3.172111 avg loss no lamb -3.172111 time 2019-02-28 07:57:49.991710
Model ind 640 epoch 933 head A head_i_epoch 0 batch 200: avg loss -3.267186 avg loss no lamb -3.267186 time 2019-02-28 08:00:54.118939
last batch sz 160
Model ind 640 epoch 933 head B head_i_epoch 0 batch 0: avg loss -1.883721 avg loss no lamb -1.883721 time 2019-02-28 08:03:07.096510
Model ind 640 epoch 933 head B head_i_epoch 0 batch 100: avg loss -1.720241 avg loss no lamb -1.720241 time 2019-02-28 08:06:11.538213
Model ind 640 epoch 933 head B head_i_epoch 0 batch 200: avg loss -1.999613 avg loss no lamb -1.999613 time 2019-02-28 08:09:13.114283
last batch sz 160
Model ind 640 epoch 933 head B head_i_epoch 1 batch 0: avg loss -1.947452 avg loss no lamb -1.947452 time 2019-02-28 08:11:20.935979
Model ind 640 epoch 933 head B head_i_epoch 1 batch 100: avg loss -1.717663 avg loss no lamb -1.717663 time 2019-02-28 08:14:22.520925
Model ind 640 epoch 933 head B head_i_epoch 1 batch 200: avg loss -1.867138 avg loss no lamb -1.867138 time 2019-02-28 08:17:20.153877
last batch sz 160
Pre: time 2019-02-28 08:20:08.742760: 
 	std: 0.05053706
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118333, 0.6149333, 0.6150333, 0.6149333, 0.51178336]
	train_accs: [0.5118333, 0.6149333, 0.6150333, 0.6149333, 0.51178336]
	best_train_sub_head: 2
	worst: 0.51178336
	avg: 0.57370335
	best: 0.6150333

Starting e_i: 934
Model ind 640 epoch 934 head A head_i_epoch 0 batch 0: avg loss -3.223952 avg loss no lamb -3.223952 time 2019-02-28 08:20:15.230559
Model ind 640 epoch 934 head A head_i_epoch 0 batch 100: avg loss -3.148273 avg loss no lamb -3.148273 time 2019-02-28 08:23:09.299126
Model ind 640 epoch 934 head A head_i_epoch 0 batch 200: avg loss -3.260044 avg loss no lamb -3.260044 time 2019-02-28 08:26:03.445824
last batch sz 160
Model ind 640 epoch 934 head B head_i_epoch 0 batch 0: avg loss -1.739514 avg loss no lamb -1.739514 time 2019-02-28 08:28:13.033172
Model ind 640 epoch 934 head B head_i_epoch 0 batch 100: avg loss -1.844602 avg loss no lamb -1.844602 time 2019-02-28 08:31:04.234293
Model ind 640 epoch 934 head B head_i_epoch 0 batch 200: avg loss -1.749649 avg loss no lamb -1.749649 time 2019-02-28 08:33:58.547958
last batch sz 160
Model ind 640 epoch 934 head B head_i_epoch 1 batch 0: avg loss -1.843886 avg loss no lamb -1.843886 time 2019-02-28 08:36:13.312185
Model ind 640 epoch 934 head B head_i_epoch 1 batch 100: avg loss -1.827192 avg loss no lamb -1.827192 time 2019-02-28 08:39:08.115011
Model ind 640 epoch 934 head B head_i_epoch 1 batch 200: avg loss -1.829778 avg loss no lamb -1.829778 time 2019-02-28 08:41:58.404541
last batch sz 160
Pre: time 2019-02-28 08:44:38.434210: 
 	std: 0.050696302
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51121664, 0.61466664, 0.6146333, 0.6146, 0.5110833]
	train_accs: [0.51121664, 0.61466664, 0.6146333, 0.6146, 0.5110833]
	best_train_sub_head: 1
	worst: 0.5110833
	avg: 0.57324
	best: 0.61466664

Starting e_i: 935
Model ind 640 epoch 935 head A head_i_epoch 0 batch 0: avg loss -3.236789 avg loss no lamb -3.236789 time 2019-02-28 08:44:45.166375
Model ind 640 epoch 935 head A head_i_epoch 0 batch 100: avg loss -3.148809 avg loss no lamb -3.148809 time 2019-02-28 08:47:29.237200
Model ind 640 epoch 935 head A head_i_epoch 0 batch 200: avg loss -3.160304 avg loss no lamb -3.160304 time 2019-02-28 08:50:22.113308
last batch sz 160
Model ind 640 epoch 935 head B head_i_epoch 0 batch 0: avg loss -1.852973 avg loss no lamb -1.852973 time 2019-02-28 08:52:15.674891
Model ind 640 epoch 935 head B head_i_epoch 0 batch 100: avg loss -1.827376 avg loss no lamb -1.827376 time 2019-02-28 08:55:07.358209
Model ind 640 epoch 935 head B head_i_epoch 0 batch 200: avg loss -1.887308 avg loss no lamb -1.887308 time 2019-02-28 08:58:03.162357
last batch sz 160
Model ind 640 epoch 935 head B head_i_epoch 1 batch 0: avg loss -1.885254 avg loss no lamb -1.885254 time 2019-02-28 09:00:16.144190
Model ind 640 epoch 935 head B head_i_epoch 1 batch 100: avg loss -1.775974 avg loss no lamb -1.775974 time 2019-02-28 09:03:06.739390
Model ind 640 epoch 935 head B head_i_epoch 1 batch 200: avg loss -1.947100 avg loss no lamb -1.947100 time 2019-02-28 09:05:53.557480
last batch sz 160
Pre: time 2019-02-28 09:08:36.259572: 
 	std: 0.051357776
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5100167, 0.6146333, 0.61473334, 0.6146333, 0.50965]
	train_accs: [0.5100167, 0.6146333, 0.61473334, 0.6146333, 0.50965]
	best_train_sub_head: 2
	worst: 0.50965
	avg: 0.57273334
	best: 0.61473334

Starting e_i: 936
Model ind 640 epoch 936 head A head_i_epoch 0 batch 0: avg loss -3.205878 avg loss no lamb -3.205878 time 2019-02-28 09:08:40.503793
Model ind 640 epoch 936 head A head_i_epoch 0 batch 100: avg loss -3.053849 avg loss no lamb -3.053849 time 2019-02-28 09:11:31.476908
Model ind 640 epoch 936 head A head_i_epoch 0 batch 200: avg loss -3.235727 avg loss no lamb -3.235727 time 2019-02-28 09:14:23.043241
last batch sz 160
Model ind 640 epoch 936 head B head_i_epoch 0 batch 0: avg loss -1.960320 avg loss no lamb -1.960320 time 2019-02-28 09:16:34.010520
Model ind 640 epoch 936 head B head_i_epoch 0 batch 100: avg loss -1.861917 avg loss no lamb -1.861917 time 2019-02-28 09:19:26.453363
Model ind 640 epoch 936 head B head_i_epoch 0 batch 200: avg loss -1.966427 avg loss no lamb -1.966427 time 2019-02-28 09:22:15.735944
last batch sz 160
Model ind 640 epoch 936 head B head_i_epoch 1 batch 0: avg loss -1.893822 avg loss no lamb -1.893822 time 2019-02-28 09:24:23.092384
Model ind 640 epoch 936 head B head_i_epoch 1 batch 100: avg loss -1.817482 avg loss no lamb -1.817482 time 2019-02-28 09:27:20.420857
Model ind 640 epoch 936 head B head_i_epoch 1 batch 200: avg loss -1.959591 avg loss no lamb -1.959591 time 2019-02-28 09:30:14.539522
last batch sz 160
Pre: time 2019-02-28 09:32:43.547970: 
 	std: 0.050993174
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101, 0.6143, 0.61425, 0.6145667, 0.5104667]
	train_accs: [0.5101, 0.6143, 0.61425, 0.6145667, 0.5104667]
	best_train_sub_head: 3
	worst: 0.5101
	avg: 0.5727366
	best: 0.6145667

Starting e_i: 937
Model ind 640 epoch 937 head A head_i_epoch 0 batch 0: avg loss -3.181330 avg loss no lamb -3.181330 time 2019-02-28 09:32:47.099453
Model ind 640 epoch 937 head A head_i_epoch 0 batch 100: avg loss -3.156629 avg loss no lamb -3.156629 time 2019-02-28 09:35:32.654608
Model ind 640 epoch 937 head A head_i_epoch 0 batch 200: avg loss -3.222324 avg loss no lamb -3.222324 time 2019-02-28 09:38:19.004802
last batch sz 160
Model ind 640 epoch 937 head B head_i_epoch 0 batch 0: avg loss -1.904625 avg loss no lamb -1.904625 time 2019-02-28 09:40:25.026693
Model ind 640 epoch 937 head B head_i_epoch 0 batch 100: avg loss -1.758423 avg loss no lamb -1.758423 time 2019-02-28 09:43:16.526608
Model ind 640 epoch 937 head B head_i_epoch 0 batch 200: avg loss -1.883931 avg loss no lamb -1.883931 time 2019-02-28 09:46:05.688213
last batch sz 160
Model ind 640 epoch 937 head B head_i_epoch 1 batch 0: avg loss -1.881374 avg loss no lamb -1.881374 time 2019-02-28 09:48:14.467910
Model ind 640 epoch 937 head B head_i_epoch 1 batch 100: avg loss -1.798726 avg loss no lamb -1.798726 time 2019-02-28 09:51:03.832312
Model ind 640 epoch 937 head B head_i_epoch 1 batch 200: avg loss -1.889396 avg loss no lamb -1.889396 time 2019-02-28 09:53:00.301760
last batch sz 160
Pre: time 2019-02-28 09:54:46.190071: 
 	std: 0.05140673
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50918335, 0.6142, 0.61401665, 0.61433333, 0.5093167]
	train_accs: [0.50918335, 0.6142, 0.61401665, 0.61433333, 0.5093167]
	best_train_sub_head: 3
	worst: 0.50918335
	avg: 0.57220995
	best: 0.61433333

Starting e_i: 938
Model ind 640 epoch 938 head A head_i_epoch 0 batch 0: avg loss -3.263759 avg loss no lamb -3.263759 time 2019-02-28 09:54:51.731913
Model ind 640 epoch 938 head A head_i_epoch 0 batch 100: avg loss -3.192379 avg loss no lamb -3.192379 time 2019-02-28 09:57:42.312601
Model ind 640 epoch 938 head A head_i_epoch 0 batch 200: avg loss -3.149553 avg loss no lamb -3.149553 time 2019-02-28 10:00:36.928931
last batch sz 160
Model ind 640 epoch 938 head B head_i_epoch 0 batch 0: avg loss -1.953206 avg loss no lamb -1.953206 time 2019-02-28 10:02:51.517771
Model ind 640 epoch 938 head B head_i_epoch 0 batch 100: avg loss -1.868032 avg loss no lamb -1.868032 time 2019-02-28 10:06:02.893386
Model ind 640 epoch 938 head B head_i_epoch 0 batch 200: avg loss -1.901173 avg loss no lamb -1.901173 time 2019-02-28 10:09:08.152859
last batch sz 160
Model ind 640 epoch 938 head B head_i_epoch 1 batch 0: avg loss -1.826739 avg loss no lamb -1.826739 time 2019-02-28 10:11:19.418340
Model ind 640 epoch 938 head B head_i_epoch 1 batch 100: avg loss -1.774554 avg loss no lamb -1.774554 time 2019-02-28 10:14:18.906947
Model ind 640 epoch 938 head B head_i_epoch 1 batch 200: avg loss -1.981958 avg loss no lamb -1.981958 time 2019-02-28 10:17:18.405171
last batch sz 160
Pre: time 2019-02-28 10:20:07.190643: 
 	std: 0.050357554
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5110667, 0.61403334, 0.6139167, 0.6141, 0.51138335]
	train_accs: [0.5110667, 0.61403334, 0.6139167, 0.6141, 0.51138335]
	best_train_sub_head: 3
	worst: 0.5110667
	avg: 0.5729
	best: 0.6141

Starting e_i: 939
Model ind 640 epoch 939 head A head_i_epoch 0 batch 0: avg loss -3.223520 avg loss no lamb -3.223520 time 2019-02-28 10:20:13.930112
Model ind 640 epoch 939 head A head_i_epoch 0 batch 100: avg loss -3.118010 avg loss no lamb -3.118010 time 2019-02-28 10:23:23.453346
Model ind 640 epoch 939 head A head_i_epoch 0 batch 200: avg loss -3.190002 avg loss no lamb -3.190002 time 2019-02-28 10:26:31.775481
last batch sz 160
Model ind 640 epoch 939 head B head_i_epoch 0 batch 0: avg loss -1.927171 avg loss no lamb -1.927171 time 2019-02-28 10:28:43.275896
Model ind 640 epoch 939 head B head_i_epoch 0 batch 100: avg loss -1.836455 avg loss no lamb -1.836455 time 2019-02-28 10:31:59.143091
Model ind 640 epoch 939 head B head_i_epoch 0 batch 200: avg loss -1.946852 avg loss no lamb -1.946852 time 2019-02-28 10:35:17.332911
last batch sz 160
Model ind 640 epoch 939 head B head_i_epoch 1 batch 0: avg loss -1.847456 avg loss no lamb -1.847456 time 2019-02-28 10:37:40.235858
Model ind 640 epoch 939 head B head_i_epoch 1 batch 100: avg loss -1.852624 avg loss no lamb -1.852624 time 2019-02-28 10:40:52.854959
Model ind 640 epoch 939 head B head_i_epoch 1 batch 200: avg loss -1.927064 avg loss no lamb -1.927064 time 2019-02-28 10:44:04.388512
last batch sz 160
Pre: time 2019-02-28 10:46:46.138492: 
 	std: 0.050701834
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51091665, 0.6142667, 0.6141833, 0.6142833, 0.51058334]
	train_accs: [0.51091665, 0.6142667, 0.6141833, 0.6142833, 0.51058334]
	best_train_sub_head: 3
	worst: 0.51058334
	avg: 0.5728467
	best: 0.6142833

Starting e_i: 940
Model ind 640 epoch 940 head A head_i_epoch 0 batch 0: avg loss -3.260861 avg loss no lamb -3.260861 time 2019-02-28 10:46:54.550444
Model ind 640 epoch 940 head A head_i_epoch 0 batch 100: avg loss -3.122148 avg loss no lamb -3.122148 time 2019-02-28 10:49:58.954985
Model ind 640 epoch 940 head A head_i_epoch 0 batch 200: avg loss -3.215138 avg loss no lamb -3.215138 time 2019-02-28 10:53:08.155378
last batch sz 160
Model ind 640 epoch 940 head B head_i_epoch 0 batch 0: avg loss -1.856648 avg loss no lamb -1.856648 time 2019-02-28 10:55:28.608892
Model ind 640 epoch 940 head B head_i_epoch 0 batch 100: avg loss -1.787745 avg loss no lamb -1.787745 time 2019-02-28 10:58:31.413250
Model ind 640 epoch 940 head B head_i_epoch 0 batch 200: avg loss -1.891741 avg loss no lamb -1.891741 time 2019-02-28 11:01:31.080974
last batch sz 160
Model ind 640 epoch 940 head B head_i_epoch 1 batch 0: avg loss -1.890066 avg loss no lamb -1.890066 time 2019-02-28 11:03:36.418912
Model ind 640 epoch 940 head B head_i_epoch 1 batch 100: avg loss -1.838920 avg loss no lamb -1.838920 time 2019-02-28 11:06:31.743184
Model ind 640 epoch 940 head B head_i_epoch 1 batch 200: avg loss -1.957201 avg loss no lamb -1.957201 time 2019-02-28 11:09:36.975382
last batch sz 160
Pre: time 2019-02-28 11:12:39.562097: 
 	std: 0.050593566
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103667, 0.61331666, 0.61305, 0.61365, 0.50976664]
	train_accs: [0.5103667, 0.61331666, 0.61305, 0.61365, 0.50976664]
	best_train_sub_head: 3
	worst: 0.50976664
	avg: 0.57202995
	best: 0.61365

Starting e_i: 941
Model ind 640 epoch 941 head A head_i_epoch 0 batch 0: avg loss -3.294095 avg loss no lamb -3.294095 time 2019-02-28 11:12:48.381935
Model ind 640 epoch 941 head A head_i_epoch 0 batch 100: avg loss -3.135913 avg loss no lamb -3.135913 time 2019-02-28 11:16:08.502728
Model ind 640 epoch 941 head A head_i_epoch 0 batch 200: avg loss -3.217822 avg loss no lamb -3.217822 time 2019-02-28 11:19:29.760709
last batch sz 160
Model ind 640 epoch 941 head B head_i_epoch 0 batch 0: avg loss -1.981950 avg loss no lamb -1.981950 time 2019-02-28 11:21:51.046853
Model ind 640 epoch 941 head B head_i_epoch 0 batch 100: avg loss -1.826097 avg loss no lamb -1.826097 time 2019-02-28 11:25:00.143993
Model ind 640 epoch 941 head B head_i_epoch 0 batch 200: avg loss -1.931837 avg loss no lamb -1.931837 time 2019-02-28 11:28:11.981450
last batch sz 160
Model ind 640 epoch 941 head B head_i_epoch 1 batch 0: avg loss -1.846773 avg loss no lamb -1.846773 time 2019-02-28 11:30:38.602039
Model ind 640 epoch 941 head B head_i_epoch 1 batch 100: avg loss -1.746561 avg loss no lamb -1.746561 time 2019-02-28 11:33:50.933972
Model ind 640 epoch 941 head B head_i_epoch 1 batch 200: avg loss -1.937196 avg loss no lamb -1.937196 time 2019-02-28 11:37:02.455721
last batch sz 160
Pre: time 2019-02-28 11:39:56.634401: 
 	std: 0.050603095
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51241666, 0.61541665, 0.61515, 0.6153333, 0.5116]
	train_accs: [0.51241666, 0.61541665, 0.61515, 0.6153333, 0.5116]
	best_train_sub_head: 1
	worst: 0.5116
	avg: 0.5739833
	best: 0.61541665

Starting e_i: 942
Model ind 640 epoch 942 head A head_i_epoch 0 batch 0: avg loss -3.206587 avg loss no lamb -3.206587 time 2019-02-28 11:40:03.830941
Model ind 640 epoch 942 head A head_i_epoch 0 batch 100: avg loss -3.093626 avg loss no lamb -3.093626 time 2019-02-28 11:43:21.518081
Model ind 640 epoch 942 head A head_i_epoch 0 batch 200: avg loss -3.219406 avg loss no lamb -3.219406 time 2019-02-28 11:46:39.820306
last batch sz 160
Model ind 640 epoch 942 head B head_i_epoch 0 batch 0: avg loss -1.904593 avg loss no lamb -1.904593 time 2019-02-28 11:48:58.650032
Model ind 640 epoch 942 head B head_i_epoch 0 batch 100: avg loss -1.813182 avg loss no lamb -1.813182 time 2019-02-28 11:52:16.442162
Model ind 640 epoch 942 head B head_i_epoch 0 batch 200: avg loss -1.889195 avg loss no lamb -1.889195 time 2019-02-28 11:55:35.884171
last batch sz 160
Model ind 640 epoch 942 head B head_i_epoch 1 batch 0: avg loss -1.848341 avg loss no lamb -1.848341 time 2019-02-28 11:58:03.206538
Model ind 640 epoch 942 head B head_i_epoch 1 batch 100: avg loss -1.868656 avg loss no lamb -1.868656 time 2019-02-28 12:01:27.102697
Model ind 640 epoch 942 head B head_i_epoch 1 batch 200: avg loss -1.889889 avg loss no lamb -1.889889 time 2019-02-28 12:04:50.719784
last batch sz 160
Pre: time 2019-02-28 12:08:01.040700: 
 	std: 0.05078225
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51045, 0.6138, 0.6138667, 0.61413336, 0.5101]
	train_accs: [0.51045, 0.6138, 0.6138667, 0.61413336, 0.5101]
	best_train_sub_head: 3
	worst: 0.5101
	avg: 0.57247
	best: 0.61413336

Starting e_i: 943
Model ind 640 epoch 943 head A head_i_epoch 0 batch 0: avg loss -3.189374 avg loss no lamb -3.189374 time 2019-02-28 12:08:10.140893
Model ind 640 epoch 943 head A head_i_epoch 0 batch 100: avg loss -3.143228 avg loss no lamb -3.143228 time 2019-02-28 12:11:27.948448
Model ind 640 epoch 943 head A head_i_epoch 0 batch 200: avg loss -3.152591 avg loss no lamb -3.152591 time 2019-02-28 12:14:46.531676
last batch sz 160
Model ind 640 epoch 943 head B head_i_epoch 0 batch 0: avg loss -1.842478 avg loss no lamb -1.842478 time 2019-02-28 12:17:18.720319
Model ind 640 epoch 943 head B head_i_epoch 0 batch 100: avg loss -1.773987 avg loss no lamb -1.773987 time 2019-02-28 12:20:34.027281
Model ind 640 epoch 943 head B head_i_epoch 0 batch 200: avg loss -1.901066 avg loss no lamb -1.901066 time 2019-02-28 12:23:51.589565
last batch sz 160
Model ind 640 epoch 943 head B head_i_epoch 1 batch 0: avg loss -1.853470 avg loss no lamb -1.853470 time 2019-02-28 12:26:11.138368
Model ind 640 epoch 943 head B head_i_epoch 1 batch 100: avg loss -1.794155 avg loss no lamb -1.794155 time 2019-02-28 12:29:20.539566
Model ind 640 epoch 943 head B head_i_epoch 1 batch 200: avg loss -1.950689 avg loss no lamb -1.950689 time 2019-02-28 12:32:34.434388
last batch sz 160
Pre: time 2019-02-28 12:35:36.451106: 
 	std: 0.050675925
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51068336, 0.6139333, 0.61401665, 0.61405, 0.5104333]
	train_accs: [0.51068336, 0.6139333, 0.61401665, 0.61405, 0.5104333]
	best_train_sub_head: 3
	worst: 0.5104333
	avg: 0.5726234
	best: 0.61405

Starting e_i: 944
Model ind 640 epoch 944 head A head_i_epoch 0 batch 0: avg loss -3.239197 avg loss no lamb -3.239197 time 2019-02-28 12:35:41.520117
Model ind 640 epoch 944 head A head_i_epoch 0 batch 100: avg loss -3.171868 avg loss no lamb -3.171868 time 2019-02-28 12:39:00.143703
Model ind 640 epoch 944 head A head_i_epoch 0 batch 200: avg loss -3.206376 avg loss no lamb -3.206376 time 2019-02-28 12:42:25.317326
last batch sz 160
Model ind 640 epoch 944 head B head_i_epoch 0 batch 0: avg loss -1.841828 avg loss no lamb -1.841828 time 2019-02-28 12:44:54.183767
Model ind 640 epoch 944 head B head_i_epoch 0 batch 100: avg loss -1.925610 avg loss no lamb -1.925610 time 2019-02-28 12:48:16.538183
Model ind 640 epoch 944 head B head_i_epoch 0 batch 200: avg loss -1.908245 avg loss no lamb -1.908245 time 2019-02-28 12:51:37.646012
last batch sz 160
Model ind 640 epoch 944 head B head_i_epoch 1 batch 0: avg loss -1.931012 avg loss no lamb -1.931012 time 2019-02-28 12:53:55.705774
Model ind 640 epoch 944 head B head_i_epoch 1 batch 100: avg loss -1.821899 avg loss no lamb -1.821899 time 2019-02-28 12:57:11.612337
Model ind 640 epoch 944 head B head_i_epoch 1 batch 200: avg loss -1.926341 avg loss no lamb -1.926341 time 2019-02-28 13:00:25.726133
last batch sz 160
Pre: time 2019-02-28 13:03:23.106555: 
 	std: 0.051020175
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50951666, 0.6135333, 0.61366665, 0.61368334, 0.50945]
	train_accs: [0.50951666, 0.6135333, 0.61366665, 0.61368334, 0.50945]
	best_train_sub_head: 3
	worst: 0.50945
	avg: 0.57197
	best: 0.61368334

Starting e_i: 945
Model ind 640 epoch 945 head A head_i_epoch 0 batch 0: avg loss -3.233902 avg loss no lamb -3.233902 time 2019-02-28 13:03:29.532548
Model ind 640 epoch 945 head A head_i_epoch 0 batch 100: avg loss -3.147234 avg loss no lamb -3.147234 time 2019-02-28 13:06:44.343010
Model ind 640 epoch 945 head A head_i_epoch 0 batch 200: avg loss -3.225124 avg loss no lamb -3.225124 time 2019-02-28 13:09:58.035207
last batch sz 160
Model ind 640 epoch 945 head B head_i_epoch 0 batch 0: avg loss -1.922929 avg loss no lamb -1.922929 time 2019-02-28 13:12:08.893321
Model ind 640 epoch 945 head B head_i_epoch 0 batch 100: avg loss -1.901765 avg loss no lamb -1.901765 time 2019-02-28 13:15:24.068276
Model ind 640 epoch 945 head B head_i_epoch 0 batch 200: avg loss -1.941461 avg loss no lamb -1.941461 time 2019-02-28 13:18:38.186316
last batch sz 160
Model ind 640 epoch 945 head B head_i_epoch 1 batch 0: avg loss -1.903764 avg loss no lamb -1.903764 time 2019-02-28 13:21:09.050985
Model ind 640 epoch 945 head B head_i_epoch 1 batch 100: avg loss -1.815924 avg loss no lamb -1.815924 time 2019-02-28 13:24:28.543402
Model ind 640 epoch 945 head B head_i_epoch 1 batch 200: avg loss -1.937437 avg loss no lamb -1.937437 time 2019-02-28 13:27:48.542274
last batch sz 160
Pre: time 2019-02-28 13:30:57.175196: 
 	std: 0.05029356
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5121, 0.6146, 0.61483335, 0.61465, 0.51196665]
	train_accs: [0.5121, 0.6146, 0.61483335, 0.61465, 0.51196665]
	best_train_sub_head: 2
	worst: 0.51196665
	avg: 0.57363
	best: 0.61483335

Starting e_i: 946
Model ind 640 epoch 946 head A head_i_epoch 0 batch 0: avg loss -3.246547 avg loss no lamb -3.246547 time 2019-02-28 13:31:04.206694
Model ind 640 epoch 946 head A head_i_epoch 0 batch 100: avg loss -3.148624 avg loss no lamb -3.148624 time 2019-02-28 13:34:23.620694
Model ind 640 epoch 946 head A head_i_epoch 0 batch 200: avg loss -3.184314 avg loss no lamb -3.184314 time 2019-02-28 13:37:37.129594
last batch sz 160
Model ind 640 epoch 946 head B head_i_epoch 0 batch 0: avg loss -1.885266 avg loss no lamb -1.885266 time 2019-02-28 13:40:04.640799
Model ind 640 epoch 946 head B head_i_epoch 0 batch 100: avg loss -1.736362 avg loss no lamb -1.736362 time 2019-02-28 13:43:21.315348
Model ind 640 epoch 946 head B head_i_epoch 0 batch 200: avg loss -1.922758 avg loss no lamb -1.922758 time 2019-02-28 13:46:41.239630
last batch sz 160
Model ind 640 epoch 946 head B head_i_epoch 1 batch 0: avg loss -1.909752 avg loss no lamb -1.909752 time 2019-02-28 13:49:02.919529
Model ind 640 epoch 946 head B head_i_epoch 1 batch 100: avg loss -1.697411 avg loss no lamb -1.697411 time 2019-02-28 13:52:19.293586
Model ind 640 epoch 946 head B head_i_epoch 1 batch 200: avg loss -1.923104 avg loss no lamb -1.923104 time 2019-02-28 13:55:29.571383
last batch sz 160
Pre: time 2019-02-28 13:58:32.127925: 
 	std: 0.05086238
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5096667, 0.6134167, 0.61328334, 0.61356664, 0.50953335]
	train_accs: [0.5096667, 0.6134167, 0.61328334, 0.61356664, 0.50953335]
	best_train_sub_head: 3
	worst: 0.50953335
	avg: 0.57189333
	best: 0.61356664

Starting e_i: 947
Model ind 640 epoch 947 head A head_i_epoch 0 batch 0: avg loss -3.190098 avg loss no lamb -3.190098 time 2019-02-28 13:58:37.012516
Model ind 640 epoch 947 head A head_i_epoch 0 batch 100: avg loss -3.186159 avg loss no lamb -3.186159 time 2019-02-28 14:01:56.234540
Model ind 640 epoch 947 head A head_i_epoch 0 batch 200: avg loss -3.305206 avg loss no lamb -3.305206 time 2019-02-28 14:05:17.294876
last batch sz 160
Model ind 640 epoch 947 head B head_i_epoch 0 batch 0: avg loss -1.934511 avg loss no lamb -1.934511 time 2019-02-28 14:07:45.307838
Model ind 640 epoch 947 head B head_i_epoch 0 batch 100: avg loss -1.811174 avg loss no lamb -1.811174 time 2019-02-28 14:11:07.978166
Model ind 640 epoch 947 head B head_i_epoch 0 batch 200: avg loss -1.959003 avg loss no lamb -1.959003 time 2019-02-28 14:14:33.742610
last batch sz 160
Model ind 640 epoch 947 head B head_i_epoch 1 batch 0: avg loss -1.842547 avg loss no lamb -1.842547 time 2019-02-28 14:17:03.598926
Model ind 640 epoch 947 head B head_i_epoch 1 batch 100: avg loss -1.788862 avg loss no lamb -1.788862 time 2019-02-28 14:20:31.812549
Model ind 640 epoch 947 head B head_i_epoch 1 batch 200: avg loss -1.879371 avg loss no lamb -1.879371 time 2019-02-28 14:24:00.875486
last batch sz 160
Pre: time 2019-02-28 14:27:12.518734: 
 	std: 0.0512774
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103833, 0.6149667, 0.6149667, 0.6151, 0.5103]
	train_accs: [0.5103833, 0.6149667, 0.6149667, 0.6151, 0.5103]
	best_train_sub_head: 3
	worst: 0.5103
	avg: 0.57314336
	best: 0.6151

Starting e_i: 948
Model ind 640 epoch 948 head A head_i_epoch 0 batch 0: avg loss -3.290686 avg loss no lamb -3.290686 time 2019-02-28 14:27:22.313936
Model ind 640 epoch 948 head A head_i_epoch 0 batch 100: avg loss -3.166992 avg loss no lamb -3.166992 time 2019-02-28 14:30:49.993714
Model ind 640 epoch 948 head A head_i_epoch 0 batch 200: avg loss -3.155812 avg loss no lamb -3.155812 time 2019-02-28 14:34:11.976515
last batch sz 160
Model ind 640 epoch 948 head B head_i_epoch 0 batch 0: avg loss -1.872548 avg loss no lamb -1.872548 time 2019-02-28 14:36:43.707184
Model ind 640 epoch 948 head B head_i_epoch 0 batch 100: avg loss -1.819845 avg loss no lamb -1.819845 time 2019-02-28 14:39:58.171852
Model ind 640 epoch 948 head B head_i_epoch 0 batch 200: avg loss -1.978968 avg loss no lamb -1.978968 time 2019-02-28 14:43:14.653580
last batch sz 160
Model ind 640 epoch 948 head B head_i_epoch 1 batch 0: avg loss -1.846038 avg loss no lamb -1.846038 time 2019-02-28 14:45:33.196319
Model ind 640 epoch 948 head B head_i_epoch 1 batch 100: avg loss -1.844322 avg loss no lamb -1.844322 time 2019-02-28 14:48:47.386723
Model ind 640 epoch 948 head B head_i_epoch 1 batch 200: avg loss -1.932277 avg loss no lamb -1.932277 time 2019-02-28 14:52:01.620741
last batch sz 160
Pre: time 2019-02-28 14:54:59.535596: 
 	std: 0.050420083
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51243335, 0.61523336, 0.6152167, 0.6153833, 0.5122833]
	train_accs: [0.51243335, 0.61523336, 0.6152167, 0.6153833, 0.5122833]
	best_train_sub_head: 3
	worst: 0.5122833
	avg: 0.57411003
	best: 0.6153833

Starting e_i: 949
Model ind 640 epoch 949 head A head_i_epoch 0 batch 0: avg loss -3.215600 avg loss no lamb -3.215600 time 2019-02-28 14:55:04.675926
Model ind 640 epoch 949 head A head_i_epoch 0 batch 100: avg loss -3.083095 avg loss no lamb -3.083095 time 2019-02-28 14:58:23.714290
Model ind 640 epoch 949 head A head_i_epoch 0 batch 200: avg loss -3.175191 avg loss no lamb -3.175191 time 2019-02-28 15:01:41.584623
last batch sz 160
Model ind 640 epoch 949 head B head_i_epoch 0 batch 0: avg loss -1.925203 avg loss no lamb -1.925203 time 2019-02-28 15:04:09.404710
Model ind 640 epoch 949 head B head_i_epoch 0 batch 100: avg loss -1.759798 avg loss no lamb -1.759798 time 2019-02-28 15:07:29.275230
Model ind 640 epoch 949 head B head_i_epoch 0 batch 200: avg loss -1.958360 avg loss no lamb -1.958360 time 2019-02-28 15:10:35.335942
last batch sz 160
Model ind 640 epoch 949 head B head_i_epoch 1 batch 0: avg loss -1.923347 avg loss no lamb -1.923347 time 2019-02-28 15:13:03.895102
Model ind 640 epoch 949 head B head_i_epoch 1 batch 100: avg loss -1.672279 avg loss no lamb -1.672279 time 2019-02-28 15:16:03.173625
Model ind 640 epoch 949 head B head_i_epoch 1 batch 200: avg loss -1.887527 avg loss no lamb -1.887527 time 2019-02-28 15:18:56.148580
last batch sz 160
Pre: time 2019-02-28 15:22:13.068441: 
 	std: 0.05082425
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50986665, 0.61363333, 0.6137667, 0.61373335, 0.5100667]
	train_accs: [0.50986665, 0.61363333, 0.6137667, 0.61373335, 0.5100667]
	best_train_sub_head: 2
	worst: 0.50986665
	avg: 0.57221335
	best: 0.6137667

Starting e_i: 950
Model ind 640 epoch 950 head A head_i_epoch 0 batch 0: avg loss -3.226960 avg loss no lamb -3.226960 time 2019-02-28 15:22:17.326280
Model ind 640 epoch 950 head A head_i_epoch 0 batch 100: avg loss -3.086639 avg loss no lamb -3.086639 time 2019-02-28 15:24:36.251418
Model ind 640 epoch 950 head A head_i_epoch 0 batch 200: avg loss -3.247190 avg loss no lamb -3.247190 time 2019-02-28 15:26:48.755359
last batch sz 160
Model ind 640 epoch 950 head B head_i_epoch 0 batch 0: avg loss -1.939666 avg loss no lamb -1.939666 time 2019-02-28 15:28:29.708766
Model ind 640 epoch 950 head B head_i_epoch 0 batch 100: avg loss -1.805044 avg loss no lamb -1.805044 time 2019-02-28 15:30:40.940644
Model ind 640 epoch 950 head B head_i_epoch 0 batch 200: avg loss -1.895609 avg loss no lamb -1.895609 time 2019-02-28 15:32:47.710194
last batch sz 160
Model ind 640 epoch 950 head B head_i_epoch 1 batch 0: avg loss -1.955332 avg loss no lamb -1.955332 time 2019-02-28 15:34:28.295053
Model ind 640 epoch 950 head B head_i_epoch 1 batch 100: avg loss -1.851929 avg loss no lamb -1.851929 time 2019-02-28 15:36:40.124501
Model ind 640 epoch 950 head B head_i_epoch 1 batch 200: avg loss -1.922461 avg loss no lamb -1.922461 time 2019-02-28 15:38:38.582122
last batch sz 160
Pre: time 2019-02-28 15:40:35.381949: 
 	std: 0.050988924
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51086664, 0.6149667, 0.61501664, 0.61513335, 0.51105]
	train_accs: [0.51086664, 0.6149667, 0.61501664, 0.61513335, 0.51105]
	best_train_sub_head: 3
	worst: 0.51086664
	avg: 0.57340664
	best: 0.61513335

Starting e_i: 951
Model ind 640 epoch 951 head A head_i_epoch 0 batch 0: avg loss -3.184029 avg loss no lamb -3.184029 time 2019-02-28 15:40:42.203794
Model ind 640 epoch 951 head A head_i_epoch 0 batch 100: avg loss -3.171750 avg loss no lamb -3.171750 time 2019-02-28 15:43:33.600514
Model ind 640 epoch 951 head A head_i_epoch 0 batch 200: avg loss -3.222162 avg loss no lamb -3.222162 time 2019-02-28 15:46:28.203871
last batch sz 160
Model ind 640 epoch 951 head B head_i_epoch 0 batch 0: avg loss -1.935689 avg loss no lamb -1.935689 time 2019-02-28 15:48:33.722410
Model ind 640 epoch 951 head B head_i_epoch 0 batch 100: avg loss -1.856372 avg loss no lamb -1.856372 time 2019-02-28 15:51:28.922213
Model ind 640 epoch 951 head B head_i_epoch 0 batch 200: avg loss -1.846542 avg loss no lamb -1.846542 time 2019-02-28 15:54:30.606665
last batch sz 160
Model ind 640 epoch 951 head B head_i_epoch 1 batch 0: avg loss -1.957552 avg loss no lamb -1.957552 time 2019-02-28 15:56:38.378312
Model ind 640 epoch 951 head B head_i_epoch 1 batch 100: avg loss -1.787730 avg loss no lamb -1.787730 time 2019-02-28 15:59:43.140622
Model ind 640 epoch 951 head B head_i_epoch 1 batch 200: avg loss -1.925401 avg loss no lamb -1.925401 time 2019-02-28 16:02:33.995800
last batch sz 160
Pre: time 2019-02-28 16:05:19.295824: 
 	std: 0.050846003
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51133335, 0.61518335, 0.61505, 0.6150333, 0.51126665]
	train_accs: [0.51133335, 0.61518335, 0.61505, 0.6150333, 0.51126665]
	best_train_sub_head: 1
	worst: 0.51126665
	avg: 0.57357335
	best: 0.61518335

Starting e_i: 952
Model ind 640 epoch 952 head A head_i_epoch 0 batch 0: avg loss -3.256477 avg loss no lamb -3.256477 time 2019-02-28 16:05:26.945033
Model ind 640 epoch 952 head A head_i_epoch 0 batch 100: avg loss -3.135607 avg loss no lamb -3.135607 time 2019-02-28 16:08:29.488975
Model ind 640 epoch 952 head A head_i_epoch 0 batch 200: avg loss -3.226000 avg loss no lamb -3.226000 time 2019-02-28 16:11:25.663148
last batch sz 160
Model ind 640 epoch 952 head B head_i_epoch 0 batch 0: avg loss -1.995736 avg loss no lamb -1.995736 time 2019-02-28 16:13:35.853493
Model ind 640 epoch 952 head B head_i_epoch 0 batch 100: avg loss -1.830137 avg loss no lamb -1.830137 time 2019-02-28 16:16:40.898893
Model ind 640 epoch 952 head B head_i_epoch 0 batch 200: avg loss -1.876689 avg loss no lamb -1.876689 time 2019-02-28 16:19:43.449396
last batch sz 160
Model ind 640 epoch 952 head B head_i_epoch 1 batch 0: avg loss -1.962653 avg loss no lamb -1.962653 time 2019-02-28 16:22:13.594433
Model ind 640 epoch 952 head B head_i_epoch 1 batch 100: avg loss -1.825539 avg loss no lamb -1.825539 time 2019-02-28 16:25:22.968044
Model ind 640 epoch 952 head B head_i_epoch 1 batch 200: avg loss -1.948496 avg loss no lamb -1.948496 time 2019-02-28 16:28:29.260454
last batch sz 160
Pre: time 2019-02-28 16:31:12.489668: 
 	std: 0.050504413
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114667, 0.6145833, 0.61445, 0.6145667, 0.5114167]
	train_accs: [0.5114667, 0.6145833, 0.61445, 0.6145667, 0.5114167]
	best_train_sub_head: 1
	worst: 0.5114167
	avg: 0.57329667
	best: 0.6145833

Starting e_i: 953
Model ind 640 epoch 953 head A head_i_epoch 0 batch 0: avg loss -3.249788 avg loss no lamb -3.249788 time 2019-02-28 16:31:17.021121
Model ind 640 epoch 953 head A head_i_epoch 0 batch 100: avg loss -3.156384 avg loss no lamb -3.156384 time 2019-02-28 16:34:07.797192
Model ind 640 epoch 953 head A head_i_epoch 0 batch 200: avg loss -3.263838 avg loss no lamb -3.263838 time 2019-02-28 16:37:01.589815
last batch sz 160
Model ind 640 epoch 953 head B head_i_epoch 0 batch 0: avg loss -1.854634 avg loss no lamb -1.854634 time 2019-02-28 16:39:10.031996
Model ind 640 epoch 953 head B head_i_epoch 0 batch 100: avg loss -1.846915 avg loss no lamb -1.846915 time 2019-02-28 16:42:13.734944
Model ind 640 epoch 953 head B head_i_epoch 0 batch 200: avg loss -1.931111 avg loss no lamb -1.931111 time 2019-02-28 16:45:11.423852
last batch sz 160
Model ind 640 epoch 953 head B head_i_epoch 1 batch 0: avg loss -1.876526 avg loss no lamb -1.876526 time 2019-02-28 16:47:16.147000
Model ind 640 epoch 953 head B head_i_epoch 1 batch 100: avg loss -1.848799 avg loss no lamb -1.848799 time 2019-02-28 16:50:09.474770
Model ind 640 epoch 953 head B head_i_epoch 1 batch 200: avg loss -1.927462 avg loss no lamb -1.927462 time 2019-02-28 16:53:06.583226
last batch sz 160
Pre: time 2019-02-28 16:55:50.777674: 
 	std: 0.05084598
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111167, 0.6149, 0.6149167, 0.6148, 0.51105]
	train_accs: [0.5111167, 0.6149, 0.6149167, 0.6148, 0.51105]
	best_train_sub_head: 2
	worst: 0.51105
	avg: 0.5733567
	best: 0.6149167

Starting e_i: 954
Model ind 640 epoch 954 head A head_i_epoch 0 batch 0: avg loss -3.226476 avg loss no lamb -3.226476 time 2019-02-28 16:55:54.892210
Model ind 640 epoch 954 head A head_i_epoch 0 batch 100: avg loss -3.207551 avg loss no lamb -3.207551 time 2019-02-28 16:58:51.956226
Model ind 640 epoch 954 head A head_i_epoch 0 batch 200: avg loss -3.187439 avg loss no lamb -3.187439 time 2019-02-28 17:01:45.510285
last batch sz 160
Model ind 640 epoch 954 head B head_i_epoch 0 batch 0: avg loss -1.960680 avg loss no lamb -1.960680 time 2019-02-28 17:03:55.369029
Model ind 640 epoch 954 head B head_i_epoch 0 batch 100: avg loss -1.867058 avg loss no lamb -1.867058 time 2019-02-28 17:06:50.675217
Model ind 640 epoch 954 head B head_i_epoch 0 batch 200: avg loss -1.970286 avg loss no lamb -1.970286 time 2019-02-28 17:09:46.122349
last batch sz 160
Model ind 640 epoch 954 head B head_i_epoch 1 batch 0: avg loss -2.005876 avg loss no lamb -2.005876 time 2019-02-28 17:11:46.094196
Model ind 640 epoch 954 head B head_i_epoch 1 batch 100: avg loss -1.760539 avg loss no lamb -1.760539 time 2019-02-28 17:14:40.745286
Model ind 640 epoch 954 head B head_i_epoch 1 batch 200: avg loss -1.854831 avg loss no lamb -1.854831 time 2019-02-28 17:17:35.238887
last batch sz 160
Pre: time 2019-02-28 17:20:05.962805: 
 	std: 0.05027595
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5125, 0.61485, 0.61495, 0.6151, 0.5121833]
	train_accs: [0.5125, 0.61485, 0.61495, 0.6151, 0.5121833]
	best_train_sub_head: 3
	worst: 0.5121833
	avg: 0.5739166
	best: 0.6151

Starting e_i: 955
Model ind 640 epoch 955 head A head_i_epoch 0 batch 0: avg loss -3.252992 avg loss no lamb -3.252992 time 2019-02-28 17:20:10.180028
Model ind 640 epoch 955 head A head_i_epoch 0 batch 100: avg loss -3.178135 avg loss no lamb -3.178135 time 2019-02-28 17:23:10.577246
Model ind 640 epoch 955 head A head_i_epoch 0 batch 200: avg loss -3.267199 avg loss no lamb -3.267199 time 2019-02-28 17:26:10.839591
last batch sz 160
Model ind 640 epoch 955 head B head_i_epoch 0 batch 0: avg loss -1.902091 avg loss no lamb -1.902091 time 2019-02-28 17:28:28.137035
Model ind 640 epoch 955 head B head_i_epoch 0 batch 100: avg loss -1.859326 avg loss no lamb -1.859326 time 2019-02-28 17:31:24.995409
Model ind 640 epoch 955 head B head_i_epoch 0 batch 200: avg loss -1.927790 avg loss no lamb -1.927790 time 2019-02-28 17:34:18.239486
last batch sz 160
Model ind 640 epoch 955 head B head_i_epoch 1 batch 0: avg loss -1.876201 avg loss no lamb -1.876201 time 2019-02-28 17:36:29.284721
Model ind 640 epoch 955 head B head_i_epoch 1 batch 100: avg loss -1.797001 avg loss no lamb -1.797001 time 2019-02-28 17:39:25.820017
Model ind 640 epoch 955 head B head_i_epoch 1 batch 200: avg loss -1.923423 avg loss no lamb -1.923423 time 2019-02-28 17:42:21.446326
last batch sz 160
Pre: time 2019-02-28 17:45:03.387194: 
 	std: 0.05076591
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51125, 0.6145833, 0.61481667, 0.6146, 0.5108333]
	train_accs: [0.51125, 0.6145833, 0.61481667, 0.6146, 0.5108333]
	best_train_sub_head: 2
	worst: 0.5108333
	avg: 0.5732166
	best: 0.61481667

Starting e_i: 956
Model ind 640 epoch 956 head A head_i_epoch 0 batch 0: avg loss -3.319796 avg loss no lamb -3.319796 time 2019-02-28 17:45:08.567291
Model ind 640 epoch 956 head A head_i_epoch 0 batch 100: avg loss -3.146610 avg loss no lamb -3.146610 time 2019-02-28 17:48:06.820767
Model ind 640 epoch 956 head A head_i_epoch 0 batch 200: avg loss -3.240754 avg loss no lamb -3.240754 time 2019-02-28 17:51:07.978866
last batch sz 160
Model ind 640 epoch 956 head B head_i_epoch 0 batch 0: avg loss -1.939077 avg loss no lamb -1.939077 time 2019-02-28 17:53:27.462341
Model ind 640 epoch 956 head B head_i_epoch 0 batch 100: avg loss -1.819949 avg loss no lamb -1.819949 time 2019-02-28 17:56:29.764199
Model ind 640 epoch 956 head B head_i_epoch 0 batch 200: avg loss -1.908696 avg loss no lamb -1.908696 time 2019-02-28 17:59:28.567673
last batch sz 160
Model ind 640 epoch 956 head B head_i_epoch 1 batch 0: avg loss -1.834304 avg loss no lamb -1.834304 time 2019-02-28 18:01:35.836274
Model ind 640 epoch 956 head B head_i_epoch 1 batch 100: avg loss -1.903436 avg loss no lamb -1.903436 time 2019-02-28 18:04:31.249548
Model ind 640 epoch 956 head B head_i_epoch 1 batch 200: avg loss -1.955024 avg loss no lamb -1.955024 time 2019-02-28 18:07:33.740344
last batch sz 160
Pre: time 2019-02-28 18:10:26.798596: 
 	std: 0.050272208
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5128833, 0.61508334, 0.6152, 0.61516666, 0.5121833]
	train_accs: [0.5128833, 0.61508334, 0.6152, 0.61516666, 0.5121833]
	best_train_sub_head: 2
	worst: 0.5121833
	avg: 0.57410336
	best: 0.6152

Starting e_i: 957
Model ind 640 epoch 957 head A head_i_epoch 0 batch 0: avg loss -3.240420 avg loss no lamb -3.240420 time 2019-02-28 18:10:30.043288
Model ind 640 epoch 957 head A head_i_epoch 0 batch 100: avg loss -3.102499 avg loss no lamb -3.102499 time 2019-02-28 18:13:37.878537
Model ind 640 epoch 957 head A head_i_epoch 0 batch 200: avg loss -3.266669 avg loss no lamb -3.266669 time 2019-02-28 18:16:44.544471
last batch sz 160
Model ind 640 epoch 957 head B head_i_epoch 0 batch 0: avg loss -1.901715 avg loss no lamb -1.901715 time 2019-02-28 18:18:59.172755
Model ind 640 epoch 957 head B head_i_epoch 0 batch 100: avg loss -1.854778 avg loss no lamb -1.854778 time 2019-02-28 18:22:00.127560
Model ind 640 epoch 957 head B head_i_epoch 0 batch 200: avg loss -1.922869 avg loss no lamb -1.922869 time 2019-02-28 18:24:58.352019
last batch sz 160
Model ind 640 epoch 957 head B head_i_epoch 1 batch 0: avg loss -1.929291 avg loss no lamb -1.929291 time 2019-02-28 18:27:08.833185
Model ind 640 epoch 957 head B head_i_epoch 1 batch 100: avg loss -1.820754 avg loss no lamb -1.820754 time 2019-02-28 18:30:09.218937
Model ind 640 epoch 957 head B head_i_epoch 1 batch 200: avg loss -1.932873 avg loss no lamb -1.932873 time 2019-02-28 18:33:14.691499
last batch sz 160
Pre: time 2019-02-28 18:36:11.958328: 
 	std: 0.050722424
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51238334, 0.6156333, 0.6156667, 0.6156333, 0.5118333]
	train_accs: [0.51238334, 0.6156333, 0.6156667, 0.6156333, 0.5118333]
	best_train_sub_head: 2
	worst: 0.5118333
	avg: 0.57423
	best: 0.6156667

Starting e_i: 958
Model ind 640 epoch 958 head A head_i_epoch 0 batch 0: avg loss -3.182550 avg loss no lamb -3.182550 time 2019-02-28 18:36:19.913515
Model ind 640 epoch 958 head A head_i_epoch 0 batch 100: avg loss -3.193638 avg loss no lamb -3.193638 time 2019-02-28 18:39:29.808147
Model ind 640 epoch 958 head A head_i_epoch 0 batch 200: avg loss -3.222578 avg loss no lamb -3.222578 time 2019-02-28 18:42:42.745651
last batch sz 160
Model ind 640 epoch 958 head B head_i_epoch 0 batch 0: avg loss -1.944915 avg loss no lamb -1.944915 time 2019-02-28 18:44:51.184488
Model ind 640 epoch 958 head B head_i_epoch 0 batch 100: avg loss -1.766074 avg loss no lamb -1.766074 time 2019-02-28 18:47:53.403490
Model ind 640 epoch 958 head B head_i_epoch 0 batch 200: avg loss -1.945549 avg loss no lamb -1.945549 time 2019-02-28 18:50:51.622409
last batch sz 160
Model ind 640 epoch 958 head B head_i_epoch 1 batch 0: avg loss -1.969740 avg loss no lamb -1.969740 time 2019-02-28 18:53:04.180867
Model ind 640 epoch 958 head B head_i_epoch 1 batch 100: avg loss -1.937752 avg loss no lamb -1.937752 time 2019-02-28 18:56:05.885115
Model ind 640 epoch 958 head B head_i_epoch 1 batch 200: avg loss -1.904590 avg loss no lamb -1.904590 time 2019-02-28 18:59:08.272517
last batch sz 160
Pre: time 2019-02-28 19:01:52.879240: 
 	std: 0.051133152
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5099667, 0.61446667, 0.6142833, 0.61425, 0.50995]
	train_accs: [0.5099667, 0.61446667, 0.6142833, 0.61425, 0.50995]
	best_train_sub_head: 1
	worst: 0.50995
	avg: 0.5725833
	best: 0.61446667

Starting e_i: 959
Model ind 640 epoch 959 head A head_i_epoch 0 batch 0: avg loss -3.248902 avg loss no lamb -3.248902 time 2019-02-28 19:02:01.750015
Model ind 640 epoch 959 head A head_i_epoch 0 batch 100: avg loss -3.174242 avg loss no lamb -3.174242 time 2019-02-28 19:05:06.769765
Model ind 640 epoch 959 head A head_i_epoch 0 batch 200: avg loss -3.193348 avg loss no lamb -3.193348 time 2019-02-28 19:08:12.355981
last batch sz 160
Model ind 640 epoch 959 head B head_i_epoch 0 batch 0: avg loss -1.924465 avg loss no lamb -1.924465 time 2019-02-28 19:10:33.292281
Model ind 640 epoch 959 head B head_i_epoch 0 batch 100: avg loss -1.794382 avg loss no lamb -1.794382 time 2019-02-28 19:13:32.376259
Model ind 640 epoch 959 head B head_i_epoch 0 batch 200: avg loss -1.945442 avg loss no lamb -1.945442 time 2019-02-28 19:16:25.985639
last batch sz 160
Model ind 640 epoch 959 head B head_i_epoch 1 batch 0: avg loss -1.912927 avg loss no lamb -1.912927 time 2019-02-28 19:18:27.865004
Model ind 640 epoch 959 head B head_i_epoch 1 batch 100: avg loss -1.820040 avg loss no lamb -1.820040 time 2019-02-28 19:21:24.450664
Model ind 640 epoch 959 head B head_i_epoch 1 batch 200: avg loss -1.936127 avg loss no lamb -1.936127 time 2019-02-28 19:24:24.419009
last batch sz 160
Pre: time 2019-02-28 19:27:15.490019: 
 	std: 0.051063716
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51093334, 0.61506665, 0.61516666, 0.61506665, 0.5108]
	train_accs: [0.51093334, 0.61506665, 0.61516666, 0.61506665, 0.5108]
	best_train_sub_head: 2
	worst: 0.5108
	avg: 0.5734066
	best: 0.61516666

Starting e_i: 960
Model ind 640 epoch 960 head A head_i_epoch 0 batch 0: avg loss -3.278805 avg loss no lamb -3.278805 time 2019-02-28 19:27:20.790141
Model ind 640 epoch 960 head A head_i_epoch 0 batch 100: avg loss -3.194215 avg loss no lamb -3.194215 time 2019-02-28 19:30:26.611724
Model ind 640 epoch 960 head A head_i_epoch 0 batch 200: avg loss -3.280130 avg loss no lamb -3.280130 time 2019-02-28 19:33:27.380159
last batch sz 160
Model ind 640 epoch 960 head B head_i_epoch 0 batch 0: avg loss -1.855221 avg loss no lamb -1.855221 time 2019-02-28 19:35:25.830590
Model ind 640 epoch 960 head B head_i_epoch 0 batch 100: avg loss -1.812999 avg loss no lamb -1.812999 time 2019-02-28 19:38:27.240789
Model ind 640 epoch 960 head B head_i_epoch 0 batch 200: avg loss -1.896663 avg loss no lamb -1.896663 time 2019-02-28 19:41:25.169094
last batch sz 160
Model ind 640 epoch 960 head B head_i_epoch 1 batch 0: avg loss -1.918543 avg loss no lamb -1.918543 time 2019-02-28 19:43:36.423258
Model ind 640 epoch 960 head B head_i_epoch 1 batch 100: avg loss -1.856918 avg loss no lamb -1.856918 time 2019-02-28 19:46:34.709089
Model ind 640 epoch 960 head B head_i_epoch 1 batch 200: avg loss -1.925900 avg loss no lamb -1.925900 time 2019-02-28 19:49:34.762918
last batch sz 160
Pre: time 2019-02-28 19:52:23.731083: 
 	std: 0.0508324
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51066667, 0.61446667, 0.6145667, 0.6145, 0.5108333]
	train_accs: [0.51066667, 0.61446667, 0.6145667, 0.6145, 0.5108333]
	best_train_sub_head: 2
	worst: 0.51066667
	avg: 0.57300663
	best: 0.6145667

Starting e_i: 961
Model ind 640 epoch 961 head A head_i_epoch 0 batch 0: avg loss -3.226263 avg loss no lamb -3.226263 time 2019-02-28 19:52:32.832063
Model ind 640 epoch 961 head A head_i_epoch 0 batch 100: avg loss -3.183641 avg loss no lamb -3.183641 time 2019-02-28 19:55:36.205229
Model ind 640 epoch 961 head A head_i_epoch 0 batch 200: avg loss -3.260634 avg loss no lamb -3.260634 time 2019-02-28 19:58:38.576056
last batch sz 160
Model ind 640 epoch 961 head B head_i_epoch 0 batch 0: avg loss -1.975372 avg loss no lamb -1.975372 time 2019-02-28 20:00:57.613830
Model ind 640 epoch 961 head B head_i_epoch 0 batch 100: avg loss -1.934978 avg loss no lamb -1.934978 time 2019-02-28 20:04:01.429283
Model ind 640 epoch 961 head B head_i_epoch 0 batch 200: avg loss -1.889209 avg loss no lamb -1.889209 time 2019-02-28 20:07:04.557646
last batch sz 160
Model ind 640 epoch 961 head B head_i_epoch 1 batch 0: avg loss -1.880472 avg loss no lamb -1.880472 time 2019-02-28 20:09:00.992009
Model ind 640 epoch 961 head B head_i_epoch 1 batch 100: avg loss -1.873371 avg loss no lamb -1.873371 time 2019-02-28 20:11:59.668444
Model ind 640 epoch 961 head B head_i_epoch 1 batch 200: avg loss -1.918387 avg loss no lamb -1.918387 time 2019-02-28 20:14:49.833694
last batch sz 160
Pre: time 2019-02-28 20:17:35.269616: 
 	std: 0.0507318
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50983334, 0.6135, 0.61365, 0.6135167, 0.51016665]
	train_accs: [0.50983334, 0.6135, 0.61365, 0.6135167, 0.51016665]
	best_train_sub_head: 2
	worst: 0.50983334
	avg: 0.57213336
	best: 0.61365

Starting e_i: 962
Model ind 640 epoch 962 head A head_i_epoch 0 batch 0: avg loss -3.240510 avg loss no lamb -3.240510 time 2019-02-28 20:17:39.617525
Model ind 640 epoch 962 head A head_i_epoch 0 batch 100: avg loss -3.228637 avg loss no lamb -3.228637 time 2019-02-28 20:20:32.792325
Model ind 640 epoch 962 head A head_i_epoch 0 batch 200: avg loss -3.166747 avg loss no lamb -3.166747 time 2019-02-28 20:23:31.638459
last batch sz 160
Model ind 640 epoch 962 head B head_i_epoch 0 batch 0: avg loss -1.865011 avg loss no lamb -1.865011 time 2019-02-28 20:25:40.729982
Model ind 640 epoch 962 head B head_i_epoch 0 batch 100: avg loss -1.858239 avg loss no lamb -1.858239 time 2019-02-28 20:28:31.819161
Model ind 640 epoch 962 head B head_i_epoch 0 batch 200: avg loss -1.939231 avg loss no lamb -1.939231 time 2019-02-28 20:31:27.524220
last batch sz 160
Model ind 640 epoch 962 head B head_i_epoch 1 batch 0: avg loss -1.887740 avg loss no lamb -1.887740 time 2019-02-28 20:33:44.659316
Model ind 640 epoch 962 head B head_i_epoch 1 batch 100: avg loss -1.830542 avg loss no lamb -1.830542 time 2019-02-28 20:36:36.909558
Model ind 640 epoch 962 head B head_i_epoch 1 batch 200: avg loss -1.992146 avg loss no lamb -1.992146 time 2019-02-28 20:39:32.239212
last batch sz 160
Pre: time 2019-02-28 20:42:23.531851: 
 	std: 0.050712716
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51185, 0.6155667, 0.6153333, 0.61555, 0.51208335]
	train_accs: [0.51185, 0.6155667, 0.6153333, 0.61555, 0.51208335]
	best_train_sub_head: 1
	worst: 0.51185
	avg: 0.57407665
	best: 0.6155667

Starting e_i: 963
Model ind 640 epoch 963 head A head_i_epoch 0 batch 0: avg loss -3.280845 avg loss no lamb -3.280845 time 2019-02-28 20:42:28.655233
Model ind 640 epoch 963 head A head_i_epoch 0 batch 100: avg loss -3.120308 avg loss no lamb -3.120308 time 2019-02-28 20:45:29.147085
Model ind 640 epoch 963 head A head_i_epoch 0 batch 200: avg loss -3.219917 avg loss no lamb -3.219917 time 2019-02-28 20:48:30.270956
last batch sz 160
Model ind 640 epoch 963 head B head_i_epoch 0 batch 0: avg loss -1.905766 avg loss no lamb -1.905766 time 2019-02-28 20:50:30.129302
Model ind 640 epoch 963 head B head_i_epoch 0 batch 100: avg loss -1.882997 avg loss no lamb -1.882997 time 2019-02-28 20:53:29.666734
Model ind 640 epoch 963 head B head_i_epoch 0 batch 200: avg loss -1.898490 avg loss no lamb -1.898490 time 2019-02-28 20:56:25.630930
last batch sz 160
Model ind 640 epoch 963 head B head_i_epoch 1 batch 0: avg loss -1.826195 avg loss no lamb -1.826195 time 2019-02-28 20:58:33.050712
Model ind 640 epoch 963 head B head_i_epoch 1 batch 100: avg loss -1.801133 avg loss no lamb -1.801133 time 2019-02-28 21:01:40.986042
Model ind 640 epoch 963 head B head_i_epoch 1 batch 200: avg loss -1.952597 avg loss no lamb -1.952597 time 2019-02-28 21:04:47.107633
last batch sz 160
Pre: time 2019-02-28 21:07:40.880787: 
 	std: 0.0506787
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51055, 0.61375, 0.6138667, 0.61395, 0.51026666]
	train_accs: [0.51055, 0.61375, 0.6138667, 0.61395, 0.51026666]
	best_train_sub_head: 3
	worst: 0.51026666
	avg: 0.5724767
	best: 0.61395

Starting e_i: 964
Model ind 640 epoch 964 head A head_i_epoch 0 batch 0: avg loss -3.287174 avg loss no lamb -3.287174 time 2019-02-28 21:07:44.870481
Model ind 640 epoch 964 head A head_i_epoch 0 batch 100: avg loss -3.086410 avg loss no lamb -3.086410 time 2019-02-28 21:10:43.150262
Model ind 640 epoch 964 head A head_i_epoch 0 batch 200: avg loss -3.317057 avg loss no lamb -3.317057 time 2019-02-28 21:13:42.855107
last batch sz 160
Model ind 640 epoch 964 head B head_i_epoch 0 batch 0: avg loss -1.990248 avg loss no lamb -1.990248 time 2019-02-28 21:15:49.042622
Model ind 640 epoch 964 head B head_i_epoch 0 batch 100: avg loss -1.797537 avg loss no lamb -1.797537 time 2019-02-28 21:18:48.290617
Model ind 640 epoch 964 head B head_i_epoch 0 batch 200: avg loss -1.961856 avg loss no lamb -1.961856 time 2019-02-28 21:21:49.766355
last batch sz 160
Model ind 640 epoch 964 head B head_i_epoch 1 batch 0: avg loss -1.869957 avg loss no lamb -1.869957 time 2019-02-28 21:24:05.738137
Model ind 640 epoch 964 head B head_i_epoch 1 batch 100: avg loss -1.914593 avg loss no lamb -1.914593 time 2019-02-28 21:27:05.134252
Model ind 640 epoch 964 head B head_i_epoch 1 batch 200: avg loss -1.888991 avg loss no lamb -1.888991 time 2019-02-28 21:30:02.670822
last batch sz 160
Pre: time 2019-02-28 21:32:55.293675: 
 	std: 0.051063802
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5092667, 0.6135833, 0.61371666, 0.61365, 0.50956666]
	train_accs: [0.5092667, 0.6135833, 0.61371666, 0.61365, 0.50956666]
	best_train_sub_head: 2
	worst: 0.5092667
	avg: 0.57195663
	best: 0.61371666

Starting e_i: 965
Model ind 640 epoch 965 head A head_i_epoch 0 batch 0: avg loss -3.229712 avg loss no lamb -3.229712 time 2019-02-28 21:33:00.355796
Model ind 640 epoch 965 head A head_i_epoch 0 batch 100: avg loss -3.167456 avg loss no lamb -3.167456 time 2019-02-28 21:35:58.208534
Model ind 640 epoch 965 head A head_i_epoch 0 batch 200: avg loss -3.200149 avg loss no lamb -3.200149 time 2019-02-28 21:38:58.086686
last batch sz 160
Model ind 640 epoch 965 head B head_i_epoch 0 batch 0: avg loss -1.932747 avg loss no lamb -1.932747 time 2019-02-28 21:41:11.830476
Model ind 640 epoch 965 head B head_i_epoch 0 batch 100: avg loss -1.919441 avg loss no lamb -1.919441 time 2019-02-28 21:44:15.182220
Model ind 640 epoch 965 head B head_i_epoch 0 batch 200: avg loss -1.886344 avg loss no lamb -1.886344 time 2019-02-28 21:47:22.554344
last batch sz 160
Model ind 640 epoch 965 head B head_i_epoch 1 batch 0: avg loss -1.946647 avg loss no lamb -1.946647 time 2019-02-28 21:49:44.698874
Model ind 640 epoch 965 head B head_i_epoch 1 batch 100: avg loss -1.798903 avg loss no lamb -1.798903 time 2019-02-28 21:52:53.648889
Model ind 640 epoch 965 head B head_i_epoch 1 batch 200: avg loss -1.910607 avg loss no lamb -1.910607 time 2019-02-28 21:55:58.486813
last batch sz 160
Pre: time 2019-02-28 21:58:45.297930: 
 	std: 0.050519373
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5122667, 0.61546665, 0.6154, 0.6154, 0.51233333]
	train_accs: [0.5122667, 0.61546665, 0.6154, 0.6154, 0.51233333]
	best_train_sub_head: 1
	worst: 0.5122667
	avg: 0.57417333
	best: 0.61546665

Starting e_i: 966
Model ind 640 epoch 966 head A head_i_epoch 0 batch 0: avg loss -3.148448 avg loss no lamb -3.148448 time 2019-02-28 21:58:50.915121
Model ind 640 epoch 966 head A head_i_epoch 0 batch 100: avg loss -3.185444 avg loss no lamb -3.185444 time 2019-02-28 22:01:47.343227
Model ind 640 epoch 966 head A head_i_epoch 0 batch 200: avg loss -3.263128 avg loss no lamb -3.263128 time 2019-02-28 22:04:44.682654
last batch sz 160
Model ind 640 epoch 966 head B head_i_epoch 0 batch 0: avg loss -1.907676 avg loss no lamb -1.907676 time 2019-02-28 22:06:57.913061
Model ind 640 epoch 966 head B head_i_epoch 0 batch 100: avg loss -1.841423 avg loss no lamb -1.841423 time 2019-02-28 22:10:12.159874
Model ind 640 epoch 966 head B head_i_epoch 0 batch 200: avg loss -1.884031 avg loss no lamb -1.884031 time 2019-02-28 22:13:27.106267
last batch sz 160
Model ind 640 epoch 966 head B head_i_epoch 1 batch 0: avg loss -1.859300 avg loss no lamb -1.859300 time 2019-02-28 22:15:46.622783
Model ind 640 epoch 966 head B head_i_epoch 1 batch 100: avg loss -1.842531 avg loss no lamb -1.842531 time 2019-02-28 22:18:58.728300
Model ind 640 epoch 966 head B head_i_epoch 1 batch 200: avg loss -2.013477 avg loss no lamb -2.013477 time 2019-02-28 22:22:03.453529
last batch sz 160
Pre: time 2019-02-28 22:24:43.126240: 
 	std: 0.050601233
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51195, 0.6153167, 0.61536664, 0.61558336, 0.51231664]
	train_accs: [0.51195, 0.6153167, 0.61536664, 0.61558336, 0.51231664]
	best_train_sub_head: 3
	worst: 0.51195
	avg: 0.5741067
	best: 0.61558336

Starting e_i: 967
Model ind 640 epoch 967 head A head_i_epoch 0 batch 0: avg loss -3.212225 avg loss no lamb -3.212225 time 2019-02-28 22:24:47.208038
Model ind 640 epoch 967 head A head_i_epoch 0 batch 100: avg loss -3.170343 avg loss no lamb -3.170343 time 2019-02-28 22:27:47.466634
Model ind 640 epoch 967 head A head_i_epoch 0 batch 200: avg loss -3.299503 avg loss no lamb -3.299503 time 2019-02-28 22:30:50.679459
last batch sz 160
Model ind 640 epoch 967 head B head_i_epoch 0 batch 0: avg loss -1.875928 avg loss no lamb -1.875928 time 2019-02-28 22:32:55.956315
Model ind 640 epoch 967 head B head_i_epoch 0 batch 100: avg loss -1.839505 avg loss no lamb -1.839505 time 2019-02-28 22:36:04.987570
Model ind 640 epoch 967 head B head_i_epoch 0 batch 200: avg loss -1.941084 avg loss no lamb -1.941084 time 2019-02-28 22:39:15.035176
last batch sz 160
Model ind 640 epoch 967 head B head_i_epoch 1 batch 0: avg loss -1.956622 avg loss no lamb -1.956622 time 2019-02-28 22:41:18.159959
Model ind 640 epoch 967 head B head_i_epoch 1 batch 100: avg loss -1.823188 avg loss no lamb -1.823188 time 2019-02-28 22:44:17.616130
Model ind 640 epoch 967 head B head_i_epoch 1 batch 200: avg loss -1.942382 avg loss no lamb -1.942382 time 2019-02-28 22:47:16.243313
last batch sz 160
Pre: time 2019-02-28 22:50:01.712908: 
 	std: 0.050853014
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108, 0.6149333, 0.61485, 0.6147, 0.51125]
	train_accs: [0.5108, 0.6149333, 0.61485, 0.6147, 0.51125]
	best_train_sub_head: 1
	worst: 0.5108
	avg: 0.5733067
	best: 0.6149333

Starting e_i: 968
Model ind 640 epoch 968 head A head_i_epoch 0 batch 0: avg loss -3.192302 avg loss no lamb -3.192302 time 2019-02-28 22:50:06.496969
Model ind 640 epoch 968 head A head_i_epoch 0 batch 100: avg loss -3.153146 avg loss no lamb -3.153146 time 2019-02-28 22:53:02.742474
Model ind 640 epoch 968 head A head_i_epoch 0 batch 200: avg loss -3.175205 avg loss no lamb -3.175205 time 2019-02-28 22:55:58.665204
last batch sz 160
Model ind 640 epoch 968 head B head_i_epoch 0 batch 0: avg loss -1.854496 avg loss no lamb -1.854496 time 2019-02-28 22:58:10.116249
Model ind 640 epoch 968 head B head_i_epoch 0 batch 100: avg loss -1.835968 avg loss no lamb -1.835968 time 2019-02-28 23:01:06.395720
Model ind 640 epoch 968 head B head_i_epoch 0 batch 200: avg loss -1.949896 avg loss no lamb -1.949896 time 2019-02-28 23:04:02.834354
last batch sz 160
Model ind 640 epoch 968 head B head_i_epoch 1 batch 0: avg loss -1.873538 avg loss no lamb -1.873538 time 2019-02-28 23:06:00.161858
Model ind 640 epoch 968 head B head_i_epoch 1 batch 100: avg loss -1.826270 avg loss no lamb -1.826270 time 2019-02-28 23:08:56.289685
Model ind 640 epoch 968 head B head_i_epoch 1 batch 200: avg loss -1.855749 avg loss no lamb -1.855749 time 2019-02-28 23:11:51.227875
last batch sz 160
Pre: time 2019-02-28 23:14:28.137658: 
 	std: 0.05042283
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51191664, 0.61465, 0.6148833, 0.6148667, 0.5118333]
	train_accs: [0.51191664, 0.61465, 0.6148833, 0.6148667, 0.5118333]
	best_train_sub_head: 2
	worst: 0.5118333
	avg: 0.57363
	best: 0.6148833

Starting e_i: 969
Model ind 640 epoch 969 head A head_i_epoch 0 batch 0: avg loss -3.228066 avg loss no lamb -3.228066 time 2019-02-28 23:14:35.503253
Model ind 640 epoch 969 head A head_i_epoch 0 batch 100: avg loss -3.127640 avg loss no lamb -3.127640 time 2019-02-28 23:17:29.649954
Model ind 640 epoch 969 head A head_i_epoch 0 batch 200: avg loss -3.299433 avg loss no lamb -3.299433 time 2019-02-28 23:20:30.296102
last batch sz 160
Model ind 640 epoch 969 head B head_i_epoch 0 batch 0: avg loss -1.875324 avg loss no lamb -1.875324 time 2019-02-28 23:22:52.368411
Model ind 640 epoch 969 head B head_i_epoch 0 batch 100: avg loss -1.877814 avg loss no lamb -1.877814 time 2019-02-28 23:25:48.968532
Model ind 640 epoch 969 head B head_i_epoch 0 batch 200: avg loss -1.879223 avg loss no lamb -1.879223 time 2019-02-28 23:28:42.043815
last batch sz 160
Model ind 640 epoch 969 head B head_i_epoch 1 batch 0: avg loss -1.901389 avg loss no lamb -1.901389 time 2019-02-28 23:30:52.471938
Model ind 640 epoch 969 head B head_i_epoch 1 batch 100: avg loss -1.868274 avg loss no lamb -1.868274 time 2019-02-28 23:33:49.542220
Model ind 640 epoch 969 head B head_i_epoch 1 batch 200: avg loss -1.917504 avg loss no lamb -1.917504 time 2019-02-28 23:36:43.863170
last batch sz 160
Pre: time 2019-02-28 23:39:21.506069: 
 	std: 0.050644584
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5112333, 0.6145, 0.6146, 0.6145833, 0.5111333]
	train_accs: [0.5112333, 0.6145, 0.6146, 0.6145833, 0.5111333]
	best_train_sub_head: 2
	worst: 0.5111333
	avg: 0.57320994
	best: 0.6146

Starting e_i: 970
Model ind 640 epoch 970 head A head_i_epoch 0 batch 0: avg loss -3.187371 avg loss no lamb -3.187371 time 2019-02-28 23:39:26.360587
Model ind 640 epoch 970 head A head_i_epoch 0 batch 100: avg loss -3.127708 avg loss no lamb -3.127708 time 2019-02-28 23:42:19.818040
Model ind 640 epoch 970 head A head_i_epoch 0 batch 200: avg loss -3.290770 avg loss no lamb -3.290770 time 2019-02-28 23:45:17.079732
last batch sz 160
Model ind 640 epoch 970 head B head_i_epoch 0 batch 0: avg loss -1.830230 avg loss no lamb -1.830230 time 2019-02-28 23:47:28.428645
Model ind 640 epoch 970 head B head_i_epoch 0 batch 100: avg loss -1.820634 avg loss no lamb -1.820634 time 2019-02-28 23:50:36.889472
Model ind 640 epoch 970 head B head_i_epoch 0 batch 200: avg loss -1.853160 avg loss no lamb -1.853160 time 2019-02-28 23:53:44.335567
last batch sz 160
Model ind 640 epoch 970 head B head_i_epoch 1 batch 0: avg loss -1.884490 avg loss no lamb -1.884490 time 2019-02-28 23:55:56.086176
Model ind 640 epoch 970 head B head_i_epoch 1 batch 100: avg loss -1.720551 avg loss no lamb -1.720551 time 2019-02-28 23:58:51.364879
Model ind 640 epoch 970 head B head_i_epoch 1 batch 200: avg loss -1.934860 avg loss no lamb -1.934860 time 2019-03-01 00:01:50.929556
last batch sz 160
Pre: time 2019-03-01 00:04:40.950202: 
 	std: 0.050424114
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51168334, 0.61465, 0.61466664, 0.61471665, 0.5118167]
	train_accs: [0.51168334, 0.61465, 0.61466664, 0.61471665, 0.5118167]
	best_train_sub_head: 3
	worst: 0.51168334
	avg: 0.5735067
	best: 0.61471665

Starting e_i: 971
Model ind 640 epoch 971 head A head_i_epoch 0 batch 0: avg loss -3.270780 avg loss no lamb -3.270780 time 2019-03-01 00:04:48.504419
Model ind 640 epoch 971 head A head_i_epoch 0 batch 100: avg loss -3.166651 avg loss no lamb -3.166651 time 2019-03-01 00:07:56.410470
Model ind 640 epoch 971 head A head_i_epoch 0 batch 200: avg loss -3.256259 avg loss no lamb -3.256259 time 2019-03-01 00:11:11.158211
last batch sz 160
Model ind 640 epoch 971 head B head_i_epoch 0 batch 0: avg loss -1.844398 avg loss no lamb -1.844398 time 2019-03-01 00:13:30.817490
Model ind 640 epoch 971 head B head_i_epoch 0 batch 100: avg loss -1.844849 avg loss no lamb -1.844849 time 2019-03-01 00:16:46.383279
Model ind 640 epoch 971 head B head_i_epoch 0 batch 200: avg loss -1.925769 avg loss no lamb -1.925769 time 2019-03-01 00:19:53.579448
last batch sz 160
Model ind 640 epoch 971 head B head_i_epoch 1 batch 0: avg loss -1.874255 avg loss no lamb -1.874255 time 2019-03-01 00:22:00.740411
Model ind 640 epoch 971 head B head_i_epoch 1 batch 100: avg loss -1.901344 avg loss no lamb -1.901344 time 2019-03-01 00:25:03.020512
Model ind 640 epoch 971 head B head_i_epoch 1 batch 200: avg loss -1.878240 avg loss no lamb -1.878240 time 2019-03-01 00:28:03.024886
last batch sz 160
Pre: time 2019-03-01 00:30:54.967909: 
 	std: 0.050548114
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111167, 0.6145667, 0.6144, 0.6145, 0.5115]
	train_accs: [0.5111167, 0.6145667, 0.6144, 0.6145, 0.5115]
	best_train_sub_head: 1
	worst: 0.5111167
	avg: 0.5732166
	best: 0.6145667

Starting e_i: 972
Model ind 640 epoch 972 head A head_i_epoch 0 batch 0: avg loss -3.229048 avg loss no lamb -3.229048 time 2019-03-01 00:31:01.730646
Model ind 640 epoch 972 head A head_i_epoch 0 batch 100: avg loss -3.129155 avg loss no lamb -3.129155 time 2019-03-01 00:34:06.777807
Model ind 640 epoch 972 head A head_i_epoch 0 batch 200: avg loss -3.206077 avg loss no lamb -3.206077 time 2019-03-01 00:37:08.844279
last batch sz 160
Model ind 640 epoch 972 head B head_i_epoch 0 batch 0: avg loss -2.035751 avg loss no lamb -2.035751 time 2019-03-01 00:39:21.584897
Model ind 640 epoch 972 head B head_i_epoch 0 batch 100: avg loss -1.871876 avg loss no lamb -1.871876 time 2019-03-01 00:42:24.149189
Model ind 640 epoch 972 head B head_i_epoch 0 batch 200: avg loss -1.913919 avg loss no lamb -1.913919 time 2019-03-01 00:45:19.474794
last batch sz 160
Model ind 640 epoch 972 head B head_i_epoch 1 batch 0: avg loss -1.863075 avg loss no lamb -1.863075 time 2019-03-01 00:47:30.631839
Model ind 640 epoch 972 head B head_i_epoch 1 batch 100: avg loss -1.907678 avg loss no lamb -1.907678 time 2019-03-01 00:50:27.159080
Model ind 640 epoch 972 head B head_i_epoch 1 batch 200: avg loss -1.926965 avg loss no lamb -1.926965 time 2019-03-01 00:53:25.267487
last batch sz 160
Pre: time 2019-03-01 00:56:05.517382: 
 	std: 0.05024735
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.6139, 0.6139333, 0.61401665, 0.51156664]
	train_accs: [0.5112, 0.6139, 0.6139333, 0.61401665, 0.51156664]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.5729233
	best: 0.61401665

Starting e_i: 973
Model ind 640 epoch 973 head A head_i_epoch 0 batch 0: avg loss -3.320018 avg loss no lamb -3.320018 time 2019-03-01 00:56:09.992486
Model ind 640 epoch 973 head A head_i_epoch 0 batch 100: avg loss -3.163990 avg loss no lamb -3.163990 time 2019-03-01 00:59:15.205311
Model ind 640 epoch 973 head A head_i_epoch 0 batch 200: avg loss -3.265786 avg loss no lamb -3.265786 time 2019-03-01 01:02:19.532159
last batch sz 160
Model ind 640 epoch 973 head B head_i_epoch 0 batch 0: avg loss -1.879966 avg loss no lamb -1.879966 time 2019-03-01 01:04:30.483405
Model ind 640 epoch 973 head B head_i_epoch 0 batch 100: avg loss -1.873274 avg loss no lamb -1.873274 time 2019-03-01 01:07:28.595820
Model ind 640 epoch 973 head B head_i_epoch 0 batch 200: avg loss -1.921316 avg loss no lamb -1.921316 time 2019-03-01 01:10:19.439265
last batch sz 160
Model ind 640 epoch 973 head B head_i_epoch 1 batch 0: avg loss -1.939869 avg loss no lamb -1.939869 time 2019-03-01 01:12:11.643095
Model ind 640 epoch 973 head B head_i_epoch 1 batch 100: avg loss -1.850405 avg loss no lamb -1.850405 time 2019-03-01 01:15:04.452301
Model ind 640 epoch 973 head B head_i_epoch 1 batch 200: avg loss -1.904471 avg loss no lamb -1.904471 time 2019-03-01 01:18:01.204361
last batch sz 160
Pre: time 2019-03-01 01:20:38.816847: 
 	std: 0.05076571
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51185, 0.61541665, 0.61558336, 0.61555, 0.5119333]
	train_accs: [0.51185, 0.61541665, 0.61558336, 0.61555, 0.5119333]
	best_train_sub_head: 2
	worst: 0.51185
	avg: 0.5740667
	best: 0.61558336

Starting e_i: 974
Model ind 640 epoch 974 head A head_i_epoch 0 batch 0: avg loss -3.254382 avg loss no lamb -3.254382 time 2019-03-01 01:20:45.036565
Model ind 640 epoch 974 head A head_i_epoch 0 batch 100: avg loss -3.197112 avg loss no lamb -3.197112 time 2019-03-01 01:23:39.216506
Model ind 640 epoch 974 head A head_i_epoch 0 batch 200: avg loss -3.263700 avg loss no lamb -3.263700 time 2019-03-01 01:26:21.349649
last batch sz 160
Model ind 640 epoch 974 head B head_i_epoch 0 batch 0: avg loss -1.903721 avg loss no lamb -1.903721 time 2019-03-01 01:28:10.768932
Model ind 640 epoch 974 head B head_i_epoch 0 batch 100: avg loss -1.828279 avg loss no lamb -1.828279 time 2019-03-01 01:30:49.163231
Model ind 640 epoch 974 head B head_i_epoch 0 batch 200: avg loss -1.910608 avg loss no lamb -1.910608 time 2019-03-01 01:32:46.773646
last batch sz 160
Model ind 640 epoch 974 head B head_i_epoch 1 batch 0: avg loss -1.915420 avg loss no lamb -1.915420 time 2019-03-01 01:34:19.413987
Model ind 640 epoch 974 head B head_i_epoch 1 batch 100: avg loss -1.864279 avg loss no lamb -1.864279 time 2019-03-01 01:36:14.656508
Model ind 640 epoch 974 head B head_i_epoch 1 batch 200: avg loss -1.921435 avg loss no lamb -1.921435 time 2019-03-01 01:38:02.164441
last batch sz 160
Pre: time 2019-03-01 01:39:42.585751: 
 	std: 0.050837904
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50988334, 0.6134, 0.6135833, 0.6135833, 0.5096167]
	train_accs: [0.50988334, 0.6134, 0.6135833, 0.6135833, 0.5096167]
	best_train_sub_head: 2
	worst: 0.5096167
	avg: 0.5720133
	best: 0.6135833

Starting e_i: 975
Model ind 640 epoch 975 head A head_i_epoch 0 batch 0: avg loss -3.251313 avg loss no lamb -3.251313 time 2019-03-01 01:39:45.028991
Model ind 640 epoch 975 head A head_i_epoch 0 batch 100: avg loss -3.204942 avg loss no lamb -3.204942 time 2019-03-01 01:41:28.317107
Model ind 640 epoch 975 head A head_i_epoch 0 batch 200: avg loss -3.280236 avg loss no lamb -3.280236 time 2019-03-01 01:43:15.804728
last batch sz 160
Model ind 640 epoch 975 head B head_i_epoch 0 batch 0: avg loss -1.876855 avg loss no lamb -1.876855 time 2019-03-01 01:44:32.224706
Model ind 640 epoch 975 head B head_i_epoch 0 batch 100: avg loss -1.822576 avg loss no lamb -1.822576 time 2019-03-01 01:46:20.537596
Model ind 640 epoch 975 head B head_i_epoch 0 batch 200: avg loss -1.889606 avg loss no lamb -1.889606 time 2019-03-01 01:48:08.654296
last batch sz 160
Model ind 640 epoch 975 head B head_i_epoch 1 batch 0: avg loss -1.878825 avg loss no lamb -1.878825 time 2019-03-01 01:49:24.320556
Model ind 640 epoch 975 head B head_i_epoch 1 batch 100: avg loss -1.773823 avg loss no lamb -1.773823 time 2019-03-01 01:51:11.191601
Model ind 640 epoch 975 head B head_i_epoch 1 batch 200: avg loss -1.930617 avg loss no lamb -1.930617 time 2019-03-01 01:52:55.032014
last batch sz 160
Pre: time 2019-03-01 01:54:35.311380: 
 	std: 0.050545238
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51061666, 0.6138, 0.6138167, 0.6138833, 0.5107]
	train_accs: [0.51061666, 0.6138, 0.6138167, 0.6138833, 0.5107]
	best_train_sub_head: 3
	worst: 0.51061666
	avg: 0.5725633
	best: 0.6138833

Starting e_i: 976
Model ind 640 epoch 976 head A head_i_epoch 0 batch 0: avg loss -3.299100 avg loss no lamb -3.299100 time 2019-03-01 01:54:37.494765
Model ind 640 epoch 976 head A head_i_epoch 0 batch 100: avg loss -3.237209 avg loss no lamb -3.237209 time 2019-03-01 01:56:21.363811
Model ind 640 epoch 976 head A head_i_epoch 0 batch 200: avg loss -3.263639 avg loss no lamb -3.263639 time 2019-03-01 01:58:08.087215
last batch sz 160
Model ind 640 epoch 976 head B head_i_epoch 0 batch 0: avg loss -1.975081 avg loss no lamb -1.975081 time 2019-03-01 01:59:24.675691
Model ind 640 epoch 976 head B head_i_epoch 0 batch 100: avg loss -1.818898 avg loss no lamb -1.818898 time 2019-03-01 02:01:10.651441
Model ind 640 epoch 976 head B head_i_epoch 0 batch 200: avg loss -1.939706 avg loss no lamb -1.939706 time 2019-03-01 02:02:58.016701
last batch sz 160
Model ind 640 epoch 976 head B head_i_epoch 1 batch 0: avg loss -1.904163 avg loss no lamb -1.904163 time 2019-03-01 02:04:13.193876
Model ind 640 epoch 976 head B head_i_epoch 1 batch 100: avg loss -1.821078 avg loss no lamb -1.821078 time 2019-03-01 02:06:00.625045
Model ind 640 epoch 976 head B head_i_epoch 1 batch 200: avg loss -1.933555 avg loss no lamb -1.933555 time 2019-03-01 02:07:43.433774
last batch sz 160
Pre: time 2019-03-01 02:09:22.297156: 
 	std: 0.051005334
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5097167, 0.61375, 0.61368334, 0.6135333, 0.5093667]
	train_accs: [0.5097167, 0.61375, 0.61368334, 0.6135333, 0.5093667]
	best_train_sub_head: 1
	worst: 0.5093667
	avg: 0.57201004
	best: 0.61375

Starting e_i: 977
Model ind 640 epoch 977 head A head_i_epoch 0 batch 0: avg loss -3.234547 avg loss no lamb -3.234547 time 2019-03-01 02:09:24.476544
Model ind 640 epoch 977 head A head_i_epoch 0 batch 100: avg loss -3.226671 avg loss no lamb -3.226671 time 2019-03-01 02:11:09.608343
Model ind 640 epoch 977 head A head_i_epoch 0 batch 200: avg loss -3.209633 avg loss no lamb -3.209633 time 2019-03-01 02:12:56.153666
last batch sz 160
Model ind 640 epoch 977 head B head_i_epoch 0 batch 0: avg loss -1.951444 avg loss no lamb -1.951444 time 2019-03-01 02:14:13.604026
Model ind 640 epoch 977 head B head_i_epoch 0 batch 100: avg loss -1.798622 avg loss no lamb -1.798622 time 2019-03-01 02:15:58.777384
Model ind 640 epoch 977 head B head_i_epoch 0 batch 200: avg loss -1.971395 avg loss no lamb -1.971395 time 2019-03-01 02:17:47.342671
last batch sz 160
Model ind 640 epoch 977 head B head_i_epoch 1 batch 0: avg loss -1.879757 avg loss no lamb -1.879757 time 2019-03-01 02:19:03.055371
Model ind 640 epoch 977 head B head_i_epoch 1 batch 100: avg loss -1.830585 avg loss no lamb -1.830585 time 2019-03-01 02:20:51.292966
Model ind 640 epoch 977 head B head_i_epoch 1 batch 200: avg loss -1.927621 avg loss no lamb -1.927621 time 2019-03-01 02:22:34.845973
last batch sz 160
Pre: time 2019-03-01 02:24:15.039282: 
 	std: 0.05082423
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107833, 0.61445, 0.6146167, 0.6145167, 0.5107833]
	train_accs: [0.5107833, 0.61445, 0.6146167, 0.6145167, 0.5107833]
	best_train_sub_head: 2
	worst: 0.5107833
	avg: 0.57303
	best: 0.6146167

Starting e_i: 978
Model ind 640 epoch 978 head A head_i_epoch 0 batch 0: avg loss -3.257466 avg loss no lamb -3.257466 time 2019-03-01 02:24:17.200875
Model ind 640 epoch 978 head A head_i_epoch 0 batch 100: avg loss -3.230821 avg loss no lamb -3.230821 time 2019-03-01 02:26:02.983500
Model ind 640 epoch 978 head A head_i_epoch 0 batch 200: avg loss -3.311653 avg loss no lamb -3.311653 time 2019-03-01 02:27:47.751353
last batch sz 160
Model ind 640 epoch 978 head B head_i_epoch 0 batch 0: avg loss -1.919506 avg loss no lamb -1.919506 time 2019-03-01 02:29:06.323915
Model ind 640 epoch 978 head B head_i_epoch 0 batch 100: avg loss -1.871313 avg loss no lamb -1.871313 time 2019-03-01 02:30:50.433130
Model ind 640 epoch 978 head B head_i_epoch 0 batch 200: avg loss -1.906026 avg loss no lamb -1.906026 time 2019-03-01 02:32:38.555696
last batch sz 160
Model ind 640 epoch 978 head B head_i_epoch 1 batch 0: avg loss -1.938444 avg loss no lamb -1.938444 time 2019-03-01 02:33:52.403611
Model ind 640 epoch 978 head B head_i_epoch 1 batch 100: avg loss -1.834064 avg loss no lamb -1.834064 time 2019-03-01 02:35:40.801410
Model ind 640 epoch 978 head B head_i_epoch 1 batch 200: avg loss -1.889979 avg loss no lamb -1.889979 time 2019-03-01 02:37:24.080406
last batch sz 160
Pre: time 2019-03-01 02:39:02.247216: 
 	std: 0.05090588
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51018333, 0.61398333, 0.61405, 0.6142, 0.51015]
	train_accs: [0.51018333, 0.61398333, 0.61405, 0.6142, 0.51015]
	best_train_sub_head: 3
	worst: 0.51015
	avg: 0.5725133
	best: 0.6142

Starting e_i: 979
Model ind 640 epoch 979 head A head_i_epoch 0 batch 0: avg loss -3.238255 avg loss no lamb -3.238255 time 2019-03-01 02:39:04.440220
Model ind 640 epoch 979 head A head_i_epoch 0 batch 100: avg loss -3.255373 avg loss no lamb -3.255373 time 2019-03-01 02:40:50.408421
Model ind 640 epoch 979 head A head_i_epoch 0 batch 200: avg loss -3.207581 avg loss no lamb -3.207581 time 2019-03-01 02:42:34.642420
last batch sz 160
Model ind 640 epoch 979 head B head_i_epoch 0 batch 0: avg loss -1.885764 avg loss no lamb -1.885764 time 2019-03-01 02:43:53.381169
Model ind 640 epoch 979 head B head_i_epoch 0 batch 100: avg loss -1.815445 avg loss no lamb -1.815445 time 2019-03-01 02:45:36.405732
Model ind 640 epoch 979 head B head_i_epoch 0 batch 200: avg loss -1.898216 avg loss no lamb -1.898216 time 2019-03-01 02:47:25.154148
last batch sz 160
Model ind 640 epoch 979 head B head_i_epoch 1 batch 0: avg loss -1.860870 avg loss no lamb -1.860870 time 2019-03-01 02:48:39.606299
Model ind 640 epoch 979 head B head_i_epoch 1 batch 100: avg loss -1.842227 avg loss no lamb -1.842227 time 2019-03-01 02:50:27.757648
Model ind 640 epoch 979 head B head_i_epoch 1 batch 200: avg loss -1.921736 avg loss no lamb -1.921736 time 2019-03-01 02:52:12.709520
last batch sz 160
Pre: time 2019-03-01 02:53:49.832908: 
 	std: 0.050586108
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51031667, 0.61355, 0.61373335, 0.6135167, 0.5103667]
	train_accs: [0.51031667, 0.61355, 0.61373335, 0.6135167, 0.5103667]
	best_train_sub_head: 2
	worst: 0.51031667
	avg: 0.5722967
	best: 0.61373335

Starting e_i: 980
Model ind 640 epoch 980 head A head_i_epoch 0 batch 0: avg loss -3.233753 avg loss no lamb -3.233753 time 2019-03-01 02:53:52.569729
Model ind 640 epoch 980 head A head_i_epoch 0 batch 100: avg loss -3.162795 avg loss no lamb -3.162795 time 2019-03-01 02:55:35.267786
Model ind 640 epoch 980 head A head_i_epoch 0 batch 200: avg loss -3.274885 avg loss no lamb -3.274885 time 2019-03-01 02:57:15.154568
last batch sz 160
Model ind 640 epoch 980 head B head_i_epoch 0 batch 0: avg loss -1.903621 avg loss no lamb -1.903621 time 2019-03-01 02:58:27.772476
Model ind 640 epoch 980 head B head_i_epoch 0 batch 100: avg loss -1.755983 avg loss no lamb -1.755983 time 2019-03-01 03:00:07.575124
Model ind 640 epoch 980 head B head_i_epoch 0 batch 200: avg loss -1.928410 avg loss no lamb -1.928410 time 2019-03-01 03:01:47.378340
last batch sz 160
Model ind 640 epoch 980 head B head_i_epoch 1 batch 0: avg loss -1.912599 avg loss no lamb -1.912599 time 2019-03-01 03:02:59.956820
Model ind 640 epoch 980 head B head_i_epoch 1 batch 100: avg loss -1.861734 avg loss no lamb -1.861734 time 2019-03-01 03:04:39.739102
Model ind 640 epoch 980 head B head_i_epoch 1 batch 200: avg loss -1.915545 avg loss no lamb -1.915545 time 2019-03-01 03:06:19.987184
last batch sz 160
Pre: time 2019-03-01 03:07:51.066850: 
 	std: 0.05048127
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.512, 0.61508334, 0.61508334, 0.6149667, 0.512]
	train_accs: [0.512, 0.61508334, 0.61508334, 0.6149667, 0.512]
	best_train_sub_head: 1
	worst: 0.512
	avg: 0.5738267
	best: 0.61508334

Starting e_i: 981
Model ind 640 epoch 981 head A head_i_epoch 0 batch 0: avg loss -3.184975 avg loss no lamb -3.184975 time 2019-03-01 03:07:56.500855
Model ind 640 epoch 981 head A head_i_epoch 0 batch 100: avg loss -3.241354 avg loss no lamb -3.241354 time 2019-03-01 03:09:36.252852
Model ind 640 epoch 981 head A head_i_epoch 0 batch 200: avg loss -3.154065 avg loss no lamb -3.154065 time 2019-03-01 03:11:15.925104
last batch sz 160
Model ind 640 epoch 981 head B head_i_epoch 0 batch 0: avg loss -1.918940 avg loss no lamb -1.918940 time 2019-03-01 03:12:28.359897
Model ind 640 epoch 981 head B head_i_epoch 0 batch 100: avg loss -1.806725 avg loss no lamb -1.806725 time 2019-03-01 03:14:08.262284
Model ind 640 epoch 981 head B head_i_epoch 0 batch 200: avg loss -1.966543 avg loss no lamb -1.966543 time 2019-03-01 03:15:47.884186
last batch sz 160
Model ind 640 epoch 981 head B head_i_epoch 1 batch 0: avg loss -1.866283 avg loss no lamb -1.866283 time 2019-03-01 03:17:00.546740
Model ind 640 epoch 981 head B head_i_epoch 1 batch 100: avg loss -1.827709 avg loss no lamb -1.827709 time 2019-03-01 03:18:40.089630
Model ind 640 epoch 981 head B head_i_epoch 1 batch 200: avg loss -1.931659 avg loss no lamb -1.931659 time 2019-03-01 03:20:19.537654
last batch sz 160
Pre: time 2019-03-01 03:21:50.491865: 
 	std: 0.050686922
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103, 0.6134833, 0.61356664, 0.61371666, 0.50995]
	train_accs: [0.5103, 0.6134833, 0.61356664, 0.61371666, 0.50995]
	best_train_sub_head: 3
	worst: 0.50995
	avg: 0.5722033
	best: 0.61371666

Starting e_i: 982
Model ind 640 epoch 982 head A head_i_epoch 0 batch 0: avg loss -3.168378 avg loss no lamb -3.168378 time 2019-03-01 03:21:53.373124
Model ind 640 epoch 982 head A head_i_epoch 0 batch 100: avg loss -3.188012 avg loss no lamb -3.188012 time 2019-03-01 03:23:33.557336
Model ind 640 epoch 982 head A head_i_epoch 0 batch 200: avg loss -3.203568 avg loss no lamb -3.203568 time 2019-03-01 03:25:13.282514
last batch sz 160
Model ind 640 epoch 982 head B head_i_epoch 0 batch 0: avg loss -1.875749 avg loss no lamb -1.875749 time 2019-03-01 03:26:25.772112
Model ind 640 epoch 982 head B head_i_epoch 0 batch 100: avg loss -1.768518 avg loss no lamb -1.768518 time 2019-03-01 03:28:05.338450
Model ind 640 epoch 982 head B head_i_epoch 0 batch 200: avg loss -1.936367 avg loss no lamb -1.936367 time 2019-03-01 03:29:44.976079
last batch sz 160
Model ind 640 epoch 982 head B head_i_epoch 1 batch 0: avg loss -1.977840 avg loss no lamb -1.977840 time 2019-03-01 03:30:57.410111
Model ind 640 epoch 982 head B head_i_epoch 1 batch 100: avg loss -1.832067 avg loss no lamb -1.832067 time 2019-03-01 03:32:37.065777
Model ind 640 epoch 982 head B head_i_epoch 1 batch 200: avg loss -2.002353 avg loss no lamb -2.002353 time 2019-03-01 03:34:16.610213
last batch sz 160
Pre: time 2019-03-01 03:35:47.575174: 
 	std: 0.05044872
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51135, 0.61433333, 0.6145667, 0.6142833, 0.5114833]
	train_accs: [0.51135, 0.61433333, 0.6145667, 0.6142833, 0.5114833]
	best_train_sub_head: 2
	worst: 0.51135
	avg: 0.57320327
	best: 0.6145667

Starting e_i: 983
Model ind 640 epoch 983 head A head_i_epoch 0 batch 0: avg loss -3.261078 avg loss no lamb -3.261078 time 2019-03-01 03:35:49.692441
Model ind 640 epoch 983 head A head_i_epoch 0 batch 100: avg loss -3.123848 avg loss no lamb -3.123848 time 2019-03-01 03:37:29.219255
Model ind 640 epoch 983 head A head_i_epoch 0 batch 200: avg loss -3.213792 avg loss no lamb -3.213792 time 2019-03-01 03:39:08.740640
last batch sz 160
Model ind 640 epoch 983 head B head_i_epoch 0 batch 0: avg loss -1.943013 avg loss no lamb -1.943013 time 2019-03-01 03:40:21.049161
Model ind 640 epoch 983 head B head_i_epoch 0 batch 100: avg loss -1.885974 avg loss no lamb -1.885974 time 2019-03-01 03:42:00.471735
Model ind 640 epoch 983 head B head_i_epoch 0 batch 200: avg loss -1.904852 avg loss no lamb -1.904852 time 2019-03-01 03:43:39.852288
last batch sz 160
Model ind 640 epoch 983 head B head_i_epoch 1 batch 0: avg loss -1.860652 avg loss no lamb -1.860652 time 2019-03-01 03:44:52.181984
Model ind 640 epoch 983 head B head_i_epoch 1 batch 100: avg loss -1.875242 avg loss no lamb -1.875242 time 2019-03-01 03:46:31.667419
Model ind 640 epoch 983 head B head_i_epoch 1 batch 200: avg loss -1.905485 avg loss no lamb -1.905485 time 2019-03-01 03:48:11.113730
last batch sz 160
Pre: time 2019-03-01 03:49:41.916091: 
 	std: 0.0506628
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5097, 0.6136, 0.6134, 0.6134167, 0.5104167]
	train_accs: [0.5097, 0.6136, 0.6134, 0.6134167, 0.5104167]
	best_train_sub_head: 1
	worst: 0.5097
	avg: 0.5721067
	best: 0.6136

Starting e_i: 984
Model ind 640 epoch 984 head A head_i_epoch 0 batch 0: avg loss -3.265235 avg loss no lamb -3.265235 time 2019-03-01 03:49:44.098543
Model ind 640 epoch 984 head A head_i_epoch 0 batch 100: avg loss -3.173062 avg loss no lamb -3.173062 time 2019-03-01 03:51:23.545626
Model ind 640 epoch 984 head A head_i_epoch 0 batch 200: avg loss -3.212426 avg loss no lamb -3.212426 time 2019-03-01 03:53:03.001089
last batch sz 160
Model ind 640 epoch 984 head B head_i_epoch 0 batch 0: avg loss -1.897607 avg loss no lamb -1.897607 time 2019-03-01 03:54:15.369500
Model ind 640 epoch 984 head B head_i_epoch 0 batch 100: avg loss -1.872781 avg loss no lamb -1.872781 time 2019-03-01 03:55:54.707041
Model ind 640 epoch 984 head B head_i_epoch 0 batch 200: avg loss -1.934676 avg loss no lamb -1.934676 time 2019-03-01 03:57:34.018365
last batch sz 160
Model ind 640 epoch 984 head B head_i_epoch 1 batch 0: avg loss -1.937649 avg loss no lamb -1.937649 time 2019-03-01 03:58:46.157819
Model ind 640 epoch 984 head B head_i_epoch 1 batch 100: avg loss -1.795837 avg loss no lamb -1.795837 time 2019-03-01 04:00:25.499394
Model ind 640 epoch 984 head B head_i_epoch 1 batch 200: avg loss -1.985349 avg loss no lamb -1.985349 time 2019-03-01 04:02:04.826645
last batch sz 160
Pre: time 2019-03-01 04:03:36.218371: 
 	std: 0.05061198
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5104167, 0.61375, 0.61395, 0.61373335, 0.51058334]
	train_accs: [0.5104167, 0.61375, 0.61395, 0.61373335, 0.51058334]
	best_train_sub_head: 2
	worst: 0.5104167
	avg: 0.5724867
	best: 0.61395

Starting e_i: 985
Model ind 640 epoch 985 head A head_i_epoch 0 batch 0: avg loss -3.259750 avg loss no lamb -3.259750 time 2019-03-01 04:03:38.305875
Model ind 640 epoch 985 head A head_i_epoch 0 batch 100: avg loss -3.147866 avg loss no lamb -3.147866 time 2019-03-01 04:05:17.937406
Model ind 640 epoch 985 head A head_i_epoch 0 batch 200: avg loss -3.291662 avg loss no lamb -3.291662 time 2019-03-01 04:06:57.510371
last batch sz 160
Model ind 640 epoch 985 head B head_i_epoch 0 batch 0: avg loss -1.966865 avg loss no lamb -1.966865 time 2019-03-01 04:08:09.990590
Model ind 640 epoch 985 head B head_i_epoch 0 batch 100: avg loss -1.840793 avg loss no lamb -1.840793 time 2019-03-01 04:09:49.486916
Model ind 640 epoch 985 head B head_i_epoch 0 batch 200: avg loss -1.971950 avg loss no lamb -1.971950 time 2019-03-01 04:11:28.987801
last batch sz 160
Model ind 640 epoch 985 head B head_i_epoch 1 batch 0: avg loss -1.896772 avg loss no lamb -1.896772 time 2019-03-01 04:12:41.338996
Model ind 640 epoch 985 head B head_i_epoch 1 batch 100: avg loss -1.796603 avg loss no lamb -1.796603 time 2019-03-01 04:14:20.920380
Model ind 640 epoch 985 head B head_i_epoch 1 batch 200: avg loss -1.966893 avg loss no lamb -1.966893 time 2019-03-01 04:16:00.389488
last batch sz 160
Pre: time 2019-03-01 04:17:31.142803: 
 	std: 0.050662313
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51178336, 0.6152833, 0.61501664, 0.6152167, 0.51173335]
	train_accs: [0.51178336, 0.6152833, 0.61501664, 0.6152167, 0.51173335]
	best_train_sub_head: 1
	worst: 0.51173335
	avg: 0.57380664
	best: 0.6152833

Starting e_i: 986
Model ind 640 epoch 986 head A head_i_epoch 0 batch 0: avg loss -3.291409 avg loss no lamb -3.291409 time 2019-03-01 04:17:33.274161
Model ind 640 epoch 986 head A head_i_epoch 0 batch 100: avg loss -3.155467 avg loss no lamb -3.155467 time 2019-03-01 04:19:12.616866
Model ind 640 epoch 986 head A head_i_epoch 0 batch 200: avg loss -3.230745 avg loss no lamb -3.230745 time 2019-03-01 04:20:52.023908
last batch sz 160
Model ind 640 epoch 986 head B head_i_epoch 0 batch 0: avg loss -1.924766 avg loss no lamb -1.924766 time 2019-03-01 04:22:04.271961
Model ind 640 epoch 986 head B head_i_epoch 0 batch 100: avg loss -1.848400 avg loss no lamb -1.848400 time 2019-03-01 04:23:43.432838
Model ind 640 epoch 986 head B head_i_epoch 0 batch 200: avg loss -1.925751 avg loss no lamb -1.925751 time 2019-03-01 04:25:22.709261
last batch sz 160
Model ind 640 epoch 986 head B head_i_epoch 1 batch 0: avg loss -1.880593 avg loss no lamb -1.880593 time 2019-03-01 04:26:34.954087
Model ind 640 epoch 986 head B head_i_epoch 1 batch 100: avg loss -1.783523 avg loss no lamb -1.783523 time 2019-03-01 04:28:14.307641
Model ind 640 epoch 986 head B head_i_epoch 1 batch 200: avg loss -1.977355 avg loss no lamb -1.977355 time 2019-03-01 04:29:53.560834
last batch sz 160
Pre: time 2019-03-01 04:31:24.174006: 
 	std: 0.050580643
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5108333, 0.61396664, 0.61403334, 0.61396664, 0.51065]
	train_accs: [0.5108333, 0.61396664, 0.61403334, 0.61396664, 0.51065]
	best_train_sub_head: 2
	worst: 0.51065
	avg: 0.57268995
	best: 0.61403334

Starting e_i: 987
Model ind 640 epoch 987 head A head_i_epoch 0 batch 0: avg loss -3.288487 avg loss no lamb -3.288487 time 2019-03-01 04:31:26.801968
Model ind 640 epoch 987 head A head_i_epoch 0 batch 100: avg loss -3.180615 avg loss no lamb -3.180615 time 2019-03-01 04:33:06.602741
Model ind 640 epoch 987 head A head_i_epoch 0 batch 200: avg loss -3.266427 avg loss no lamb -3.266427 time 2019-03-01 04:34:46.445634
last batch sz 160
Model ind 640 epoch 987 head B head_i_epoch 0 batch 0: avg loss -1.932626 avg loss no lamb -1.932626 time 2019-03-01 04:35:59.082711
Model ind 640 epoch 987 head B head_i_epoch 0 batch 100: avg loss -1.885409 avg loss no lamb -1.885409 time 2019-03-01 04:37:38.827715
Model ind 640 epoch 987 head B head_i_epoch 0 batch 200: avg loss -1.968015 avg loss no lamb -1.968015 time 2019-03-01 04:39:18.585051
last batch sz 160
Model ind 640 epoch 987 head B head_i_epoch 1 batch 0: avg loss -1.888572 avg loss no lamb -1.888572 time 2019-03-01 04:40:31.056556
Model ind 640 epoch 987 head B head_i_epoch 1 batch 100: avg loss -1.852103 avg loss no lamb -1.852103 time 2019-03-01 04:42:10.748269
Model ind 640 epoch 987 head B head_i_epoch 1 batch 200: avg loss -1.920505 avg loss no lamb -1.920505 time 2019-03-01 04:43:50.514544
last batch sz 160
Pre: time 2019-03-01 04:45:21.564282: 
 	std: 0.050459508
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5103, 0.61326665, 0.61338335, 0.6133, 0.51033336]
	train_accs: [0.5103, 0.61326665, 0.61338335, 0.6133, 0.51033336]
	best_train_sub_head: 2
	worst: 0.5103
	avg: 0.5721167
	best: 0.61338335

Starting e_i: 988
Model ind 640 epoch 988 head A head_i_epoch 0 batch 0: avg loss -3.213677 avg loss no lamb -3.213677 time 2019-03-01 04:45:23.675315
Model ind 640 epoch 988 head A head_i_epoch 0 batch 100: avg loss -3.137342 avg loss no lamb -3.137342 time 2019-03-01 04:47:03.490281
Model ind 640 epoch 988 head A head_i_epoch 0 batch 200: avg loss -3.249379 avg loss no lamb -3.249379 time 2019-03-01 04:48:43.344096
last batch sz 160
Model ind 640 epoch 988 head B head_i_epoch 0 batch 0: avg loss -1.964181 avg loss no lamb -1.964181 time 2019-03-01 04:49:55.962351
Model ind 640 epoch 988 head B head_i_epoch 0 batch 100: avg loss -1.869036 avg loss no lamb -1.869036 time 2019-03-01 04:51:35.627094
Model ind 640 epoch 988 head B head_i_epoch 0 batch 200: avg loss -1.899954 avg loss no lamb -1.899954 time 2019-03-01 04:53:15.258359
last batch sz 160
Model ind 640 epoch 988 head B head_i_epoch 1 batch 0: avg loss -1.880864 avg loss no lamb -1.880864 time 2019-03-01 04:54:27.641108
Model ind 640 epoch 988 head B head_i_epoch 1 batch 100: avg loss -1.886999 avg loss no lamb -1.886999 time 2019-03-01 04:56:07.243390
Model ind 640 epoch 988 head B head_i_epoch 1 batch 200: avg loss -1.890914 avg loss no lamb -1.890914 time 2019-03-01 04:57:47.008142
last batch sz 160
Pre: time 2019-03-01 04:59:17.916317: 
 	std: 0.05062562
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51021665, 0.61336666, 0.6134667, 0.61338335, 0.50991666]
	train_accs: [0.51021665, 0.61336666, 0.6134667, 0.61338335, 0.50991666]
	best_train_sub_head: 2
	worst: 0.50991666
	avg: 0.57207
	best: 0.6134667

Starting e_i: 989
Model ind 640 epoch 989 head A head_i_epoch 0 batch 0: avg loss -3.288341 avg loss no lamb -3.288341 time 2019-03-01 04:59:20.050033
Model ind 640 epoch 989 head A head_i_epoch 0 batch 100: avg loss -3.147974 avg loss no lamb -3.147974 time 2019-03-01 05:01:00.331663
Model ind 640 epoch 989 head A head_i_epoch 0 batch 200: avg loss -3.150391 avg loss no lamb -3.150391 time 2019-03-01 05:02:40.167985
last batch sz 160
Model ind 640 epoch 989 head B head_i_epoch 0 batch 0: avg loss -1.924089 avg loss no lamb -1.924089 time 2019-03-01 05:03:52.688257
Model ind 640 epoch 989 head B head_i_epoch 0 batch 100: avg loss -1.807924 avg loss no lamb -1.807924 time 2019-03-01 05:05:32.482220
Model ind 640 epoch 989 head B head_i_epoch 0 batch 200: avg loss -1.958696 avg loss no lamb -1.958696 time 2019-03-01 05:07:12.214108
last batch sz 160
Model ind 640 epoch 989 head B head_i_epoch 1 batch 0: avg loss -1.900527 avg loss no lamb -1.900527 time 2019-03-01 05:08:24.691796
Model ind 640 epoch 989 head B head_i_epoch 1 batch 100: avg loss -1.840772 avg loss no lamb -1.840772 time 2019-03-01 05:10:04.470889
Model ind 640 epoch 989 head B head_i_epoch 1 batch 200: avg loss -1.904616 avg loss no lamb -1.904616 time 2019-03-01 05:11:44.251462
last batch sz 160
Pre: time 2019-03-01 05:13:15.323208: 
 	std: 0.050431132
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51121664, 0.61385, 0.61396664, 0.61398333, 0.5107667]
	train_accs: [0.51121664, 0.61385, 0.61396664, 0.61398333, 0.5107667]
	best_train_sub_head: 3
	worst: 0.5107667
	avg: 0.57275665
	best: 0.61398333

Starting e_i: 990
Model ind 640 epoch 990 head A head_i_epoch 0 batch 0: avg loss -3.268134 avg loss no lamb -3.268134 time 2019-03-01 05:13:17.460133
Model ind 640 epoch 990 head A head_i_epoch 0 batch 100: avg loss -3.118680 avg loss no lamb -3.118680 time 2019-03-01 05:14:57.329471
Model ind 640 epoch 990 head A head_i_epoch 0 batch 200: avg loss -3.230470 avg loss no lamb -3.230470 time 2019-03-01 05:16:37.110510
last batch sz 160
Model ind 640 epoch 990 head B head_i_epoch 0 batch 0: avg loss -1.932139 avg loss no lamb -1.932139 time 2019-03-01 05:17:49.618642
Model ind 640 epoch 990 head B head_i_epoch 0 batch 100: avg loss -1.808539 avg loss no lamb -1.808539 time 2019-03-01 05:19:29.301839
Model ind 640 epoch 990 head B head_i_epoch 0 batch 200: avg loss -1.957597 avg loss no lamb -1.957597 time 2019-03-01 05:21:08.883986
last batch sz 160
Model ind 640 epoch 990 head B head_i_epoch 1 batch 0: avg loss -1.847103 avg loss no lamb -1.847103 time 2019-03-01 05:22:21.220236
Model ind 640 epoch 990 head B head_i_epoch 1 batch 100: avg loss -1.847839 avg loss no lamb -1.847839 time 2019-03-01 05:24:00.898063
Model ind 640 epoch 990 head B head_i_epoch 1 batch 200: avg loss -1.945816 avg loss no lamb -1.945816 time 2019-03-01 05:25:40.501940
last batch sz 160
Pre: time 2019-03-01 05:27:11.484789: 
 	std: 0.05019825
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118333, 0.6142, 0.61438334, 0.61431664, 0.5118333]
	train_accs: [0.5118333, 0.6142, 0.61438334, 0.61431664, 0.5118333]
	best_train_sub_head: 2
	worst: 0.5118333
	avg: 0.57331336
	best: 0.61438334

Starting e_i: 991
Model ind 640 epoch 991 head A head_i_epoch 0 batch 0: avg loss -3.217874 avg loss no lamb -3.217874 time 2019-03-01 05:27:16.617630
Model ind 640 epoch 991 head A head_i_epoch 0 batch 100: avg loss -3.187787 avg loss no lamb -3.187787 time 2019-03-01 05:28:56.359506
Model ind 640 epoch 991 head A head_i_epoch 0 batch 200: avg loss -3.271107 avg loss no lamb -3.271107 time 2019-03-01 05:30:36.107847
last batch sz 160
Model ind 640 epoch 991 head B head_i_epoch 0 batch 0: avg loss -1.853824 avg loss no lamb -1.853824 time 2019-03-01 05:31:48.641678
Model ind 640 epoch 991 head B head_i_epoch 0 batch 100: avg loss -1.845606 avg loss no lamb -1.845606 time 2019-03-01 05:33:28.238057
Model ind 640 epoch 991 head B head_i_epoch 0 batch 200: avg loss -1.934517 avg loss no lamb -1.934517 time 2019-03-01 05:35:07.877357
last batch sz 160
Model ind 640 epoch 991 head B head_i_epoch 1 batch 0: avg loss -1.876141 avg loss no lamb -1.876141 time 2019-03-01 05:36:20.311234
Model ind 640 epoch 991 head B head_i_epoch 1 batch 100: avg loss -1.893481 avg loss no lamb -1.893481 time 2019-03-01 05:37:59.971102
Model ind 640 epoch 991 head B head_i_epoch 1 batch 200: avg loss -1.918267 avg loss no lamb -1.918267 time 2019-03-01 05:39:39.605692
last batch sz 160
Pre: time 2019-03-01 05:41:11.043370: 
 	std: 0.05021726
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5109, 0.6134167, 0.61335, 0.6134, 0.51086664]
	train_accs: [0.5109, 0.6134167, 0.61335, 0.6134, 0.51086664]
	best_train_sub_head: 1
	worst: 0.51086664
	avg: 0.5723866
	best: 0.6134167

Starting e_i: 992
Model ind 640 epoch 992 head A head_i_epoch 0 batch 0: avg loss -3.316545 avg loss no lamb -3.316545 time 2019-03-01 05:41:13.233590
Model ind 640 epoch 992 head A head_i_epoch 0 batch 100: avg loss -3.242577 avg loss no lamb -3.242577 time 2019-03-01 05:42:53.487884
Model ind 640 epoch 992 head A head_i_epoch 0 batch 200: avg loss -3.220143 avg loss no lamb -3.220143 time 2019-03-01 05:44:33.392490
last batch sz 160
Model ind 640 epoch 992 head B head_i_epoch 0 batch 0: avg loss -1.888802 avg loss no lamb -1.888802 time 2019-03-01 05:45:45.936203
Model ind 640 epoch 992 head B head_i_epoch 0 batch 100: avg loss -1.916318 avg loss no lamb -1.916318 time 2019-03-01 05:47:25.986208
Model ind 640 epoch 992 head B head_i_epoch 0 batch 200: avg loss -1.962513 avg loss no lamb -1.962513 time 2019-03-01 05:49:06.330268
last batch sz 160
Model ind 640 epoch 992 head B head_i_epoch 1 batch 0: avg loss -1.869121 avg loss no lamb -1.869121 time 2019-03-01 05:50:18.867598
Model ind 640 epoch 992 head B head_i_epoch 1 batch 100: avg loss -1.828782 avg loss no lamb -1.828782 time 2019-03-01 05:51:58.573677
Model ind 640 epoch 992 head B head_i_epoch 1 batch 200: avg loss -1.917367 avg loss no lamb -1.917367 time 2019-03-01 05:53:38.227327
last batch sz 160
Pre: time 2019-03-01 05:55:09.210904: 
 	std: 0.050349303
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51236665, 0.61513335, 0.61501664, 0.6152, 0.51231664]
	train_accs: [0.51236665, 0.61513335, 0.61501664, 0.6152, 0.51231664]
	best_train_sub_head: 3
	worst: 0.51231664
	avg: 0.5740067
	best: 0.6152

Starting e_i: 993
Model ind 640 epoch 993 head A head_i_epoch 0 batch 0: avg loss -3.247660 avg loss no lamb -3.247660 time 2019-03-01 05:55:11.325810
Model ind 640 epoch 993 head A head_i_epoch 0 batch 100: avg loss -3.180046 avg loss no lamb -3.180046 time 2019-03-01 05:56:50.966437
Model ind 640 epoch 993 head A head_i_epoch 0 batch 200: avg loss -3.251727 avg loss no lamb -3.251727 time 2019-03-01 05:58:30.556434
last batch sz 160
Model ind 640 epoch 993 head B head_i_epoch 0 batch 0: avg loss -1.872066 avg loss no lamb -1.872066 time 2019-03-01 05:59:43.011172
Model ind 640 epoch 993 head B head_i_epoch 0 batch 100: avg loss -1.746113 avg loss no lamb -1.746113 time 2019-03-01 06:01:22.822603
Model ind 640 epoch 993 head B head_i_epoch 0 batch 200: avg loss -1.856913 avg loss no lamb -1.856913 time 2019-03-01 06:03:02.302799
last batch sz 160
Model ind 640 epoch 993 head B head_i_epoch 1 batch 0: avg loss -1.914879 avg loss no lamb -1.914879 time 2019-03-01 06:04:14.600323
Model ind 640 epoch 993 head B head_i_epoch 1 batch 100: avg loss -1.821794 avg loss no lamb -1.821794 time 2019-03-01 06:05:54.721240
Model ind 640 epoch 993 head B head_i_epoch 1 batch 200: avg loss -1.982512 avg loss no lamb -1.982512 time 2019-03-01 06:07:34.268808
last batch sz 160
Pre: time 2019-03-01 06:09:05.435093: 
 	std: 0.050254013
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51161665, 0.6142167, 0.6141667, 0.6142333, 0.51163334]
	train_accs: [0.51161665, 0.6142167, 0.6141667, 0.6142333, 0.51163334]
	best_train_sub_head: 3
	worst: 0.51161665
	avg: 0.57317334
	best: 0.6142333

Starting e_i: 994
Model ind 640 epoch 994 head A head_i_epoch 0 batch 0: avg loss -3.123972 avg loss no lamb -3.123972 time 2019-03-01 06:09:08.065229
Model ind 640 epoch 994 head A head_i_epoch 0 batch 100: avg loss -3.120902 avg loss no lamb -3.120902 time 2019-03-01 06:10:47.965109
Model ind 640 epoch 994 head A head_i_epoch 0 batch 200: avg loss -3.268684 avg loss no lamb -3.268684 time 2019-03-01 06:12:27.910673
last batch sz 160
Model ind 640 epoch 994 head B head_i_epoch 0 batch 0: avg loss -1.926590 avg loss no lamb -1.926590 time 2019-03-01 06:13:40.618998
Model ind 640 epoch 994 head B head_i_epoch 0 batch 100: avg loss -1.872712 avg loss no lamb -1.872712 time 2019-03-01 06:15:20.854649
Model ind 640 epoch 994 head B head_i_epoch 0 batch 200: avg loss -1.931894 avg loss no lamb -1.931894 time 2019-03-01 06:17:00.696591
last batch sz 160
Model ind 640 epoch 994 head B head_i_epoch 1 batch 0: avg loss -1.895703 avg loss no lamb -1.895703 time 2019-03-01 06:18:13.504651
Model ind 640 epoch 994 head B head_i_epoch 1 batch 100: avg loss -1.827514 avg loss no lamb -1.827514 time 2019-03-01 06:19:53.445528
Model ind 640 epoch 994 head B head_i_epoch 1 batch 200: avg loss -1.939475 avg loss no lamb -1.939475 time 2019-03-01 06:21:33.573998
last batch sz 160
Pre: time 2019-03-01 06:23:05.382468: 
 	std: 0.05108693
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5109, 0.6152667, 0.6154, 0.61515, 0.5110833]
	train_accs: [0.5109, 0.6152667, 0.6154, 0.61515, 0.5110833]
	best_train_sub_head: 2
	worst: 0.5109
	avg: 0.57356
	best: 0.6154

Starting e_i: 995
Model ind 640 epoch 995 head A head_i_epoch 0 batch 0: avg loss -3.205061 avg loss no lamb -3.205061 time 2019-03-01 06:23:07.497446
Model ind 640 epoch 995 head A head_i_epoch 0 batch 100: avg loss -3.228781 avg loss no lamb -3.228781 time 2019-03-01 06:24:47.259552
Model ind 640 epoch 995 head A head_i_epoch 0 batch 200: avg loss -3.253572 avg loss no lamb -3.253572 time 2019-03-01 06:26:27.103766
last batch sz 160
Model ind 640 epoch 995 head B head_i_epoch 0 batch 0: avg loss -1.977705 avg loss no lamb -1.977705 time 2019-03-01 06:27:39.743988
Model ind 640 epoch 995 head B head_i_epoch 0 batch 100: avg loss -1.861060 avg loss no lamb -1.861060 time 2019-03-01 06:29:20.170168
Model ind 640 epoch 995 head B head_i_epoch 0 batch 200: avg loss -1.938580 avg loss no lamb -1.938580 time 2019-03-01 06:30:59.938059
last batch sz 160
Model ind 640 epoch 995 head B head_i_epoch 1 batch 0: avg loss -1.926100 avg loss no lamb -1.926100 time 2019-03-01 06:32:12.428102
Model ind 640 epoch 995 head B head_i_epoch 1 batch 100: avg loss -1.855564 avg loss no lamb -1.855564 time 2019-03-01 06:33:52.204868
Model ind 640 epoch 995 head B head_i_epoch 1 batch 200: avg loss -1.964722 avg loss no lamb -1.964722 time 2019-03-01 06:35:31.868104
last batch sz 160
Pre: time 2019-03-01 06:37:02.981567: 
 	std: 0.0502678
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5117, 0.61405, 0.6141833, 0.61406666, 0.51128334]
	train_accs: [0.5117, 0.61405, 0.6141833, 0.61406666, 0.51128334]
	best_train_sub_head: 2
	worst: 0.51128334
	avg: 0.57305664
	best: 0.6141833

Starting e_i: 996
Model ind 640 epoch 996 head A head_i_epoch 0 batch 0: avg loss -3.264818 avg loss no lamb -3.264818 time 2019-03-01 06:37:05.105633
Model ind 640 epoch 996 head A head_i_epoch 0 batch 100: avg loss -3.238760 avg loss no lamb -3.238760 time 2019-03-01 06:38:44.957343
Model ind 640 epoch 996 head A head_i_epoch 0 batch 200: avg loss -3.257941 avg loss no lamb -3.257941 time 2019-03-01 06:40:25.319900
last batch sz 160
Model ind 640 epoch 996 head B head_i_epoch 0 batch 0: avg loss -1.934647 avg loss no lamb -1.934647 time 2019-03-01 06:41:37.905370
Model ind 640 epoch 996 head B head_i_epoch 0 batch 100: avg loss -1.820670 avg loss no lamb -1.820670 time 2019-03-01 06:43:17.542989
Model ind 640 epoch 996 head B head_i_epoch 0 batch 200: avg loss -1.952884 avg loss no lamb -1.952884 time 2019-03-01 06:44:57.216505
last batch sz 160
Model ind 640 epoch 996 head B head_i_epoch 1 batch 0: avg loss -1.903142 avg loss no lamb -1.903142 time 2019-03-01 06:46:10.030339
Model ind 640 epoch 996 head B head_i_epoch 1 batch 100: avg loss -1.889646 avg loss no lamb -1.889646 time 2019-03-01 06:47:50.348546
Model ind 640 epoch 996 head B head_i_epoch 1 batch 200: avg loss -1.935477 avg loss no lamb -1.935477 time 2019-03-01 06:49:30.532608
last batch sz 160
Pre: time 2019-03-01 06:51:01.585082: 
 	std: 0.050175183
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51163334, 0.6142167, 0.61425, 0.6141667, 0.51195]
	train_accs: [0.51163334, 0.6142167, 0.61425, 0.6141667, 0.51195]
	best_train_sub_head: 2
	worst: 0.51163334
	avg: 0.5732433
	best: 0.61425

Starting e_i: 997
Model ind 640 epoch 997 head A head_i_epoch 0 batch 0: avg loss -3.301505 avg loss no lamb -3.301505 time 2019-03-01 06:51:03.699401
Model ind 640 epoch 997 head A head_i_epoch 0 batch 100: avg loss -3.201941 avg loss no lamb -3.201941 time 2019-03-01 06:52:43.565153
Model ind 640 epoch 997 head A head_i_epoch 0 batch 200: avg loss -3.253157 avg loss no lamb -3.253157 time 2019-03-01 06:54:23.942168
last batch sz 160
Model ind 640 epoch 997 head B head_i_epoch 0 batch 0: avg loss -1.856104 avg loss no lamb -1.856104 time 2019-03-01 06:55:36.552052
Model ind 640 epoch 997 head B head_i_epoch 0 batch 100: avg loss -1.853136 avg loss no lamb -1.853136 time 2019-03-01 06:57:16.707688
Model ind 640 epoch 997 head B head_i_epoch 0 batch 200: avg loss -1.937890 avg loss no lamb -1.937890 time 2019-03-01 06:58:56.817634
last batch sz 160
Model ind 640 epoch 997 head B head_i_epoch 1 batch 0: avg loss -1.930366 avg loss no lamb -1.930366 time 2019-03-01 07:00:09.603204
Model ind 640 epoch 997 head B head_i_epoch 1 batch 100: avg loss -1.861503 avg loss no lamb -1.861503 time 2019-03-01 07:01:49.421520
Model ind 640 epoch 997 head B head_i_epoch 1 batch 200: avg loss -1.952809 avg loss no lamb -1.952809 time 2019-03-01 07:03:29.226003
last batch sz 160
Pre: time 2019-03-01 07:05:00.680771: 
 	std: 0.050869163
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51005, 0.6139167, 0.61401665, 0.61405, 0.51026666]
	train_accs: [0.51005, 0.6139167, 0.61401665, 0.61405, 0.51026666]
	best_train_sub_head: 3
	worst: 0.51005
	avg: 0.57246
	best: 0.61405

Starting e_i: 998
Model ind 640 epoch 998 head A head_i_epoch 0 batch 0: avg loss -3.238597 avg loss no lamb -3.238597 time 2019-03-01 07:05:02.814493
Model ind 640 epoch 998 head A head_i_epoch 0 batch 100: avg loss -3.201542 avg loss no lamb -3.201542 time 2019-03-01 07:06:42.651069
Model ind 640 epoch 998 head A head_i_epoch 0 batch 200: avg loss -3.223193 avg loss no lamb -3.223193 time 2019-03-01 07:08:22.572518
last batch sz 160
Model ind 640 epoch 998 head B head_i_epoch 0 batch 0: avg loss -1.928196 avg loss no lamb -1.928196 time 2019-03-01 07:09:34.996520
Model ind 640 epoch 998 head B head_i_epoch 0 batch 100: avg loss -1.869904 avg loss no lamb -1.869904 time 2019-03-01 07:11:14.440559
Model ind 640 epoch 998 head B head_i_epoch 0 batch 200: avg loss -1.956052 avg loss no lamb -1.956052 time 2019-03-01 07:12:53.898864
last batch sz 160
Model ind 640 epoch 998 head B head_i_epoch 1 batch 0: avg loss -1.942160 avg loss no lamb -1.942160 time 2019-03-01 07:14:06.227695
Model ind 640 epoch 998 head B head_i_epoch 1 batch 100: avg loss -1.928200 avg loss no lamb -1.928200 time 2019-03-01 07:15:46.138660
Model ind 640 epoch 998 head B head_i_epoch 1 batch 200: avg loss -1.983186 avg loss no lamb -1.983186 time 2019-03-01 07:17:25.561748
last batch sz 160
Pre: time 2019-03-01 07:18:57.955418: 
 	std: 0.051016197
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5100333, 0.6142833, 0.61441666, 0.6142833, 0.51035]
	train_accs: [0.5100333, 0.6142833, 0.61441666, 0.6142833, 0.51035]
	best_train_sub_head: 2
	worst: 0.5100333
	avg: 0.5726733
	best: 0.61441666

Starting e_i: 999
Model ind 640 epoch 999 head A head_i_epoch 0 batch 0: avg loss -3.249627 avg loss no lamb -3.249627 time 2019-03-01 07:19:00.066355
Model ind 640 epoch 999 head A head_i_epoch 0 batch 100: avg loss -3.199869 avg loss no lamb -3.199869 time 2019-03-01 07:20:39.749663
Model ind 640 epoch 999 head A head_i_epoch 0 batch 200: avg loss -3.210747 avg loss no lamb -3.210747 time 2019-03-01 07:22:19.774433
last batch sz 160
Model ind 640 epoch 999 head B head_i_epoch 0 batch 0: avg loss -1.997337 avg loss no lamb -1.997337 time 2019-03-01 07:23:32.566431
Model ind 640 epoch 999 head B head_i_epoch 0 batch 100: avg loss -1.813568 avg loss no lamb -1.813568 time 2019-03-01 07:25:12.192932
Model ind 640 epoch 999 head B head_i_epoch 0 batch 200: avg loss -2.021231 avg loss no lamb -2.021231 time 2019-03-01 07:26:51.797702
last batch sz 160
Model ind 640 epoch 999 head B head_i_epoch 1 batch 0: avg loss -1.972328 avg loss no lamb -1.972328 time 2019-03-01 07:28:04.212808
Model ind 640 epoch 999 head B head_i_epoch 1 batch 100: avg loss -1.860575 avg loss no lamb -1.860575 time 2019-03-01 07:29:44.707718
Model ind 640 epoch 999 head B head_i_epoch 1 batch 200: avg loss -1.968774 avg loss no lamb -1.968774 time 2019-03-01 07:31:24.620601
last batch sz 160
Pre: time 2019-03-01 07:32:55.848571: 
 	std: 0.05084751
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103, 0.6142333, 0.61431664, 0.61435, 0.5107167]
	train_accs: [0.5103, 0.6142333, 0.61431664, 0.61435, 0.5107167]
	best_train_sub_head: 3
	worst: 0.5103
	avg: 0.57278335
	best: 0.61435

Starting e_i: 1000
Model ind 640 epoch 1000 head A head_i_epoch 0 batch 0: avg loss -3.262120 avg loss no lamb -3.262120 time 2019-03-01 07:32:57.970477
Model ind 640 epoch 1000 head A head_i_epoch 0 batch 100: avg loss -3.165282 avg loss no lamb -3.165282 time 2019-03-01 07:34:38.129780
Model ind 640 epoch 1000 head A head_i_epoch 0 batch 200: avg loss -3.220818 avg loss no lamb -3.220818 time 2019-03-01 07:36:18.004952
last batch sz 160
Model ind 640 epoch 1000 head B head_i_epoch 0 batch 0: avg loss -1.881963 avg loss no lamb -1.881963 time 2019-03-01 07:37:30.580399
Model ind 640 epoch 1000 head B head_i_epoch 0 batch 100: avg loss -1.792282 avg loss no lamb -1.792282 time 2019-03-01 07:39:10.903267
Model ind 640 epoch 1000 head B head_i_epoch 0 batch 200: avg loss -1.923344 avg loss no lamb -1.923344 time 2019-03-01 07:40:50.426646
last batch sz 160
Model ind 640 epoch 1000 head B head_i_epoch 1 batch 0: avg loss -1.912947 avg loss no lamb -1.912947 time 2019-03-01 07:42:02.748481
Model ind 640 epoch 1000 head B head_i_epoch 1 batch 100: avg loss -1.864958 avg loss no lamb -1.864958 time 2019-03-01 07:43:42.063881
Model ind 640 epoch 1000 head B head_i_epoch 1 batch 200: avg loss -1.951613 avg loss no lamb -1.951613 time 2019-03-01 07:45:21.479604
last batch sz 160
Pre: time 2019-03-01 07:46:52.386628: 
 	std: 0.05090069
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50946665, 0.61366665, 0.61366665, 0.61356664, 0.51]
	train_accs: [0.50946665, 0.61366665, 0.61366665, 0.61356664, 0.51]
	best_train_sub_head: 1
	worst: 0.50946665
	avg: 0.57207334
	best: 0.61366665

Starting e_i: 1001
Model ind 640 epoch 1001 head A head_i_epoch 0 batch 0: avg loss -3.268261 avg loss no lamb -3.268261 time 2019-03-01 07:46:58.633433
Model ind 640 epoch 1001 head A head_i_epoch 0 batch 100: avg loss -3.194646 avg loss no lamb -3.194646 time 2019-03-01 07:48:38.408000
Model ind 640 epoch 1001 head A head_i_epoch 0 batch 200: avg loss -3.274688 avg loss no lamb -3.274688 time 2019-03-01 07:50:18.220991
last batch sz 160
Model ind 640 epoch 1001 head B head_i_epoch 0 batch 0: avg loss -1.930836 avg loss no lamb -1.930836 time 2019-03-01 07:51:30.773015
Model ind 640 epoch 1001 head B head_i_epoch 0 batch 100: avg loss -1.846481 avg loss no lamb -1.846481 time 2019-03-01 07:53:10.588042
Model ind 640 epoch 1001 head B head_i_epoch 0 batch 200: avg loss -1.927474 avg loss no lamb -1.927474 time 2019-03-01 07:54:50.256942
last batch sz 160
Model ind 640 epoch 1001 head B head_i_epoch 1 batch 0: avg loss -1.824734 avg loss no lamb -1.824734 time 2019-03-01 07:56:02.671537
Model ind 640 epoch 1001 head B head_i_epoch 1 batch 100: avg loss -1.796300 avg loss no lamb -1.796300 time 2019-03-01 07:57:42.462142
Model ind 640 epoch 1001 head B head_i_epoch 1 batch 200: avg loss -1.963768 avg loss no lamb -1.963768 time 2019-03-01 07:59:22.136963
last batch sz 160
Pre: time 2019-03-01 08:00:53.082109: 
 	std: 0.05108731
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107167, 0.6152, 0.61545, 0.6153167, 0.51136667]
	train_accs: [0.5107167, 0.6152, 0.61545, 0.6153167, 0.51136667]
	best_train_sub_head: 2
	worst: 0.5107167
	avg: 0.57361
	best: 0.61545

Starting e_i: 1002
Model ind 640 epoch 1002 head A head_i_epoch 0 batch 0: avg loss -3.265847 avg loss no lamb -3.265847 time 2019-03-01 08:00:55.196672
Model ind 640 epoch 1002 head A head_i_epoch 0 batch 100: avg loss -3.239649 avg loss no lamb -3.239649 time 2019-03-01 08:02:34.961955
Model ind 640 epoch 1002 head A head_i_epoch 0 batch 200: avg loss -3.353766 avg loss no lamb -3.353766 time 2019-03-01 08:04:14.731541
last batch sz 160
Model ind 640 epoch 1002 head B head_i_epoch 0 batch 0: avg loss -1.897942 avg loss no lamb -1.897942 time 2019-03-01 08:05:27.242480
Model ind 640 epoch 1002 head B head_i_epoch 0 batch 100: avg loss -1.881989 avg loss no lamb -1.881989 time 2019-03-01 08:07:06.912049
Model ind 640 epoch 1002 head B head_i_epoch 0 batch 200: avg loss -1.945038 avg loss no lamb -1.945038 time 2019-03-01 08:08:48.012673
last batch sz 160
Model ind 640 epoch 1002 head B head_i_epoch 1 batch 0: avg loss -1.868100 avg loss no lamb -1.868100 time 2019-03-01 08:10:01.608192
Model ind 640 epoch 1002 head B head_i_epoch 1 batch 100: avg loss -1.853796 avg loss no lamb -1.853796 time 2019-03-01 08:11:41.824185
Model ind 640 epoch 1002 head B head_i_epoch 1 batch 200: avg loss -1.918469 avg loss no lamb -1.918469 time 2019-03-01 08:13:21.600895
last batch sz 160
Pre: time 2019-03-01 08:14:52.724609: 
 	std: 0.05044861
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5113, 0.6142, 0.61435, 0.6142833, 0.5113]
	train_accs: [0.5113, 0.6142, 0.61435, 0.6142833, 0.5113]
	best_train_sub_head: 2
	worst: 0.5113
	avg: 0.5730867
	best: 0.61435

Starting e_i: 1003
Model ind 640 epoch 1003 head A head_i_epoch 0 batch 0: avg loss -3.232308 avg loss no lamb -3.232308 time 2019-03-01 08:14:54.864282
Model ind 640 epoch 1003 head A head_i_epoch 0 batch 100: avg loss -3.224855 avg loss no lamb -3.224855 time 2019-03-01 08:16:34.701986
Model ind 640 epoch 1003 head A head_i_epoch 0 batch 200: avg loss -3.259167 avg loss no lamb -3.259167 time 2019-03-01 08:18:15.154605
last batch sz 160
Model ind 640 epoch 1003 head B head_i_epoch 0 batch 0: avg loss -1.899911 avg loss no lamb -1.899911 time 2019-03-01 08:19:27.870768
Model ind 640 epoch 1003 head B head_i_epoch 0 batch 100: avg loss -1.859786 avg loss no lamb -1.859786 time 2019-03-01 08:21:07.715512
Model ind 640 epoch 1003 head B head_i_epoch 0 batch 200: avg loss -1.866814 avg loss no lamb -1.866814 time 2019-03-01 08:22:47.536642
last batch sz 160
Model ind 640 epoch 1003 head B head_i_epoch 1 batch 0: avg loss -1.931736 avg loss no lamb -1.931736 time 2019-03-01 08:24:00.098875
Model ind 640 epoch 1003 head B head_i_epoch 1 batch 100: avg loss -1.796605 avg loss no lamb -1.796605 time 2019-03-01 08:25:39.876742
Model ind 640 epoch 1003 head B head_i_epoch 1 batch 200: avg loss -1.885815 avg loss no lamb -1.885815 time 2019-03-01 08:27:19.745917
last batch sz 160
Pre: time 2019-03-01 08:28:50.958242: 
 	std: 0.050680302
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5105, 0.61405, 0.61436665, 0.6142333, 0.51103336]
	train_accs: [0.5105, 0.61405, 0.61436665, 0.6142333, 0.51103336]
	best_train_sub_head: 2
	worst: 0.5105
	avg: 0.57283664
	best: 0.61436665

Starting e_i: 1004
Model ind 640 epoch 1004 head A head_i_epoch 0 batch 0: avg loss -3.240863 avg loss no lamb -3.240863 time 2019-03-01 08:28:53.109790
Model ind 640 epoch 1004 head A head_i_epoch 0 batch 100: avg loss -3.225000 avg loss no lamb -3.225000 time 2019-03-01 08:30:33.059184
Model ind 640 epoch 1004 head A head_i_epoch 0 batch 200: avg loss -3.258753 avg loss no lamb -3.258753 time 2019-03-01 08:32:12.917062
last batch sz 160
Model ind 640 epoch 1004 head B head_i_epoch 0 batch 0: avg loss -1.863396 avg loss no lamb -1.863396 time 2019-03-01 08:33:25.478917
Model ind 640 epoch 1004 head B head_i_epoch 0 batch 100: avg loss -1.855145 avg loss no lamb -1.855145 time 2019-03-01 08:35:05.207881
Model ind 640 epoch 1004 head B head_i_epoch 0 batch 200: avg loss -1.970583 avg loss no lamb -1.970583 time 2019-03-01 08:36:44.974703
last batch sz 160
Model ind 640 epoch 1004 head B head_i_epoch 1 batch 0: avg loss -1.848762 avg loss no lamb -1.848762 time 2019-03-01 08:37:57.327444
Model ind 640 epoch 1004 head B head_i_epoch 1 batch 100: avg loss -1.884439 avg loss no lamb -1.884439 time 2019-03-01 08:39:36.934158
Model ind 640 epoch 1004 head B head_i_epoch 1 batch 200: avg loss -1.933767 avg loss no lamb -1.933767 time 2019-03-01 08:41:16.486728
last batch sz 160
Pre: time 2019-03-01 08:42:47.390576: 
 	std: 0.050386094
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51115, 0.6141667, 0.614, 0.6141833, 0.51138335]
	train_accs: [0.51115, 0.6141667, 0.614, 0.6141833, 0.51138335]
	best_train_sub_head: 3
	worst: 0.51115
	avg: 0.57297665
	best: 0.6141833

Starting e_i: 1005
Model ind 640 epoch 1005 head A head_i_epoch 0 batch 0: avg loss -3.305521 avg loss no lamb -3.305521 time 2019-03-01 08:42:49.617190
Model ind 640 epoch 1005 head A head_i_epoch 0 batch 100: avg loss -3.204369 avg loss no lamb -3.204369 time 2019-03-01 08:44:29.358769
Model ind 640 epoch 1005 head A head_i_epoch 0 batch 200: avg loss -3.246346 avg loss no lamb -3.246346 time 2019-03-01 08:46:08.984149
last batch sz 160
Model ind 640 epoch 1005 head B head_i_epoch 0 batch 0: avg loss -1.945809 avg loss no lamb -1.945809 time 2019-03-01 08:47:21.394873
Model ind 640 epoch 1005 head B head_i_epoch 0 batch 100: avg loss -1.728806 avg loss no lamb -1.728806 time 2019-03-01 08:49:00.866108
Model ind 640 epoch 1005 head B head_i_epoch 0 batch 200: avg loss -1.867171 avg loss no lamb -1.867171 time 2019-03-01 08:50:40.319774
last batch sz 160
Model ind 640 epoch 1005 head B head_i_epoch 1 batch 0: avg loss -1.858135 avg loss no lamb -1.858135 time 2019-03-01 08:51:52.560353
Model ind 640 epoch 1005 head B head_i_epoch 1 batch 100: avg loss -1.885970 avg loss no lamb -1.885970 time 2019-03-01 08:53:31.964932
Model ind 640 epoch 1005 head B head_i_epoch 1 batch 200: avg loss -1.946733 avg loss no lamb -1.946733 time 2019-03-01 08:55:11.541198
last batch sz 160
Pre: time 2019-03-01 08:56:42.762536: 
 	std: 0.050542567
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.511, 0.6142, 0.61431664, 0.61431664, 0.51121664]
	train_accs: [0.511, 0.6142, 0.61431664, 0.61431664, 0.51121664]
	best_train_sub_head: 2
	worst: 0.511
	avg: 0.57301
	best: 0.61431664

Starting e_i: 1006
Model ind 640 epoch 1006 head A head_i_epoch 0 batch 0: avg loss -3.306349 avg loss no lamb -3.306349 time 2019-03-01 08:56:44.874100
Model ind 640 epoch 1006 head A head_i_epoch 0 batch 100: avg loss -3.134145 avg loss no lamb -3.134145 time 2019-03-01 08:58:24.433115
Model ind 640 epoch 1006 head A head_i_epoch 0 batch 200: avg loss -3.248525 avg loss no lamb -3.248525 time 2019-03-01 09:00:04.470002
last batch sz 160
Model ind 640 epoch 1006 head B head_i_epoch 0 batch 0: avg loss -1.865538 avg loss no lamb -1.865538 time 2019-03-01 09:01:16.958531
Model ind 640 epoch 1006 head B head_i_epoch 0 batch 100: avg loss -1.800287 avg loss no lamb -1.800287 time 2019-03-01 09:02:57.037902
Model ind 640 epoch 1006 head B head_i_epoch 0 batch 200: avg loss -1.961001 avg loss no lamb -1.961001 time 2019-03-01 09:04:36.868154
last batch sz 160
Model ind 640 epoch 1006 head B head_i_epoch 1 batch 0: avg loss -1.973096 avg loss no lamb -1.973096 time 2019-03-01 09:05:49.179468
Model ind 640 epoch 1006 head B head_i_epoch 1 batch 100: avg loss -1.903419 avg loss no lamb -1.903419 time 2019-03-01 09:07:28.717299
Model ind 640 epoch 1006 head B head_i_epoch 1 batch 200: avg loss -1.971185 avg loss no lamb -1.971185 time 2019-03-01 09:09:08.205530
last batch sz 160
Pre: time 2019-03-01 09:10:39.187824: 
 	std: 0.050424244
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51063335, 0.61361665, 0.61385, 0.61366665, 0.51093334]
	train_accs: [0.51063335, 0.61361665, 0.61385, 0.61366665, 0.51093334]
	best_train_sub_head: 2
	worst: 0.51063335
	avg: 0.57254
	best: 0.61385

Starting e_i: 1007
Model ind 640 epoch 1007 head A head_i_epoch 0 batch 0: avg loss -3.173843 avg loss no lamb -3.173843 time 2019-03-01 09:10:41.309898
Model ind 640 epoch 1007 head A head_i_epoch 0 batch 100: avg loss -3.109399 avg loss no lamb -3.109399 time 2019-03-01 09:12:20.789474
Model ind 640 epoch 1007 head A head_i_epoch 0 batch 200: avg loss -3.201247 avg loss no lamb -3.201247 time 2019-03-01 09:14:00.670606
last batch sz 160
Model ind 640 epoch 1007 head B head_i_epoch 0 batch 0: avg loss -1.963902 avg loss no lamb -1.963902 time 2019-03-01 09:15:13.098181
Model ind 640 epoch 1007 head B head_i_epoch 0 batch 100: avg loss -1.822859 avg loss no lamb -1.822859 time 2019-03-01 09:16:52.857780
Model ind 640 epoch 1007 head B head_i_epoch 0 batch 200: avg loss -1.972050 avg loss no lamb -1.972050 time 2019-03-01 09:18:32.176800
last batch sz 160
Model ind 640 epoch 1007 head B head_i_epoch 1 batch 0: avg loss -1.895398 avg loss no lamb -1.895398 time 2019-03-01 09:19:44.858150
Model ind 640 epoch 1007 head B head_i_epoch 1 batch 100: avg loss -1.857191 avg loss no lamb -1.857191 time 2019-03-01 09:21:24.259184
Model ind 640 epoch 1007 head B head_i_epoch 1 batch 200: avg loss -1.901122 avg loss no lamb -1.901122 time 2019-03-01 09:23:03.657661
last batch sz 160
Pre: time 2019-03-01 09:24:34.334861: 
 	std: 0.050650034
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51058334, 0.61395, 0.6141, 0.61396664, 0.51065]
	train_accs: [0.51058334, 0.61395, 0.6141, 0.61396664, 0.51065]
	best_train_sub_head: 2
	worst: 0.51058334
	avg: 0.57265
	best: 0.6141

Starting e_i: 1008
Model ind 640 epoch 1008 head A head_i_epoch 0 batch 0: avg loss -3.300115 avg loss no lamb -3.300115 time 2019-03-01 09:24:36.976319
Model ind 640 epoch 1008 head A head_i_epoch 0 batch 100: avg loss -3.148479 avg loss no lamb -3.148479 time 2019-03-01 09:26:16.793615
Model ind 640 epoch 1008 head A head_i_epoch 0 batch 200: avg loss -3.262383 avg loss no lamb -3.262383 time 2019-03-01 09:27:56.625852
last batch sz 160
Model ind 640 epoch 1008 head B head_i_epoch 0 batch 0: avg loss -1.936684 avg loss no lamb -1.936684 time 2019-03-01 09:29:09.509917
Model ind 640 epoch 1008 head B head_i_epoch 0 batch 100: avg loss -1.824894 avg loss no lamb -1.824894 time 2019-03-01 09:30:49.262591
Model ind 640 epoch 1008 head B head_i_epoch 0 batch 200: avg loss -1.940501 avg loss no lamb -1.940501 time 2019-03-01 09:32:29.293363
last batch sz 160
Model ind 640 epoch 1008 head B head_i_epoch 1 batch 0: avg loss -1.940255 avg loss no lamb -1.940255 time 2019-03-01 09:33:41.784996
Model ind 640 epoch 1008 head B head_i_epoch 1 batch 100: avg loss -1.860874 avg loss no lamb -1.860874 time 2019-03-01 09:35:21.842678
Model ind 640 epoch 1008 head B head_i_epoch 1 batch 200: avg loss -1.933557 avg loss no lamb -1.933557 time 2019-03-01 09:37:01.572137
last batch sz 160
Pre: time 2019-03-01 09:38:32.523204: 
 	std: 0.050899077
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51121664, 0.61515, 0.6152667, 0.61515, 0.51136667]
	train_accs: [0.51121664, 0.61515, 0.6152667, 0.61515, 0.51136667]
	best_train_sub_head: 2
	worst: 0.51121664
	avg: 0.57363
	best: 0.6152667

Starting e_i: 1009
Model ind 640 epoch 1009 head A head_i_epoch 0 batch 0: avg loss -3.293676 avg loss no lamb -3.293676 time 2019-03-01 09:38:34.665889
Model ind 640 epoch 1009 head A head_i_epoch 0 batch 100: avg loss -3.212702 avg loss no lamb -3.212702 time 2019-03-01 09:40:14.429165
Model ind 640 epoch 1009 head A head_i_epoch 0 batch 200: avg loss -3.218871 avg loss no lamb -3.218871 time 2019-03-01 09:41:54.263049
last batch sz 160
Model ind 640 epoch 1009 head B head_i_epoch 0 batch 0: avg loss -1.886677 avg loss no lamb -1.886677 time 2019-03-01 09:43:07.135328
Model ind 640 epoch 1009 head B head_i_epoch 0 batch 100: avg loss -1.818984 avg loss no lamb -1.818984 time 2019-03-01 09:44:47.209519
Model ind 640 epoch 1009 head B head_i_epoch 0 batch 200: avg loss -1.942531 avg loss no lamb -1.942531 time 2019-03-01 09:46:27.188890
last batch sz 160
Model ind 640 epoch 1009 head B head_i_epoch 1 batch 0: avg loss -1.936958 avg loss no lamb -1.936958 time 2019-03-01 09:47:39.670168
Model ind 640 epoch 1009 head B head_i_epoch 1 batch 100: avg loss -1.813969 avg loss no lamb -1.813969 time 2019-03-01 09:49:19.352433
Model ind 640 epoch 1009 head B head_i_epoch 1 batch 200: avg loss -1.971040 avg loss no lamb -1.971040 time 2019-03-01 09:50:58.978884
last batch sz 160
Pre: time 2019-03-01 09:52:29.964339: 
 	std: 0.05071406
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51165, 0.6149833, 0.61505, 0.615, 0.51133335]
	train_accs: [0.51165, 0.6149833, 0.61505, 0.615, 0.51133335]
	best_train_sub_head: 2
	worst: 0.51133335
	avg: 0.57360333
	best: 0.61505

Starting e_i: 1010
Model ind 640 epoch 1010 head A head_i_epoch 0 batch 0: avg loss -3.345772 avg loss no lamb -3.345772 time 2019-03-01 09:52:32.179228
Model ind 640 epoch 1010 head A head_i_epoch 0 batch 100: avg loss -3.223754 avg loss no lamb -3.223754 time 2019-03-01 09:54:12.450833
Model ind 640 epoch 1010 head A head_i_epoch 0 batch 200: avg loss -3.190941 avg loss no lamb -3.190941 time 2019-03-01 09:55:52.436519
last batch sz 160
Model ind 640 epoch 1010 head B head_i_epoch 0 batch 0: avg loss -1.925941 avg loss no lamb -1.925941 time 2019-03-01 09:57:04.881149
Model ind 640 epoch 1010 head B head_i_epoch 0 batch 100: avg loss -1.836263 avg loss no lamb -1.836263 time 2019-03-01 09:58:44.338683
Model ind 640 epoch 1010 head B head_i_epoch 0 batch 200: avg loss -1.916583 avg loss no lamb -1.916583 time 2019-03-01 10:00:24.498724
last batch sz 160
Model ind 640 epoch 1010 head B head_i_epoch 1 batch 0: avg loss -1.860503 avg loss no lamb -1.860503 time 2019-03-01 10:01:37.010039
Model ind 640 epoch 1010 head B head_i_epoch 1 batch 100: avg loss -1.832308 avg loss no lamb -1.832308 time 2019-03-01 10:03:16.687133
Model ind 640 epoch 1010 head B head_i_epoch 1 batch 200: avg loss -1.904056 avg loss no lamb -1.904056 time 2019-03-01 10:04:56.379478
last batch sz 160
Pre: time 2019-03-01 10:06:27.314753: 
 	std: 0.05032478
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114167, 0.6141833, 0.61405, 0.61406666, 0.51133335]
	train_accs: [0.5114167, 0.6141833, 0.61405, 0.61406666, 0.51133335]
	best_train_sub_head: 1
	worst: 0.51133335
	avg: 0.57300997
	best: 0.6141833

Starting e_i: 1011
Model ind 640 epoch 1011 head A head_i_epoch 0 batch 0: avg loss -3.307045 avg loss no lamb -3.307045 time 2019-03-01 10:06:32.843132
Model ind 640 epoch 1011 head A head_i_epoch 0 batch 100: avg loss -3.204993 avg loss no lamb -3.204993 time 2019-03-01 10:08:12.480816
Model ind 640 epoch 1011 head A head_i_epoch 0 batch 200: avg loss -3.196494 avg loss no lamb -3.196494 time 2019-03-01 10:09:52.308667
last batch sz 160
Model ind 640 epoch 1011 head B head_i_epoch 0 batch 0: avg loss -1.941441 avg loss no lamb -1.941441 time 2019-03-01 10:11:04.753324
Model ind 640 epoch 1011 head B head_i_epoch 0 batch 100: avg loss -1.816010 avg loss no lamb -1.816010 time 2019-03-01 10:12:44.288075
Model ind 640 epoch 1011 head B head_i_epoch 0 batch 200: avg loss -1.939655 avg loss no lamb -1.939655 time 2019-03-01 10:14:23.769595
last batch sz 160
Model ind 640 epoch 1011 head B head_i_epoch 1 batch 0: avg loss -1.853989 avg loss no lamb -1.853989 time 2019-03-01 10:15:36.388910
Model ind 640 epoch 1011 head B head_i_epoch 1 batch 100: avg loss -1.775703 avg loss no lamb -1.775703 time 2019-03-01 10:17:15.902335
Model ind 640 epoch 1011 head B head_i_epoch 1 batch 200: avg loss -1.909764 avg loss no lamb -1.909764 time 2019-03-01 10:18:55.429994
last batch sz 160
Pre: time 2019-03-01 10:20:26.238264: 
 	std: 0.05068312
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5089167, 0.61263335, 0.6127167, 0.6127667, 0.50958335]
	train_accs: [0.5089167, 0.61263335, 0.6127167, 0.6127667, 0.50958335]
	best_train_sub_head: 3
	worst: 0.5089167
	avg: 0.5713234
	best: 0.6127667

Starting e_i: 1012
Model ind 640 epoch 1012 head A head_i_epoch 0 batch 0: avg loss -3.247957 avg loss no lamb -3.247957 time 2019-03-01 10:20:28.374634
Model ind 640 epoch 1012 head A head_i_epoch 0 batch 100: avg loss -3.238962 avg loss no lamb -3.238962 time 2019-03-01 10:22:07.815232
Model ind 640 epoch 1012 head A head_i_epoch 0 batch 200: avg loss -3.248749 avg loss no lamb -3.248749 time 2019-03-01 10:23:47.448725
last batch sz 160
Model ind 640 epoch 1012 head B head_i_epoch 0 batch 0: avg loss -1.973030 avg loss no lamb -1.973030 time 2019-03-01 10:24:59.963212
Model ind 640 epoch 1012 head B head_i_epoch 0 batch 100: avg loss -1.880183 avg loss no lamb -1.880183 time 2019-03-01 10:26:39.336353
Model ind 640 epoch 1012 head B head_i_epoch 0 batch 200: avg loss -1.994311 avg loss no lamb -1.994311 time 2019-03-01 10:28:18.762625
last batch sz 160
Model ind 640 epoch 1012 head B head_i_epoch 1 batch 0: avg loss -1.957374 avg loss no lamb -1.957374 time 2019-03-01 10:29:30.974808
Model ind 640 epoch 1012 head B head_i_epoch 1 batch 100: avg loss -1.809919 avg loss no lamb -1.809919 time 2019-03-01 10:31:10.365199
Model ind 640 epoch 1012 head B head_i_epoch 1 batch 200: avg loss -1.926041 avg loss no lamb -1.926041 time 2019-03-01 10:32:53.787128
last batch sz 160
Pre: time 2019-03-01 10:34:31.042831: 
 	std: 0.05100518
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5100167, 0.6142, 0.61415, 0.61411667, 0.5100667]
	train_accs: [0.5100167, 0.6142, 0.61415, 0.61411667, 0.5100667]
	best_train_sub_head: 1
	worst: 0.5100167
	avg: 0.57251
	best: 0.6142

Starting e_i: 1013
Model ind 640 epoch 1013 head A head_i_epoch 0 batch 0: avg loss -3.297184 avg loss no lamb -3.297184 time 2019-03-01 10:34:33.244337
Model ind 640 epoch 1013 head A head_i_epoch 0 batch 100: avg loss -3.252905 avg loss no lamb -3.252905 time 2019-03-01 10:36:22.274243
Model ind 640 epoch 1013 head A head_i_epoch 0 batch 200: avg loss -3.190817 avg loss no lamb -3.190817 time 2019-03-01 10:38:09.668529
last batch sz 160
Model ind 640 epoch 1013 head B head_i_epoch 0 batch 0: avg loss -1.991723 avg loss no lamb -1.991723 time 2019-03-01 10:39:32.593543
Model ind 640 epoch 1013 head B head_i_epoch 0 batch 100: avg loss -1.868984 avg loss no lamb -1.868984 time 2019-03-01 10:41:23.818497
Model ind 640 epoch 1013 head B head_i_epoch 0 batch 200: avg loss -1.874780 avg loss no lamb -1.874780 time 2019-03-01 10:43:17.804747
last batch sz 160
Model ind 640 epoch 1013 head B head_i_epoch 1 batch 0: avg loss -1.913839 avg loss no lamb -1.913839 time 2019-03-01 10:44:39.279852
Model ind 640 epoch 1013 head B head_i_epoch 1 batch 100: avg loss -1.887875 avg loss no lamb -1.887875 time 2019-03-01 10:46:31.851435
Model ind 640 epoch 1013 head B head_i_epoch 1 batch 200: avg loss -1.880257 avg loss no lamb -1.880257 time 2019-03-01 10:48:24.888896
last batch sz 160
Pre: time 2019-03-01 10:50:08.118149: 
 	std: 0.05083102
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107333, 0.6145667, 0.6145833, 0.61445, 0.5108167]
	train_accs: [0.5107333, 0.6145667, 0.6145833, 0.61445, 0.5108167]
	best_train_sub_head: 2
	worst: 0.5107333
	avg: 0.57303
	best: 0.6145833

Starting e_i: 1014
Model ind 640 epoch 1014 head A head_i_epoch 0 batch 0: avg loss -3.171450 avg loss no lamb -3.171450 time 2019-03-01 10:50:10.691243
Model ind 640 epoch 1014 head A head_i_epoch 0 batch 100: avg loss -3.169330 avg loss no lamb -3.169330 time 2019-03-01 10:52:04.026146
Model ind 640 epoch 1014 head A head_i_epoch 0 batch 200: avg loss -3.300203 avg loss no lamb -3.300203 time 2019-03-01 10:53:55.412088
last batch sz 160
Model ind 640 epoch 1014 head B head_i_epoch 0 batch 0: avg loss -1.923212 avg loss no lamb -1.923212 time 2019-03-01 10:55:16.952628
Model ind 640 epoch 1014 head B head_i_epoch 0 batch 100: avg loss -1.865141 avg loss no lamb -1.865141 time 2019-03-01 10:57:09.385126
Model ind 640 epoch 1014 head B head_i_epoch 0 batch 200: avg loss -1.961167 avg loss no lamb -1.961167 time 2019-03-01 10:59:00.071999
last batch sz 160
Model ind 640 epoch 1014 head B head_i_epoch 1 batch 0: avg loss -1.927914 avg loss no lamb -1.927914 time 2019-03-01 11:00:23.576352
Model ind 640 epoch 1014 head B head_i_epoch 1 batch 100: avg loss -1.869277 avg loss no lamb -1.869277 time 2019-03-01 11:02:13.815795
Model ind 640 epoch 1014 head B head_i_epoch 1 batch 200: avg loss -1.978697 avg loss no lamb -1.978697 time 2019-03-01 11:04:07.073419
last batch sz 160
Pre: time 2019-03-01 11:05:49.834266: 
 	std: 0.050070412
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51165, 0.61403334, 0.61368334, 0.6139, 0.51168334]
	train_accs: [0.51165, 0.61403334, 0.61368334, 0.6139, 0.51168334]
	best_train_sub_head: 1
	worst: 0.51165
	avg: 0.57299006
	best: 0.61403334

Starting e_i: 1015
Model ind 640 epoch 1015 head A head_i_epoch 0 batch 0: avg loss -3.261989 avg loss no lamb -3.261989 time 2019-03-01 11:05:53.693151
Model ind 640 epoch 1015 head A head_i_epoch 0 batch 100: avg loss -3.214160 avg loss no lamb -3.214160 time 2019-03-01 11:07:46.160327
Model ind 640 epoch 1015 head A head_i_epoch 0 batch 200: avg loss -3.240415 avg loss no lamb -3.240415 time 2019-03-01 11:09:40.129437
last batch sz 160
Model ind 640 epoch 1015 head B head_i_epoch 0 batch 0: avg loss -1.926901 avg loss no lamb -1.926901 time 2019-03-01 11:11:00.569770
Model ind 640 epoch 1015 head B head_i_epoch 0 batch 100: avg loss -1.880160 avg loss no lamb -1.880160 time 2019-03-01 11:12:55.186625
Model ind 640 epoch 1015 head B head_i_epoch 0 batch 200: avg loss -1.899765 avg loss no lamb -1.899765 time 2019-03-01 11:14:47.578778
last batch sz 160
Model ind 640 epoch 1015 head B head_i_epoch 1 batch 0: avg loss -1.867859 avg loss no lamb -1.867859 time 2019-03-01 11:16:09.743892
Model ind 640 epoch 1015 head B head_i_epoch 1 batch 100: avg loss -1.873359 avg loss no lamb -1.873359 time 2019-03-01 11:18:03.037765
Model ind 640 epoch 1015 head B head_i_epoch 1 batch 200: avg loss -2.010804 avg loss no lamb -2.010804 time 2019-03-01 11:19:53.074233
last batch sz 160
Pre: time 2019-03-01 11:21:38.747494: 
 	std: 0.050358865
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114833, 0.61438334, 0.61433333, 0.61446667, 0.51171666]
	train_accs: [0.5114833, 0.61438334, 0.61433333, 0.61446667, 0.51171666]
	best_train_sub_head: 3
	worst: 0.5114833
	avg: 0.57327664
	best: 0.61446667

Starting e_i: 1016
Model ind 640 epoch 1016 head A head_i_epoch 0 batch 0: avg loss -3.315816 avg loss no lamb -3.315816 time 2019-03-01 11:21:41.539220
Model ind 640 epoch 1016 head A head_i_epoch 0 batch 100: avg loss -3.188297 avg loss no lamb -3.188297 time 2019-03-01 11:23:32.090780
Model ind 640 epoch 1016 head A head_i_epoch 0 batch 200: avg loss -3.263555 avg loss no lamb -3.263555 time 2019-03-01 11:25:22.001232
last batch sz 160
Model ind 640 epoch 1016 head B head_i_epoch 0 batch 0: avg loss -1.900552 avg loss no lamb -1.900552 time 2019-03-01 11:26:46.706927
Model ind 640 epoch 1016 head B head_i_epoch 0 batch 100: avg loss -1.866377 avg loss no lamb -1.866377 time 2019-03-01 11:28:35.850555
Model ind 640 epoch 1016 head B head_i_epoch 0 batch 200: avg loss -1.936464 avg loss no lamb -1.936464 time 2019-03-01 11:30:31.779395
last batch sz 160
Model ind 640 epoch 1016 head B head_i_epoch 1 batch 0: avg loss -1.824979 avg loss no lamb -1.824979 time 2019-03-01 11:31:51.034674
Model ind 640 epoch 1016 head B head_i_epoch 1 batch 100: avg loss -1.728594 avg loss no lamb -1.728594 time 2019-03-01 11:33:46.977947
Model ind 640 epoch 1016 head B head_i_epoch 1 batch 200: avg loss -1.971199 avg loss no lamb -1.971199 time 2019-03-01 11:35:42.474544
last batch sz 160
Pre: time 2019-03-01 11:37:21.990782: 
 	std: 0.051364556
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50916666, 0.6139, 0.61411667, 0.6142, 0.5092833]
	train_accs: [0.50916666, 0.6139, 0.61411667, 0.6142, 0.5092833]
	best_train_sub_head: 3
	worst: 0.50916666
	avg: 0.5721333
	best: 0.6142

Starting e_i: 1017
Model ind 640 epoch 1017 head A head_i_epoch 0 batch 0: avg loss -3.286704 avg loss no lamb -3.286704 time 2019-03-01 11:37:24.716608
Model ind 640 epoch 1017 head A head_i_epoch 0 batch 100: avg loss -3.142492 avg loss no lamb -3.142492 time 2019-03-01 11:39:21.330723
Model ind 640 epoch 1017 head A head_i_epoch 0 batch 200: avg loss -3.323727 avg loss no lamb -3.323727 time 2019-03-01 11:41:13.369352
last batch sz 160
Model ind 640 epoch 1017 head B head_i_epoch 0 batch 0: avg loss -1.946460 avg loss no lamb -1.946460 time 2019-03-01 11:42:34.502888
Model ind 640 epoch 1017 head B head_i_epoch 0 batch 100: avg loss -1.862604 avg loss no lamb -1.862604 time 2019-03-01 11:44:29.539304
Model ind 640 epoch 1017 head B head_i_epoch 0 batch 200: avg loss -1.967111 avg loss no lamb -1.967111 time 2019-03-01 11:46:18.236329
last batch sz 160
Model ind 640 epoch 1017 head B head_i_epoch 1 batch 0: avg loss -1.920883 avg loss no lamb -1.920883 time 2019-03-01 11:47:45.468947
Model ind 640 epoch 1017 head B head_i_epoch 1 batch 100: avg loss -1.842690 avg loss no lamb -1.842690 time 2019-03-01 11:49:32.993379
Model ind 640 epoch 1017 head B head_i_epoch 1 batch 200: avg loss -1.932520 avg loss no lamb -1.932520 time 2019-03-01 11:51:28.472456
last batch sz 160
Pre: time 2019-03-01 11:53:14.402332: 
 	std: 0.050335776
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51095, 0.61363333, 0.6139167, 0.6138167, 0.5111333]
	train_accs: [0.51095, 0.61363333, 0.6139167, 0.6138167, 0.5111333]
	best_train_sub_head: 2
	worst: 0.51095
	avg: 0.57269
	best: 0.6139167

Starting e_i: 1018
Model ind 640 epoch 1018 head A head_i_epoch 0 batch 0: avg loss -3.205549 avg loss no lamb -3.205549 time 2019-03-01 11:53:17.314068
Model ind 640 epoch 1018 head A head_i_epoch 0 batch 100: avg loss -3.187766 avg loss no lamb -3.187766 time 2019-03-01 11:55:06.815563
Model ind 640 epoch 1018 head A head_i_epoch 0 batch 200: avg loss -3.226830 avg loss no lamb -3.226830 time 2019-03-01 11:57:02.641233
last batch sz 160
Model ind 640 epoch 1018 head B head_i_epoch 0 batch 0: avg loss -2.012592 avg loss no lamb -2.012592 time 2019-03-01 11:58:22.163768
Model ind 640 epoch 1018 head B head_i_epoch 0 batch 100: avg loss -1.861145 avg loss no lamb -1.861145 time 2019-03-01 12:00:16.743855
Model ind 640 epoch 1018 head B head_i_epoch 0 batch 200: avg loss -1.981007 avg loss no lamb -1.981007 time 2019-03-01 12:02:11.369534
last batch sz 160
Model ind 640 epoch 1018 head B head_i_epoch 1 batch 0: avg loss -1.870315 avg loss no lamb -1.870315 time 2019-03-01 12:03:31.188800
Model ind 640 epoch 1018 head B head_i_epoch 1 batch 100: avg loss -1.776903 avg loss no lamb -1.776903 time 2019-03-01 12:05:25.564504
Model ind 640 epoch 1018 head B head_i_epoch 1 batch 200: avg loss -1.938421 avg loss no lamb -1.938421 time 2019-03-01 12:07:16.074086
last batch sz 160
Pre: time 2019-03-01 12:09:00.737918: 
 	std: 0.050694976
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107333, 0.61425, 0.61433333, 0.61438334, 0.51095]
	train_accs: [0.5107333, 0.61425, 0.61433333, 0.61438334, 0.51095]
	best_train_sub_head: 3
	worst: 0.5107333
	avg: 0.57293004
	best: 0.61438334

Starting e_i: 1019
Model ind 640 epoch 1019 head A head_i_epoch 0 batch 0: avg loss -3.289421 avg loss no lamb -3.289421 time 2019-03-01 12:09:03.525145
Model ind 640 epoch 1019 head A head_i_epoch 0 batch 100: avg loss -3.265785 avg loss no lamb -3.265785 time 2019-03-01 12:10:55.659533
Model ind 640 epoch 1019 head A head_i_epoch 0 batch 200: avg loss -3.258854 avg loss no lamb -3.258854 time 2019-03-01 12:12:47.887601
last batch sz 160
Model ind 640 epoch 1019 head B head_i_epoch 0 batch 0: avg loss -1.913924 avg loss no lamb -1.913924 time 2019-03-01 12:14:09.962775
Model ind 640 epoch 1019 head B head_i_epoch 0 batch 100: avg loss -1.802557 avg loss no lamb -1.802557 time 2019-03-01 12:16:02.153653
Model ind 640 epoch 1019 head B head_i_epoch 0 batch 200: avg loss -1.922332 avg loss no lamb -1.922332 time 2019-03-01 12:17:54.344052
last batch sz 160
Model ind 640 epoch 1019 head B head_i_epoch 1 batch 0: avg loss -1.890180 avg loss no lamb -1.890180 time 2019-03-01 12:19:15.788673
Model ind 640 epoch 1019 head B head_i_epoch 1 batch 100: avg loss -1.823149 avg loss no lamb -1.823149 time 2019-03-01 12:21:08.588259
Model ind 640 epoch 1019 head B head_i_epoch 1 batch 200: avg loss -1.911516 avg loss no lamb -1.911516 time 2019-03-01 12:23:00.937308
last batch sz 160
Pre: time 2019-03-01 12:24:43.918273: 
 	std: 0.051049028
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50963336, 0.6139167, 0.6142, 0.61411667, 0.5101167]
	train_accs: [0.50963336, 0.6139167, 0.6142, 0.61411667, 0.5101167]
	best_train_sub_head: 2
	worst: 0.50963336
	avg: 0.57239664
	best: 0.6142

Starting e_i: 1020
Model ind 640 epoch 1020 head A head_i_epoch 0 batch 0: avg loss -3.204094 avg loss no lamb -3.204094 time 2019-03-01 12:24:47.413633
Model ind 640 epoch 1020 head A head_i_epoch 0 batch 100: avg loss -3.206004 avg loss no lamb -3.206004 time 2019-03-01 12:26:40.650034
Model ind 640 epoch 1020 head A head_i_epoch 0 batch 200: avg loss -3.272739 avg loss no lamb -3.272739 time 2019-03-01 12:28:33.419991
last batch sz 160
Model ind 640 epoch 1020 head B head_i_epoch 0 batch 0: avg loss -1.909147 avg loss no lamb -1.909147 time 2019-03-01 12:29:57.315854
Model ind 640 epoch 1020 head B head_i_epoch 0 batch 100: avg loss -1.870745 avg loss no lamb -1.870745 time 2019-03-01 12:31:50.226808
Model ind 640 epoch 1020 head B head_i_epoch 0 batch 200: avg loss -1.967927 avg loss no lamb -1.967927 time 2019-03-01 12:33:43.854016
last batch sz 160
Model ind 640 epoch 1020 head B head_i_epoch 1 batch 0: avg loss -2.048851 avg loss no lamb -2.048851 time 2019-03-01 12:35:06.201947
Model ind 640 epoch 1020 head B head_i_epoch 1 batch 100: avg loss -1.886139 avg loss no lamb -1.886139 time 2019-03-01 12:36:59.139092
Model ind 640 epoch 1020 head B head_i_epoch 1 batch 200: avg loss -1.992366 avg loss no lamb -1.992366 time 2019-03-01 12:38:52.123859
last batch sz 160
Pre: time 2019-03-01 12:40:34.234282: 
 	std: 0.050987527
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51005, 0.61415, 0.61425, 0.6142333, 0.51021665]
	train_accs: [0.51005, 0.61415, 0.61425, 0.6142333, 0.51021665]
	best_train_sub_head: 2
	worst: 0.51005
	avg: 0.57258
	best: 0.61425

Starting e_i: 1021
Model ind 640 epoch 1021 head A head_i_epoch 0 batch 0: avg loss -3.293296 avg loss no lamb -3.293296 time 2019-03-01 12:40:40.617805
Model ind 640 epoch 1021 head A head_i_epoch 0 batch 100: avg loss -3.197320 avg loss no lamb -3.197320 time 2019-03-01 12:42:34.844607
Model ind 640 epoch 1021 head A head_i_epoch 0 batch 200: avg loss -3.245978 avg loss no lamb -3.245978 time 2019-03-01 12:44:26.776460
last batch sz 160
Model ind 640 epoch 1021 head B head_i_epoch 0 batch 0: avg loss -1.920889 avg loss no lamb -1.920889 time 2019-03-01 12:45:47.696732
Model ind 640 epoch 1021 head B head_i_epoch 0 batch 100: avg loss -1.923687 avg loss no lamb -1.923687 time 2019-03-01 12:47:41.370413
Model ind 640 epoch 1021 head B head_i_epoch 0 batch 200: avg loss -1.951708 avg loss no lamb -1.951708 time 2019-03-01 12:49:31.325825
last batch sz 160
Model ind 640 epoch 1021 head B head_i_epoch 1 batch 0: avg loss -1.992444 avg loss no lamb -1.992444 time 2019-03-01 12:50:55.391114
Model ind 640 epoch 1021 head B head_i_epoch 1 batch 100: avg loss -1.922099 avg loss no lamb -1.922099 time 2019-03-01 12:52:45.277344
Model ind 640 epoch 1021 head B head_i_epoch 1 batch 200: avg loss -1.970265 avg loss no lamb -1.970265 time 2019-03-01 12:54:37.027056
last batch sz 160
Pre: time 2019-03-01 12:56:20.887106: 
 	std: 0.050067585
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51318336, 0.6153167, 0.61541665, 0.61546665, 0.5132167]
	train_accs: [0.51318336, 0.6153167, 0.61541665, 0.61546665, 0.5132167]
	best_train_sub_head: 3
	worst: 0.51318336
	avg: 0.57452
	best: 0.61546665

Starting e_i: 1022
Model ind 640 epoch 1022 head A head_i_epoch 0 batch 0: avg loss -3.241117 avg loss no lamb -3.241117 time 2019-03-01 12:56:23.785585
Model ind 640 epoch 1022 head A head_i_epoch 0 batch 100: avg loss -3.128623 avg loss no lamb -3.128623 time 2019-03-01 12:58:12.812373
Model ind 640 epoch 1022 head A head_i_epoch 0 batch 200: avg loss -3.318681 avg loss no lamb -3.318681 time 2019-03-01 13:00:07.257697
last batch sz 160
Model ind 640 epoch 1022 head B head_i_epoch 0 batch 0: avg loss -1.915852 avg loss no lamb -1.915852 time 2019-03-01 13:01:26.314411
Model ind 640 epoch 1022 head B head_i_epoch 0 batch 100: avg loss -1.859324 avg loss no lamb -1.859324 time 2019-03-01 13:03:19.078794
Model ind 640 epoch 1022 head B head_i_epoch 0 batch 200: avg loss -1.910358 avg loss no lamb -1.910358 time 2019-03-01 13:05:11.019822
last batch sz 160
Model ind 640 epoch 1022 head B head_i_epoch 1 batch 0: avg loss -1.953489 avg loss no lamb -1.953489 time 2019-03-01 13:06:30.451692
Model ind 640 epoch 1022 head B head_i_epoch 1 batch 100: avg loss -1.895709 avg loss no lamb -1.895709 time 2019-03-01 13:08:24.247149
Model ind 640 epoch 1022 head B head_i_epoch 1 batch 200: avg loss -1.928419 avg loss no lamb -1.928419 time 2019-03-01 13:10:14.125678
last batch sz 160
Pre: time 2019-03-01 13:12:00.058860: 
 	std: 0.05047585
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51131666, 0.61435, 0.6143, 0.61445, 0.51135]
	train_accs: [0.51131666, 0.61435, 0.6143, 0.61445, 0.51135]
	best_train_sub_head: 3
	worst: 0.51131666
	avg: 0.5731533
	best: 0.61445

Starting e_i: 1023
Model ind 640 epoch 1023 head A head_i_epoch 0 batch 0: avg loss -3.246035 avg loss no lamb -3.246035 time 2019-03-01 13:12:02.483702
Model ind 640 epoch 1023 head A head_i_epoch 0 batch 100: avg loss -3.151145 avg loss no lamb -3.151145 time 2019-03-01 13:13:54.901989
Model ind 640 epoch 1023 head A head_i_epoch 0 batch 200: avg loss -3.266205 avg loss no lamb -3.266205 time 2019-03-01 13:15:43.533196
last batch sz 160
Model ind 640 epoch 1023 head B head_i_epoch 0 batch 0: avg loss -1.946231 avg loss no lamb -1.946231 time 2019-03-01 13:17:08.775023
Model ind 640 epoch 1023 head B head_i_epoch 0 batch 100: avg loss -1.859365 avg loss no lamb -1.859365 time 2019-03-01 13:18:57.227924
Model ind 640 epoch 1023 head B head_i_epoch 0 batch 200: avg loss -1.895316 avg loss no lamb -1.895316 time 2019-03-01 13:20:53.655528
last batch sz 160
Model ind 640 epoch 1023 head B head_i_epoch 1 batch 0: avg loss -1.929640 avg loss no lamb -1.929640 time 2019-03-01 13:22:12.174169
Model ind 640 epoch 1023 head B head_i_epoch 1 batch 100: avg loss -1.847086 avg loss no lamb -1.847086 time 2019-03-01 13:24:06.785931
Model ind 640 epoch 1023 head B head_i_epoch 1 batch 200: avg loss -1.926664 avg loss no lamb -1.926664 time 2019-03-01 13:26:00.817474
last batch sz 160
Pre: time 2019-03-01 13:27:39.695370: 
 	std: 0.05068818
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51126665, 0.61465, 0.6146, 0.61485, 0.5112]
	train_accs: [0.51126665, 0.61465, 0.6146, 0.61485, 0.5112]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.57331336
	best: 0.61485

Starting e_i: 1024
Model ind 640 epoch 1024 head A head_i_epoch 0 batch 0: avg loss -3.255644 avg loss no lamb -3.255644 time 2019-03-01 13:27:42.279776
Model ind 640 epoch 1024 head A head_i_epoch 0 batch 100: avg loss -3.179868 avg loss no lamb -3.179868 time 2019-03-01 13:29:39.283144
Model ind 640 epoch 1024 head A head_i_epoch 0 batch 200: avg loss -3.268404 avg loss no lamb -3.268404 time 2019-03-01 13:31:31.590089
last batch sz 160
Model ind 640 epoch 1024 head B head_i_epoch 0 batch 0: avg loss -1.934164 avg loss no lamb -1.934164 time 2019-03-01 13:32:52.928937
Model ind 640 epoch 1024 head B head_i_epoch 0 batch 100: avg loss -1.893872 avg loss no lamb -1.893872 time 2019-03-01 13:34:46.497364
Model ind 640 epoch 1024 head B head_i_epoch 0 batch 200: avg loss -1.992813 avg loss no lamb -1.992813 time 2019-03-01 13:36:34.163763
last batch sz 160
Model ind 640 epoch 1024 head B head_i_epoch 1 batch 0: avg loss -1.939014 avg loss no lamb -1.939014 time 2019-03-01 13:37:59.337046
Model ind 640 epoch 1024 head B head_i_epoch 1 batch 100: avg loss -1.884286 avg loss no lamb -1.884286 time 2019-03-01 13:39:46.957134
Model ind 640 epoch 1024 head B head_i_epoch 1 batch 200: avg loss -1.949447 avg loss no lamb -1.949447 time 2019-03-01 13:41:42.669296
last batch sz 160
Pre: time 2019-03-01 13:43:24.806373: 
 	std: 0.050303087
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118, 0.6145167, 0.61445, 0.61425, 0.51165]
	train_accs: [0.5118, 0.6145167, 0.61445, 0.61425, 0.51165]
	best_train_sub_head: 1
	worst: 0.51165
	avg: 0.5733334
	best: 0.6145167

Starting e_i: 1025
Model ind 640 epoch 1025 head A head_i_epoch 0 batch 0: avg loss -3.254980 avg loss no lamb -3.254980 time 2019-03-01 13:43:29.284179
Model ind 640 epoch 1025 head A head_i_epoch 0 batch 100: avg loss -3.088862 avg loss no lamb -3.088862 time 2019-03-01 13:45:21.055478
Model ind 640 epoch 1025 head A head_i_epoch 0 batch 200: avg loss -3.199845 avg loss no lamb -3.199845 time 2019-03-01 13:47:16.866721
last batch sz 160
Model ind 640 epoch 1025 head B head_i_epoch 0 batch 0: avg loss -1.968197 avg loss no lamb -1.968197 time 2019-03-01 13:48:36.963649
Model ind 640 epoch 1025 head B head_i_epoch 0 batch 100: avg loss -1.846979 avg loss no lamb -1.846979 time 2019-03-01 13:50:31.574330
Model ind 640 epoch 1025 head B head_i_epoch 0 batch 200: avg loss -1.970338 avg loss no lamb -1.970338 time 2019-03-01 13:52:24.034230
last batch sz 160
Model ind 640 epoch 1025 head B head_i_epoch 1 batch 0: avg loss -1.896031 avg loss no lamb -1.896031 time 2019-03-01 13:53:46.181599
Model ind 640 epoch 1025 head B head_i_epoch 1 batch 100: avg loss -1.930307 avg loss no lamb -1.930307 time 2019-03-01 13:55:40.332271
Model ind 640 epoch 1025 head B head_i_epoch 1 batch 200: avg loss -1.968710 avg loss no lamb -1.968710 time 2019-03-01 13:57:32.952056
last batch sz 160
Pre: time 2019-03-01 13:59:17.105635: 
 	std: 0.050323416
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5121667, 0.6149667, 0.6148667, 0.6148833, 0.5122]
	train_accs: [0.5121667, 0.6149667, 0.6148667, 0.6148833, 0.5122]
	best_train_sub_head: 1
	worst: 0.5121667
	avg: 0.57381666
	best: 0.6149667

Starting e_i: 1026
Model ind 640 epoch 1026 head A head_i_epoch 0 batch 0: avg loss -3.258460 avg loss no lamb -3.258460 time 2019-03-01 13:59:19.922075
Model ind 640 epoch 1026 head A head_i_epoch 0 batch 100: avg loss -3.205068 avg loss no lamb -3.205068 time 2019-03-01 14:01:12.094605
Model ind 640 epoch 1026 head A head_i_epoch 0 batch 200: avg loss -3.273681 avg loss no lamb -3.273681 time 2019-03-01 14:03:04.856640
last batch sz 160
Model ind 640 epoch 1026 head B head_i_epoch 0 batch 0: avg loss -1.850255 avg loss no lamb -1.850255 time 2019-03-01 14:04:26.214305
Model ind 640 epoch 1026 head B head_i_epoch 0 batch 100: avg loss -1.865589 avg loss no lamb -1.865589 time 2019-03-01 14:06:19.649066
Model ind 640 epoch 1026 head B head_i_epoch 0 batch 200: avg loss -1.949176 avg loss no lamb -1.949176 time 2019-03-01 14:08:11.725230
last batch sz 160
Model ind 640 epoch 1026 head B head_i_epoch 1 batch 0: avg loss -1.900733 avg loss no lamb -1.900733 time 2019-03-01 14:09:33.790839
Model ind 640 epoch 1026 head B head_i_epoch 1 batch 100: avg loss -1.880069 avg loss no lamb -1.880069 time 2019-03-01 14:11:26.548199
Model ind 640 epoch 1026 head B head_i_epoch 1 batch 200: avg loss -2.005197 avg loss no lamb -2.005197 time 2019-03-01 14:13:16.865223
last batch sz 160
Pre: time 2019-03-01 14:15:02.031945: 
 	std: 0.050322067
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5116, 0.61433333, 0.61443335, 0.61436665, 0.51171666]
	train_accs: [0.5116, 0.61433333, 0.61443335, 0.61436665, 0.51171666]
	best_train_sub_head: 2
	worst: 0.5116
	avg: 0.57329
	best: 0.61443335

Starting e_i: 1027
Model ind 640 epoch 1027 head A head_i_epoch 0 batch 0: avg loss -3.280434 avg loss no lamb -3.280434 time 2019-03-01 14:15:04.837898
Model ind 640 epoch 1027 head A head_i_epoch 0 batch 100: avg loss -3.143258 avg loss no lamb -3.143258 time 2019-03-01 14:16:55.650357
Model ind 640 epoch 1027 head A head_i_epoch 0 batch 200: avg loss -3.298892 avg loss no lamb -3.298892 time 2019-03-01 14:18:47.017675
last batch sz 160
Model ind 640 epoch 1027 head B head_i_epoch 0 batch 0: avg loss -1.972937 avg loss no lamb -1.972937 time 2019-03-01 14:20:07.785468
Model ind 640 epoch 1027 head B head_i_epoch 0 batch 100: avg loss -1.847412 avg loss no lamb -1.847412 time 2019-03-01 14:21:58.850914
Model ind 640 epoch 1027 head B head_i_epoch 0 batch 200: avg loss -1.997495 avg loss no lamb -1.997495 time 2019-03-01 14:23:51.587591
last batch sz 160
Model ind 640 epoch 1027 head B head_i_epoch 1 batch 0: avg loss -1.901699 avg loss no lamb -1.901699 time 2019-03-01 14:25:11.039686
Model ind 640 epoch 1027 head B head_i_epoch 1 batch 100: avg loss -1.867650 avg loss no lamb -1.867650 time 2019-03-01 14:27:05.891651
Model ind 640 epoch 1027 head B head_i_epoch 1 batch 200: avg loss -1.970534 avg loss no lamb -1.970534 time 2019-03-01 14:29:56.501265
last batch sz 160
Pre: time 2019-03-01 14:32:34.992002: 
 	std: 0.050484028
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51138335, 0.6145333, 0.61455, 0.61436665, 0.5114833]
	train_accs: [0.51138335, 0.6145333, 0.61455, 0.61436665, 0.5114833]
	best_train_sub_head: 2
	worst: 0.51138335
	avg: 0.57326335
	best: 0.61455

Starting e_i: 1028
Model ind 640 epoch 1028 head A head_i_epoch 0 batch 0: avg loss -3.272172 avg loss no lamb -3.272172 time 2019-03-01 14:32:37.848892
Model ind 640 epoch 1028 head A head_i_epoch 0 batch 100: avg loss -3.148604 avg loss no lamb -3.148604 time 2019-03-01 14:34:28.038874
Model ind 640 epoch 1028 head A head_i_epoch 0 batch 200: avg loss -3.331892 avg loss no lamb -3.331892 time 2019-03-01 14:36:21.582137
last batch sz 160
Model ind 640 epoch 1028 head B head_i_epoch 0 batch 0: avg loss -1.852268 avg loss no lamb -1.852268 time 2019-03-01 14:37:40.797739
Model ind 640 epoch 1028 head B head_i_epoch 0 batch 100: avg loss -1.785552 avg loss no lamb -1.785552 time 2019-03-01 14:39:35.472730
Model ind 640 epoch 1028 head B head_i_epoch 0 batch 200: avg loss -1.915674 avg loss no lamb -1.915674 time 2019-03-01 14:41:25.953643
last batch sz 160
Model ind 640 epoch 1028 head B head_i_epoch 1 batch 0: avg loss -1.875213 avg loss no lamb -1.875213 time 2019-03-01 14:42:47.295517
Model ind 640 epoch 1028 head B head_i_epoch 1 batch 100: avg loss -1.812618 avg loss no lamb -1.812618 time 2019-03-01 14:44:39.490259
Model ind 640 epoch 1028 head B head_i_epoch 1 batch 200: avg loss -1.944701 avg loss no lamb -1.944701 time 2019-03-01 14:46:28.556930
last batch sz 160
Pre: time 2019-03-01 14:48:15.107050: 
 	std: 0.05074394
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111667, 0.61475, 0.6146167, 0.6147, 0.51105]
	train_accs: [0.5111667, 0.61475, 0.6146167, 0.6147, 0.51105]
	best_train_sub_head: 1
	worst: 0.51105
	avg: 0.5732567
	best: 0.61475

Starting e_i: 1029
Model ind 640 epoch 1029 head A head_i_epoch 0 batch 0: avg loss -3.207093 avg loss no lamb -3.207093 time 2019-03-01 14:48:17.801235
Model ind 640 epoch 1029 head A head_i_epoch 0 batch 100: avg loss -3.162600 avg loss no lamb -3.162600 time 2019-03-01 14:50:09.094704
Model ind 640 epoch 1029 head A head_i_epoch 0 batch 200: avg loss -3.220955 avg loss no lamb -3.220955 time 2019-03-01 14:52:00.063382
last batch sz 160
Model ind 640 epoch 1029 head B head_i_epoch 0 batch 0: avg loss -1.937497 avg loss no lamb -1.937497 time 2019-03-01 14:53:25.363656
Model ind 640 epoch 1029 head B head_i_epoch 0 batch 100: avg loss -1.782178 avg loss no lamb -1.782178 time 2019-03-01 14:55:13.640352
Model ind 640 epoch 1029 head B head_i_epoch 0 batch 200: avg loss -1.844645 avg loss no lamb -1.844645 time 2019-03-01 14:57:11.130739
last batch sz 160
Model ind 640 epoch 1029 head B head_i_epoch 1 batch 0: avg loss -1.891992 avg loss no lamb -1.891992 time 2019-03-01 14:58:28.359021
Model ind 640 epoch 1029 head B head_i_epoch 1 batch 100: avg loss -1.835971 avg loss no lamb -1.835971 time 2019-03-01 15:00:25.160113
Model ind 640 epoch 1029 head B head_i_epoch 1 batch 200: avg loss -1.851107 avg loss no lamb -1.851107 time 2019-03-01 15:02:18.881462
last batch sz 160
Pre: time 2019-03-01 15:03:58.490530: 
 	std: 0.051036503
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5104333, 0.6147, 0.6146167, 0.6145667, 0.5104667]
	train_accs: [0.5104333, 0.6147, 0.6146167, 0.6145667, 0.5104667]
	best_train_sub_head: 1
	worst: 0.5104333
	avg: 0.5729567
	best: 0.6147

Starting e_i: 1030
Model ind 640 epoch 1030 head A head_i_epoch 0 batch 0: avg loss -3.207899 avg loss no lamb -3.207899 time 2019-03-01 15:04:01.754650
Model ind 640 epoch 1030 head A head_i_epoch 0 batch 100: avg loss -3.226643 avg loss no lamb -3.226643 time 2019-03-01 15:05:59.958443
Model ind 640 epoch 1030 head A head_i_epoch 0 batch 200: avg loss -3.212732 avg loss no lamb -3.212732 time 2019-03-01 15:07:50.019806
last batch sz 160
Model ind 640 epoch 1030 head B head_i_epoch 0 batch 0: avg loss -1.865884 avg loss no lamb -1.865884 time 2019-03-01 15:09:13.809406
Model ind 640 epoch 1030 head B head_i_epoch 0 batch 100: avg loss -1.823785 avg loss no lamb -1.823785 time 2019-03-01 15:11:07.231575
Model ind 640 epoch 1030 head B head_i_epoch 0 batch 200: avg loss -1.940383 avg loss no lamb -1.940383 time 2019-03-01 15:12:57.244074
last batch sz 160
Model ind 640 epoch 1030 head B head_i_epoch 1 batch 0: avg loss -1.874292 avg loss no lamb -1.874292 time 2019-03-01 15:14:23.699682
Model ind 640 epoch 1030 head B head_i_epoch 1 batch 100: avg loss -1.768472 avg loss no lamb -1.768472 time 2019-03-01 15:16:13.792861
Model ind 640 epoch 1030 head B head_i_epoch 1 batch 200: avg loss -1.993946 avg loss no lamb -1.993946 time 2019-03-01 15:18:08.753569
last batch sz 160
Pre: time 2019-03-01 15:19:51.438966: 
 	std: 0.05071127
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108167, 0.6143, 0.6141833, 0.6142833, 0.51066667]
	train_accs: [0.5108167, 0.6143, 0.6141833, 0.6142833, 0.51066667]
	best_train_sub_head: 1
	worst: 0.51066667
	avg: 0.57285
	best: 0.6143

Starting e_i: 1031
Model ind 640 epoch 1031 head A head_i_epoch 0 batch 0: avg loss -3.180332 avg loss no lamb -3.180332 time 2019-03-01 15:19:59.587074
Model ind 640 epoch 1031 head A head_i_epoch 0 batch 100: avg loss -3.184398 avg loss no lamb -3.184398 time 2019-03-01 15:21:51.860977
Model ind 640 epoch 1031 head A head_i_epoch 0 batch 200: avg loss -3.289959 avg loss no lamb -3.289959 time 2019-03-01 15:23:47.353759
last batch sz 160
Model ind 640 epoch 1031 head B head_i_epoch 0 batch 0: avg loss -1.897534 avg loss no lamb -1.897534 time 2019-03-01 15:25:08.798333
Model ind 640 epoch 1031 head B head_i_epoch 0 batch 100: avg loss -1.841284 avg loss no lamb -1.841284 time 2019-03-01 15:27:02.097404
Model ind 640 epoch 1031 head B head_i_epoch 0 batch 200: avg loss -1.964660 avg loss no lamb -1.964660 time 2019-03-01 15:28:54.794917
last batch sz 160
Model ind 640 epoch 1031 head B head_i_epoch 1 batch 0: avg loss -1.887056 avg loss no lamb -1.887056 time 2019-03-01 15:30:16.600274
Model ind 640 epoch 1031 head B head_i_epoch 1 batch 100: avg loss -1.819853 avg loss no lamb -1.819853 time 2019-03-01 15:32:10.761753
Model ind 640 epoch 1031 head B head_i_epoch 1 batch 200: avg loss -1.895983 avg loss no lamb -1.895983 time 2019-03-01 15:34:03.878223
last batch sz 160
Pre: time 2019-03-01 15:35:47.820154: 
 	std: 0.051390417
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50836664, 0.6135167, 0.61325, 0.61333334, 0.5085667]
	train_accs: [0.50836664, 0.6135167, 0.61325, 0.61333334, 0.5085667]
	best_train_sub_head: 1
	worst: 0.50836664
	avg: 0.5714067
	best: 0.6135167

Starting e_i: 1032
Model ind 640 epoch 1032 head A head_i_epoch 0 batch 0: avg loss -3.283366 avg loss no lamb -3.283366 time 2019-03-01 15:35:50.833454
Model ind 640 epoch 1032 head A head_i_epoch 0 batch 100: avg loss -3.168015 avg loss no lamb -3.168015 time 2019-03-01 15:37:42.777535
Model ind 640 epoch 1032 head A head_i_epoch 0 batch 200: avg loss -3.307319 avg loss no lamb -3.307319 time 2019-03-01 15:39:36.511158
last batch sz 160
Model ind 640 epoch 1032 head B head_i_epoch 0 batch 0: avg loss -1.975629 avg loss no lamb -1.975629 time 2019-03-01 15:40:58.007048
Model ind 640 epoch 1032 head B head_i_epoch 0 batch 100: avg loss -1.876911 avg loss no lamb -1.876911 time 2019-03-01 15:42:50.809561
Model ind 640 epoch 1032 head B head_i_epoch 0 batch 200: avg loss -1.934305 avg loss no lamb -1.934305 time 2019-03-01 15:44:43.084237
last batch sz 160
Model ind 640 epoch 1032 head B head_i_epoch 1 batch 0: avg loss -1.943265 avg loss no lamb -1.943265 time 2019-03-01 15:46:03.932171
Model ind 640 epoch 1032 head B head_i_epoch 1 batch 100: avg loss -1.799467 avg loss no lamb -1.799467 time 2019-03-01 15:47:57.812699
Model ind 640 epoch 1032 head B head_i_epoch 1 batch 200: avg loss -1.922231 avg loss no lamb -1.922231 time 2019-03-01 15:49:49.191696
last batch sz 160
Pre: time 2019-03-01 15:51:33.465905: 
 	std: 0.051056933
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50993335, 0.6141667, 0.61411667, 0.6143, 0.5100167]
	train_accs: [0.50993335, 0.6141667, 0.61411667, 0.6143, 0.5100167]
	best_train_sub_head: 3
	worst: 0.50993335
	avg: 0.57250667
	best: 0.6143

Starting e_i: 1033
Model ind 640 epoch 1033 head A head_i_epoch 0 batch 0: avg loss -3.311125 avg loss no lamb -3.311125 time 2019-03-01 15:51:36.164808
Model ind 640 epoch 1033 head A head_i_epoch 0 batch 100: avg loss -3.154869 avg loss no lamb -3.154869 time 2019-03-01 15:53:28.613450
Model ind 640 epoch 1033 head A head_i_epoch 0 batch 200: avg loss -3.294395 avg loss no lamb -3.294395 time 2019-03-01 15:55:18.783115
last batch sz 160
Model ind 640 epoch 1033 head B head_i_epoch 0 batch 0: avg loss -1.936512 avg loss no lamb -1.936512 time 2019-03-01 15:56:42.858942
Model ind 640 epoch 1033 head B head_i_epoch 0 batch 100: avg loss -1.915260 avg loss no lamb -1.915260 time 2019-03-01 15:58:31.878629
Model ind 640 epoch 1033 head B head_i_epoch 0 batch 200: avg loss -1.907536 avg loss no lamb -1.907536 time 2019-03-01 16:00:25.903062
last batch sz 160
Model ind 640 epoch 1033 head B head_i_epoch 1 batch 0: avg loss -1.919558 avg loss no lamb -1.919558 time 2019-03-01 16:01:45.718796
Model ind 640 epoch 1033 head B head_i_epoch 1 batch 100: avg loss -1.844514 avg loss no lamb -1.844514 time 2019-03-01 16:03:38.738094
Model ind 640 epoch 1033 head B head_i_epoch 1 batch 200: avg loss -1.936907 avg loss no lamb -1.936907 time 2019-03-01 16:05:32.497461
last batch sz 160
Pre: time 2019-03-01 16:07:11.323146: 
 	std: 0.050964404
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5092667, 0.61345, 0.61336666, 0.6133, 0.50941664]
	train_accs: [0.5092667, 0.61345, 0.61336666, 0.6133, 0.50941664]
	best_train_sub_head: 1
	worst: 0.5092667
	avg: 0.57176
	best: 0.61345

Starting e_i: 1034
Model ind 640 epoch 1034 head A head_i_epoch 0 batch 0: avg loss -3.282574 avg loss no lamb -3.282574 time 2019-03-01 16:07:14.091213
Model ind 640 epoch 1034 head A head_i_epoch 0 batch 100: avg loss -3.233350 avg loss no lamb -3.233350 time 2019-03-01 16:09:09.874744
Model ind 640 epoch 1034 head A head_i_epoch 0 batch 200: avg loss -3.292741 avg loss no lamb -3.292741 time 2019-03-01 16:11:02.193239
last batch sz 160
Model ind 640 epoch 1034 head B head_i_epoch 0 batch 0: avg loss -1.916340 avg loss no lamb -1.916340 time 2019-03-01 16:12:22.791856
Model ind 640 epoch 1034 head B head_i_epoch 0 batch 100: avg loss -1.804598 avg loss no lamb -1.804598 time 2019-03-01 16:14:16.895016
Model ind 640 epoch 1034 head B head_i_epoch 0 batch 200: avg loss -1.890158 avg loss no lamb -1.890158 time 2019-03-01 16:16:05.709598
last batch sz 160
Model ind 640 epoch 1034 head B head_i_epoch 1 batch 0: avg loss -1.862362 avg loss no lamb -1.862362 time 2019-03-01 16:17:31.537854
Model ind 640 epoch 1034 head B head_i_epoch 1 batch 100: avg loss -1.819563 avg loss no lamb -1.819563 time 2019-03-01 16:19:19.430513
Model ind 640 epoch 1034 head B head_i_epoch 1 batch 200: avg loss -1.951196 avg loss no lamb -1.951196 time 2019-03-01 16:21:14.326183
last batch sz 160
Pre: time 2019-03-01 16:22:59.473537: 
 	std: 0.05108013
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51091665, 0.6153333, 0.61513335, 0.6153833, 0.5111167]
	train_accs: [0.51091665, 0.6153333, 0.61513335, 0.6153833, 0.5111167]
	best_train_sub_head: 3
	worst: 0.51091665
	avg: 0.5735767
	best: 0.6153833

Starting e_i: 1035
Model ind 640 epoch 1035 head A head_i_epoch 0 batch 0: avg loss -3.246263 avg loss no lamb -3.246263 time 2019-03-01 16:23:03.161503
Model ind 640 epoch 1035 head A head_i_epoch 0 batch 100: avg loss -3.117029 avg loss no lamb -3.117029 time 2019-03-01 16:24:52.553654
Model ind 640 epoch 1035 head A head_i_epoch 0 batch 200: avg loss -3.270387 avg loss no lamb -3.270387 time 2019-03-01 16:26:49.792902
last batch sz 160
Model ind 640 epoch 1035 head B head_i_epoch 0 batch 0: avg loss -1.934562 avg loss no lamb -1.934562 time 2019-03-01 16:28:09.146089
Model ind 640 epoch 1035 head B head_i_epoch 0 batch 100: avg loss -1.865175 avg loss no lamb -1.865175 time 2019-03-01 16:30:03.736386
Model ind 640 epoch 1035 head B head_i_epoch 0 batch 200: avg loss -1.959658 avg loss no lamb -1.959658 time 2019-03-01 16:31:58.799663
last batch sz 160
Model ind 640 epoch 1035 head B head_i_epoch 1 batch 0: avg loss -1.910845 avg loss no lamb -1.910845 time 2019-03-01 16:33:18.336037
Model ind 640 epoch 1035 head B head_i_epoch 1 batch 100: avg loss -1.868101 avg loss no lamb -1.868101 time 2019-03-01 16:35:14.715872
Model ind 640 epoch 1035 head B head_i_epoch 1 batch 200: avg loss -1.965536 avg loss no lamb -1.965536 time 2019-03-01 16:37:06.188873
last batch sz 160
Pre: time 2019-03-01 16:38:51.649830: 
 	std: 0.050693598
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51075, 0.61438334, 0.6143, 0.6143, 0.51095]
	train_accs: [0.51075, 0.61438334, 0.6143, 0.6143, 0.51095]
	best_train_sub_head: 1
	worst: 0.51075
	avg: 0.57293665
	best: 0.61438334

Starting e_i: 1036
Model ind 640 epoch 1036 head A head_i_epoch 0 batch 0: avg loss -3.184974 avg loss no lamb -3.184974 time 2019-03-01 16:38:54.222329
Model ind 640 epoch 1036 head A head_i_epoch 0 batch 100: avg loss -3.185280 avg loss no lamb -3.185280 time 2019-03-01 16:40:48.270754
Model ind 640 epoch 1036 head A head_i_epoch 0 batch 200: avg loss -3.272787 avg loss no lamb -3.272787 time 2019-03-01 16:42:40.218353
last batch sz 160
Model ind 640 epoch 1036 head B head_i_epoch 0 batch 0: avg loss -1.951276 avg loss no lamb -1.951276 time 2019-03-01 16:44:03.222482
Model ind 640 epoch 1036 head B head_i_epoch 0 batch 100: avg loss -1.850967 avg loss no lamb -1.850967 time 2019-03-01 16:45:55.300136
Model ind 640 epoch 1036 head B head_i_epoch 0 batch 200: avg loss -1.964352 avg loss no lamb -1.964352 time 2019-03-01 16:47:48.448500
last batch sz 160
Model ind 640 epoch 1036 head B head_i_epoch 1 batch 0: avg loss -1.849190 avg loss no lamb -1.849190 time 2019-03-01 16:49:09.517207
Model ind 640 epoch 1036 head B head_i_epoch 1 batch 100: avg loss -1.818587 avg loss no lamb -1.818587 time 2019-03-01 16:51:03.253959
Model ind 640 epoch 1036 head B head_i_epoch 1 batch 200: avg loss -1.963625 avg loss no lamb -1.963625 time 2019-03-01 16:52:55.947281
last batch sz 160
Pre: time 2019-03-01 16:54:39.152338: 
 	std: 0.051204156
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50986665, 0.6146333, 0.61475, 0.6145, 0.51035]
	train_accs: [0.50986665, 0.6146333, 0.61475, 0.6145, 0.51035]
	best_train_sub_head: 2
	worst: 0.50986665
	avg: 0.57282
	best: 0.61475

Starting e_i: 1037
Model ind 640 epoch 1037 head A head_i_epoch 0 batch 0: avg loss -3.240820 avg loss no lamb -3.240820 time 2019-03-01 16:54:41.846544
Model ind 640 epoch 1037 head A head_i_epoch 0 batch 100: avg loss -3.153500 avg loss no lamb -3.153500 time 2019-03-01 16:56:33.418193
Model ind 640 epoch 1037 head A head_i_epoch 0 batch 200: avg loss -3.308428 avg loss no lamb -3.308428 time 2019-03-01 16:58:23.802076
last batch sz 160
Model ind 640 epoch 1037 head B head_i_epoch 0 batch 0: avg loss -1.882486 avg loss no lamb -1.882486 time 2019-03-01 16:59:47.015462
Model ind 640 epoch 1037 head B head_i_epoch 0 batch 100: avg loss -1.854629 avg loss no lamb -1.854629 time 2019-03-01 17:01:37.486244
Model ind 640 epoch 1037 head B head_i_epoch 0 batch 200: avg loss -1.935160 avg loss no lamb -1.935160 time 2019-03-01 17:03:30.220416
last batch sz 160
Model ind 640 epoch 1037 head B head_i_epoch 1 batch 0: avg loss -1.925679 avg loss no lamb -1.925679 time 2019-03-01 17:04:50.904782
Model ind 640 epoch 1037 head B head_i_epoch 1 batch 100: avg loss -1.879942 avg loss no lamb -1.879942 time 2019-03-01 17:06:42.990658
Model ind 640 epoch 1037 head B head_i_epoch 1 batch 200: avg loss -1.977964 avg loss no lamb -1.977964 time 2019-03-01 17:08:35.659342
last batch sz 160
Pre: time 2019-03-01 17:10:16.268980: 
 	std: 0.050941817
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51075, 0.61516666, 0.61501664, 0.61516666, 0.5115167]
	train_accs: [0.51075, 0.61516666, 0.61501664, 0.61516666, 0.5115167]
	best_train_sub_head: 1
	worst: 0.51075
	avg: 0.57352334
	best: 0.61516666

Starting e_i: 1038
Model ind 640 epoch 1038 head A head_i_epoch 0 batch 0: avg loss -3.296552 avg loss no lamb -3.296552 time 2019-03-01 17:10:19.621043
Model ind 640 epoch 1038 head A head_i_epoch 0 batch 100: avg loss -3.253082 avg loss no lamb -3.253082 time 2019-03-01 17:12:13.076511
Model ind 640 epoch 1038 head A head_i_epoch 0 batch 200: avg loss -3.293941 avg loss no lamb -3.293941 time 2019-03-01 17:14:04.374892
last batch sz 160
Model ind 640 epoch 1038 head B head_i_epoch 0 batch 0: avg loss -1.935060 avg loss no lamb -1.935060 time 2019-03-01 17:15:25.064190
Model ind 640 epoch 1038 head B head_i_epoch 0 batch 100: avg loss -1.806010 avg loss no lamb -1.806010 time 2019-03-01 17:17:19.325464
Model ind 640 epoch 1038 head B head_i_epoch 0 batch 200: avg loss -1.963521 avg loss no lamb -1.963521 time 2019-03-01 17:19:07.695347
last batch sz 160
Model ind 640 epoch 1038 head B head_i_epoch 1 batch 0: avg loss -1.892022 avg loss no lamb -1.892022 time 2019-03-01 17:20:31.042784
Model ind 640 epoch 1038 head B head_i_epoch 1 batch 100: avg loss -1.841850 avg loss no lamb -1.841850 time 2019-03-01 17:22:22.350703
Model ind 640 epoch 1038 head B head_i_epoch 1 batch 200: avg loss -1.991060 avg loss no lamb -1.991060 time 2019-03-01 17:24:13.919843
last batch sz 160
Pre: time 2019-03-01 17:25:59.217973: 
 	std: 0.05114679
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112333, 0.6156833, 0.6156333, 0.61586666, 0.5114167]
	train_accs: [0.5112333, 0.6156833, 0.6156333, 0.61586666, 0.5114167]
	best_train_sub_head: 3
	worst: 0.5112333
	avg: 0.5739666
	best: 0.61586666

Starting e_i: 1039
Model ind 640 epoch 1039 head A head_i_epoch 0 batch 0: avg loss -3.305823 avg loss no lamb -3.305823 time 2019-03-01 17:26:02.171487
Model ind 640 epoch 1039 head A head_i_epoch 0 batch 100: avg loss -3.123144 avg loss no lamb -3.123144 time 2019-03-01 17:27:51.015455
Model ind 640 epoch 1039 head A head_i_epoch 0 batch 200: avg loss -3.307054 avg loss no lamb -3.307054 time 2019-03-01 17:29:46.580528
last batch sz 160
Model ind 640 epoch 1039 head B head_i_epoch 0 batch 0: avg loss -1.861230 avg loss no lamb -1.861230 time 2019-03-01 17:31:06.336772
Model ind 640 epoch 1039 head B head_i_epoch 0 batch 100: avg loss -1.877313 avg loss no lamb -1.877313 time 2019-03-01 17:32:59.621601
Model ind 640 epoch 1039 head B head_i_epoch 0 batch 200: avg loss -1.965766 avg loss no lamb -1.965766 time 2019-03-01 17:34:55.152422
last batch sz 160
Model ind 640 epoch 1039 head B head_i_epoch 1 batch 0: avg loss -1.964903 avg loss no lamb -1.964903 time 2019-03-01 17:36:13.701545
Model ind 640 epoch 1039 head B head_i_epoch 1 batch 100: avg loss -1.880768 avg loss no lamb -1.880768 time 2019-03-01 17:38:09.937999
Model ind 640 epoch 1039 head B head_i_epoch 1 batch 200: avg loss -1.954080 avg loss no lamb -1.954080 time 2019-03-01 17:39:57.282629
last batch sz 160
Pre: time 2019-03-01 17:41:42.587663: 
 	std: 0.05110456
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51101667, 0.6152833, 0.61525, 0.61516666, 0.5108167]
	train_accs: [0.51101667, 0.6152833, 0.61525, 0.61516666, 0.5108167]
	best_train_sub_head: 1
	worst: 0.5108167
	avg: 0.57350665
	best: 0.6152833

Starting e_i: 1040
Model ind 640 epoch 1040 head A head_i_epoch 0 batch 0: avg loss -3.289884 avg loss no lamb -3.289884 time 2019-03-01 17:41:45.636000
Model ind 640 epoch 1040 head A head_i_epoch 0 batch 100: avg loss -3.185482 avg loss no lamb -3.185482 time 2019-03-01 17:43:40.146885
Model ind 640 epoch 1040 head A head_i_epoch 0 batch 200: avg loss -3.213842 avg loss no lamb -3.213842 time 2019-03-01 17:45:30.214562
last batch sz 160
Model ind 640 epoch 1040 head B head_i_epoch 0 batch 0: avg loss -1.974198 avg loss no lamb -1.974198 time 2019-03-01 17:46:56.800799
Model ind 640 epoch 1040 head B head_i_epoch 0 batch 100: avg loss -1.923648 avg loss no lamb -1.923648 time 2019-03-01 17:48:46.993619
Model ind 640 epoch 1040 head B head_i_epoch 0 batch 200: avg loss -1.915984 avg loss no lamb -1.915984 time 2019-03-01 17:50:41.572180
last batch sz 160
Model ind 640 epoch 1040 head B head_i_epoch 1 batch 0: avg loss -1.909415 avg loss no lamb -1.909415 time 2019-03-01 17:52:02.919177
Model ind 640 epoch 1040 head B head_i_epoch 1 batch 100: avg loss -1.763412 avg loss no lamb -1.763412 time 2019-03-01 17:53:56.839842
Model ind 640 epoch 1040 head B head_i_epoch 1 batch 200: avg loss -1.935220 avg loss no lamb -1.935220 time 2019-03-01 17:55:51.434172
last batch sz 160
Pre: time 2019-03-01 17:57:34.328249: 
 	std: 0.05111826
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51155, 0.61586666, 0.6156667, 0.6157, 0.51125]
	train_accs: [0.51155, 0.61586666, 0.6156667, 0.6157, 0.51125]
	best_train_sub_head: 1
	worst: 0.51125
	avg: 0.5740067
	best: 0.61586666

Starting e_i: 1041
Model ind 640 epoch 1041 head A head_i_epoch 0 batch 0: avg loss -3.281336 avg loss no lamb -3.281336 time 2019-03-01 17:57:42.670559
Model ind 640 epoch 1041 head A head_i_epoch 0 batch 100: avg loss -3.315510 avg loss no lamb -3.315510 time 2019-03-01 17:59:36.117679
Model ind 640 epoch 1041 head A head_i_epoch 0 batch 200: avg loss -3.203669 avg loss no lamb -3.203669 time 2019-03-01 18:01:29.634565
last batch sz 160
Model ind 640 epoch 1041 head B head_i_epoch 0 batch 0: avg loss -1.937285 avg loss no lamb -1.937285 time 2019-03-01 18:02:51.865062
Model ind 640 epoch 1041 head B head_i_epoch 0 batch 100: avg loss -1.875869 avg loss no lamb -1.875869 time 2019-03-01 18:04:44.897569
Model ind 640 epoch 1041 head B head_i_epoch 0 batch 200: avg loss -1.931632 avg loss no lamb -1.931632 time 2019-03-01 18:06:37.457432
last batch sz 160
Model ind 640 epoch 1041 head B head_i_epoch 1 batch 0: avg loss -1.918055 avg loss no lamb -1.918055 time 2019-03-01 18:07:59.776110
Model ind 640 epoch 1041 head B head_i_epoch 1 batch 100: avg loss -1.865997 avg loss no lamb -1.865997 time 2019-03-01 18:09:52.352586
Model ind 640 epoch 1041 head B head_i_epoch 1 batch 200: avg loss -1.945392 avg loss no lamb -1.945392 time 2019-03-01 18:11:45.345925
last batch sz 160
Pre: time 2019-03-01 18:13:27.566897: 
 	std: 0.050986197
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5100667, 0.61413336, 0.614, 0.61396664, 0.50985]
	train_accs: [0.5100667, 0.61413336, 0.614, 0.61396664, 0.50985]
	best_train_sub_head: 1
	worst: 0.50985
	avg: 0.5724034
	best: 0.61413336

Starting e_i: 1042
Model ind 640 epoch 1042 head A head_i_epoch 0 batch 0: avg loss -3.296233 avg loss no lamb -3.296233 time 2019-03-01 18:13:30.446171
Model ind 640 epoch 1042 head A head_i_epoch 0 batch 100: avg loss -3.233006 avg loss no lamb -3.233006 time 2019-03-01 18:15:22.584274
Model ind 640 epoch 1042 head A head_i_epoch 0 batch 200: avg loss -3.260438 avg loss no lamb -3.260438 time 2019-03-01 18:17:15.384881
last batch sz 160
Model ind 640 epoch 1042 head B head_i_epoch 0 batch 0: avg loss -1.860647 avg loss no lamb -1.860647 time 2019-03-01 18:18:36.384520
Model ind 640 epoch 1042 head B head_i_epoch 0 batch 100: avg loss -1.881951 avg loss no lamb -1.881951 time 2019-03-01 18:20:29.547685
Model ind 640 epoch 1042 head B head_i_epoch 0 batch 200: avg loss -1.927873 avg loss no lamb -1.927873 time 2019-03-01 18:22:20.043434
last batch sz 160
Model ind 640 epoch 1042 head B head_i_epoch 1 batch 0: avg loss -1.788395 avg loss no lamb -1.788395 time 2019-03-01 18:23:42.797010
Model ind 640 epoch 1042 head B head_i_epoch 1 batch 100: avg loss -1.761463 avg loss no lamb -1.761463 time 2019-03-01 18:25:35.442090
Model ind 640 epoch 1042 head B head_i_epoch 1 batch 200: avg loss -1.940862 avg loss no lamb -1.940862 time 2019-03-01 18:27:27.030866
last batch sz 160
Pre: time 2019-03-01 18:29:11.940581: 
 	std: 0.051293854
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50918335, 0.61408335, 0.61405, 0.61415, 0.5096]
	train_accs: [0.50918335, 0.61408335, 0.61405, 0.61415, 0.5096]
	best_train_sub_head: 3
	worst: 0.50918335
	avg: 0.5722133
	best: 0.61415

Starting e_i: 1043
Model ind 640 epoch 1043 head A head_i_epoch 0 batch 0: avg loss -3.278682 avg loss no lamb -3.278682 time 2019-03-01 18:29:14.897976
Model ind 640 epoch 1043 head A head_i_epoch 0 batch 100: avg loss -3.228757 avg loss no lamb -3.228757 time 2019-03-01 18:31:05.383073
Model ind 640 epoch 1043 head A head_i_epoch 0 batch 200: avg loss -3.155126 avg loss no lamb -3.155126 time 2019-03-01 18:32:58.915905
last batch sz 160
Model ind 640 epoch 1043 head B head_i_epoch 0 batch 0: avg loss -1.948291 avg loss no lamb -1.948291 time 2019-03-01 18:34:20.170630
Model ind 640 epoch 1043 head B head_i_epoch 0 batch 100: avg loss -1.803531 avg loss no lamb -1.803531 time 2019-03-01 18:36:11.465842
Model ind 640 epoch 1043 head B head_i_epoch 0 batch 200: avg loss -1.887729 avg loss no lamb -1.887729 time 2019-03-01 18:38:05.564402
last batch sz 160
Model ind 640 epoch 1043 head B head_i_epoch 1 batch 0: avg loss -1.930859 avg loss no lamb -1.930859 time 2019-03-01 18:39:24.192607
Model ind 640 epoch 1043 head B head_i_epoch 1 batch 100: avg loss -1.854428 avg loss no lamb -1.854428 time 2019-03-01 18:41:19.591177
Model ind 640 epoch 1043 head B head_i_epoch 1 batch 200: avg loss -1.941213 avg loss no lamb -1.941213 time 2019-03-01 18:43:10.507667
last batch sz 160
Pre: time 2019-03-01 18:44:54.449024: 
 	std: 0.050700523
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114833, 0.6152167, 0.6151, 0.61518335, 0.5118667]
	train_accs: [0.5114833, 0.6152167, 0.6151, 0.61518335, 0.5118667]
	best_train_sub_head: 1
	worst: 0.5114833
	avg: 0.5737699
	best: 0.6152167

Starting e_i: 1044
Model ind 640 epoch 1044 head A head_i_epoch 0 batch 0: avg loss -3.221013 avg loss no lamb -3.221013 time 2019-03-01 18:44:57.059841
Model ind 640 epoch 1044 head A head_i_epoch 0 batch 100: avg loss -3.253962 avg loss no lamb -3.253962 time 2019-03-01 18:46:52.241891
Model ind 640 epoch 1044 head A head_i_epoch 0 batch 200: avg loss -3.191395 avg loss no lamb -3.191395 time 2019-03-01 18:48:40.299712
last batch sz 160
Model ind 640 epoch 1044 head B head_i_epoch 0 batch 0: avg loss -1.937330 avg loss no lamb -1.937330 time 2019-03-01 18:50:07.174811
Model ind 640 epoch 1044 head B head_i_epoch 0 batch 100: avg loss -1.917787 avg loss no lamb -1.917787 time 2019-03-01 18:51:55.416911
Model ind 640 epoch 1044 head B head_i_epoch 0 batch 200: avg loss -1.985540 avg loss no lamb -1.985540 time 2019-03-01 18:53:49.497030
last batch sz 160
Model ind 640 epoch 1044 head B head_i_epoch 1 batch 0: avg loss -1.885123 avg loss no lamb -1.885123 time 2019-03-01 18:55:11.379672
Model ind 640 epoch 1044 head B head_i_epoch 1 batch 100: avg loss -1.847665 avg loss no lamb -1.847665 time 2019-03-01 18:57:03.151512
Model ind 640 epoch 1044 head B head_i_epoch 1 batch 200: avg loss -1.920644 avg loss no lamb -1.920644 time 2019-03-01 18:58:58.542858
last batch sz 160
Pre: time 2019-03-01 19:00:39.238931: 
 	std: 0.051044937
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50956666, 0.614, 0.61398333, 0.6141, 0.5101]
	train_accs: [0.50956666, 0.614, 0.61398333, 0.6141, 0.5101]
	best_train_sub_head: 3
	worst: 0.50956666
	avg: 0.57235
	best: 0.6141

Starting e_i: 1045
Model ind 640 epoch 1045 head A head_i_epoch 0 batch 0: avg loss -3.257201 avg loss no lamb -3.257201 time 2019-03-01 19:00:42.728412
Model ind 640 epoch 1045 head A head_i_epoch 0 batch 100: avg loss -3.248258 avg loss no lamb -3.248258 time 2019-03-01 19:02:37.821093
Model ind 640 epoch 1045 head A head_i_epoch 0 batch 200: avg loss -3.267122 avg loss no lamb -3.267122 time 2019-03-01 19:04:32.659340
last batch sz 160
Model ind 640 epoch 1045 head B head_i_epoch 0 batch 0: avg loss -1.970414 avg loss no lamb -1.970414 time 2019-03-01 19:05:53.210833
Model ind 640 epoch 1045 head B head_i_epoch 0 batch 100: avg loss -1.851568 avg loss no lamb -1.851568 time 2019-03-01 19:07:47.670919
Model ind 640 epoch 1045 head B head_i_epoch 0 batch 200: avg loss -1.999482 avg loss no lamb -1.999482 time 2019-03-01 19:09:39.778932
last batch sz 160
Model ind 640 epoch 1045 head B head_i_epoch 1 batch 0: avg loss -1.937089 avg loss no lamb -1.937089 time 2019-03-01 19:11:01.748421
Model ind 640 epoch 1045 head B head_i_epoch 1 batch 100: avg loss -1.928687 avg loss no lamb -1.928687 time 2019-03-01 19:12:53.631654
Model ind 640 epoch 1045 head B head_i_epoch 1 batch 200: avg loss -1.898212 avg loss no lamb -1.898212 time 2019-03-01 19:14:47.051771
last batch sz 160
Pre: time 2019-03-01 19:16:30.947693: 
 	std: 0.050844908
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51065, 0.61475, 0.61466664, 0.61471665, 0.5112]
	train_accs: [0.51065, 0.61475, 0.61466664, 0.61471665, 0.5112]
	best_train_sub_head: 1
	worst: 0.51065
	avg: 0.5731967
	best: 0.61475

Starting e_i: 1046
Model ind 640 epoch 1046 head A head_i_epoch 0 batch 0: avg loss -3.356703 avg loss no lamb -3.356703 time 2019-03-01 19:16:33.759278
Model ind 640 epoch 1046 head A head_i_epoch 0 batch 100: avg loss -3.216515 avg loss no lamb -3.216515 time 2019-03-01 19:18:26.295231
Model ind 640 epoch 1046 head A head_i_epoch 0 batch 200: avg loss -3.259788 avg loss no lamb -3.259788 time 2019-03-01 19:20:18.829140
last batch sz 160
Model ind 640 epoch 1046 head B head_i_epoch 0 batch 0: avg loss -1.969280 avg loss no lamb -1.969280 time 2019-03-01 19:21:39.734472
Model ind 640 epoch 1046 head B head_i_epoch 0 batch 100: avg loss -1.878956 avg loss no lamb -1.878956 time 2019-03-01 19:23:32.439296
Model ind 640 epoch 1046 head B head_i_epoch 0 batch 200: avg loss -1.917451 avg loss no lamb -1.917451 time 2019-03-01 19:25:24.417154
last batch sz 160
Model ind 640 epoch 1046 head B head_i_epoch 1 batch 0: avg loss -1.930324 avg loss no lamb -1.930324 time 2019-03-01 19:26:47.658574
Model ind 640 epoch 1046 head B head_i_epoch 1 batch 100: avg loss -1.866585 avg loss no lamb -1.866585 time 2019-03-01 19:28:39.325674
Model ind 640 epoch 1046 head B head_i_epoch 1 batch 200: avg loss -1.945989 avg loss no lamb -1.945989 time 2019-03-01 19:30:31.398969
last batch sz 160
Pre: time 2019-03-01 19:32:16.000161: 
 	std: 0.05134029
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5097833, 0.61483335, 0.6148667, 0.6149167, 0.5103667]
	train_accs: [0.5097833, 0.61483335, 0.6148667, 0.6149167, 0.5103667]
	best_train_sub_head: 3
	worst: 0.5097833
	avg: 0.57295334
	best: 0.6149167

Starting e_i: 1047
Model ind 640 epoch 1047 head A head_i_epoch 0 batch 0: avg loss -3.183408 avg loss no lamb -3.183408 time 2019-03-01 19:32:19.008551
Model ind 640 epoch 1047 head A head_i_epoch 0 batch 100: avg loss -3.155957 avg loss no lamb -3.155957 time 2019-03-01 19:34:09.377530
Model ind 640 epoch 1047 head A head_i_epoch 0 batch 200: avg loss -3.271857 avg loss no lamb -3.271857 time 2019-03-01 19:36:03.580268
last batch sz 160
Model ind 640 epoch 1047 head B head_i_epoch 0 batch 0: avg loss -1.905073 avg loss no lamb -1.905073 time 2019-03-01 19:37:23.962750
Model ind 640 epoch 1047 head B head_i_epoch 0 batch 100: avg loss -1.770091 avg loss no lamb -1.770091 time 2019-03-01 19:39:15.844992
Model ind 640 epoch 1047 head B head_i_epoch 0 batch 200: avg loss -1.918230 avg loss no lamb -1.918230 time 2019-03-01 19:41:09.060298
last batch sz 160
Model ind 640 epoch 1047 head B head_i_epoch 1 batch 0: avg loss -1.893062 avg loss no lamb -1.893062 time 2019-03-01 19:42:27.657631
Model ind 640 epoch 1047 head B head_i_epoch 1 batch 100: avg loss -1.846584 avg loss no lamb -1.846584 time 2019-03-01 19:44:22.460788
Model ind 640 epoch 1047 head B head_i_epoch 1 batch 200: avg loss -1.969485 avg loss no lamb -1.969485 time 2019-03-01 19:46:13.874873
last batch sz 160
Pre: time 2019-03-01 19:47:57.551299: 
 	std: 0.051414896
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5096833, 0.61468333, 0.61478335, 0.61483335, 0.50995]
	train_accs: [0.5096833, 0.61468333, 0.61478335, 0.61483335, 0.50995]
	best_train_sub_head: 3
	worst: 0.5096833
	avg: 0.5727867
	best: 0.61483335

Starting e_i: 1048
Model ind 640 epoch 1048 head A head_i_epoch 0 batch 0: avg loss -3.291628 avg loss no lamb -3.291628 time 2019-03-01 19:47:59.931066
Model ind 640 epoch 1048 head A head_i_epoch 0 batch 100: avg loss -3.133509 avg loss no lamb -3.133509 time 2019-03-01 19:49:54.120039
Model ind 640 epoch 1048 head A head_i_epoch 0 batch 200: avg loss -3.306892 avg loss no lamb -3.306892 time 2019-03-01 19:51:43.822880
last batch sz 160
Model ind 640 epoch 1048 head B head_i_epoch 0 batch 0: avg loss -1.890275 avg loss no lamb -1.890275 time 2019-03-01 19:53:10.065635
Model ind 640 epoch 1048 head B head_i_epoch 0 batch 100: avg loss -1.834173 avg loss no lamb -1.834173 time 2019-03-01 19:54:58.142943
Model ind 640 epoch 1048 head B head_i_epoch 0 batch 200: avg loss -2.019310 avg loss no lamb -2.019310 time 2019-03-01 19:56:52.707745
last batch sz 160
Model ind 640 epoch 1048 head B head_i_epoch 1 batch 0: avg loss -1.837514 avg loss no lamb -1.837514 time 2019-03-01 19:58:14.564952
Model ind 640 epoch 1048 head B head_i_epoch 1 batch 100: avg loss -1.911952 avg loss no lamb -1.911952 time 2019-03-01 20:00:05.892193
Model ind 640 epoch 1048 head B head_i_epoch 1 batch 200: avg loss -1.906715 avg loss no lamb -1.906715 time 2019-03-01 20:02:01.655258
last batch sz 160
Pre: time 2019-03-01 20:03:40.575146: 
 	std: 0.05103119
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5099, 0.6138, 0.6138833, 0.61396664, 0.50953335]
	train_accs: [0.5099, 0.6138, 0.6138833, 0.61396664, 0.50953335]
	best_train_sub_head: 3
	worst: 0.50953335
	avg: 0.57221663
	best: 0.61396664

Starting e_i: 1049
Model ind 640 epoch 1049 head A head_i_epoch 0 batch 0: avg loss -3.231487 avg loss no lamb -3.231487 time 2019-03-01 20:03:43.411973
Model ind 640 epoch 1049 head A head_i_epoch 0 batch 100: avg loss -3.202727 avg loss no lamb -3.202727 time 2019-03-01 20:05:40.214007
Model ind 640 epoch 1049 head A head_i_epoch 0 batch 200: avg loss -3.318378 avg loss no lamb -3.318378 time 2019-03-01 20:07:33.984696
last batch sz 160
Model ind 640 epoch 1049 head B head_i_epoch 0 batch 0: avg loss -1.916423 avg loss no lamb -1.916423 time 2019-03-01 20:08:53.679481
Model ind 640 epoch 1049 head B head_i_epoch 0 batch 100: avg loss -1.852432 avg loss no lamb -1.852432 time 2019-03-01 20:10:49.046979
Model ind 640 epoch 1049 head B head_i_epoch 0 batch 200: avg loss -1.911302 avg loss no lamb -1.911302 time 2019-03-01 20:12:37.006059
last batch sz 160
Model ind 640 epoch 1049 head B head_i_epoch 1 batch 0: avg loss -1.876864 avg loss no lamb -1.876864 time 2019-03-01 20:14:03.251796
Model ind 640 epoch 1049 head B head_i_epoch 1 batch 100: avg loss -1.882737 avg loss no lamb -1.882737 time 2019-03-01 20:15:52.977215
Model ind 640 epoch 1049 head B head_i_epoch 1 batch 200: avg loss -2.003386 avg loss no lamb -2.003386 time 2019-03-01 20:17:46.454642
last batch sz 160
Pre: time 2019-03-01 20:19:32.992149: 
 	std: 0.050726272
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5123, 0.61581665, 0.61565, 0.61576664, 0.5121]
	train_accs: [0.5123, 0.61581665, 0.61565, 0.61576664, 0.5121]
	best_train_sub_head: 1
	worst: 0.5121
	avg: 0.57432663
	best: 0.61581665

Starting e_i: 1050
Model ind 640 epoch 1050 head A head_i_epoch 0 batch 0: avg loss -3.248433 avg loss no lamb -3.248433 time 2019-03-01 20:19:36.909089
Model ind 640 epoch 1050 head A head_i_epoch 0 batch 100: avg loss -3.135527 avg loss no lamb -3.135527 time 2019-03-01 20:21:24.618517
Model ind 640 epoch 1050 head A head_i_epoch 0 batch 200: avg loss -3.330735 avg loss no lamb -3.330735 time 2019-03-01 20:23:13.362091
last batch sz 160
Model ind 640 epoch 1050 head B head_i_epoch 0 batch 0: avg loss -1.925047 avg loss no lamb -1.925047 time 2019-03-01 20:24:27.996803
Model ind 640 epoch 1050 head B head_i_epoch 0 batch 100: avg loss -1.833074 avg loss no lamb -1.833074 time 2019-03-01 20:26:17.280549
Model ind 640 epoch 1050 head B head_i_epoch 0 batch 200: avg loss -2.023961 avg loss no lamb -2.023961 time 2019-03-01 20:28:00.664717
last batch sz 160
Model ind 640 epoch 1050 head B head_i_epoch 1 batch 0: avg loss -1.982507 avg loss no lamb -1.982507 time 2019-03-01 20:29:21.056074
Model ind 640 epoch 1050 head B head_i_epoch 1 batch 100: avg loss -1.816046 avg loss no lamb -1.816046 time 2019-03-01 20:31:04.493308
Model ind 640 epoch 1050 head B head_i_epoch 1 batch 200: avg loss -2.018708 avg loss no lamb -2.018708 time 2019-03-01 20:32:44.290975
last batch sz 160
Pre: time 2019-03-01 20:34:15.281158: 
 	std: 0.05081336
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51121664, 0.6148, 0.61485, 0.6149667, 0.5110833]
	train_accs: [0.51121664, 0.6148, 0.61485, 0.6149667, 0.5110833]
	best_train_sub_head: 3
	worst: 0.5110833
	avg: 0.57338333
	best: 0.6149667

Starting e_i: 1051
Model ind 640 epoch 1051 head A head_i_epoch 0 batch 0: avg loss -3.257975 avg loss no lamb -3.257975 time 2019-03-01 20:34:22.104250
Model ind 640 epoch 1051 head A head_i_epoch 0 batch 100: avg loss -3.213448 avg loss no lamb -3.213448 time 2019-03-01 20:36:02.642128
Model ind 640 epoch 1051 head A head_i_epoch 0 batch 200: avg loss -3.309409 avg loss no lamb -3.309409 time 2019-03-01 20:37:42.691007
last batch sz 160
Model ind 640 epoch 1051 head B head_i_epoch 0 batch 0: avg loss -1.852088 avg loss no lamb -1.852088 time 2019-03-01 20:38:55.505429
Model ind 640 epoch 1051 head B head_i_epoch 0 batch 100: avg loss -1.859288 avg loss no lamb -1.859288 time 2019-03-01 20:40:35.294205
Model ind 640 epoch 1051 head B head_i_epoch 0 batch 200: avg loss -1.924544 avg loss no lamb -1.924544 time 2019-03-01 20:42:15.839244
last batch sz 160
Model ind 640 epoch 1051 head B head_i_epoch 1 batch 0: avg loss -1.888687 avg loss no lamb -1.888687 time 2019-03-01 20:43:28.436562
Model ind 640 epoch 1051 head B head_i_epoch 1 batch 100: avg loss -1.843728 avg loss no lamb -1.843728 time 2019-03-01 20:45:08.696859
Model ind 640 epoch 1051 head B head_i_epoch 1 batch 200: avg loss -1.993959 avg loss no lamb -1.993959 time 2019-03-01 20:46:48.290222
last batch sz 160
Pre: time 2019-03-01 20:48:19.273862: 
 	std: 0.050946847
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.511, 0.6148833, 0.61466664, 0.6148833, 0.51063335]
	train_accs: [0.511, 0.6148833, 0.61466664, 0.6148833, 0.51063335]
	best_train_sub_head: 1
	worst: 0.51063335
	avg: 0.5732134
	best: 0.6148833

Starting e_i: 1052
Model ind 640 epoch 1052 head A head_i_epoch 0 batch 0: avg loss -3.235803 avg loss no lamb -3.235803 time 2019-03-01 20:48:21.474062
Model ind 640 epoch 1052 head A head_i_epoch 0 batch 100: avg loss -3.140221 avg loss no lamb -3.140221 time 2019-03-01 20:50:01.279062
Model ind 640 epoch 1052 head A head_i_epoch 0 batch 200: avg loss -3.284354 avg loss no lamb -3.284354 time 2019-03-01 20:51:41.105454
last batch sz 160
Model ind 640 epoch 1052 head B head_i_epoch 0 batch 0: avg loss -1.929137 avg loss no lamb -1.929137 time 2019-03-01 20:52:53.696511
Model ind 640 epoch 1052 head B head_i_epoch 0 batch 100: avg loss -1.857498 avg loss no lamb -1.857498 time 2019-03-01 20:54:33.370854
Model ind 640 epoch 1052 head B head_i_epoch 0 batch 200: avg loss -1.887952 avg loss no lamb -1.887952 time 2019-03-01 20:56:13.055468
last batch sz 160
Model ind 640 epoch 1052 head B head_i_epoch 1 batch 0: avg loss -1.928188 avg loss no lamb -1.928188 time 2019-03-01 20:57:25.593847
Model ind 640 epoch 1052 head B head_i_epoch 1 batch 100: avg loss -1.798913 avg loss no lamb -1.798913 time 2019-03-01 20:59:05.597722
Model ind 640 epoch 1052 head B head_i_epoch 1 batch 200: avg loss -1.910916 avg loss no lamb -1.910916 time 2019-03-01 21:00:45.529228
last batch sz 160
Pre: time 2019-03-01 21:02:17.576978: 
 	std: 0.05057528
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51145, 0.61473334, 0.6145, 0.61485, 0.5114667]
	train_accs: [0.51145, 0.61473334, 0.6145, 0.61485, 0.5114667]
	best_train_sub_head: 3
	worst: 0.51145
	avg: 0.5734
	best: 0.61485

Starting e_i: 1053
Model ind 640 epoch 1053 head A head_i_epoch 0 batch 0: avg loss -3.223482 avg loss no lamb -3.223482 time 2019-03-01 21:02:19.933414
Model ind 640 epoch 1053 head A head_i_epoch 0 batch 100: avg loss -3.219995 avg loss no lamb -3.219995 time 2019-03-01 21:03:59.779365
Model ind 640 epoch 1053 head A head_i_epoch 0 batch 200: avg loss -3.318157 avg loss no lamb -3.318157 time 2019-03-01 21:05:39.600761
last batch sz 160
Model ind 640 epoch 1053 head B head_i_epoch 0 batch 0: avg loss -1.919083 avg loss no lamb -1.919083 time 2019-03-01 21:06:52.153237
Model ind 640 epoch 1053 head B head_i_epoch 0 batch 100: avg loss -1.870687 avg loss no lamb -1.870687 time 2019-03-01 21:08:31.896504
Model ind 640 epoch 1053 head B head_i_epoch 0 batch 200: avg loss -1.937181 avg loss no lamb -1.937181 time 2019-03-01 21:10:11.441051
last batch sz 160
Model ind 640 epoch 1053 head B head_i_epoch 1 batch 0: avg loss -1.953438 avg loss no lamb -1.953438 time 2019-03-01 21:11:23.973331
Model ind 640 epoch 1053 head B head_i_epoch 1 batch 100: avg loss -1.923120 avg loss no lamb -1.923120 time 2019-03-01 21:13:04.231417
Model ind 640 epoch 1053 head B head_i_epoch 1 batch 200: avg loss -1.907956 avg loss no lamb -1.907956 time 2019-03-01 21:14:44.004707
last batch sz 160
Pre: time 2019-03-01 21:16:15.024613: 
 	std: 0.05074403
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51075, 0.6141, 0.6141667, 0.6142, 0.5104]
	train_accs: [0.51075, 0.6141, 0.6141667, 0.6142, 0.5104]
	best_train_sub_head: 3
	worst: 0.5104
	avg: 0.5727233
	best: 0.6142

Starting e_i: 1054
Model ind 640 epoch 1054 head A head_i_epoch 0 batch 0: avg loss -3.214087 avg loss no lamb -3.214087 time 2019-03-01 21:16:17.204562
Model ind 640 epoch 1054 head A head_i_epoch 0 batch 100: avg loss -3.194542 avg loss no lamb -3.194542 time 2019-03-01 21:17:56.914727
Model ind 640 epoch 1054 head A head_i_epoch 0 batch 200: avg loss -3.287508 avg loss no lamb -3.287508 time 2019-03-01 21:19:37.090161
last batch sz 160
Model ind 640 epoch 1054 head B head_i_epoch 0 batch 0: avg loss -1.929302 avg loss no lamb -1.929302 time 2019-03-01 21:20:49.630809
Model ind 640 epoch 1054 head B head_i_epoch 0 batch 100: avg loss -1.787251 avg loss no lamb -1.787251 time 2019-03-01 21:22:29.995420
Model ind 640 epoch 1054 head B head_i_epoch 0 batch 200: avg loss -1.990250 avg loss no lamb -1.990250 time 2019-03-01 21:24:09.600296
last batch sz 160
Model ind 640 epoch 1054 head B head_i_epoch 1 batch 0: avg loss -1.842489 avg loss no lamb -1.842489 time 2019-03-01 21:25:22.608622
Model ind 640 epoch 1054 head B head_i_epoch 1 batch 100: avg loss -1.890676 avg loss no lamb -1.890676 time 2019-03-01 21:27:02.212917
Model ind 640 epoch 1054 head B head_i_epoch 1 batch 200: avg loss -1.938626 avg loss no lamb -1.938626 time 2019-03-01 21:28:41.966223
last batch sz 160
Pre: time 2019-03-01 21:30:13.261230: 
 	std: 0.050216366
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51233333, 0.6145667, 0.6144, 0.6145667, 0.51168334]
	train_accs: [0.51233333, 0.6145667, 0.6144, 0.6145667, 0.51168334]
	best_train_sub_head: 1
	worst: 0.51168334
	avg: 0.57351
	best: 0.6145667

Starting e_i: 1055
Model ind 640 epoch 1055 head A head_i_epoch 0 batch 0: avg loss -3.311434 avg loss no lamb -3.311434 time 2019-03-01 21:30:16.097550
Model ind 640 epoch 1055 head A head_i_epoch 0 batch 100: avg loss -3.252059 avg loss no lamb -3.252059 time 2019-03-01 21:31:56.221130
Model ind 640 epoch 1055 head A head_i_epoch 0 batch 200: avg loss -3.220409 avg loss no lamb -3.220409 time 2019-03-01 21:33:36.559929
last batch sz 160
Model ind 640 epoch 1055 head B head_i_epoch 0 batch 0: avg loss -1.888297 avg loss no lamb -1.888297 time 2019-03-01 21:34:49.600887
Model ind 640 epoch 1055 head B head_i_epoch 0 batch 100: avg loss -1.870177 avg loss no lamb -1.870177 time 2019-03-01 21:36:29.388955
Model ind 640 epoch 1055 head B head_i_epoch 0 batch 200: avg loss -2.002193 avg loss no lamb -2.002193 time 2019-03-01 21:38:09.654995
last batch sz 160
Model ind 640 epoch 1055 head B head_i_epoch 1 batch 0: avg loss -1.899453 avg loss no lamb -1.899453 time 2019-03-01 21:39:22.519185
Model ind 640 epoch 1055 head B head_i_epoch 1 batch 100: avg loss -1.904524 avg loss no lamb -1.904524 time 2019-03-01 21:41:02.740911
Model ind 640 epoch 1055 head B head_i_epoch 1 batch 200: avg loss -1.998078 avg loss no lamb -1.998078 time 2019-03-01 21:42:42.747403
last batch sz 160
Pre: time 2019-03-01 21:44:13.871558: 
 	std: 0.05039707
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51103336, 0.61408335, 0.61403334, 0.6142, 0.51143336]
	train_accs: [0.51103336, 0.61408335, 0.61403334, 0.6142, 0.51143336]
	best_train_sub_head: 3
	worst: 0.51103336
	avg: 0.5729567
	best: 0.6142

Starting e_i: 1056
Model ind 640 epoch 1056 head A head_i_epoch 0 batch 0: avg loss -3.236308 avg loss no lamb -3.236308 time 2019-03-01 21:44:16.061501
Model ind 640 epoch 1056 head A head_i_epoch 0 batch 100: avg loss -3.183795 avg loss no lamb -3.183795 time 2019-03-01 21:45:56.586149
Model ind 640 epoch 1056 head A head_i_epoch 0 batch 200: avg loss -3.275702 avg loss no lamb -3.275702 time 2019-03-01 21:47:36.610928
last batch sz 160
Model ind 640 epoch 1056 head B head_i_epoch 0 batch 0: avg loss -1.935365 avg loss no lamb -1.935365 time 2019-03-01 21:48:49.672607
Model ind 640 epoch 1056 head B head_i_epoch 0 batch 100: avg loss -1.856229 avg loss no lamb -1.856229 time 2019-03-01 21:50:29.559941
Model ind 640 epoch 1056 head B head_i_epoch 0 batch 200: avg loss -1.992119 avg loss no lamb -1.992119 time 2019-03-01 21:52:09.478065
last batch sz 160
Model ind 640 epoch 1056 head B head_i_epoch 1 batch 0: avg loss -1.951896 avg loss no lamb -1.951896 time 2019-03-01 21:53:22.076702
Model ind 640 epoch 1056 head B head_i_epoch 1 batch 100: avg loss -1.825127 avg loss no lamb -1.825127 time 2019-03-01 21:55:02.310911
Model ind 640 epoch 1056 head B head_i_epoch 1 batch 200: avg loss -1.953645 avg loss no lamb -1.953645 time 2019-03-01 21:56:42.177420
last batch sz 160
Pre: time 2019-03-01 21:58:13.687716: 
 	std: 0.05024997
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115833, 0.614, 0.61415, 0.61406666, 0.5114167]
	train_accs: [0.5115833, 0.614, 0.61415, 0.61406666, 0.5114167]
	best_train_sub_head: 2
	worst: 0.5114167
	avg: 0.57304335
	best: 0.61415

Starting e_i: 1057
Model ind 640 epoch 1057 head A head_i_epoch 0 batch 0: avg loss -3.253911 avg loss no lamb -3.253911 time 2019-03-01 21:58:15.888637
Model ind 640 epoch 1057 head A head_i_epoch 0 batch 100: avg loss -3.186948 avg loss no lamb -3.186948 time 2019-03-01 21:59:55.679065
Model ind 640 epoch 1057 head A head_i_epoch 0 batch 200: avg loss -3.186793 avg loss no lamb -3.186793 time 2019-03-01 22:01:35.482824
last batch sz 160
Model ind 640 epoch 1057 head B head_i_epoch 0 batch 0: avg loss -1.915756 avg loss no lamb -1.915756 time 2019-03-01 22:02:48.748571
Model ind 640 epoch 1057 head B head_i_epoch 0 batch 100: avg loss -1.938276 avg loss no lamb -1.938276 time 2019-03-01 22:04:28.435120
Model ind 640 epoch 1057 head B head_i_epoch 0 batch 200: avg loss -1.983092 avg loss no lamb -1.983092 time 2019-03-01 22:06:08.112064
last batch sz 160
Model ind 640 epoch 1057 head B head_i_epoch 1 batch 0: avg loss -1.860832 avg loss no lamb -1.860832 time 2019-03-01 22:07:21.384137
Model ind 640 epoch 1057 head B head_i_epoch 1 batch 100: avg loss -1.937448 avg loss no lamb -1.937448 time 2019-03-01 22:09:01.569328
Model ind 640 epoch 1057 head B head_i_epoch 1 batch 200: avg loss -1.953479 avg loss no lamb -1.953479 time 2019-03-01 22:10:41.799151
last batch sz 160
Pre: time 2019-03-01 22:12:13.050389: 
 	std: 0.050724957
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50913334, 0.6128333, 0.6128833, 0.61268336, 0.5093833]
	train_accs: [0.50913334, 0.6128333, 0.6128833, 0.61268336, 0.5093833]
	best_train_sub_head: 2
	worst: 0.50913334
	avg: 0.5713833
	best: 0.6128833

Starting e_i: 1058
Model ind 640 epoch 1058 head A head_i_epoch 0 batch 0: avg loss -3.240482 avg loss no lamb -3.240482 time 2019-03-01 22:12:15.446239
Model ind 640 epoch 1058 head A head_i_epoch 0 batch 100: avg loss -3.177047 avg loss no lamb -3.177047 time 2019-03-01 22:13:55.492625
Model ind 640 epoch 1058 head A head_i_epoch 0 batch 200: avg loss -3.253953 avg loss no lamb -3.253953 time 2019-03-01 22:15:35.278292
last batch sz 160
Model ind 640 epoch 1058 head B head_i_epoch 0 batch 0: avg loss -1.848267 avg loss no lamb -1.848267 time 2019-03-01 22:16:47.889000
Model ind 640 epoch 1058 head B head_i_epoch 0 batch 100: avg loss -1.903344 avg loss no lamb -1.903344 time 2019-03-01 22:18:27.523223
Model ind 640 epoch 1058 head B head_i_epoch 0 batch 200: avg loss -1.937346 avg loss no lamb -1.937346 time 2019-03-01 22:20:07.134228
last batch sz 160
Model ind 640 epoch 1058 head B head_i_epoch 1 batch 0: avg loss -1.967151 avg loss no lamb -1.967151 time 2019-03-01 22:21:19.524248
Model ind 640 epoch 1058 head B head_i_epoch 1 batch 100: avg loss -1.758001 avg loss no lamb -1.758001 time 2019-03-01 22:22:59.447726
Model ind 640 epoch 1058 head B head_i_epoch 1 batch 200: avg loss -1.983082 avg loss no lamb -1.983082 time 2019-03-01 22:24:39.400895
last batch sz 160
Pre: time 2019-03-01 22:26:10.932056: 
 	std: 0.050601076
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.6145167, 0.6146333, 0.61466664, 0.51143336]
	train_accs: [0.5112, 0.6145167, 0.6146333, 0.61466664, 0.51143336]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.57329
	best: 0.61466664

Starting e_i: 1059
Model ind 640 epoch 1059 head A head_i_epoch 0 batch 0: avg loss -3.193591 avg loss no lamb -3.193591 time 2019-03-01 22:26:13.230939
Model ind 640 epoch 1059 head A head_i_epoch 0 batch 100: avg loss -3.126811 avg loss no lamb -3.126811 time 2019-03-01 22:27:53.412191
Model ind 640 epoch 1059 head A head_i_epoch 0 batch 200: avg loss -3.288377 avg loss no lamb -3.288377 time 2019-03-01 22:29:33.580110
last batch sz 160
Model ind 640 epoch 1059 head B head_i_epoch 0 batch 0: avg loss -1.937269 avg loss no lamb -1.937269 time 2019-03-01 22:30:48.551304
Model ind 640 epoch 1059 head B head_i_epoch 0 batch 100: avg loss -1.845525 avg loss no lamb -1.845525 time 2019-03-01 22:32:28.554334
Model ind 640 epoch 1059 head B head_i_epoch 0 batch 200: avg loss -1.941659 avg loss no lamb -1.941659 time 2019-03-01 22:34:10.552688
last batch sz 160
Model ind 640 epoch 1059 head B head_i_epoch 1 batch 0: avg loss -1.877125 avg loss no lamb -1.877125 time 2019-03-01 22:35:22.871320
Model ind 640 epoch 1059 head B head_i_epoch 1 batch 100: avg loss -1.959787 avg loss no lamb -1.959787 time 2019-03-01 22:37:04.547501
Model ind 640 epoch 1059 head B head_i_epoch 1 batch 200: avg loss -1.992319 avg loss no lamb -1.992319 time 2019-03-01 22:38:44.189386
last batch sz 160
Pre: time 2019-03-01 22:40:14.937680: 
 	std: 0.051232524
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50985, 0.61436665, 0.61446667, 0.61465, 0.50998336]
	train_accs: [0.50985, 0.61436665, 0.61446667, 0.61465, 0.50998336]
	best_train_sub_head: 3
	worst: 0.50985
	avg: 0.57266337
	best: 0.61465

Starting e_i: 1060
Model ind 640 epoch 1060 head A head_i_epoch 0 batch 0: avg loss -3.239300 avg loss no lamb -3.239300 time 2019-03-01 22:40:17.814578
Model ind 640 epoch 1060 head A head_i_epoch 0 batch 100: avg loss -3.176903 avg loss no lamb -3.176903 time 2019-03-01 22:41:57.604957
Model ind 640 epoch 1060 head A head_i_epoch 0 batch 200: avg loss -3.361485 avg loss no lamb -3.361485 time 2019-03-01 22:43:37.761025
last batch sz 160
Model ind 640 epoch 1060 head B head_i_epoch 0 batch 0: avg loss -1.918679 avg loss no lamb -1.918679 time 2019-03-01 22:44:50.322373
Model ind 640 epoch 1060 head B head_i_epoch 0 batch 100: avg loss -1.858827 avg loss no lamb -1.858827 time 2019-03-01 22:46:32.054326
Model ind 640 epoch 1060 head B head_i_epoch 0 batch 200: avg loss -2.012657 avg loss no lamb -2.012657 time 2019-03-01 22:48:12.222390
last batch sz 160
Model ind 640 epoch 1060 head B head_i_epoch 1 batch 0: avg loss -1.981845 avg loss no lamb -1.981845 time 2019-03-01 22:49:25.841679
Model ind 640 epoch 1060 head B head_i_epoch 1 batch 100: avg loss -1.866142 avg loss no lamb -1.866142 time 2019-03-01 22:51:09.560482
Model ind 640 epoch 1060 head B head_i_epoch 1 batch 200: avg loss -1.922697 avg loss no lamb -1.922697 time 2019-03-01 22:52:52.258519
last batch sz 160
Pre: time 2019-03-01 22:54:24.488515: 
 	std: 0.05015204
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115833, 0.61385, 0.61373335, 0.6138833, 0.51131666]
	train_accs: [0.5115833, 0.61385, 0.61373335, 0.6138833, 0.51131666]
	best_train_sub_head: 3
	worst: 0.51131666
	avg: 0.5728733
	best: 0.6138833

Starting e_i: 1061
Model ind 640 epoch 1061 head A head_i_epoch 0 batch 0: avg loss -3.216980 avg loss no lamb -3.216980 time 2019-03-01 22:54:31.745252
Model ind 640 epoch 1061 head A head_i_epoch 0 batch 100: avg loss -3.294592 avg loss no lamb -3.294592 time 2019-03-01 22:56:13.670654
Model ind 640 epoch 1061 head A head_i_epoch 0 batch 200: avg loss -3.295830 avg loss no lamb -3.295830 time 2019-03-01 22:57:56.784206
last batch sz 160
Model ind 640 epoch 1061 head B head_i_epoch 0 batch 0: avg loss -1.926334 avg loss no lamb -1.926334 time 2019-03-01 22:59:09.960665
Model ind 640 epoch 1061 head B head_i_epoch 0 batch 100: avg loss -1.939365 avg loss no lamb -1.939365 time 2019-03-01 23:00:59.305847
Model ind 640 epoch 1061 head B head_i_epoch 0 batch 200: avg loss -1.950752 avg loss no lamb -1.950752 time 2019-03-01 23:02:44.790716
last batch sz 160
Model ind 640 epoch 1061 head B head_i_epoch 1 batch 0: avg loss -1.943904 avg loss no lamb -1.943904 time 2019-03-01 23:04:14.145147
Model ind 640 epoch 1061 head B head_i_epoch 1 batch 100: avg loss -1.835361 avg loss no lamb -1.835361 time 2019-03-01 23:05:59.613326
Model ind 640 epoch 1061 head B head_i_epoch 1 batch 200: avg loss -1.941797 avg loss no lamb -1.941797 time 2019-03-01 23:07:42.815805
last batch sz 160
Pre: time 2019-03-01 23:09:18.959003: 
 	std: 0.050305776
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5128833, 0.61545, 0.61546665, 0.6156167, 0.51276666]
	train_accs: [0.5128833, 0.61545, 0.61546665, 0.6156167, 0.51276666]
	best_train_sub_head: 3
	worst: 0.51276666
	avg: 0.5744366
	best: 0.6156167

Starting e_i: 1062
Model ind 640 epoch 1062 head A head_i_epoch 0 batch 0: avg loss -3.273514 avg loss no lamb -3.273514 time 2019-03-01 23:09:21.277350
Model ind 640 epoch 1062 head A head_i_epoch 0 batch 100: avg loss -3.236331 avg loss no lamb -3.236331 time 2019-03-01 23:11:05.933047
Model ind 640 epoch 1062 head A head_i_epoch 0 batch 200: avg loss -3.277821 avg loss no lamb -3.277821 time 2019-03-01 23:12:50.286409
last batch sz 160
Model ind 640 epoch 1062 head B head_i_epoch 0 batch 0: avg loss -1.907725 avg loss no lamb -1.907725 time 2019-03-01 23:14:05.337364
Model ind 640 epoch 1062 head B head_i_epoch 0 batch 100: avg loss -1.845087 avg loss no lamb -1.845087 time 2019-03-01 23:15:47.895296
Model ind 640 epoch 1062 head B head_i_epoch 0 batch 200: avg loss -1.946564 avg loss no lamb -1.946564 time 2019-03-01 23:17:30.757926
last batch sz 160
Model ind 640 epoch 1062 head B head_i_epoch 1 batch 0: avg loss -1.957359 avg loss no lamb -1.957359 time 2019-03-01 23:18:45.592802
Model ind 640 epoch 1062 head B head_i_epoch 1 batch 100: avg loss -1.876761 avg loss no lamb -1.876761 time 2019-03-01 23:20:28.692722
Model ind 640 epoch 1062 head B head_i_epoch 1 batch 200: avg loss -2.031414 avg loss no lamb -2.031414 time 2019-03-01 23:22:11.840240
last batch sz 160
Pre: time 2019-03-01 23:23:46.233058: 
 	std: 0.050410923
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51126665, 0.61455, 0.6144, 0.6145, 0.5119]
	train_accs: [0.51126665, 0.61455, 0.6144, 0.6145, 0.5119]
	best_train_sub_head: 1
	worst: 0.51126665
	avg: 0.5733233
	best: 0.61455

Starting e_i: 1063
Model ind 640 epoch 1063 head A head_i_epoch 0 batch 0: avg loss -3.281231 avg loss no lamb -3.281231 time 2019-03-01 23:23:48.469587
Model ind 640 epoch 1063 head A head_i_epoch 0 batch 100: avg loss -3.190922 avg loss no lamb -3.190922 time 2019-03-01 23:25:30.626030
Model ind 640 epoch 1063 head A head_i_epoch 0 batch 200: avg loss -3.285507 avg loss no lamb -3.285507 time 2019-03-01 23:27:14.032217
last batch sz 160
Model ind 640 epoch 1063 head B head_i_epoch 0 batch 0: avg loss -1.931808 avg loss no lamb -1.931808 time 2019-03-01 23:28:28.705031
Model ind 640 epoch 1063 head B head_i_epoch 0 batch 100: avg loss -1.903062 avg loss no lamb -1.903062 time 2019-03-01 23:30:11.157631
Model ind 640 epoch 1063 head B head_i_epoch 0 batch 200: avg loss -1.945784 avg loss no lamb -1.945784 time 2019-03-01 23:31:54.741614
last batch sz 160
Model ind 640 epoch 1063 head B head_i_epoch 1 batch 0: avg loss -1.905428 avg loss no lamb -1.905428 time 2019-03-01 23:33:10.409979
Model ind 640 epoch 1063 head B head_i_epoch 1 batch 100: avg loss -1.869877 avg loss no lamb -1.869877 time 2019-03-01 23:34:54.057752
Model ind 640 epoch 1063 head B head_i_epoch 1 batch 200: avg loss -1.866239 avg loss no lamb -1.866239 time 2019-03-01 23:36:37.977437
last batch sz 160
Pre: time 2019-03-01 23:38:13.637267: 
 	std: 0.05022409
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51233333, 0.61475, 0.61481667, 0.6148667, 0.51225]
	train_accs: [0.51233333, 0.61475, 0.61481667, 0.6148667, 0.51225]
	best_train_sub_head: 3
	worst: 0.51225
	avg: 0.5738033
	best: 0.6148667

Starting e_i: 1064
Model ind 640 epoch 1064 head A head_i_epoch 0 batch 0: avg loss -3.256918 avg loss no lamb -3.256918 time 2019-03-01 23:38:15.976474
Model ind 640 epoch 1064 head A head_i_epoch 0 batch 100: avg loss -3.221275 avg loss no lamb -3.221275 time 2019-03-01 23:39:57.313547
Model ind 640 epoch 1064 head A head_i_epoch 0 batch 200: avg loss -3.294716 avg loss no lamb -3.294716 time 2019-03-01 23:41:38.852719
last batch sz 160
Model ind 640 epoch 1064 head B head_i_epoch 0 batch 0: avg loss -1.975469 avg loss no lamb -1.975469 time 2019-03-01 23:42:54.335062
Model ind 640 epoch 1064 head B head_i_epoch 0 batch 100: avg loss -1.921795 avg loss no lamb -1.921795 time 2019-03-01 23:44:36.875687
Model ind 640 epoch 1064 head B head_i_epoch 0 batch 200: avg loss -1.987925 avg loss no lamb -1.987925 time 2019-03-01 23:46:19.885267
last batch sz 160
Model ind 640 epoch 1064 head B head_i_epoch 1 batch 0: avg loss -1.929316 avg loss no lamb -1.929316 time 2019-03-01 23:47:34.968287
Model ind 640 epoch 1064 head B head_i_epoch 1 batch 100: avg loss -1.865573 avg loss no lamb -1.865573 time 2019-03-01 23:49:18.206439
Model ind 640 epoch 1064 head B head_i_epoch 1 batch 200: avg loss -1.978766 avg loss no lamb -1.978766 time 2019-03-01 23:51:01.994400
last batch sz 160
Pre: time 2019-03-01 23:52:36.977143: 
 	std: 0.050696306
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51168334, 0.61501664, 0.61516666, 0.61506665, 0.5115167]
	train_accs: [0.51168334, 0.61501664, 0.61516666, 0.61506665, 0.5115167]
	best_train_sub_head: 2
	worst: 0.5115167
	avg: 0.57368994
	best: 0.61516666

Starting e_i: 1065
Model ind 640 epoch 1065 head A head_i_epoch 0 batch 0: avg loss -3.371575 avg loss no lamb -3.371575 time 2019-03-01 23:52:39.338001
Model ind 640 epoch 1065 head A head_i_epoch 0 batch 100: avg loss -3.202088 avg loss no lamb -3.202088 time 2019-03-01 23:54:21.790235
Model ind 640 epoch 1065 head A head_i_epoch 0 batch 200: avg loss -3.217498 avg loss no lamb -3.217498 time 2019-03-01 23:56:04.586982
last batch sz 160
Model ind 640 epoch 1065 head B head_i_epoch 0 batch 0: avg loss -1.970939 avg loss no lamb -1.970939 time 2019-03-01 23:57:19.648881
Model ind 640 epoch 1065 head B head_i_epoch 0 batch 100: avg loss -1.911363 avg loss no lamb -1.911363 time 2019-03-01 23:59:02.946652
Model ind 640 epoch 1065 head B head_i_epoch 0 batch 200: avg loss -1.917959 avg loss no lamb -1.917959 time 2019-03-02 00:00:45.722969
last batch sz 160
Model ind 640 epoch 1065 head B head_i_epoch 1 batch 0: avg loss -1.896726 avg loss no lamb -1.896726 time 2019-03-02 00:02:00.589801
Model ind 640 epoch 1065 head B head_i_epoch 1 batch 100: avg loss -1.833474 avg loss no lamb -1.833474 time 2019-03-02 00:03:44.082089
Model ind 640 epoch 1065 head B head_i_epoch 1 batch 200: avg loss -1.995890 avg loss no lamb -1.995890 time 2019-03-02 00:05:27.589056
last batch sz 160
Pre: time 2019-03-02 00:07:01.710914: 
 	std: 0.050229546
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.512, 0.61445, 0.6146167, 0.6146, 0.51205]
	train_accs: [0.512, 0.61445, 0.6146167, 0.6146, 0.51205]
	best_train_sub_head: 2
	worst: 0.512
	avg: 0.5735433
	best: 0.6146167

Starting e_i: 1066
Model ind 640 epoch 1066 head A head_i_epoch 0 batch 0: avg loss -3.298239 avg loss no lamb -3.298239 time 2019-03-02 00:07:04.115711
Model ind 640 epoch 1066 head A head_i_epoch 0 batch 100: avg loss -3.193473 avg loss no lamb -3.193473 time 2019-03-02 00:08:46.809182
Model ind 640 epoch 1066 head A head_i_epoch 0 batch 200: avg loss -3.319308 avg loss no lamb -3.319308 time 2019-03-02 00:10:30.025060
last batch sz 160
Model ind 640 epoch 1066 head B head_i_epoch 0 batch 0: avg loss -1.944890 avg loss no lamb -1.944890 time 2019-03-02 00:11:43.137820
Model ind 640 epoch 1066 head B head_i_epoch 0 batch 100: avg loss -1.806070 avg loss no lamb -1.806070 time 2019-03-02 00:13:24.680236
Model ind 640 epoch 1066 head B head_i_epoch 0 batch 200: avg loss -1.971125 avg loss no lamb -1.971125 time 2019-03-02 00:15:08.295030
last batch sz 160
Model ind 640 epoch 1066 head B head_i_epoch 1 batch 0: avg loss -1.967205 avg loss no lamb -1.967205 time 2019-03-02 00:16:23.253550
Model ind 640 epoch 1066 head B head_i_epoch 1 batch 100: avg loss -1.801664 avg loss no lamb -1.801664 time 2019-03-02 00:18:05.652684
Model ind 640 epoch 1066 head B head_i_epoch 1 batch 200: avg loss -1.987941 avg loss no lamb -1.987941 time 2019-03-02 00:19:47.696063
last batch sz 160
Pre: time 2019-03-02 00:21:21.317467: 
 	std: 0.050639145
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51, 0.6134167, 0.61345, 0.61343336, 0.5101333]
	train_accs: [0.51, 0.6134167, 0.61345, 0.61343336, 0.5101333]
	best_train_sub_head: 2
	worst: 0.51
	avg: 0.5720867
	best: 0.61345

Starting e_i: 1067
Model ind 640 epoch 1067 head A head_i_epoch 0 batch 0: avg loss -3.253143 avg loss no lamb -3.253143 time 2019-03-02 00:21:24.369894
Model ind 640 epoch 1067 head A head_i_epoch 0 batch 100: avg loss -3.211742 avg loss no lamb -3.211742 time 2019-03-02 00:23:06.958350
Model ind 640 epoch 1067 head A head_i_epoch 0 batch 200: avg loss -3.284626 avg loss no lamb -3.284626 time 2019-03-02 00:24:50.577206
last batch sz 160
Model ind 640 epoch 1067 head B head_i_epoch 0 batch 0: avg loss -1.980949 avg loss no lamb -1.980949 time 2019-03-02 00:26:06.098433
Model ind 640 epoch 1067 head B head_i_epoch 0 batch 100: avg loss -1.813759 avg loss no lamb -1.813759 time 2019-03-02 00:27:48.738161
Model ind 640 epoch 1067 head B head_i_epoch 0 batch 200: avg loss -2.063569 avg loss no lamb -2.063569 time 2019-03-02 00:29:31.489928
last batch sz 160
Model ind 640 epoch 1067 head B head_i_epoch 1 batch 0: avg loss -1.852808 avg loss no lamb -1.852808 time 2019-03-02 00:30:47.430439
Model ind 640 epoch 1067 head B head_i_epoch 1 batch 100: avg loss -1.866154 avg loss no lamb -1.866154 time 2019-03-02 00:32:30.267395
Model ind 640 epoch 1067 head B head_i_epoch 1 batch 200: avg loss -1.974741 avg loss no lamb -1.974741 time 2019-03-02 00:34:12.973032
last batch sz 160
Pre: time 2019-03-02 00:35:46.892900: 
 	std: 0.05078065
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51086664, 0.6145, 0.6145, 0.6145167, 0.5108333]
	train_accs: [0.51086664, 0.6145, 0.6145, 0.6145167, 0.5108333]
	best_train_sub_head: 3
	worst: 0.5108333
	avg: 0.57304335
	best: 0.6145167

Starting e_i: 1068
Model ind 640 epoch 1068 head A head_i_epoch 0 batch 0: avg loss -3.190185 avg loss no lamb -3.190185 time 2019-03-02 00:35:49.241529
Model ind 640 epoch 1068 head A head_i_epoch 0 batch 100: avg loss -3.187716 avg loss no lamb -3.187716 time 2019-03-02 00:37:32.026782
Model ind 640 epoch 1068 head A head_i_epoch 0 batch 200: avg loss -3.266926 avg loss no lamb -3.266926 time 2019-03-02 00:39:14.680250
last batch sz 160
Model ind 640 epoch 1068 head B head_i_epoch 0 batch 0: avg loss -1.926595 avg loss no lamb -1.926595 time 2019-03-02 00:40:29.380043
Model ind 640 epoch 1068 head B head_i_epoch 0 batch 100: avg loss -1.816822 avg loss no lamb -1.816822 time 2019-03-02 00:42:11.766461
Model ind 640 epoch 1068 head B head_i_epoch 0 batch 200: avg loss -2.002886 avg loss no lamb -2.002886 time 2019-03-02 00:43:52.099072
last batch sz 160
Model ind 640 epoch 1068 head B head_i_epoch 1 batch 0: avg loss -1.865818 avg loss no lamb -1.865818 time 2019-03-02 00:45:06.243466
Model ind 640 epoch 1068 head B head_i_epoch 1 batch 100: avg loss -1.835764 avg loss no lamb -1.835764 time 2019-03-02 00:46:49.816250
Model ind 640 epoch 1068 head B head_i_epoch 1 batch 200: avg loss -1.944979 avg loss no lamb -1.944979 time 2019-03-02 00:48:32.411607
last batch sz 160
Pre: time 2019-03-02 00:50:06.661406: 
 	std: 0.050443195
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51103336, 0.61403334, 0.61398333, 0.61413336, 0.5111333]
	train_accs: [0.51103336, 0.61403334, 0.61398333, 0.61413336, 0.5111333]
	best_train_sub_head: 3
	worst: 0.51103336
	avg: 0.5728634
	best: 0.61413336

Starting e_i: 1069
Model ind 640 epoch 1069 head A head_i_epoch 0 batch 0: avg loss -3.211298 avg loss no lamb -3.211298 time 2019-03-02 00:50:09.058846
Model ind 640 epoch 1069 head A head_i_epoch 0 batch 100: avg loss -3.189580 avg loss no lamb -3.189580 time 2019-03-02 00:51:52.331877
Model ind 640 epoch 1069 head A head_i_epoch 0 batch 200: avg loss -3.280242 avg loss no lamb -3.280242 time 2019-03-02 00:53:35.400128
last batch sz 160
Model ind 640 epoch 1069 head B head_i_epoch 0 batch 0: avg loss -1.892238 avg loss no lamb -1.892238 time 2019-03-02 00:54:50.342578
Model ind 640 epoch 1069 head B head_i_epoch 0 batch 100: avg loss -1.852970 avg loss no lamb -1.852970 time 2019-03-02 00:56:34.342380
Model ind 640 epoch 1069 head B head_i_epoch 0 batch 200: avg loss -1.965104 avg loss no lamb -1.965104 time 2019-03-02 00:58:19.577857
last batch sz 160
Model ind 640 epoch 1069 head B head_i_epoch 1 batch 0: avg loss -1.959620 avg loss no lamb -1.959620 time 2019-03-02 00:59:33.414883
Model ind 640 epoch 1069 head B head_i_epoch 1 batch 100: avg loss -1.769714 avg loss no lamb -1.769714 time 2019-03-02 01:01:41.785578
Model ind 640 epoch 1069 head B head_i_epoch 1 batch 200: avg loss -1.893923 avg loss no lamb -1.893923 time 2019-03-02 01:04:59.778459
last batch sz 160
Pre: time 2019-03-02 01:07:59.136151: 
 	std: 0.05068148
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104833, 0.61405, 0.61413336, 0.6142, 0.51086664]
	train_accs: [0.5104833, 0.61405, 0.61413336, 0.6142, 0.51086664]
	best_train_sub_head: 3
	worst: 0.5104833
	avg: 0.57274663
	best: 0.6142

Starting e_i: 1070
Model ind 640 epoch 1070 head A head_i_epoch 0 batch 0: avg loss -3.216958 avg loss no lamb -3.216958 time 2019-03-02 01:08:02.766269
Model ind 640 epoch 1070 head A head_i_epoch 0 batch 100: avg loss -3.207785 avg loss no lamb -3.207785 time 2019-03-02 01:11:13.905813
Model ind 640 epoch 1070 head A head_i_epoch 0 batch 200: avg loss -3.297862 avg loss no lamb -3.297862 time 2019-03-02 01:13:49.625740
last batch sz 160
Model ind 640 epoch 1070 head B head_i_epoch 0 batch 0: avg loss -1.886366 avg loss no lamb -1.886366 time 2019-03-02 01:16:07.145801
Model ind 640 epoch 1070 head B head_i_epoch 0 batch 100: avg loss -1.864404 avg loss no lamb -1.864404 time 2019-03-02 01:19:20.048218
Model ind 640 epoch 1070 head B head_i_epoch 0 batch 200: avg loss -1.958978 avg loss no lamb -1.958978 time 2019-03-02 01:22:28.420133
last batch sz 160
Model ind 640 epoch 1070 head B head_i_epoch 1 batch 0: avg loss -1.941159 avg loss no lamb -1.941159 time 2019-03-02 01:24:46.300216
Model ind 640 epoch 1070 head B head_i_epoch 1 batch 100: avg loss -1.832192 avg loss no lamb -1.832192 time 2019-03-02 01:27:20.939597
Model ind 640 epoch 1070 head B head_i_epoch 1 batch 200: avg loss -1.987721 avg loss no lamb -1.987721 time 2019-03-02 01:30:31.278984
last batch sz 160
Pre: time 2019-03-02 01:33:24.252042: 
 	std: 0.0509318
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5100167, 0.6139167, 0.6142, 0.614, 0.5101333]
	train_accs: [0.5100167, 0.6139167, 0.6142, 0.614, 0.5101333]
	best_train_sub_head: 2
	worst: 0.5100167
	avg: 0.5724533
	best: 0.6142

Starting e_i: 1071
Model ind 640 epoch 1071 head A head_i_epoch 0 batch 0: avg loss -3.290972 avg loss no lamb -3.290972 time 2019-03-02 01:33:33.319553
Model ind 640 epoch 1071 head A head_i_epoch 0 batch 100: avg loss -3.146566 avg loss no lamb -3.146566 time 2019-03-02 01:36:43.590714
Model ind 640 epoch 1071 head A head_i_epoch 0 batch 200: avg loss -3.332105 avg loss no lamb -3.332105 time 2019-03-02 01:39:37.770706
last batch sz 160
Model ind 640 epoch 1071 head B head_i_epoch 0 batch 0: avg loss -1.957881 avg loss no lamb -1.957881 time 2019-03-02 01:41:38.688743
Model ind 640 epoch 1071 head B head_i_epoch 0 batch 100: avg loss -1.880553 avg loss no lamb -1.880553 time 2019-03-02 01:44:50.491831
Model ind 640 epoch 1071 head B head_i_epoch 0 batch 200: avg loss -1.934032 avg loss no lamb -1.934032 time 2019-03-02 01:48:00.829662
last batch sz 160
Model ind 640 epoch 1071 head B head_i_epoch 1 batch 0: avg loss -1.950479 avg loss no lamb -1.950479 time 2019-03-02 01:50:17.657073
Model ind 640 epoch 1071 head B head_i_epoch 1 batch 100: avg loss -1.859089 avg loss no lamb -1.859089 time 2019-03-02 01:53:10.976594
Model ind 640 epoch 1071 head B head_i_epoch 1 batch 200: avg loss -1.875437 avg loss no lamb -1.875437 time 2019-03-02 01:56:03.578989
last batch sz 160
Pre: time 2019-03-02 01:58:55.817180: 
 	std: 0.051139936
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.6155, 0.61545, 0.6155667, 0.51103336]
	train_accs: [0.5112, 0.6155, 0.61545, 0.6155667, 0.51103336]
	best_train_sub_head: 3
	worst: 0.51103336
	avg: 0.57374996
	best: 0.6155667

Starting e_i: 1072
Model ind 640 epoch 1072 head A head_i_epoch 0 batch 0: avg loss -3.216327 avg loss no lamb -3.216327 time 2019-03-02 01:58:59.988662
Model ind 640 epoch 1072 head A head_i_epoch 0 batch 100: avg loss -3.197151 avg loss no lamb -3.197151 time 2019-03-02 02:02:08.374730
Model ind 640 epoch 1072 head A head_i_epoch 0 batch 200: avg loss -3.280735 avg loss no lamb -3.280735 time 2019-03-02 02:05:22.323326
last batch sz 160
Model ind 640 epoch 1072 head B head_i_epoch 0 batch 0: avg loss -1.927753 avg loss no lamb -1.927753 time 2019-03-02 02:07:09.892091
Model ind 640 epoch 1072 head B head_i_epoch 0 batch 100: avg loss -1.904878 avg loss no lamb -1.904878 time 2019-03-02 02:10:20.540542
Model ind 640 epoch 1072 head B head_i_epoch 0 batch 200: avg loss -1.942704 avg loss no lamb -1.942704 time 2019-03-02 02:13:29.376043
last batch sz 160
Model ind 640 epoch 1072 head B head_i_epoch 1 batch 0: avg loss -1.962653 avg loss no lamb -1.962653 time 2019-03-02 02:15:47.413165
Model ind 640 epoch 1072 head B head_i_epoch 1 batch 100: avg loss -1.845791 avg loss no lamb -1.845791 time 2019-03-02 02:18:59.678673
Model ind 640 epoch 1072 head B head_i_epoch 1 batch 200: avg loss -2.026215 avg loss no lamb -2.026215 time 2019-03-02 02:21:34.172722
last batch sz 160
Pre: time 2019-03-02 02:24:27.907952: 
 	std: 0.050311234
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51163334, 0.6141833, 0.6142, 0.6141833, 0.51135]
	train_accs: [0.51163334, 0.6141833, 0.6142, 0.6141833, 0.51135]
	best_train_sub_head: 2
	worst: 0.51135
	avg: 0.57311
	best: 0.6142

Starting e_i: 1073
Model ind 640 epoch 1073 head A head_i_epoch 0 batch 0: avg loss -3.292520 avg loss no lamb -3.292520 time 2019-03-02 02:24:31.039739
Model ind 640 epoch 1073 head A head_i_epoch 0 batch 100: avg loss -3.173809 avg loss no lamb -3.173809 time 2019-03-02 02:27:41.122900
Model ind 640 epoch 1073 head A head_i_epoch 0 batch 200: avg loss -3.227378 avg loss no lamb -3.227378 time 2019-03-02 02:30:50.271928
last batch sz 160
Model ind 640 epoch 1073 head B head_i_epoch 0 batch 0: avg loss -1.880336 avg loss no lamb -1.880336 time 2019-03-02 02:32:53.700938
Model ind 640 epoch 1073 head B head_i_epoch 0 batch 100: avg loss -1.898854 avg loss no lamb -1.898854 time 2019-03-02 02:35:48.256678
Model ind 640 epoch 1073 head B head_i_epoch 0 batch 200: avg loss -1.946265 avg loss no lamb -1.946265 time 2019-03-02 02:39:00.584468
last batch sz 160
Model ind 640 epoch 1073 head B head_i_epoch 1 batch 0: avg loss -1.998470 avg loss no lamb -1.998470 time 2019-03-02 02:41:18.303876
Model ind 640 epoch 1073 head B head_i_epoch 1 batch 100: avg loss -1.856083 avg loss no lamb -1.856083 time 2019-03-02 02:44:28.313602
Model ind 640 epoch 1073 head B head_i_epoch 1 batch 200: avg loss -1.990325 avg loss no lamb -1.990325 time 2019-03-02 02:47:05.709147
last batch sz 160
Pre: time 2019-03-02 02:49:58.467890: 
 	std: 0.050628245
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51115, 0.61446667, 0.61445, 0.61441666, 0.51105]
	train_accs: [0.51115, 0.61446667, 0.61445, 0.61441666, 0.51105]
	best_train_sub_head: 1
	worst: 0.51105
	avg: 0.57310665
	best: 0.61446667

Starting e_i: 1074
Model ind 640 epoch 1074 head A head_i_epoch 0 batch 0: avg loss -3.253775 avg loss no lamb -3.253775 time 2019-03-02 02:50:02.086304
Model ind 640 epoch 1074 head A head_i_epoch 0 batch 100: avg loss -3.168247 avg loss no lamb -3.168247 time 2019-03-02 02:53:15.819734
Model ind 640 epoch 1074 head A head_i_epoch 0 batch 200: avg loss -3.178224 avg loss no lamb -3.178224 time 2019-03-02 02:56:26.653002
last batch sz 160
Model ind 640 epoch 1074 head B head_i_epoch 0 batch 0: avg loss -1.967605 avg loss no lamb -1.967605 time 2019-03-02 02:58:44.749352
Model ind 640 epoch 1074 head B head_i_epoch 0 batch 100: avg loss -1.871807 avg loss no lamb -1.871807 time 2019-03-02 03:01:21.993179
Model ind 640 epoch 1074 head B head_i_epoch 0 batch 200: avg loss -1.950548 avg loss no lamb -1.950548 time 2019-03-02 03:04:32.225983
last batch sz 160
Model ind 640 epoch 1074 head B head_i_epoch 1 batch 0: avg loss -1.887891 avg loss no lamb -1.887891 time 2019-03-02 03:06:48.238991
Model ind 640 epoch 1074 head B head_i_epoch 1 batch 100: avg loss -1.909208 avg loss no lamb -1.909208 time 2019-03-02 03:09:59.685185
Model ind 640 epoch 1074 head B head_i_epoch 1 batch 200: avg loss -1.932392 avg loss no lamb -1.932392 time 2019-03-02 03:12:54.421092
last batch sz 160
Pre: time 2019-03-02 03:15:28.629761: 
 	std: 0.05074394
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50955, 0.6131833, 0.61325, 0.61308336, 0.50963336]
	train_accs: [0.50955, 0.6131833, 0.61325, 0.61308336, 0.50963336]
	best_train_sub_head: 2
	worst: 0.50955
	avg: 0.57174003
	best: 0.61325

Starting e_i: 1075
Model ind 640 epoch 1075 head A head_i_epoch 0 batch 0: avg loss -3.249393 avg loss no lamb -3.249393 time 2019-03-02 03:15:31.631723
Model ind 640 epoch 1075 head A head_i_epoch 0 batch 100: avg loss -3.227937 avg loss no lamb -3.227937 time 2019-03-02 03:18:46.117744
Model ind 640 epoch 1075 head A head_i_epoch 0 batch 200: avg loss -3.293837 avg loss no lamb -3.293837 time 2019-03-02 03:21:55.235394
last batch sz 160
Model ind 640 epoch 1075 head B head_i_epoch 0 batch 0: avg loss -1.968390 avg loss no lamb -1.968390 time 2019-03-02 03:24:13.064372
Model ind 640 epoch 1075 head B head_i_epoch 0 batch 100: avg loss -1.864293 avg loss no lamb -1.864293 time 2019-03-02 03:26:51.876597
Model ind 640 epoch 1075 head B head_i_epoch 0 batch 200: avg loss -1.907640 avg loss no lamb -1.907640 time 2019-03-02 03:30:00.697927
last batch sz 160
Model ind 640 epoch 1075 head B head_i_epoch 1 batch 0: avg loss -1.945606 avg loss no lamb -1.945606 time 2019-03-02 03:32:16.379802
Model ind 640 epoch 1075 head B head_i_epoch 1 batch 100: avg loss -1.864866 avg loss no lamb -1.864866 time 2019-03-02 03:35:30.381252
Model ind 640 epoch 1075 head B head_i_epoch 1 batch 200: avg loss -1.931969 avg loss no lamb -1.931969 time 2019-03-02 03:38:40.387715
last batch sz 160
Pre: time 2019-03-02 03:40:58.291605: 
 	std: 0.05080927
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5103833, 0.61403334, 0.61395, 0.61408335, 0.51023334]
	train_accs: [0.5103833, 0.61403334, 0.61395, 0.61408335, 0.51023334]
	best_train_sub_head: 3
	worst: 0.51023334
	avg: 0.57253665
	best: 0.61408335

Starting e_i: 1076
Model ind 640 epoch 1076 head A head_i_epoch 0 batch 0: avg loss -3.224075 avg loss no lamb -3.224075 time 2019-03-02 03:41:01.881670
Model ind 640 epoch 1076 head A head_i_epoch 0 batch 100: avg loss -3.209422 avg loss no lamb -3.209422 time 2019-03-02 03:44:16.475984
Model ind 640 epoch 1076 head A head_i_epoch 0 batch 200: avg loss -3.270928 avg loss no lamb -3.270928 time 2019-03-02 03:47:27.919008
last batch sz 160
Model ind 640 epoch 1076 head B head_i_epoch 0 batch 0: avg loss -1.941836 avg loss no lamb -1.941836 time 2019-03-02 03:49:46.333299
Model ind 640 epoch 1076 head B head_i_epoch 0 batch 100: avg loss -1.877251 avg loss no lamb -1.877251 time 2019-03-02 03:52:40.754654
Model ind 640 epoch 1076 head B head_i_epoch 0 batch 200: avg loss -1.963519 avg loss no lamb -1.963519 time 2019-03-02 03:55:34.196480
last batch sz 160
Model ind 640 epoch 1076 head B head_i_epoch 1 batch 0: avg loss -1.993633 avg loss no lamb -1.993633 time 2019-03-02 03:57:53.309557
Model ind 640 epoch 1076 head B head_i_epoch 1 batch 100: avg loss -1.937573 avg loss no lamb -1.937573 time 2019-03-02 04:01:05.707693
Model ind 640 epoch 1076 head B head_i_epoch 1 batch 200: avg loss -1.909448 avg loss no lamb -1.909448 time 2019-03-02 04:04:14.907908
last batch sz 160
Pre: time 2019-03-02 04:06:42.100501: 
 	std: 0.050268978
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51173335, 0.6142833, 0.61433333, 0.61436665, 0.5117]
	train_accs: [0.51173335, 0.6142833, 0.61433333, 0.61436665, 0.5117]
	best_train_sub_head: 3
	worst: 0.5117
	avg: 0.5732833
	best: 0.61436665

Starting e_i: 1077
Model ind 640 epoch 1077 head A head_i_epoch 0 batch 0: avg loss -3.275716 avg loss no lamb -3.275716 time 2019-03-02 04:06:45.998993
Model ind 640 epoch 1077 head A head_i_epoch 0 batch 100: avg loss -3.124331 avg loss no lamb -3.124331 time 2019-03-02 04:09:50.394291
Model ind 640 epoch 1077 head A head_i_epoch 0 batch 200: avg loss -3.207001 avg loss no lamb -3.207001 time 2019-03-02 04:13:00.587178
last batch sz 160
Model ind 640 epoch 1077 head B head_i_epoch 0 batch 0: avg loss -1.972749 avg loss no lamb -1.972749 time 2019-03-02 04:15:18.461752
Model ind 640 epoch 1077 head B head_i_epoch 0 batch 100: avg loss -1.930851 avg loss no lamb -1.930851 time 2019-03-02 04:18:29.045253
Model ind 640 epoch 1077 head B head_i_epoch 0 batch 200: avg loss -2.011389 avg loss no lamb -2.011389 time 2019-03-02 04:21:08.372412
last batch sz 160
Model ind 640 epoch 1077 head B head_i_epoch 1 batch 0: avg loss -1.892716 avg loss no lamb -1.892716 time 2019-03-02 04:23:27.990870
Model ind 640 epoch 1077 head B head_i_epoch 1 batch 100: avg loss -1.845722 avg loss no lamb -1.845722 time 2019-03-02 04:26:38.010069
Model ind 640 epoch 1077 head B head_i_epoch 1 batch 200: avg loss -1.962153 avg loss no lamb -1.962153 time 2019-03-02 04:29:48.115203
last batch sz 160
Pre: time 2019-03-02 04:32:31.741125: 
 	std: 0.050152015
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51285, 0.61516666, 0.61505, 0.6151, 0.5126167]
	train_accs: [0.51285, 0.61516666, 0.61505, 0.6151, 0.5126167]
	best_train_sub_head: 1
	worst: 0.5126167
	avg: 0.57415664
	best: 0.61516666

Starting e_i: 1078
Model ind 640 epoch 1078 head A head_i_epoch 0 batch 0: avg loss -3.233887 avg loss no lamb -3.233887 time 2019-03-02 04:32:34.637514
Model ind 640 epoch 1078 head A head_i_epoch 0 batch 100: avg loss -3.216748 avg loss no lamb -3.216748 time 2019-03-02 04:35:23.651864
Model ind 640 epoch 1078 head A head_i_epoch 0 batch 200: avg loss -3.214823 avg loss no lamb -3.214823 time 2019-03-02 04:38:36.193751
last batch sz 160
Model ind 640 epoch 1078 head B head_i_epoch 0 batch 0: avg loss -1.881611 avg loss no lamb -1.881611 time 2019-03-02 04:40:57.468893
Model ind 640 epoch 1078 head B head_i_epoch 0 batch 100: avg loss -1.831287 avg loss no lamb -1.831287 time 2019-03-02 04:44:10.676664
Model ind 640 epoch 1078 head B head_i_epoch 0 batch 200: avg loss -1.953223 avg loss no lamb -1.953223 time 2019-03-02 04:46:49.162381
last batch sz 160
Model ind 640 epoch 1078 head B head_i_epoch 1 batch 0: avg loss -1.853932 avg loss no lamb -1.853932 time 2019-03-02 04:49:07.610988
Model ind 640 epoch 1078 head B head_i_epoch 1 batch 100: avg loss -1.914481 avg loss no lamb -1.914481 time 2019-03-02 04:52:17.010802
Model ind 640 epoch 1078 head B head_i_epoch 1 batch 200: avg loss -1.939013 avg loss no lamb -1.939013 time 2019-03-02 04:55:24.620394
last batch sz 160
Pre: time 2019-03-02 04:58:17.760548: 
 	std: 0.050190274
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51088333, 0.61305, 0.6131833, 0.6130667, 0.5104167]
	train_accs: [0.51088333, 0.61305, 0.6131833, 0.6130667, 0.5104167]
	best_train_sub_head: 2
	worst: 0.5104167
	avg: 0.57212
	best: 0.6131833

Starting e_i: 1079
Model ind 640 epoch 1079 head A head_i_epoch 0 batch 0: avg loss -3.186201 avg loss no lamb -3.186201 time 2019-03-02 04:58:20.775265
Model ind 640 epoch 1079 head A head_i_epoch 0 batch 100: avg loss -3.166500 avg loss no lamb -3.166500 time 2019-03-02 05:00:58.433631
Model ind 640 epoch 1079 head A head_i_epoch 0 batch 200: avg loss -3.283293 avg loss no lamb -3.283293 time 2019-03-02 05:04:10.377301
last batch sz 160
Model ind 640 epoch 1079 head B head_i_epoch 0 batch 0: avg loss -1.972326 avg loss no lamb -1.972326 time 2019-03-02 05:06:27.249844
Model ind 640 epoch 1079 head B head_i_epoch 0 batch 100: avg loss -1.923020 avg loss no lamb -1.923020 time 2019-03-02 05:09:39.801099
Model ind 640 epoch 1079 head B head_i_epoch 0 batch 200: avg loss -2.006016 avg loss no lamb -2.006016 time 2019-03-02 05:12:35.398128
last batch sz 160
Model ind 640 epoch 1079 head B head_i_epoch 1 batch 0: avg loss -1.961075 avg loss no lamb -1.961075 time 2019-03-02 05:14:36.087029
Model ind 640 epoch 1079 head B head_i_epoch 1 batch 100: avg loss -1.903743 avg loss no lamb -1.903743 time 2019-03-02 05:17:44.369742
Model ind 640 epoch 1079 head B head_i_epoch 1 batch 200: avg loss -1.968947 avg loss no lamb -1.968947 time 2019-03-02 05:20:59.173127
last batch sz 160
Pre: time 2019-03-02 05:23:48.199135: 
 	std: 0.05059834
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111667, 0.6145833, 0.6145833, 0.6145333, 0.5114]
	train_accs: [0.5111667, 0.6145833, 0.6145833, 0.6145333, 0.5114]
	best_train_sub_head: 1
	worst: 0.5111667
	avg: 0.5732533
	best: 0.6145833

Starting e_i: 1080
Model ind 640 epoch 1080 head A head_i_epoch 0 batch 0: avg loss -3.259936 avg loss no lamb -3.259936 time 2019-03-02 05:23:51.827283
Model ind 640 epoch 1080 head A head_i_epoch 0 batch 100: avg loss -3.260904 avg loss no lamb -3.260904 time 2019-03-02 05:26:36.340800
Model ind 640 epoch 1080 head A head_i_epoch 0 batch 200: avg loss -3.336129 avg loss no lamb -3.336129 time 2019-03-02 05:29:39.150576
last batch sz 160
Model ind 640 epoch 1080 head B head_i_epoch 0 batch 0: avg loss -1.942379 avg loss no lamb -1.942379 time 2019-03-02 05:31:59.891853
Model ind 640 epoch 1080 head B head_i_epoch 0 batch 100: avg loss -1.853465 avg loss no lamb -1.853465 time 2019-03-02 05:35:09.454025
Model ind 640 epoch 1080 head B head_i_epoch 0 batch 200: avg loss -2.003058 avg loss no lamb -2.003058 time 2019-03-02 05:38:22.269015
last batch sz 160
Model ind 640 epoch 1080 head B head_i_epoch 1 batch 0: avg loss -2.009562 avg loss no lamb -2.009562 time 2019-03-02 05:40:10.006543
Model ind 640 epoch 1080 head B head_i_epoch 1 batch 100: avg loss -1.948824 avg loss no lamb -1.948824 time 2019-03-02 05:43:20.126457
Model ind 640 epoch 1080 head B head_i_epoch 1 batch 200: avg loss -1.951324 avg loss no lamb -1.951324 time 2019-03-02 05:46:32.577876
last batch sz 160
Pre: time 2019-03-02 05:49:21.524764: 
 	std: 0.05049895
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51138335, 0.61443335, 0.61445, 0.61438334, 0.5113]
	train_accs: [0.51138335, 0.61443335, 0.61445, 0.61438334, 0.5113]
	best_train_sub_head: 2
	worst: 0.5113
	avg: 0.57319003
	best: 0.61445

Starting e_i: 1081
Model ind 640 epoch 1081 head A head_i_epoch 0 batch 0: avg loss -3.293098 avg loss no lamb -3.293098 time 2019-03-02 05:49:29.703519
Model ind 640 epoch 1081 head A head_i_epoch 0 batch 100: avg loss -3.196627 avg loss no lamb -3.196627 time 2019-03-02 05:52:25.298904
Model ind 640 epoch 1081 head A head_i_epoch 0 batch 200: avg loss -3.300189 avg loss no lamb -3.300189 time 2019-03-02 05:55:17.606548
last batch sz 160
Model ind 640 epoch 1081 head B head_i_epoch 0 batch 0: avg loss -1.862138 avg loss no lamb -1.862138 time 2019-03-02 05:57:38.500011
Model ind 640 epoch 1081 head B head_i_epoch 0 batch 100: avg loss -1.899106 avg loss no lamb -1.899106 time 2019-03-02 06:00:49.203349
Model ind 640 epoch 1081 head B head_i_epoch 0 batch 200: avg loss -2.021328 avg loss no lamb -2.021328 time 2019-03-02 06:04:00.502854
last batch sz 160
Model ind 640 epoch 1081 head B head_i_epoch 1 batch 0: avg loss -1.935991 avg loss no lamb -1.935991 time 2019-03-02 06:06:00.639096
Model ind 640 epoch 1081 head B head_i_epoch 1 batch 100: avg loss -1.794520 avg loss no lamb -1.794520 time 2019-03-02 06:08:55.782803
Model ind 640 epoch 1081 head B head_i_epoch 1 batch 200: avg loss -1.939104 avg loss no lamb -1.939104 time 2019-03-02 06:12:05.058592
last batch sz 160
Pre: time 2019-03-02 06:14:56.465573: 
 	std: 0.050707337
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5121833, 0.61551666, 0.6154, 0.6156, 0.5118167]
	train_accs: [0.5121833, 0.61551666, 0.6154, 0.6156, 0.5118167]
	best_train_sub_head: 3
	worst: 0.5118167
	avg: 0.5741033
	best: 0.6156

Starting e_i: 1082
Model ind 640 epoch 1082 head A head_i_epoch 0 batch 0: avg loss -3.224817 avg loss no lamb -3.224817 time 2019-03-02 06:15:00.588520
Model ind 640 epoch 1082 head A head_i_epoch 0 batch 100: avg loss -3.189096 avg loss no lamb -3.189096 time 2019-03-02 06:18:08.037806
Model ind 640 epoch 1082 head A head_i_epoch 0 batch 200: avg loss -3.249546 avg loss no lamb -3.249546 time 2019-03-02 06:20:47.884595
last batch sz 160
Model ind 640 epoch 1082 head B head_i_epoch 0 batch 0: avg loss -1.964746 avg loss no lamb -1.964746 time 2019-03-02 06:23:05.916519
Model ind 640 epoch 1082 head B head_i_epoch 0 batch 100: avg loss -1.897995 avg loss no lamb -1.897995 time 2019-03-02 06:26:14.999942
Model ind 640 epoch 1082 head B head_i_epoch 0 batch 200: avg loss -2.025601 avg loss no lamb -2.025601 time 2019-03-02 06:29:22.893095
last batch sz 160
Model ind 640 epoch 1082 head B head_i_epoch 1 batch 0: avg loss -1.948032 avg loss no lamb -1.948032 time 2019-03-02 06:31:40.426333
Model ind 640 epoch 1082 head B head_i_epoch 1 batch 100: avg loss -1.869293 avg loss no lamb -1.869293 time 2019-03-02 06:34:17.996261
Model ind 640 epoch 1082 head B head_i_epoch 1 batch 200: avg loss -1.947534 avg loss no lamb -1.947534 time 2019-03-02 06:37:28.013747
last batch sz 160
Pre: time 2019-03-02 06:40:20.711543: 
 	std: 0.0507168
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51065, 0.61408335, 0.61405, 0.61436665, 0.51063335]
	train_accs: [0.51065, 0.61408335, 0.61405, 0.61436665, 0.51063335]
	best_train_sub_head: 3
	worst: 0.51063335
	avg: 0.57275665
	best: 0.61436665

Starting e_i: 1083
Model ind 640 epoch 1083 head A head_i_epoch 0 batch 0: avg loss -3.194083 avg loss no lamb -3.194083 time 2019-03-02 06:40:23.737680
Model ind 640 epoch 1083 head A head_i_epoch 0 batch 100: avg loss -3.158457 avg loss no lamb -3.158457 time 2019-03-02 06:43:35.848774
Model ind 640 epoch 1083 head A head_i_epoch 0 batch 200: avg loss -3.228062 avg loss no lamb -3.228062 time 2019-03-02 06:46:18.484054
last batch sz 160
Model ind 640 epoch 1083 head B head_i_epoch 0 batch 0: avg loss -1.866932 avg loss no lamb -1.866932 time 2019-03-02 06:48:32.068088
Model ind 640 epoch 1083 head B head_i_epoch 0 batch 100: avg loss -1.887519 avg loss no lamb -1.887519 time 2019-03-02 06:51:44.509929
Model ind 640 epoch 1083 head B head_i_epoch 0 batch 200: avg loss -1.957787 avg loss no lamb -1.957787 time 2019-03-02 06:54:52.962084
last batch sz 160
Model ind 640 epoch 1083 head B head_i_epoch 1 batch 0: avg loss -1.863149 avg loss no lamb -1.863149 time 2019-03-02 06:57:12.268029
Model ind 640 epoch 1083 head B head_i_epoch 1 batch 100: avg loss -1.911911 avg loss no lamb -1.911911 time 2019-03-02 06:59:47.860442
Model ind 640 epoch 1083 head B head_i_epoch 1 batch 200: avg loss -1.984455 avg loss no lamb -1.984455 time 2019-03-02 07:02:58.564166
last batch sz 160
Pre: time 2019-03-02 07:05:51.025652: 
 	std: 0.050798476
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107167, 0.61445, 0.6145667, 0.61468333, 0.51103336]
	train_accs: [0.5107167, 0.61445, 0.6145667, 0.61468333, 0.51103336]
	best_train_sub_head: 3
	worst: 0.5107167
	avg: 0.57308996
	best: 0.61468333

Starting e_i: 1084
Model ind 640 epoch 1084 head A head_i_epoch 0 batch 0: avg loss -3.321037 avg loss no lamb -3.321037 time 2019-03-02 07:05:54.894865
Model ind 640 epoch 1084 head A head_i_epoch 0 batch 100: avg loss -3.189633 avg loss no lamb -3.189633 time 2019-03-02 07:09:05.901778
Model ind 640 epoch 1084 head A head_i_epoch 0 batch 200: avg loss -3.321194 avg loss no lamb -3.321194 time 2019-03-02 07:12:02.512723
last batch sz 160
Model ind 640 epoch 1084 head B head_i_epoch 0 batch 0: avg loss -1.978922 avg loss no lamb -1.978922 time 2019-03-02 07:14:01.792638
Model ind 640 epoch 1084 head B head_i_epoch 0 batch 100: avg loss -1.963224 avg loss no lamb -1.963224 time 2019-03-02 07:17:15.763693
Model ind 640 epoch 1084 head B head_i_epoch 0 batch 200: avg loss -2.001562 avg loss no lamb -2.001562 time 2019-03-02 07:20:24.290470
last batch sz 160
Model ind 640 epoch 1084 head B head_i_epoch 1 batch 0: avg loss -1.910720 avg loss no lamb -1.910720 time 2019-03-02 07:22:44.877229
Model ind 640 epoch 1084 head B head_i_epoch 1 batch 100: avg loss -1.889211 avg loss no lamb -1.889211 time 2019-03-02 07:25:41.222444
Model ind 640 epoch 1084 head B head_i_epoch 1 batch 200: avg loss -1.993538 avg loss no lamb -1.993538 time 2019-03-02 07:28:35.657700
last batch sz 160
Pre: time 2019-03-02 07:31:28.913201: 
 	std: 0.050579276
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5119333, 0.61513335, 0.61506665, 0.61523336, 0.5118667]
	train_accs: [0.5119333, 0.61513335, 0.61506665, 0.61523336, 0.5118667]
	best_train_sub_head: 3
	worst: 0.5118667
	avg: 0.57384664
	best: 0.61523336

Starting e_i: 1085
Model ind 640 epoch 1085 head A head_i_epoch 0 batch 0: avg loss -3.254246 avg loss no lamb -3.254246 time 2019-03-02 07:31:31.991091
Model ind 640 epoch 1085 head A head_i_epoch 0 batch 100: avg loss -3.144866 avg loss no lamb -3.144866 time 2019-03-02 07:34:42.196759
Model ind 640 epoch 1085 head A head_i_epoch 0 batch 200: avg loss -3.278301 avg loss no lamb -3.278301 time 2019-03-02 07:37:49.859290
last batch sz 160
Model ind 640 epoch 1085 head B head_i_epoch 0 batch 0: avg loss -1.937272 avg loss no lamb -1.937272 time 2019-03-02 07:39:36.466687
Model ind 640 epoch 1085 head B head_i_epoch 0 batch 100: avg loss -1.915672 avg loss no lamb -1.915672 time 2019-03-02 07:42:45.799007
Model ind 640 epoch 1085 head B head_i_epoch 0 batch 200: avg loss -1.963477 avg loss no lamb -1.963477 time 2019-03-02 07:45:58.847695
last batch sz 160
Model ind 640 epoch 1085 head B head_i_epoch 1 batch 0: avg loss -1.961506 avg loss no lamb -1.961506 time 2019-03-02 07:48:16.687155
Model ind 640 epoch 1085 head B head_i_epoch 1 batch 100: avg loss -1.819734 avg loss no lamb -1.819734 time 2019-03-02 07:51:24.534175
Model ind 640 epoch 1085 head B head_i_epoch 1 batch 200: avg loss -1.949219 avg loss no lamb -1.949219 time 2019-03-02 07:54:04.858816
last batch sz 160
Pre: time 2019-03-02 07:56:58.784635: 
 	std: 0.050759062
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51138335, 0.61473334, 0.61475, 0.61485, 0.51095]
	train_accs: [0.51138335, 0.61473334, 0.61475, 0.61485, 0.51095]
	best_train_sub_head: 3
	worst: 0.51095
	avg: 0.5733334
	best: 0.61485

Starting e_i: 1086
Model ind 640 epoch 1086 head A head_i_epoch 0 batch 0: avg loss -3.257223 avg loss no lamb -3.257223 time 2019-03-02 07:57:01.795678
Model ind 640 epoch 1086 head A head_i_epoch 0 batch 100: avg loss -3.252755 avg loss no lamb -3.252755 time 2019-03-02 08:00:09.948002
Model ind 640 epoch 1086 head A head_i_epoch 0 batch 200: avg loss -3.317686 avg loss no lamb -3.317686 time 2019-03-02 08:03:21.459368
last batch sz 160
Model ind 640 epoch 1086 head B head_i_epoch 0 batch 0: avg loss -1.917558 avg loss no lamb -1.917558 time 2019-03-02 08:05:20.548243
Model ind 640 epoch 1086 head B head_i_epoch 0 batch 100: avg loss -1.896776 avg loss no lamb -1.896776 time 2019-03-02 08:08:15.532701
Model ind 640 epoch 1086 head B head_i_epoch 0 batch 200: avg loss -1.915739 avg loss no lamb -1.915739 time 2019-03-02 08:11:28.714217
last batch sz 160
Model ind 640 epoch 1086 head B head_i_epoch 1 batch 0: avg loss -1.905928 avg loss no lamb -1.905928 time 2019-03-02 08:13:44.955686
Model ind 640 epoch 1086 head B head_i_epoch 1 batch 100: avg loss -1.848252 avg loss no lamb -1.848252 time 2019-03-02 08:16:54.581190
Model ind 640 epoch 1086 head B head_i_epoch 1 batch 200: avg loss -1.901242 avg loss no lamb -1.901242 time 2019-03-02 08:19:30.894391
last batch sz 160
Pre: time 2019-03-02 08:22:23.676490: 
 	std: 0.050983425
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51128334, 0.6153, 0.61525, 0.6153833, 0.5112]
	train_accs: [0.51128334, 0.6153, 0.61525, 0.6153833, 0.5112]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.5736833
	best: 0.6153833

Starting e_i: 1087
Model ind 640 epoch 1087 head A head_i_epoch 0 batch 0: avg loss -3.259855 avg loss no lamb -3.259855 time 2019-03-02 08:22:27.718143
Model ind 640 epoch 1087 head A head_i_epoch 0 batch 100: avg loss -3.213488 avg loss no lamb -3.213488 time 2019-03-02 08:25:36.016881
Model ind 640 epoch 1087 head A head_i_epoch 0 batch 200: avg loss -3.242048 avg loss no lamb -3.242048 time 2019-03-02 08:28:44.646397
last batch sz 160
Model ind 640 epoch 1087 head B head_i_epoch 0 batch 0: avg loss -1.889668 avg loss no lamb -1.889668 time 2019-03-02 08:31:05.647024
Model ind 640 epoch 1087 head B head_i_epoch 0 batch 100: avg loss -1.788473 avg loss no lamb -1.788473 time 2019-03-02 08:33:48.495263
Model ind 640 epoch 1087 head B head_i_epoch 0 batch 200: avg loss -1.917117 avg loss no lamb -1.917117 time 2019-03-02 08:36:58.722274
last batch sz 160
Model ind 640 epoch 1087 head B head_i_epoch 1 batch 0: avg loss -1.937709 avg loss no lamb -1.937709 time 2019-03-02 08:39:15.849255
Model ind 640 epoch 1087 head B head_i_epoch 1 batch 100: avg loss -1.859382 avg loss no lamb -1.859382 time 2019-03-02 08:42:26.594253
Model ind 640 epoch 1087 head B head_i_epoch 1 batch 200: avg loss -2.033713 avg loss no lamb -2.033713 time 2019-03-02 08:45:22.974421
last batch sz 160
Pre: time 2019-03-02 08:47:57.608165: 
 	std: 0.050601024
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5111167, 0.61435, 0.61436665, 0.6144, 0.51105]
	train_accs: [0.5111167, 0.61435, 0.61436665, 0.6144, 0.51105]
	best_train_sub_head: 3
	worst: 0.51105
	avg: 0.57305664
	best: 0.6144

Starting e_i: 1088
Model ind 640 epoch 1088 head A head_i_epoch 0 batch 0: avg loss -3.319731 avg loss no lamb -3.319731 time 2019-03-02 08:48:00.750418
Model ind 640 epoch 1088 head A head_i_epoch 0 batch 100: avg loss -3.181835 avg loss no lamb -3.181835 time 2019-03-02 08:51:08.756217
Model ind 640 epoch 1088 head A head_i_epoch 0 batch 200: avg loss -3.252203 avg loss no lamb -3.252203 time 2019-03-02 08:54:16.510133
last batch sz 160
Model ind 640 epoch 1088 head B head_i_epoch 0 batch 0: avg loss -2.028739 avg loss no lamb -2.028739 time 2019-03-02 08:56:33.531803
Model ind 640 epoch 1088 head B head_i_epoch 0 batch 100: avg loss -1.910492 avg loss no lamb -1.910492 time 2019-03-02 08:59:21.483027
Model ind 640 epoch 1088 head B head_i_epoch 0 batch 200: avg loss -1.982318 avg loss no lamb -1.982318 time 2019-03-02 09:02:25.718577
last batch sz 160
Model ind 640 epoch 1088 head B head_i_epoch 1 batch 0: avg loss -2.012228 avg loss no lamb -2.012228 time 2019-03-02 09:04:45.582616
Model ind 640 epoch 1088 head B head_i_epoch 1 batch 100: avg loss -1.821677 avg loss no lamb -1.821677 time 2019-03-02 09:07:52.486316
Model ind 640 epoch 1088 head B head_i_epoch 1 batch 200: avg loss -1.976276 avg loss no lamb -1.976276 time 2019-03-02 09:11:02.525352
last batch sz 160
Pre: time 2019-03-02 09:13:23.779843: 
 	std: 0.050656866
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5103833, 0.61373335, 0.6139167, 0.6138833, 0.5105]
	train_accs: [0.5103833, 0.61373335, 0.6139167, 0.6138833, 0.5105]
	best_train_sub_head: 2
	worst: 0.5103833
	avg: 0.5724833
	best: 0.6139167

Starting e_i: 1089
Model ind 640 epoch 1089 head A head_i_epoch 0 batch 0: avg loss -3.263180 avg loss no lamb -3.263180 time 2019-03-02 09:13:27.658535
Model ind 640 epoch 1089 head A head_i_epoch 0 batch 100: avg loss -3.219157 avg loss no lamb -3.219157 time 2019-03-02 09:16:37.167670
Model ind 640 epoch 1089 head A head_i_epoch 0 batch 200: avg loss -3.216005 avg loss no lamb -3.216005 time 2019-03-02 09:19:51.849602
last batch sz 160
Model ind 640 epoch 1089 head B head_i_epoch 0 batch 0: avg loss -1.967782 avg loss no lamb -1.967782 time 2019-03-02 09:22:10.142845
Model ind 640 epoch 1089 head B head_i_epoch 0 batch 100: avg loss -1.942423 avg loss no lamb -1.942423 time 2019-03-02 09:25:04.756081
Model ind 640 epoch 1089 head B head_i_epoch 0 batch 200: avg loss -1.954547 avg loss no lamb -1.954547 time 2019-03-02 09:27:55.817226
last batch sz 160
Model ind 640 epoch 1089 head B head_i_epoch 1 batch 0: avg loss -1.857973 avg loss no lamb -1.857973 time 2019-03-02 09:30:15.261383
Model ind 640 epoch 1089 head B head_i_epoch 1 batch 100: avg loss -1.843425 avg loss no lamb -1.843425 time 2019-03-02 09:33:23.753458
Model ind 640 epoch 1089 head B head_i_epoch 1 batch 200: avg loss -1.945255 avg loss no lamb -1.945255 time 2019-03-02 09:36:35.541793
last batch sz 160
Pre: time 2019-03-02 09:39:05.598220: 
 	std: 0.050969906
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51056665, 0.6145833, 0.61478335, 0.61478335, 0.5107833]
	train_accs: [0.51056665, 0.6145833, 0.61478335, 0.61478335, 0.5107833]
	best_train_sub_head: 2
	worst: 0.51056665
	avg: 0.5731
	best: 0.61478335

Starting e_i: 1090
Model ind 640 epoch 1090 head A head_i_epoch 0 batch 0: avg loss -3.302795 avg loss no lamb -3.302795 time 2019-03-02 09:39:08.950682
Model ind 640 epoch 1090 head A head_i_epoch 0 batch 100: avg loss -3.197016 avg loss no lamb -3.197016 time 2019-03-02 09:42:14.821511
Model ind 640 epoch 1090 head A head_i_epoch 0 batch 200: avg loss -3.225206 avg loss no lamb -3.225206 time 2019-03-02 09:45:24.835752
last batch sz 160
Model ind 640 epoch 1090 head B head_i_epoch 0 batch 0: avg loss -2.004442 avg loss no lamb -2.004442 time 2019-03-02 09:47:45.039952
Model ind 640 epoch 1090 head B head_i_epoch 0 batch 100: avg loss -1.847139 avg loss no lamb -1.847139 time 2019-03-02 09:50:53.861604
Model ind 640 epoch 1090 head B head_i_epoch 0 batch 200: avg loss -2.043731 avg loss no lamb -2.043731 time 2019-03-02 09:53:31.614263
last batch sz 160
Model ind 640 epoch 1090 head B head_i_epoch 1 batch 0: avg loss -1.913289 avg loss no lamb -1.913289 time 2019-03-02 09:55:53.851577
Model ind 640 epoch 1090 head B head_i_epoch 1 batch 100: avg loss -1.775187 avg loss no lamb -1.775187 time 2019-03-02 09:59:02.932148
Model ind 640 epoch 1090 head B head_i_epoch 1 batch 200: avg loss -1.939843 avg loss no lamb -1.939843 time 2019-03-02 10:02:12.040768
last batch sz 160
Pre: time 2019-03-02 10:04:56.759212: 
 	std: 0.05076039
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51023334, 0.6137, 0.6139333, 0.61408335, 0.51035]
	train_accs: [0.51023334, 0.6137, 0.6139333, 0.61408335, 0.51035]
	best_train_sub_head: 3
	worst: 0.51023334
	avg: 0.57246
	best: 0.61408335

Starting e_i: 1091
Model ind 640 epoch 1091 head A head_i_epoch 0 batch 0: avg loss -3.303289 avg loss no lamb -3.303289 time 2019-03-02 10:05:04.591694
Model ind 640 epoch 1091 head A head_i_epoch 0 batch 100: avg loss -3.246449 avg loss no lamb -3.246449 time 2019-03-02 10:07:55.970947
Model ind 640 epoch 1091 head A head_i_epoch 0 batch 200: avg loss -3.287137 avg loss no lamb -3.287137 time 2019-03-02 10:11:05.641484
last batch sz 160
Model ind 640 epoch 1091 head B head_i_epoch 0 batch 0: avg loss -2.020439 avg loss no lamb -2.020439 time 2019-03-02 10:13:23.443507
Model ind 640 epoch 1091 head B head_i_epoch 0 batch 100: avg loss -1.900438 avg loss no lamb -1.900438 time 2019-03-02 10:16:34.807699
Model ind 640 epoch 1091 head B head_i_epoch 0 batch 200: avg loss -2.008687 avg loss no lamb -2.008687 time 2019-03-02 10:19:16.243114
last batch sz 160
Model ind 640 epoch 1091 head B head_i_epoch 1 batch 0: avg loss -1.992701 avg loss no lamb -1.992701 time 2019-03-02 10:21:35.231609
Model ind 640 epoch 1091 head B head_i_epoch 1 batch 100: avg loss -1.909600 avg loss no lamb -1.909600 time 2019-03-02 10:24:48.136525
Model ind 640 epoch 1091 head B head_i_epoch 1 batch 200: avg loss -2.043522 avg loss no lamb -2.043522 time 2019-03-02 10:27:58.845524
last batch sz 160
Pre: time 2019-03-02 10:30:49.639699: 
 	std: 0.050455447
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5121167, 0.61516666, 0.61518335, 0.61525, 0.5123]
	train_accs: [0.5121167, 0.61516666, 0.61518335, 0.61525, 0.5123]
	best_train_sub_head: 3
	worst: 0.5121167
	avg: 0.57400334
	best: 0.61525

Starting e_i: 1092
Model ind 640 epoch 1092 head A head_i_epoch 0 batch 0: avg loss -3.245007 avg loss no lamb -3.245007 time 2019-03-02 10:30:54.305099
Model ind 640 epoch 1092 head A head_i_epoch 0 batch 100: avg loss -3.202907 avg loss no lamb -3.202907 time 2019-03-02 10:33:31.599565
Model ind 640 epoch 1092 head A head_i_epoch 0 batch 200: avg loss -3.305647 avg loss no lamb -3.305647 time 2019-03-02 10:36:45.603748
last batch sz 160
Model ind 640 epoch 1092 head B head_i_epoch 0 batch 0: avg loss -1.965147 avg loss no lamb -1.965147 time 2019-03-02 10:39:03.174118
Model ind 640 epoch 1092 head B head_i_epoch 0 batch 100: avg loss -1.763282 avg loss no lamb -1.763282 time 2019-03-02 10:42:12.391455
Model ind 640 epoch 1092 head B head_i_epoch 0 batch 200: avg loss -2.019559 avg loss no lamb -2.019559 time 2019-03-02 10:45:04.781814
last batch sz 160
Model ind 640 epoch 1092 head B head_i_epoch 1 batch 0: avg loss -1.927981 avg loss no lamb -1.927981 time 2019-03-02 10:47:09.166175
Model ind 640 epoch 1092 head B head_i_epoch 1 batch 100: avg loss -1.902808 avg loss no lamb -1.902808 time 2019-03-02 10:50:20.926935
Model ind 640 epoch 1092 head B head_i_epoch 1 batch 200: avg loss -1.987941 avg loss no lamb -1.987941 time 2019-03-02 10:53:30.851971
last batch sz 160
Pre: time 2019-03-02 10:56:23.573991: 
 	std: 0.050597005
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118333, 0.61505, 0.6149833, 0.61523336, 0.51178336]
	train_accs: [0.5118333, 0.61505, 0.6149833, 0.61523336, 0.51178336]
	best_train_sub_head: 3
	worst: 0.51178336
	avg: 0.57377666
	best: 0.61523336

Starting e_i: 1093
Model ind 640 epoch 1093 head A head_i_epoch 0 batch 0: avg loss -3.212051 avg loss no lamb -3.212051 time 2019-03-02 10:56:26.549328
Model ind 640 epoch 1093 head A head_i_epoch 0 batch 100: avg loss -3.187848 avg loss no lamb -3.187848 time 2019-03-02 10:59:04.030009
Model ind 640 epoch 1093 head A head_i_epoch 0 batch 200: avg loss -3.245237 avg loss no lamb -3.245237 time 2019-03-02 11:02:12.832215
last batch sz 160
Model ind 640 epoch 1093 head B head_i_epoch 0 batch 0: avg loss -1.972650 avg loss no lamb -1.972650 time 2019-03-02 11:04:33.250016
Model ind 640 epoch 1093 head B head_i_epoch 0 batch 100: avg loss -1.905449 avg loss no lamb -1.905449 time 2019-03-02 11:07:46.774467
Model ind 640 epoch 1093 head B head_i_epoch 0 batch 200: avg loss -2.015028 avg loss no lamb -2.015028 time 2019-03-02 11:10:55.204325
last batch sz 160
Model ind 640 epoch 1093 head B head_i_epoch 1 batch 0: avg loss -1.949850 avg loss no lamb -1.949850 time 2019-03-02 11:12:42.601001
Model ind 640 epoch 1093 head B head_i_epoch 1 batch 100: avg loss -1.871705 avg loss no lamb -1.871705 time 2019-03-02 11:15:49.938376
Model ind 640 epoch 1093 head B head_i_epoch 1 batch 200: avg loss -2.002733 avg loss no lamb -2.002733 time 2019-03-02 11:19:03.759011
last batch sz 160
Pre: time 2019-03-02 11:21:53.660069: 
 	std: 0.05073192
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51101667, 0.61481667, 0.61475, 0.6149, 0.5115167]
	train_accs: [0.51101667, 0.61481667, 0.61475, 0.6149, 0.5115167]
	best_train_sub_head: 3
	worst: 0.51101667
	avg: 0.5734
	best: 0.6149

Starting e_i: 1094
Model ind 640 epoch 1094 head A head_i_epoch 0 batch 0: avg loss -3.221819 avg loss no lamb -3.221819 time 2019-03-02 11:21:57.268637
Model ind 640 epoch 1094 head A head_i_epoch 0 batch 100: avg loss -3.189095 avg loss no lamb -3.189095 time 2019-03-02 11:24:58.329808
Model ind 640 epoch 1094 head A head_i_epoch 0 batch 200: avg loss -3.284272 avg loss no lamb -3.284272 time 2019-03-02 11:27:52.068262
last batch sz 160
Model ind 640 epoch 1094 head B head_i_epoch 0 batch 0: avg loss -1.952245 avg loss no lamb -1.952245 time 2019-03-02 11:30:10.492111
Model ind 640 epoch 1094 head B head_i_epoch 0 batch 100: avg loss -1.918718 avg loss no lamb -1.918718 time 2019-03-02 11:33:19.121141
Model ind 640 epoch 1094 head B head_i_epoch 0 batch 200: avg loss -1.973353 avg loss no lamb -1.973353 time 2019-03-02 11:36:25.721758
last batch sz 160
Model ind 640 epoch 1094 head B head_i_epoch 1 batch 0: avg loss -1.950924 avg loss no lamb -1.950924 time 2019-03-02 11:38:30.536396
Model ind 640 epoch 1094 head B head_i_epoch 1 batch 100: avg loss -1.839323 avg loss no lamb -1.839323 time 2019-03-02 11:41:23.618643
Model ind 640 epoch 1094 head B head_i_epoch 1 batch 200: avg loss -1.896554 avg loss no lamb -1.896554 time 2019-03-02 11:44:33.620473
last batch sz 160
Pre: time 2019-03-02 11:47:23.460195: 
 	std: 0.050641842
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5122167, 0.61565, 0.6156333, 0.6156833, 0.51235]
	train_accs: [0.5122167, 0.61565, 0.6156333, 0.6156833, 0.51235]
	best_train_sub_head: 3
	worst: 0.5122167
	avg: 0.57430667
	best: 0.6156833

Starting e_i: 1095
Model ind 640 epoch 1095 head A head_i_epoch 0 batch 0: avg loss -3.315558 avg loss no lamb -3.315558 time 2019-03-02 11:47:27.030590
Model ind 640 epoch 1095 head A head_i_epoch 0 batch 100: avg loss -3.299021 avg loss no lamb -3.299021 time 2019-03-02 11:50:35.480080
Model ind 640 epoch 1095 head A head_i_epoch 0 batch 200: avg loss -3.301269 avg loss no lamb -3.301269 time 2019-03-02 11:53:15.531980
last batch sz 160
Model ind 640 epoch 1095 head B head_i_epoch 0 batch 0: avg loss -1.907037 avg loss no lamb -1.907037 time 2019-03-02 11:55:37.052839
Model ind 640 epoch 1095 head B head_i_epoch 0 batch 100: avg loss -1.863285 avg loss no lamb -1.863285 time 2019-03-02 11:58:44.094623
Model ind 640 epoch 1095 head B head_i_epoch 0 batch 200: avg loss -1.929970 avg loss no lamb -1.929970 time 2019-03-02 12:01:52.530781
last batch sz 160
Model ind 640 epoch 1095 head B head_i_epoch 1 batch 0: avg loss -1.916104 avg loss no lamb -1.916104 time 2019-03-02 12:04:10.351830
Model ind 640 epoch 1095 head B head_i_epoch 1 batch 100: avg loss -1.892774 avg loss no lamb -1.892774 time 2019-03-02 12:06:47.187439
Model ind 640 epoch 1095 head B head_i_epoch 1 batch 200: avg loss -1.928347 avg loss no lamb -1.928347 time 2019-03-02 12:09:59.076982
last batch sz 160
Pre: time 2019-03-02 12:12:51.523505: 
 	std: 0.050791524
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51103336, 0.61475, 0.61471665, 0.61471665, 0.5110667]
	train_accs: [0.51103336, 0.61475, 0.61471665, 0.61471665, 0.5110667]
	best_train_sub_head: 1
	worst: 0.51103336
	avg: 0.5732567
	best: 0.61475

Starting e_i: 1096
Model ind 640 epoch 1096 head A head_i_epoch 0 batch 0: avg loss -3.356502 avg loss no lamb -3.356502 time 2019-03-02 12:12:56.252414
Model ind 640 epoch 1096 head A head_i_epoch 0 batch 100: avg loss -3.198337 avg loss no lamb -3.198337 time 2019-03-02 12:16:06.688964
Model ind 640 epoch 1096 head A head_i_epoch 0 batch 200: avg loss -3.323016 avg loss no lamb -3.323016 time 2019-03-02 12:18:48.883335
last batch sz 160
Model ind 640 epoch 1096 head B head_i_epoch 0 batch 0: avg loss -1.939221 avg loss no lamb -1.939221 time 2019-03-02 12:21:05.759820
Model ind 640 epoch 1096 head B head_i_epoch 0 batch 100: avg loss -1.813172 avg loss no lamb -1.813172 time 2019-03-02 12:24:12.092579
Model ind 640 epoch 1096 head B head_i_epoch 0 batch 200: avg loss -2.001480 avg loss no lamb -2.001480 time 2019-03-02 12:27:21.241102
last batch sz 160
Model ind 640 epoch 1096 head B head_i_epoch 1 batch 0: avg loss -1.925159 avg loss no lamb -1.925159 time 2019-03-02 12:29:44.837495
Model ind 640 epoch 1096 head B head_i_epoch 1 batch 100: avg loss -1.873512 avg loss no lamb -1.873512 time 2019-03-02 12:32:18.113441
Model ind 640 epoch 1096 head B head_i_epoch 1 batch 200: avg loss -1.989876 avg loss no lamb -1.989876 time 2019-03-02 12:35:29.103552
last batch sz 160
Pre: time 2019-03-02 12:38:20.726112: 
 	std: 0.050122373
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51171666, 0.61433333, 0.61436665, 0.6142833, 0.51231664]
	train_accs: [0.51171666, 0.61433333, 0.61436665, 0.6142833, 0.51231664]
	best_train_sub_head: 2
	worst: 0.51171666
	avg: 0.57340336
	best: 0.61436665

Starting e_i: 1097
Model ind 640 epoch 1097 head A head_i_epoch 0 batch 0: avg loss -3.400412 avg loss no lamb -3.400412 time 2019-03-02 12:38:25.219054
Model ind 640 epoch 1097 head A head_i_epoch 0 batch 100: avg loss -3.212164 avg loss no lamb -3.212164 time 2019-03-02 12:41:33.760998
Model ind 640 epoch 1097 head A head_i_epoch 0 batch 200: avg loss -3.281353 avg loss no lamb -3.281353 time 2019-03-02 12:44:31.619346
last batch sz 160
Model ind 640 epoch 1097 head B head_i_epoch 0 batch 0: avg loss -1.969891 avg loss no lamb -1.969891 time 2019-03-02 12:46:31.160095
Model ind 640 epoch 1097 head B head_i_epoch 0 batch 100: avg loss -1.989611 avg loss no lamb -1.989611 time 2019-03-02 12:49:42.590461
Model ind 640 epoch 1097 head B head_i_epoch 0 batch 200: avg loss -1.891016 avg loss no lamb -1.891016 time 2019-03-02 12:52:51.946924
last batch sz 160
Model ind 640 epoch 1097 head B head_i_epoch 1 batch 0: avg loss -2.021203 avg loss no lamb -2.021203 time 2019-03-02 12:55:12.118234
Model ind 640 epoch 1097 head B head_i_epoch 1 batch 100: avg loss -1.966942 avg loss no lamb -1.966942 time 2019-03-02 12:58:08.200402
Model ind 640 epoch 1097 head B head_i_epoch 1 batch 200: avg loss -1.946528 avg loss no lamb -1.946528 time 2019-03-02 13:01:02.750958
last batch sz 160
Pre: time 2019-03-02 13:03:52.184477: 
 	std: 0.05107202
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51021665, 0.6147, 0.6145667, 0.61468333, 0.51058334]
	train_accs: [0.51021665, 0.6147, 0.6145667, 0.61468333, 0.51058334]
	best_train_sub_head: 1
	worst: 0.51021665
	avg: 0.57295
	best: 0.6147

Starting e_i: 1098
Model ind 640 epoch 1098 head A head_i_epoch 0 batch 0: avg loss -3.276994 avg loss no lamb -3.276994 time 2019-03-02 13:03:56.413337
Model ind 640 epoch 1098 head A head_i_epoch 0 batch 100: avg loss -3.182461 avg loss no lamb -3.182461 time 2019-03-02 13:07:05.551262
Model ind 640 epoch 1098 head A head_i_epoch 0 batch 200: avg loss -3.262862 avg loss no lamb -3.262862 time 2019-03-02 13:10:17.291014
last batch sz 160
Model ind 640 epoch 1098 head B head_i_epoch 0 batch 0: avg loss -1.980122 avg loss no lamb -1.980122 time 2019-03-02 13:12:04.899187
Model ind 640 epoch 1098 head B head_i_epoch 0 batch 100: avg loss -1.909639 avg loss no lamb -1.909639 time 2019-03-02 13:15:10.812049
Model ind 640 epoch 1098 head B head_i_epoch 0 batch 200: avg loss -1.993443 avg loss no lamb -1.993443 time 2019-03-02 13:18:22.460209
last batch sz 160
Model ind 640 epoch 1098 head B head_i_epoch 1 batch 0: avg loss -1.862388 avg loss no lamb -1.862388 time 2019-03-02 13:20:41.628144
Model ind 640 epoch 1098 head B head_i_epoch 1 batch 100: avg loss -1.961378 avg loss no lamb -1.961378 time 2019-03-02 13:23:54.616358
Model ind 640 epoch 1098 head B head_i_epoch 1 batch 200: avg loss -2.083204 avg loss no lamb -2.083204 time 2019-03-02 13:26:30.879499
last batch sz 160
Pre: time 2019-03-02 13:29:18.206941: 
 	std: 0.050760504
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51088333, 0.6148, 0.61473334, 0.61473334, 0.5114]
	train_accs: [0.51088333, 0.6148, 0.61473334, 0.61473334, 0.5114]
	best_train_sub_head: 1
	worst: 0.51088333
	avg: 0.57331
	best: 0.6148

Starting e_i: 1099
Model ind 640 epoch 1099 head A head_i_epoch 0 batch 0: avg loss -3.206992 avg loss no lamb -3.206992 time 2019-03-02 13:29:21.211782
Model ind 640 epoch 1099 head A head_i_epoch 0 batch 100: avg loss -3.090595 avg loss no lamb -3.090595 time 2019-03-02 13:32:30.578625
Model ind 640 epoch 1099 head A head_i_epoch 0 batch 200: avg loss -3.299094 avg loss no lamb -3.299094 time 2019-03-02 13:35:45.985696
last batch sz 160
Model ind 640 epoch 1099 head B head_i_epoch 0 batch 0: avg loss -1.984651 avg loss no lamb -1.984651 time 2019-03-02 13:37:51.259844
Model ind 640 epoch 1099 head B head_i_epoch 0 batch 100: avg loss -1.801347 avg loss no lamb -1.801347 time 2019-03-02 13:40:39.421049
Model ind 640 epoch 1099 head B head_i_epoch 0 batch 200: avg loss -2.006313 avg loss no lamb -2.006313 time 2019-03-02 13:43:48.481216
last batch sz 160
Model ind 640 epoch 1099 head B head_i_epoch 1 batch 0: avg loss -1.894812 avg loss no lamb -1.894812 time 2019-03-02 13:46:07.640618
Model ind 640 epoch 1099 head B head_i_epoch 1 batch 100: avg loss -1.879023 avg loss no lamb -1.879023 time 2019-03-02 13:49:16.522785
Model ind 640 epoch 1099 head B head_i_epoch 1 batch 200: avg loss -1.941685 avg loss no lamb -1.941685 time 2019-03-02 13:51:59.901640
last batch sz 160
Pre: time 2019-03-02 13:54:47.208288: 
 	std: 0.050330244
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114667, 0.61433333, 0.61425, 0.6143, 0.51165]
	train_accs: [0.5114667, 0.61433333, 0.61425, 0.6143, 0.51165]
	best_train_sub_head: 1
	worst: 0.5114667
	avg: 0.57320005
	best: 0.61433333

Starting e_i: 1100
Model ind 640 epoch 1100 head A head_i_epoch 0 batch 0: avg loss -3.192029 avg loss no lamb -3.192029 time 2019-03-02 13:54:50.576897
Model ind 640 epoch 1100 head A head_i_epoch 0 batch 100: avg loss -3.218056 avg loss no lamb -3.218056 time 2019-03-02 13:58:02.011518
Model ind 640 epoch 1100 head A head_i_epoch 0 batch 200: avg loss -3.308723 avg loss no lamb -3.308723 time 2019-03-02 14:01:12.120631
last batch sz 160
Model ind 640 epoch 1100 head B head_i_epoch 0 batch 0: avg loss -1.915479 avg loss no lamb -1.915479 time 2019-03-02 14:03:31.934688
Model ind 640 epoch 1100 head B head_i_epoch 0 batch 100: avg loss -1.923324 avg loss no lamb -1.923324 time 2019-03-02 14:06:07.163836
Model ind 640 epoch 1100 head B head_i_epoch 0 batch 200: avg loss -1.904665 avg loss no lamb -1.904665 time 2019-03-02 14:09:19.297285
last batch sz 160
Model ind 640 epoch 1100 head B head_i_epoch 1 batch 0: avg loss -1.889404 avg loss no lamb -1.889404 time 2019-03-02 14:11:36.993509
Model ind 640 epoch 1100 head B head_i_epoch 1 batch 100: avg loss -1.916343 avg loss no lamb -1.916343 time 2019-03-02 14:14:43.762367
Model ind 640 epoch 1100 head B head_i_epoch 1 batch 200: avg loss -1.897370 avg loss no lamb -1.897370 time 2019-03-02 14:17:46.855126
last batch sz 160
Pre: time 2019-03-02 14:20:11.856338: 
 	std: 0.050900456
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5100333, 0.61378336, 0.6139, 0.6139167, 0.5099]
	train_accs: [0.5100333, 0.61378336, 0.6139, 0.6139167, 0.5099]
	best_train_sub_head: 3
	worst: 0.5099
	avg: 0.57230663
	best: 0.6139167

Starting e_i: 1101
Model ind 640 epoch 1101 head A head_i_epoch 0 batch 0: avg loss -3.199800 avg loss no lamb -3.199800 time 2019-03-02 14:20:19.668563
Model ind 640 epoch 1101 head A head_i_epoch 0 batch 100: avg loss -3.195601 avg loss no lamb -3.195601 time 2019-03-02 14:23:26.605902
Model ind 640 epoch 1101 head A head_i_epoch 0 batch 200: avg loss -3.224220 avg loss no lamb -3.224220 time 2019-03-02 14:26:34.584592
last batch sz 160
Model ind 640 epoch 1101 head B head_i_epoch 0 batch 0: avg loss -1.864660 avg loss no lamb -1.864660 time 2019-03-02 14:28:53.116729
Model ind 640 epoch 1101 head B head_i_epoch 0 batch 100: avg loss -1.908648 avg loss no lamb -1.908648 time 2019-03-02 14:31:48.028355
Model ind 640 epoch 1101 head B head_i_epoch 0 batch 200: avg loss -1.957280 avg loss no lamb -1.957280 time 2019-03-02 14:34:43.064908
last batch sz 160
Model ind 640 epoch 1101 head B head_i_epoch 1 batch 0: avg loss -1.837534 avg loss no lamb -1.837534 time 2019-03-02 14:36:59.153589
Model ind 640 epoch 1101 head B head_i_epoch 1 batch 100: avg loss -1.911373 avg loss no lamb -1.911373 time 2019-03-02 14:40:11.888626
Model ind 640 epoch 1101 head B head_i_epoch 1 batch 200: avg loss -1.984179 avg loss no lamb -1.984179 time 2019-03-02 14:43:21.265496
last batch sz 160
Pre: time 2019-03-02 14:45:41.458251: 
 	std: 0.05078244
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5110833, 0.6144, 0.61443335, 0.61441666, 0.5104333]
	train_accs: [0.5110833, 0.6144, 0.61443335, 0.61441666, 0.5104333]
	best_train_sub_head: 2
	worst: 0.5104333
	avg: 0.57295334
	best: 0.61443335

Starting e_i: 1102
Model ind 640 epoch 1102 head A head_i_epoch 0 batch 0: avg loss -3.285368 avg loss no lamb -3.285368 time 2019-03-02 14:45:45.974920
Model ind 640 epoch 1102 head A head_i_epoch 0 batch 100: avg loss -3.188334 avg loss no lamb -3.188334 time 2019-03-02 14:48:55.421380
Model ind 640 epoch 1102 head A head_i_epoch 0 batch 200: avg loss -3.258169 avg loss no lamb -3.258169 time 2019-03-02 14:52:04.612860
last batch sz 160
Model ind 640 epoch 1102 head B head_i_epoch 0 batch 0: avg loss -1.933583 avg loss no lamb -1.933583 time 2019-03-02 14:54:22.601570
Model ind 640 epoch 1102 head B head_i_epoch 0 batch 100: avg loss -1.881212 avg loss no lamb -1.881212 time 2019-03-02 14:57:35.223443
Model ind 640 epoch 1102 head B head_i_epoch 0 batch 200: avg loss -2.018129 avg loss no lamb -2.018129 time 2019-03-02 15:00:10.862659
last batch sz 160
Model ind 640 epoch 1102 head B head_i_epoch 1 batch 0: avg loss -1.988875 avg loss no lamb -1.988875 time 2019-03-02 15:02:28.053436
Model ind 640 epoch 1102 head B head_i_epoch 1 batch 100: avg loss -1.848410 avg loss no lamb -1.848410 time 2019-03-02 15:05:43.337537
Model ind 640 epoch 1102 head B head_i_epoch 1 batch 200: avg loss -2.003301 avg loss no lamb -2.003301 time 2019-03-02 15:08:53.032721
last batch sz 160
Pre: time 2019-03-02 15:11:32.329923: 
 	std: 0.050602403
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51115, 0.61436665, 0.61431664, 0.61446667, 0.51103336]
	train_accs: [0.51115, 0.61436665, 0.61431664, 0.61446667, 0.51103336]
	best_train_sub_head: 3
	worst: 0.51103336
	avg: 0.57306665
	best: 0.61446667

Starting e_i: 1103
Model ind 640 epoch 1103 head A head_i_epoch 0 batch 0: avg loss -3.334059 avg loss no lamb -3.334059 time 2019-03-02 15:11:35.771717
Model ind 640 epoch 1103 head A head_i_epoch 0 batch 100: avg loss -3.186285 avg loss no lamb -3.186285 time 2019-03-02 15:14:29.245918
Model ind 640 epoch 1103 head A head_i_epoch 0 batch 200: avg loss -3.341819 avg loss no lamb -3.341819 time 2019-03-02 15:17:41.089340
last batch sz 160
Model ind 640 epoch 1103 head B head_i_epoch 0 batch 0: avg loss -1.923931 avg loss no lamb -1.923931 time 2019-03-02 15:19:59.052551
Model ind 640 epoch 1103 head B head_i_epoch 0 batch 100: avg loss -1.854919 avg loss no lamb -1.854919 time 2019-03-02 15:23:10.466489
Model ind 640 epoch 1103 head B head_i_epoch 0 batch 200: avg loss -1.922712 avg loss no lamb -1.922712 time 2019-03-02 15:25:46.255553
last batch sz 160
Model ind 640 epoch 1103 head B head_i_epoch 1 batch 0: avg loss -1.875523 avg loss no lamb -1.875523 time 2019-03-02 15:28:07.102070
Model ind 640 epoch 1103 head B head_i_epoch 1 batch 100: avg loss -1.821370 avg loss no lamb -1.821370 time 2019-03-02 15:31:19.715580
Model ind 640 epoch 1103 head B head_i_epoch 1 batch 200: avg loss -1.967634 avg loss no lamb -1.967634 time 2019-03-02 15:34:28.965091
last batch sz 160
Pre: time 2019-03-02 15:37:22.674668: 
 	std: 0.050330292
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111833, 0.6138667, 0.61375, 0.6137667, 0.51093334]
	train_accs: [0.5111833, 0.6138667, 0.61375, 0.6137667, 0.51093334]
	best_train_sub_head: 1
	worst: 0.51093334
	avg: 0.5727
	best: 0.6138667

Starting e_i: 1104
Model ind 640 epoch 1104 head A head_i_epoch 0 batch 0: avg loss -3.271867 avg loss no lamb -3.271867 time 2019-03-02 15:37:25.543739
Model ind 640 epoch 1104 head A head_i_epoch 0 batch 100: avg loss -3.226078 avg loss no lamb -3.226078 time 2019-03-02 15:40:02.980506
Model ind 640 epoch 1104 head A head_i_epoch 0 batch 200: avg loss -3.208531 avg loss no lamb -3.208531 time 2019-03-02 15:43:15.556226
last batch sz 160
Model ind 640 epoch 1104 head B head_i_epoch 0 batch 0: avg loss -1.964377 avg loss no lamb -1.964377 time 2019-03-02 15:45:34.067610
Model ind 640 epoch 1104 head B head_i_epoch 0 batch 100: avg loss -1.828666 avg loss no lamb -1.828666 time 2019-03-02 15:48:44.546002
Model ind 640 epoch 1104 head B head_i_epoch 0 batch 200: avg loss -1.967096 avg loss no lamb -1.967096 time 2019-03-02 15:51:34.505477
last batch sz 160
Model ind 640 epoch 1104 head B head_i_epoch 1 batch 0: avg loss -1.951414 avg loss no lamb -1.951414 time 2019-03-02 15:53:37.072749
Model ind 640 epoch 1104 head B head_i_epoch 1 batch 100: avg loss -1.892531 avg loss no lamb -1.892531 time 2019-03-02 15:56:47.199350
Model ind 640 epoch 1104 head B head_i_epoch 1 batch 200: avg loss -1.958286 avg loss no lamb -1.958286 time 2019-03-02 15:59:56.995870
last batch sz 160
Pre: time 2019-03-02 16:02:51.322764: 
 	std: 0.05037794
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51103336, 0.61373335, 0.61368334, 0.61373335, 0.5107333]
	train_accs: [0.51103336, 0.61373335, 0.61368334, 0.61373335, 0.5107333]
	best_train_sub_head: 1
	worst: 0.5107333
	avg: 0.5725833
	best: 0.61373335

Starting e_i: 1105
Model ind 640 epoch 1105 head A head_i_epoch 0 batch 0: avg loss -3.210754 avg loss no lamb -3.210754 time 2019-03-02 16:02:54.610821
Model ind 640 epoch 1105 head A head_i_epoch 0 batch 100: avg loss -3.194542 avg loss no lamb -3.194542 time 2019-03-02 16:05:33.535069
Model ind 640 epoch 1105 head A head_i_epoch 0 batch 200: avg loss -3.206955 avg loss no lamb -3.206955 time 2019-03-02 16:08:40.919367
last batch sz 160
Model ind 640 epoch 1105 head B head_i_epoch 0 batch 0: avg loss -1.834083 avg loss no lamb -1.834083 time 2019-03-02 16:10:58.999588
Model ind 640 epoch 1105 head B head_i_epoch 0 batch 100: avg loss -1.888570 avg loss no lamb -1.888570 time 2019-03-02 16:14:09.889462
Model ind 640 epoch 1105 head B head_i_epoch 0 batch 200: avg loss -1.991526 avg loss no lamb -1.991526 time 2019-03-02 16:17:21.133569
last batch sz 160
Model ind 640 epoch 1105 head B head_i_epoch 1 batch 0: avg loss -1.911612 avg loss no lamb -1.911612 time 2019-03-02 16:19:03.584540
Model ind 640 epoch 1105 head B head_i_epoch 1 batch 100: avg loss -1.865533 avg loss no lamb -1.865533 time 2019-03-02 16:22:13.940340
Model ind 640 epoch 1105 head B head_i_epoch 1 batch 200: avg loss -2.038754 avg loss no lamb -2.038754 time 2019-03-02 16:25:26.176969
last batch sz 160
Pre: time 2019-03-02 16:28:17.696068: 
 	std: 0.05086918
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51138335, 0.61523336, 0.615, 0.61515, 0.5112]
	train_accs: [0.51138335, 0.61523336, 0.615, 0.61515, 0.5112]
	best_train_sub_head: 1
	worst: 0.5112
	avg: 0.5735933
	best: 0.61523336

Starting e_i: 1106
Model ind 640 epoch 1106 head A head_i_epoch 0 batch 0: avg loss -3.225291 avg loss no lamb -3.225291 time 2019-03-02 16:28:21.669449
Model ind 640 epoch 1106 head A head_i_epoch 0 batch 100: avg loss -3.202578 avg loss no lamb -3.202578 time 2019-03-02 16:31:18.912218
Model ind 640 epoch 1106 head A head_i_epoch 0 batch 200: avg loss -3.319169 avg loss no lamb -3.319169 time 2019-03-02 16:34:11.437806
last batch sz 160
Model ind 640 epoch 1106 head B head_i_epoch 0 batch 0: avg loss -1.886616 avg loss no lamb -1.886616 time 2019-03-02 16:36:31.403888
Model ind 640 epoch 1106 head B head_i_epoch 0 batch 100: avg loss -1.875026 avg loss no lamb -1.875026 time 2019-03-02 16:39:44.325852
Model ind 640 epoch 1106 head B head_i_epoch 0 batch 200: avg loss -1.957455 avg loss no lamb -1.957455 time 2019-03-02 16:42:55.210355
last batch sz 160
Model ind 640 epoch 1106 head B head_i_epoch 1 batch 0: avg loss -1.930691 avg loss no lamb -1.930691 time 2019-03-02 16:44:57.734728
Model ind 640 epoch 1106 head B head_i_epoch 1 batch 100: avg loss -1.903712 avg loss no lamb -1.903712 time 2019-03-02 16:47:52.281395
Model ind 640 epoch 1106 head B head_i_epoch 1 batch 200: avg loss -1.939755 avg loss no lamb -1.939755 time 2019-03-02 16:51:02.843196
last batch sz 160
Pre: time 2019-03-02 16:53:57.923343: 
 	std: 0.050964434
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51023334, 0.6142, 0.61406666, 0.6143, 0.5100833]
	train_accs: [0.51023334, 0.6142, 0.61406666, 0.6143, 0.5100833]
	best_train_sub_head: 3
	worst: 0.5100833
	avg: 0.57257664
	best: 0.6143

Starting e_i: 1107
Model ind 640 epoch 1107 head A head_i_epoch 0 batch 0: avg loss -3.284652 avg loss no lamb -3.284652 time 2019-03-02 16:54:01.792623
Model ind 640 epoch 1107 head A head_i_epoch 0 batch 100: avg loss -3.176924 avg loss no lamb -3.176924 time 2019-03-02 16:57:12.815401
Model ind 640 epoch 1107 head A head_i_epoch 0 batch 200: avg loss -3.245901 avg loss no lamb -3.245901 time 2019-03-02 16:59:51.765312
last batch sz 160
Model ind 640 epoch 1107 head B head_i_epoch 0 batch 0: avg loss -1.935270 avg loss no lamb -1.935270 time 2019-03-02 17:02:10.445080
Model ind 640 epoch 1107 head B head_i_epoch 0 batch 100: avg loss -1.901560 avg loss no lamb -1.901560 time 2019-03-02 17:05:23.740104
Model ind 640 epoch 1107 head B head_i_epoch 0 batch 200: avg loss -1.980589 avg loss no lamb -1.980589 time 2019-03-02 17:08:31.769322
last batch sz 160
Model ind 640 epoch 1107 head B head_i_epoch 1 batch 0: avg loss -1.995043 avg loss no lamb -1.995043 time 2019-03-02 17:10:53.851416
Model ind 640 epoch 1107 head B head_i_epoch 1 batch 100: avg loss -1.842575 avg loss no lamb -1.842575 time 2019-03-02 17:13:32.810417
Model ind 640 epoch 1107 head B head_i_epoch 1 batch 200: avg loss -1.923793 avg loss no lamb -1.923793 time 2019-03-02 17:16:42.684435
last batch sz 160
Pre: time 2019-03-02 17:19:32.555044: 
 	std: 0.051289625
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5096167, 0.61433333, 0.6141833, 0.6142667, 0.50951666]
	train_accs: [0.5096167, 0.61433333, 0.6141833, 0.6142667, 0.50951666]
	best_train_sub_head: 1
	worst: 0.50951666
	avg: 0.57238334
	best: 0.61433333

Starting e_i: 1108
Model ind 640 epoch 1108 head A head_i_epoch 0 batch 0: avg loss -3.270997 avg loss no lamb -3.270997 time 2019-03-02 17:19:36.605954
Model ind 640 epoch 1108 head A head_i_epoch 0 batch 100: avg loss -3.166523 avg loss no lamb -3.166523 time 2019-03-02 17:22:48.581571
Model ind 640 epoch 1108 head A head_i_epoch 0 batch 200: avg loss -3.281016 avg loss no lamb -3.281016 time 2019-03-02 17:25:31.005098
last batch sz 160
Model ind 640 epoch 1108 head B head_i_epoch 0 batch 0: avg loss -1.924878 avg loss no lamb -1.924878 time 2019-03-02 17:27:49.075937
Model ind 640 epoch 1108 head B head_i_epoch 0 batch 100: avg loss -1.813369 avg loss no lamb -1.813369 time 2019-03-02 17:30:57.418084
Model ind 640 epoch 1108 head B head_i_epoch 0 batch 200: avg loss -1.983366 avg loss no lamb -1.983366 time 2019-03-02 17:34:05.952140
last batch sz 160
Model ind 640 epoch 1108 head B head_i_epoch 1 batch 0: avg loss -1.887069 avg loss no lamb -1.887069 time 2019-03-02 17:36:25.395679
Model ind 640 epoch 1108 head B head_i_epoch 1 batch 100: avg loss -1.903634 avg loss no lamb -1.903634 time 2019-03-02 17:39:05.222235
Model ind 640 epoch 1108 head B head_i_epoch 1 batch 200: avg loss -1.919404 avg loss no lamb -1.919404 time 2019-03-02 17:42:17.207562
last batch sz 160
Pre: time 2019-03-02 17:45:07.919420: 
 	std: 0.05077797
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5116, 0.61541665, 0.6152167, 0.6153167, 0.51173335]
	train_accs: [0.5116, 0.61541665, 0.6152167, 0.6153167, 0.51173335]
	best_train_sub_head: 1
	worst: 0.5116
	avg: 0.57385665
	best: 0.61541665

Starting e_i: 1109
Model ind 640 epoch 1109 head A head_i_epoch 0 batch 0: avg loss -3.302612 avg loss no lamb -3.302612 time 2019-03-02 17:45:11.772764
Model ind 640 epoch 1109 head A head_i_epoch 0 batch 100: avg loss -3.158441 avg loss no lamb -3.158441 time 2019-03-02 17:48:23.952844
Model ind 640 epoch 1109 head A head_i_epoch 0 batch 200: avg loss -3.241225 avg loss no lamb -3.241225 time 2019-03-02 17:51:18.497545
last batch sz 160
Model ind 640 epoch 1109 head B head_i_epoch 0 batch 0: avg loss -1.880420 avg loss no lamb -1.880420 time 2019-03-02 17:53:22.062756
Model ind 640 epoch 1109 head B head_i_epoch 0 batch 100: avg loss -1.922377 avg loss no lamb -1.922377 time 2019-03-02 17:56:34.254029
Model ind 640 epoch 1109 head B head_i_epoch 0 batch 200: avg loss -1.962126 avg loss no lamb -1.962126 time 2019-03-02 17:59:43.466482
last batch sz 160
Model ind 640 epoch 1109 head B head_i_epoch 1 batch 0: avg loss -1.938828 avg loss no lamb -1.938828 time 2019-03-02 18:02:01.346501
Model ind 640 epoch 1109 head B head_i_epoch 1 batch 100: avg loss -1.919133 avg loss no lamb -1.919133 time 2019-03-02 18:04:57.064101
Model ind 640 epoch 1109 head B head_i_epoch 1 batch 200: avg loss -1.951263 avg loss no lamb -1.951263 time 2019-03-02 18:07:48.966286
last batch sz 160
Pre: time 2019-03-02 18:10:43.435626: 
 	std: 0.050931703
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50975, 0.61371666, 0.61363333, 0.61371666, 0.5097]
	train_accs: [0.50975, 0.61371666, 0.61363333, 0.61371666, 0.5097]
	best_train_sub_head: 1
	worst: 0.5097
	avg: 0.5721034
	best: 0.61371666

Starting e_i: 1110
Model ind 640 epoch 1110 head A head_i_epoch 0 batch 0: avg loss -3.231070 avg loss no lamb -3.231070 time 2019-03-02 18:10:47.131143
Model ind 640 epoch 1110 head A head_i_epoch 0 batch 100: avg loss -3.200780 avg loss no lamb -3.200780 time 2019-03-02 18:13:57.198882
Model ind 640 epoch 1110 head A head_i_epoch 0 batch 200: avg loss -3.182483 avg loss no lamb -3.182483 time 2019-03-02 18:17:08.980338
last batch sz 160
Model ind 640 epoch 1110 head B head_i_epoch 0 batch 0: avg loss -1.963744 avg loss no lamb -1.963744 time 2019-03-02 18:18:52.123989
Model ind 640 epoch 1110 head B head_i_epoch 0 batch 100: avg loss -1.851877 avg loss no lamb -1.851877 time 2019-03-02 18:22:02.323439
Model ind 640 epoch 1110 head B head_i_epoch 0 batch 200: avg loss -1.962374 avg loss no lamb -1.962374 time 2019-03-02 18:25:12.659797
last batch sz 160
Model ind 640 epoch 1110 head B head_i_epoch 1 batch 0: avg loss -1.922369 avg loss no lamb -1.922369 time 2019-03-02 18:27:31.065374
Model ind 640 epoch 1110 head B head_i_epoch 1 batch 100: avg loss -1.816351 avg loss no lamb -1.816351 time 2019-03-02 18:30:43.451595
Model ind 640 epoch 1110 head B head_i_epoch 1 batch 200: avg loss -1.938731 avg loss no lamb -1.938731 time 2019-03-02 18:33:27.265498
last batch sz 160
Pre: time 2019-03-02 18:36:17.688204: 
 	std: 0.05068136
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51061666, 0.6142167, 0.61415, 0.6141667, 0.5108333]
	train_accs: [0.51061666, 0.6142167, 0.61415, 0.6141667, 0.5108333]
	best_train_sub_head: 1
	worst: 0.51061666
	avg: 0.5727967
	best: 0.6142167

Starting e_i: 1111
Model ind 640 epoch 1111 head A head_i_epoch 0 batch 0: avg loss -3.319373 avg loss no lamb -3.319373 time 2019-03-02 18:36:25.995125
Model ind 640 epoch 1111 head A head_i_epoch 0 batch 100: avg loss -3.226311 avg loss no lamb -3.226311 time 2019-03-02 18:39:33.374463
Model ind 640 epoch 1111 head A head_i_epoch 0 batch 200: avg loss -3.273082 avg loss no lamb -3.273082 time 2019-03-02 18:42:42.797556
last batch sz 160
Model ind 640 epoch 1111 head B head_i_epoch 0 batch 0: avg loss -1.940792 avg loss no lamb -1.940792 time 2019-03-02 18:44:45.334208
Model ind 640 epoch 1111 head B head_i_epoch 0 batch 100: avg loss -1.903238 avg loss no lamb -1.903238 time 2019-03-02 18:47:35.153942
Model ind 640 epoch 1111 head B head_i_epoch 0 batch 200: avg loss -1.942613 avg loss no lamb -1.942613 time 2019-03-02 18:50:46.212739
last batch sz 160
Model ind 640 epoch 1111 head B head_i_epoch 1 batch 0: avg loss -1.938600 avg loss no lamb -1.938600 time 2019-03-02 18:53:05.883905
Model ind 640 epoch 1111 head B head_i_epoch 1 batch 100: avg loss -1.832990 avg loss no lamb -1.832990 time 2019-03-02 18:56:16.348438
Model ind 640 epoch 1111 head B head_i_epoch 1 batch 200: avg loss -1.976727 avg loss no lamb -1.976727 time 2019-03-02 18:58:51.402025
last batch sz 160
Pre: time 2019-03-02 19:01:43.328486: 
 	std: 0.05096711
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51068336, 0.61473334, 0.61473334, 0.6148667, 0.5108]
	train_accs: [0.51068336, 0.61473334, 0.61473334, 0.6148667, 0.5108]
	best_train_sub_head: 3
	worst: 0.51068336
	avg: 0.57316333
	best: 0.6148667

Starting e_i: 1112
Model ind 640 epoch 1112 head A head_i_epoch 0 batch 0: avg loss -3.285843 avg loss no lamb -3.285843 time 2019-03-02 19:01:48.112730
Model ind 640 epoch 1112 head A head_i_epoch 0 batch 100: avg loss -3.171155 avg loss no lamb -3.171155 time 2019-03-02 19:04:55.373515
Model ind 640 epoch 1112 head A head_i_epoch 0 batch 200: avg loss -3.271346 avg loss no lamb -3.271346 time 2019-03-02 19:08:07.407251
last batch sz 160
Model ind 640 epoch 1112 head B head_i_epoch 0 batch 0: avg loss -1.929741 avg loss no lamb -1.929741 time 2019-03-02 19:10:28.471671
Model ind 640 epoch 1112 head B head_i_epoch 0 batch 100: avg loss -1.966565 avg loss no lamb -1.966565 time 2019-03-02 19:13:04.772057
Model ind 640 epoch 1112 head B head_i_epoch 0 batch 200: avg loss -1.918527 avg loss no lamb -1.918527 time 2019-03-02 19:16:16.264022
last batch sz 160
Model ind 640 epoch 1112 head B head_i_epoch 1 batch 0: avg loss -1.920229 avg loss no lamb -1.920229 time 2019-03-02 19:18:37.218200
Model ind 640 epoch 1112 head B head_i_epoch 1 batch 100: avg loss -1.895020 avg loss no lamb -1.895020 time 2019-03-02 19:21:49.048047
Model ind 640 epoch 1112 head B head_i_epoch 1 batch 200: avg loss -2.007094 avg loss no lamb -2.007094 time 2019-03-02 19:24:46.761008
last batch sz 160
Pre: time 2019-03-02 19:27:21.162050: 
 	std: 0.050787475
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51095, 0.6145833, 0.61473334, 0.6146167, 0.511]
	train_accs: [0.51095, 0.6145833, 0.61473334, 0.6146167, 0.511]
	best_train_sub_head: 2
	worst: 0.51095
	avg: 0.5731767
	best: 0.61473334

Starting e_i: 1113
Model ind 640 epoch 1113 head A head_i_epoch 0 batch 0: avg loss -3.262388 avg loss no lamb -3.262388 time 2019-03-02 19:27:25.116974
Model ind 640 epoch 1113 head A head_i_epoch 0 batch 100: avg loss -3.193739 avg loss no lamb -3.193739 time 2019-03-02 19:30:30.685401
Model ind 640 epoch 1113 head A head_i_epoch 0 batch 200: avg loss -3.297593 avg loss no lamb -3.297593 time 2019-03-02 19:33:41.545434
last batch sz 160
Model ind 640 epoch 1113 head B head_i_epoch 0 batch 0: avg loss -1.873674 avg loss no lamb -1.873674 time 2019-03-02 19:35:59.573942
Model ind 640 epoch 1113 head B head_i_epoch 0 batch 100: avg loss -1.919528 avg loss no lamb -1.919528 time 2019-03-02 19:38:37.134922
Model ind 640 epoch 1113 head B head_i_epoch 0 batch 200: avg loss -1.969133 avg loss no lamb -1.969133 time 2019-03-02 19:41:47.912603
last batch sz 160
Model ind 640 epoch 1113 head B head_i_epoch 1 batch 0: avg loss -1.891978 avg loss no lamb -1.891978 time 2019-03-02 19:44:04.378121
Model ind 640 epoch 1113 head B head_i_epoch 1 batch 100: avg loss -1.818397 avg loss no lamb -1.818397 time 2019-03-02 19:47:18.645134
Model ind 640 epoch 1113 head B head_i_epoch 1 batch 200: avg loss -2.001901 avg loss no lamb -2.001901 time 2019-03-02 19:50:31.907832
last batch sz 160
Pre: time 2019-03-02 19:52:46.787967: 
 	std: 0.0506868
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51246667, 0.6160167, 0.6160667, 0.6160333, 0.51268333]
	train_accs: [0.51246667, 0.6160167, 0.6160667, 0.6160333, 0.51268333]
	best_train_sub_head: 2
	worst: 0.51246667
	avg: 0.5746533
	best: 0.6160667

Starting e_i: 1114
Model ind 640 epoch 1114 head A head_i_epoch 0 batch 0: avg loss -3.318989 avg loss no lamb -3.318989 time 2019-03-02 19:52:51.006857
Model ind 640 epoch 1114 head A head_i_epoch 0 batch 100: avg loss -3.112657 avg loss no lamb -3.112657 time 2019-03-02 19:56:00.539010
Model ind 640 epoch 1114 head A head_i_epoch 0 batch 200: avg loss -3.313933 avg loss no lamb -3.313933 time 2019-03-02 19:59:12.546282
last batch sz 160
Model ind 640 epoch 1114 head B head_i_epoch 0 batch 0: avg loss -1.893633 avg loss no lamb -1.893633 time 2019-03-02 20:01:30.895598
Model ind 640 epoch 1114 head B head_i_epoch 0 batch 100: avg loss -1.874880 avg loss no lamb -1.874880 time 2019-03-02 20:04:25.122112
Model ind 640 epoch 1114 head B head_i_epoch 0 batch 200: avg loss -1.904233 avg loss no lamb -1.904233 time 2019-03-02 20:07:19.735080
last batch sz 160
Model ind 640 epoch 1114 head B head_i_epoch 1 batch 0: avg loss -2.015533 avg loss no lamb -2.015533 time 2019-03-02 20:09:38.452084
Model ind 640 epoch 1114 head B head_i_epoch 1 batch 100: avg loss -1.792925 avg loss no lamb -1.792925 time 2019-03-02 20:12:48.719012
Model ind 640 epoch 1114 head B head_i_epoch 1 batch 200: avg loss -1.918177 avg loss no lamb -1.918177 time 2019-03-02 20:16:01.819273
last batch sz 160
Pre: time 2019-03-02 20:18:29.667129: 
 	std: 0.050729483
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51091665, 0.6146167, 0.6149167, 0.6148667, 0.5115833]
	train_accs: [0.51091665, 0.6146167, 0.6149167, 0.6148667, 0.5115833]
	best_train_sub_head: 2
	worst: 0.51091665
	avg: 0.57338
	best: 0.6149167

Starting e_i: 1115
Model ind 640 epoch 1115 head A head_i_epoch 0 batch 0: avg loss -3.246399 avg loss no lamb -3.246399 time 2019-03-02 20:18:33.036610
Model ind 640 epoch 1115 head A head_i_epoch 0 batch 100: avg loss -3.171180 avg loss no lamb -3.171180 time 2019-03-02 20:21:44.083498
Model ind 640 epoch 1115 head A head_i_epoch 0 batch 200: avg loss -3.262129 avg loss no lamb -3.262129 time 2019-03-02 20:24:57.997918
last batch sz 160
Model ind 640 epoch 1115 head B head_i_epoch 0 batch 0: avg loss -1.891016 avg loss no lamb -1.891016 time 2019-03-02 20:27:16.772299
Model ind 640 epoch 1115 head B head_i_epoch 0 batch 100: avg loss -1.820641 avg loss no lamb -1.820641 time 2019-03-02 20:30:26.733620
Model ind 640 epoch 1115 head B head_i_epoch 0 batch 200: avg loss -1.978351 avg loss no lamb -1.978351 time 2019-03-02 20:33:02.798628
last batch sz 160
Model ind 640 epoch 1115 head B head_i_epoch 1 batch 0: avg loss -1.870804 avg loss no lamb -1.870804 time 2019-03-02 20:35:20.701744
Model ind 640 epoch 1115 head B head_i_epoch 1 batch 100: avg loss -1.849336 avg loss no lamb -1.849336 time 2019-03-02 20:38:31.438111
Model ind 640 epoch 1115 head B head_i_epoch 1 batch 200: avg loss -1.947425 avg loss no lamb -1.947425 time 2019-03-02 20:41:42.826601
last batch sz 160
Pre: time 2019-03-02 20:44:20.462065: 
 	std: 0.050749347
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104167, 0.61401665, 0.61403334, 0.6141, 0.5105]
	train_accs: [0.5104167, 0.61401665, 0.61403334, 0.6141, 0.5105]
	best_train_sub_head: 3
	worst: 0.5104167
	avg: 0.57261336
	best: 0.6141

Starting e_i: 1116
Model ind 640 epoch 1116 head A head_i_epoch 0 batch 0: avg loss -3.263921 avg loss no lamb -3.263921 time 2019-03-02 20:44:23.978264
Model ind 640 epoch 1116 head A head_i_epoch 0 batch 100: avg loss -3.208714 avg loss no lamb -3.208714 time 2019-03-02 20:47:14.584271
Model ind 640 epoch 1116 head A head_i_epoch 0 batch 200: avg loss -3.233090 avg loss no lamb -3.233090 time 2019-03-02 20:50:26.771777
last batch sz 160
Model ind 640 epoch 1116 head B head_i_epoch 0 batch 0: avg loss -2.054331 avg loss no lamb -2.054331 time 2019-03-02 20:52:45.154658
Model ind 640 epoch 1116 head B head_i_epoch 0 batch 100: avg loss -1.865354 avg loss no lamb -1.865354 time 2019-03-02 20:55:54.719555
Model ind 640 epoch 1116 head B head_i_epoch 0 batch 200: avg loss -1.974867 avg loss no lamb -1.974867 time 2019-03-02 20:58:34.956971
last batch sz 160
Model ind 640 epoch 1116 head B head_i_epoch 1 batch 0: avg loss -1.871096 avg loss no lamb -1.871096 time 2019-03-02 21:00:53.779350
Model ind 640 epoch 1116 head B head_i_epoch 1 batch 100: avg loss -1.888515 avg loss no lamb -1.888515 time 2019-03-02 21:04:06.382407
Model ind 640 epoch 1116 head B head_i_epoch 1 batch 200: avg loss -1.996688 avg loss no lamb -1.996688 time 2019-03-02 21:07:12.611161
last batch sz 160
Pre: time 2019-03-02 21:10:08.563430: 
 	std: 0.05096184
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51105, 0.6153333, 0.61525, 0.6153167, 0.5115]
	train_accs: [0.51105, 0.6153333, 0.61525, 0.6153167, 0.5115]
	best_train_sub_head: 1
	worst: 0.51105
	avg: 0.57369006
	best: 0.6153333

Starting e_i: 1117
Model ind 640 epoch 1117 head A head_i_epoch 0 batch 0: avg loss -3.246663 avg loss no lamb -3.246663 time 2019-03-02 21:10:13.070112
Model ind 640 epoch 1117 head A head_i_epoch 0 batch 100: avg loss -3.204108 avg loss no lamb -3.204108 time 2019-03-02 21:12:51.937295
Model ind 640 epoch 1117 head A head_i_epoch 0 batch 200: avg loss -3.179272 avg loss no lamb -3.179272 time 2019-03-02 21:16:01.393344
last batch sz 160
Model ind 640 epoch 1117 head B head_i_epoch 0 batch 0: avg loss -1.980929 avg loss no lamb -1.980929 time 2019-03-02 21:18:23.048757
Model ind 640 epoch 1117 head B head_i_epoch 0 batch 100: avg loss -1.828092 avg loss no lamb -1.828092 time 2019-03-02 21:21:35.301880
Model ind 640 epoch 1117 head B head_i_epoch 0 batch 200: avg loss -2.012497 avg loss no lamb -2.012497 time 2019-03-02 21:24:29.533151
last batch sz 160
Model ind 640 epoch 1117 head B head_i_epoch 1 batch 0: avg loss -1.945305 avg loss no lamb -1.945305 time 2019-03-02 21:26:29.282477
Model ind 640 epoch 1117 head B head_i_epoch 1 batch 100: avg loss -1.836432 avg loss no lamb -1.836432 time 2019-03-02 21:29:41.308623
Model ind 640 epoch 1117 head B head_i_epoch 1 batch 200: avg loss -2.041301 avg loss no lamb -2.041301 time 2019-03-02 21:32:53.758115
last batch sz 160
Pre: time 2019-03-02 21:35:48.418316: 
 	std: 0.05099305
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5099, 0.61405, 0.6142333, 0.61413336, 0.5102]
	train_accs: [0.5099, 0.61405, 0.6142333, 0.61413336, 0.5102]
	best_train_sub_head: 2
	worst: 0.5099
	avg: 0.5725033
	best: 0.6142333

Starting e_i: 1118
Model ind 640 epoch 1118 head A head_i_epoch 0 batch 0: avg loss -3.290149 avg loss no lamb -3.290149 time 2019-03-02 21:35:51.818283
Model ind 640 epoch 1118 head A head_i_epoch 0 batch 100: avg loss -3.117156 avg loss no lamb -3.117156 time 2019-03-02 21:38:25.218021
Model ind 640 epoch 1118 head A head_i_epoch 0 batch 200: avg loss -3.256282 avg loss no lamb -3.256282 time 2019-03-02 21:41:37.386844
last batch sz 160
Model ind 640 epoch 1118 head B head_i_epoch 0 batch 0: avg loss -1.943613 avg loss no lamb -1.943613 time 2019-03-02 21:43:55.999468
Model ind 640 epoch 1118 head B head_i_epoch 0 batch 100: avg loss -1.853560 avg loss no lamb -1.853560 time 2019-03-02 21:47:05.989912
Model ind 640 epoch 1118 head B head_i_epoch 0 batch 200: avg loss -1.948553 avg loss no lamb -1.948553 time 2019-03-02 21:50:17.963744
last batch sz 160
Model ind 640 epoch 1118 head B head_i_epoch 1 batch 0: avg loss -1.970468 avg loss no lamb -1.970468 time 2019-03-02 21:52:01.327595
Model ind 640 epoch 1118 head B head_i_epoch 1 batch 100: avg loss -1.872360 avg loss no lamb -1.872360 time 2019-03-02 21:55:14.764383
Model ind 640 epoch 1118 head B head_i_epoch 1 batch 200: avg loss -1.976447 avg loss no lamb -1.976447 time 2019-03-02 21:58:24.020869
last batch sz 160
Pre: time 2019-03-02 22:01:15.213126: 
 	std: 0.050783508
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51056665, 0.6141833, 0.61406666, 0.61443335, 0.51056665]
	train_accs: [0.51056665, 0.6141833, 0.61406666, 0.61443335, 0.51056665]
	best_train_sub_head: 3
	worst: 0.51056665
	avg: 0.5727633
	best: 0.61443335

Starting e_i: 1119
Model ind 640 epoch 1119 head A head_i_epoch 0 batch 0: avg loss -3.357687 avg loss no lamb -3.357687 time 2019-03-02 22:01:18.733121
Model ind 640 epoch 1119 head A head_i_epoch 0 batch 100: avg loss -3.220509 avg loss no lamb -3.220509 time 2019-03-02 22:04:12.792396
Model ind 640 epoch 1119 head A head_i_epoch 0 batch 200: avg loss -3.297905 avg loss no lamb -3.297905 time 2019-03-02 22:07:07.252846
last batch sz 160
Model ind 640 epoch 1119 head B head_i_epoch 0 batch 0: avg loss -1.966493 avg loss no lamb -1.966493 time 2019-03-02 22:09:25.952845
Model ind 640 epoch 1119 head B head_i_epoch 0 batch 100: avg loss -1.820848 avg loss no lamb -1.820848 time 2019-03-02 22:12:34.486644
Model ind 640 epoch 1119 head B head_i_epoch 0 batch 200: avg loss -1.975207 avg loss no lamb -1.975207 time 2019-03-02 22:15:49.830905
last batch sz 160
Model ind 640 epoch 1119 head B head_i_epoch 1 batch 0: avg loss -1.842631 avg loss no lamb -1.842631 time 2019-03-02 22:17:47.919395
Model ind 640 epoch 1119 head B head_i_epoch 1 batch 100: avg loss -1.881950 avg loss no lamb -1.881950 time 2019-03-02 22:20:43.256828
Model ind 640 epoch 1119 head B head_i_epoch 1 batch 200: avg loss -1.946347 avg loss no lamb -1.946347 time 2019-03-02 22:23:54.293003
last batch sz 160
Pre: time 2019-03-02 22:26:41.801699: 
 	std: 0.05029219
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51161665, 0.6144, 0.61433333, 0.61446667, 0.5118667]
	train_accs: [0.51161665, 0.6144, 0.61433333, 0.61446667, 0.5118667]
	best_train_sub_head: 3
	worst: 0.51161665
	avg: 0.5733367
	best: 0.61446667

Starting e_i: 1120
Model ind 640 epoch 1120 head A head_i_epoch 0 batch 0: avg loss -3.232436 avg loss no lamb -3.232436 time 2019-03-02 22:26:45.266350
Model ind 640 epoch 1120 head A head_i_epoch 0 batch 100: avg loss -3.155337 avg loss no lamb -3.155337 time 2019-03-02 22:29:59.234146
Model ind 640 epoch 1120 head A head_i_epoch 0 batch 200: avg loss -3.195121 avg loss no lamb -3.195121 time 2019-03-02 22:32:37.009157
last batch sz 160
Model ind 640 epoch 1120 head B head_i_epoch 0 batch 0: avg loss -1.919469 avg loss no lamb -1.919469 time 2019-03-02 22:34:55.027601
Model ind 640 epoch 1120 head B head_i_epoch 0 batch 100: avg loss -1.929810 avg loss no lamb -1.929810 time 2019-03-02 22:38:05.428934
Model ind 640 epoch 1120 head B head_i_epoch 0 batch 200: avg loss -1.948301 avg loss no lamb -1.948301 time 2019-03-02 22:41:16.260808
last batch sz 160
Model ind 640 epoch 1120 head B head_i_epoch 1 batch 0: avg loss -1.935119 avg loss no lamb -1.935119 time 2019-03-02 22:43:34.059645
Model ind 640 epoch 1120 head B head_i_epoch 1 batch 100: avg loss -1.884245 avg loss no lamb -1.884245 time 2019-03-02 22:46:12.744903
Model ind 640 epoch 1120 head B head_i_epoch 1 batch 200: avg loss -2.000756 avg loss no lamb -2.000756 time 2019-03-02 22:49:20.350417
last batch sz 160
Pre: time 2019-03-02 22:52:11.835258: 
 	std: 0.05094846
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5096833, 0.61408335, 0.6138667, 0.61401665, 0.5103]
	train_accs: [0.5096833, 0.61408335, 0.6138667, 0.61401665, 0.5103]
	best_train_sub_head: 1
	worst: 0.5096833
	avg: 0.57238996
	best: 0.61408335

Starting e_i: 1121
Model ind 640 epoch 1121 head A head_i_epoch 0 batch 0: avg loss -3.241391 avg loss no lamb -3.241391 time 2019-03-02 22:52:24.095333
Model ind 640 epoch 1121 head A head_i_epoch 0 batch 100: avg loss -3.226674 avg loss no lamb -3.226674 time 2019-03-02 22:55:10.804434
Model ind 640 epoch 1121 head A head_i_epoch 0 batch 200: avg loss -3.333382 avg loss no lamb -3.333382 time 2019-03-02 22:56:56.376280
last batch sz 160
Model ind 640 epoch 1121 head B head_i_epoch 0 batch 0: avg loss -1.903186 avg loss no lamb -1.903186 time 2019-03-02 22:58:26.757084
Model ind 640 epoch 1121 head B head_i_epoch 0 batch 100: avg loss -1.855892 avg loss no lamb -1.855892 time 2019-03-02 23:01:57.437786
Model ind 640 epoch 1121 head B head_i_epoch 0 batch 200: avg loss -1.984278 avg loss no lamb -1.984278 time 2019-03-02 23:05:31.604196
last batch sz 160
Model ind 640 epoch 1121 head B head_i_epoch 1 batch 0: avg loss -1.872089 avg loss no lamb -1.872089 time 2019-03-02 23:08:08.340320
Model ind 640 epoch 1121 head B head_i_epoch 1 batch 100: avg loss -1.846579 avg loss no lamb -1.846579 time 2019-03-02 23:11:04.551624
Model ind 640 epoch 1121 head B head_i_epoch 1 batch 200: avg loss -1.989764 avg loss no lamb -1.989764 time 2019-03-02 23:14:27.996027
last batch sz 160
Pre: time 2019-03-02 23:17:40.837418: 
 	std: 0.05070369
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5099667, 0.6139, 0.61385, 0.6138167, 0.51075]
	train_accs: [0.5099667, 0.6139, 0.61385, 0.6138167, 0.51075]
	best_train_sub_head: 1
	worst: 0.5099667
	avg: 0.5724567
	best: 0.6139

Starting e_i: 1122
Model ind 640 epoch 1122 head A head_i_epoch 0 batch 0: avg loss -3.348642 avg loss no lamb -3.348642 time 2019-03-02 23:17:45.140419
Model ind 640 epoch 1122 head A head_i_epoch 0 batch 100: avg loss -3.230048 avg loss no lamb -3.230048 time 2019-03-02 23:21:19.009379
Model ind 640 epoch 1122 head A head_i_epoch 0 batch 200: avg loss -3.196771 avg loss no lamb -3.196771 time 2019-03-02 23:24:12.397332
last batch sz 160
Model ind 640 epoch 1122 head B head_i_epoch 0 batch 0: avg loss -1.888910 avg loss no lamb -1.888910 time 2019-03-02 23:26:42.409960
Model ind 640 epoch 1122 head B head_i_epoch 0 batch 100: avg loss -1.912420 avg loss no lamb -1.912420 time 2019-03-02 23:30:19.940192
Model ind 640 epoch 1122 head B head_i_epoch 0 batch 200: avg loss -1.938180 avg loss no lamb -1.938180 time 2019-03-02 23:33:55.049258
last batch sz 160
Model ind 640 epoch 1122 head B head_i_epoch 1 batch 0: avg loss -1.954062 avg loss no lamb -1.954062 time 2019-03-02 23:36:20.150564
Model ind 640 epoch 1122 head B head_i_epoch 1 batch 100: avg loss -1.845260 avg loss no lamb -1.845260 time 2019-03-02 23:39:13.990494
Model ind 640 epoch 1122 head B head_i_epoch 1 batch 200: avg loss -1.936760 avg loss no lamb -1.936760 time 2019-03-02 23:42:47.608398
last batch sz 160
Pre: time 2019-03-02 23:45:58.859033: 
 	std: 0.050837
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104333, 0.6145833, 0.61455, 0.6146, 0.5111833]
	train_accs: [0.5104333, 0.6145833, 0.61455, 0.6146, 0.5111833]
	best_train_sub_head: 3
	worst: 0.5104333
	avg: 0.57307
	best: 0.6146

Starting e_i: 1123
Model ind 640 epoch 1123 head A head_i_epoch 0 batch 0: avg loss -3.242714 avg loss no lamb -3.242714 time 2019-03-02 23:46:02.456549
Model ind 640 epoch 1123 head A head_i_epoch 0 batch 100: avg loss -3.255951 avg loss no lamb -3.255951 time 2019-03-02 23:49:28.152403
Model ind 640 epoch 1123 head A head_i_epoch 0 batch 200: avg loss -3.285797 avg loss no lamb -3.285797 time 2019-03-02 23:52:18.850251
last batch sz 160
Model ind 640 epoch 1123 head B head_i_epoch 0 batch 0: avg loss -1.969579 avg loss no lamb -1.969579 time 2019-03-02 23:54:55.677839
Model ind 640 epoch 1123 head B head_i_epoch 0 batch 100: avg loss -1.947375 avg loss no lamb -1.947375 time 2019-03-02 23:58:29.709395
Model ind 640 epoch 1123 head B head_i_epoch 0 batch 200: avg loss -1.977785 avg loss no lamb -1.977785 time 2019-03-03 00:02:04.841837
last batch sz 160
Model ind 640 epoch 1123 head B head_i_epoch 1 batch 0: avg loss -1.929623 avg loss no lamb -1.929623 time 2019-03-03 00:03:50.414937
Model ind 640 epoch 1123 head B head_i_epoch 1 batch 100: avg loss -1.785400 avg loss no lamb -1.785400 time 2019-03-03 00:07:24.233578
Model ind 640 epoch 1123 head B head_i_epoch 1 batch 200: avg loss -2.020519 avg loss no lamb -2.020519 time 2019-03-03 00:10:58.621284
last batch sz 160
Pre: time 2019-03-03 00:14:12.484511: 
 	std: 0.050741345
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50995, 0.6142, 0.61408335, 0.61413336, 0.5111833]
	train_accs: [0.50995, 0.6142, 0.61408335, 0.61413336, 0.5111833]
	best_train_sub_head: 1
	worst: 0.50995
	avg: 0.57271
	best: 0.6142

Starting e_i: 1124
Model ind 640 epoch 1124 head A head_i_epoch 0 batch 0: avg loss -3.256241 avg loss no lamb -3.256241 time 2019-03-03 00:14:15.571334
Model ind 640 epoch 1124 head A head_i_epoch 0 batch 100: avg loss -3.256339 avg loss no lamb -3.256339 time 2019-03-03 00:16:59.111530
Model ind 640 epoch 1124 head A head_i_epoch 0 batch 200: avg loss -3.275342 avg loss no lamb -3.275342 time 2019-03-03 00:20:33.600530
last batch sz 160
Model ind 640 epoch 1124 head B head_i_epoch 0 batch 0: avg loss -1.938294 avg loss no lamb -1.938294 time 2019-03-03 00:23:08.616976
Model ind 640 epoch 1124 head B head_i_epoch 0 batch 100: avg loss -1.820402 avg loss no lamb -1.820402 time 2019-03-03 00:26:42.312039
Model ind 640 epoch 1124 head B head_i_epoch 0 batch 200: avg loss -1.953165 avg loss no lamb -1.953165 time 2019-03-03 00:29:47.590866
last batch sz 160
Model ind 640 epoch 1124 head B head_i_epoch 1 batch 0: avg loss -1.969932 avg loss no lamb -1.969932 time 2019-03-03 00:31:59.583684
Model ind 640 epoch 1124 head B head_i_epoch 1 batch 100: avg loss -1.933467 avg loss no lamb -1.933467 time 2019-03-03 00:35:34.152113
Model ind 640 epoch 1124 head B head_i_epoch 1 batch 200: avg loss -1.945820 avg loss no lamb -1.945820 time 2019-03-03 00:39:09.744071
last batch sz 160
Pre: time 2019-03-03 00:42:13.260136: 
 	std: 0.050609972
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51126665, 0.6150333, 0.61501664, 0.61501664, 0.5121667]
	train_accs: [0.51126665, 0.6150333, 0.61501664, 0.61501664, 0.5121667]
	best_train_sub_head: 1
	worst: 0.51126665
	avg: 0.5737
	best: 0.6150333

Starting e_i: 1125
Model ind 640 epoch 1125 head A head_i_epoch 0 batch 0: avg loss -3.340274 avg loss no lamb -3.340274 time 2019-03-03 00:42:16.273710
Model ind 640 epoch 1125 head A head_i_epoch 0 batch 100: avg loss -3.266951 avg loss no lamb -3.266951 time 2019-03-03 00:45:16.586404
Model ind 640 epoch 1125 head A head_i_epoch 0 batch 200: avg loss -3.300130 avg loss no lamb -3.300130 time 2019-03-03 00:48:48.653547
last batch sz 160
Model ind 640 epoch 1125 head B head_i_epoch 0 batch 0: avg loss -1.922518 avg loss no lamb -1.922518 time 2019-03-03 00:51:27.554234
Model ind 640 epoch 1125 head B head_i_epoch 0 batch 100: avg loss -1.855065 avg loss no lamb -1.855065 time 2019-03-03 00:54:59.644164
Model ind 640 epoch 1125 head B head_i_epoch 0 batch 200: avg loss -1.907199 avg loss no lamb -1.907199 time 2019-03-03 00:57:43.522940
last batch sz 160
Model ind 640 epoch 1125 head B head_i_epoch 1 batch 0: avg loss -1.867931 avg loss no lamb -1.867931 time 2019-03-03 01:00:20.345115
Model ind 640 epoch 1125 head B head_i_epoch 1 batch 100: avg loss -1.880911 avg loss no lamb -1.880911 time 2019-03-03 01:03:56.566768
Model ind 640 epoch 1125 head B head_i_epoch 1 batch 200: avg loss -1.915568 avg loss no lamb -1.915568 time 2019-03-03 01:07:31.431270
last batch sz 160
Pre: time 2019-03-03 01:09:55.550591: 
 	std: 0.05083057
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5093, 0.61361665, 0.6134833, 0.6135167, 0.51026666]
	train_accs: [0.5093, 0.61361665, 0.6134833, 0.6135167, 0.51026666]
	best_train_sub_head: 1
	worst: 0.5093
	avg: 0.5720366
	best: 0.61361665

Starting e_i: 1126
Model ind 640 epoch 1126 head A head_i_epoch 0 batch 0: avg loss -3.285016 avg loss no lamb -3.285016 time 2019-03-03 01:09:59.123147
Model ind 640 epoch 1126 head A head_i_epoch 0 batch 100: avg loss -3.247929 avg loss no lamb -3.247929 time 2019-03-03 01:13:27.291990
Model ind 640 epoch 1126 head A head_i_epoch 0 batch 200: avg loss -3.239803 avg loss no lamb -3.239803 time 2019-03-03 01:17:03.869281
last batch sz 160
Model ind 640 epoch 1126 head B head_i_epoch 0 batch 0: avg loss -1.925937 avg loss no lamb -1.925937 time 2019-03-03 01:19:40.871168
Model ind 640 epoch 1126 head B head_i_epoch 0 batch 100: avg loss -1.886128 avg loss no lamb -1.886128 time 2019-03-03 01:22:39.710970
Model ind 640 epoch 1126 head B head_i_epoch 0 batch 200: avg loss -1.959948 avg loss no lamb -1.959948 time 2019-03-03 01:25:59.148602
last batch sz 160
Model ind 640 epoch 1126 head B head_i_epoch 1 batch 0: avg loss -1.835203 avg loss no lamb -1.835203 time 2019-03-03 01:28:36.002620
Model ind 640 epoch 1126 head B head_i_epoch 1 batch 100: avg loss -1.928441 avg loss no lamb -1.928441 time 2019-03-03 01:32:09.174447
Model ind 640 epoch 1126 head B head_i_epoch 1 batch 200: avg loss -1.961805 avg loss no lamb -1.961805 time 2019-03-03 01:35:18.865284
last batch sz 160
Pre: time 2019-03-03 01:38:08.011930: 
 	std: 0.05017933
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5112333, 0.6139, 0.6138667, 0.6138167, 0.51163334]
	train_accs: [0.5112333, 0.6139, 0.6138667, 0.6138167, 0.51163334]
	best_train_sub_head: 1
	worst: 0.5112333
	avg: 0.57289
	best: 0.6139

Starting e_i: 1127
Model ind 640 epoch 1127 head A head_i_epoch 0 batch 0: avg loss -3.218121 avg loss no lamb -3.218121 time 2019-03-03 01:38:12.740958
Model ind 640 epoch 1127 head A head_i_epoch 0 batch 100: avg loss -3.202894 avg loss no lamb -3.202894 time 2019-03-03 01:41:41.674295
Model ind 640 epoch 1127 head A head_i_epoch 0 batch 200: avg loss -3.286135 avg loss no lamb -3.286135 time 2019-03-03 01:45:18.509731
last batch sz 160
Model ind 640 epoch 1127 head B head_i_epoch 0 batch 0: avg loss -1.904074 avg loss no lamb -1.904074 time 2019-03-03 01:47:54.070812
Model ind 640 epoch 1127 head B head_i_epoch 0 batch 100: avg loss -1.819718 avg loss no lamb -1.819718 time 2019-03-03 01:50:35.107216
Model ind 640 epoch 1127 head B head_i_epoch 0 batch 200: avg loss -1.991789 avg loss no lamb -1.991789 time 2019-03-03 01:54:11.045195
last batch sz 160
Model ind 640 epoch 1127 head B head_i_epoch 1 batch 0: avg loss -1.915915 avg loss no lamb -1.915915 time 2019-03-03 01:56:49.571116
Model ind 640 epoch 1127 head B head_i_epoch 1 batch 100: avg loss -1.857427 avg loss no lamb -1.857427 time 2019-03-03 02:00:24.332766
Model ind 640 epoch 1127 head B head_i_epoch 1 batch 200: avg loss -2.035480 avg loss no lamb -2.035480 time 2019-03-03 02:03:08.168146
last batch sz 160
Pre: time 2019-03-03 02:06:20.344452: 
 	std: 0.05040287
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51163334, 0.6149, 0.6149, 0.61485, 0.51236665]
	train_accs: [0.51163334, 0.6149, 0.6149, 0.61485, 0.51236665]
	best_train_sub_head: 1
	worst: 0.51163334
	avg: 0.57373
	best: 0.6149

Starting e_i: 1128
Model ind 640 epoch 1128 head A head_i_epoch 0 batch 0: avg loss -3.359999 avg loss no lamb -3.359999 time 2019-03-03 02:06:24.212209
Model ind 640 epoch 1128 head A head_i_epoch 0 batch 100: avg loss -3.299080 avg loss no lamb -3.299080 time 2019-03-03 02:09:56.861364
Model ind 640 epoch 1128 head A head_i_epoch 0 batch 200: avg loss -3.283427 avg loss no lamb -3.283427 time 2019-03-03 02:13:34.903935
last batch sz 160
Model ind 640 epoch 1128 head B head_i_epoch 0 batch 0: avg loss -1.931458 avg loss no lamb -1.931458 time 2019-03-03 02:15:29.062076
Model ind 640 epoch 1128 head B head_i_epoch 0 batch 100: avg loss -1.839967 avg loss no lamb -1.839967 time 2019-03-03 02:18:51.101418
Model ind 640 epoch 1128 head B head_i_epoch 0 batch 200: avg loss -1.942997 avg loss no lamb -1.942997 time 2019-03-03 02:22:26.377887
last batch sz 160
Model ind 640 epoch 1128 head B head_i_epoch 1 batch 0: avg loss -1.925791 avg loss no lamb -1.925791 time 2019-03-03 02:25:01.464914
Model ind 640 epoch 1128 head B head_i_epoch 1 batch 100: avg loss -1.836232 avg loss no lamb -1.836232 time 2019-03-03 02:28:07.739936
Model ind 640 epoch 1128 head B head_i_epoch 1 batch 200: avg loss -2.011112 avg loss no lamb -2.011112 time 2019-03-03 02:31:14.848412
last batch sz 160
Pre: time 2019-03-03 02:34:25.618462: 
 	std: 0.05053062
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51093334, 0.61433333, 0.61436665, 0.61443335, 0.5115333]
	train_accs: [0.51093334, 0.61433333, 0.61436665, 0.61443335, 0.5115333]
	best_train_sub_head: 3
	worst: 0.51093334
	avg: 0.57312
	best: 0.61443335

Starting e_i: 1129
Model ind 640 epoch 1129 head A head_i_epoch 0 batch 0: avg loss -3.316865 avg loss no lamb -3.316865 time 2019-03-03 02:34:29.241669
Model ind 640 epoch 1129 head A head_i_epoch 0 batch 100: avg loss -3.254123 avg loss no lamb -3.254123 time 2019-03-03 02:38:04.162200
Model ind 640 epoch 1129 head A head_i_epoch 0 batch 200: avg loss -3.291410 avg loss no lamb -3.291410 time 2019-03-03 02:41:13.802123
last batch sz 160
Model ind 640 epoch 1129 head B head_i_epoch 0 batch 0: avg loss -2.028360 avg loss no lamb -2.028360 time 2019-03-03 02:43:23.136787
Model ind 640 epoch 1129 head B head_i_epoch 0 batch 100: avg loss -1.914329 avg loss no lamb -1.914329 time 2019-03-03 02:46:55.380995
Model ind 640 epoch 1129 head B head_i_epoch 0 batch 200: avg loss -1.974891 avg loss no lamb -1.974891 time 2019-03-03 02:50:31.027709
last batch sz 160
Model ind 640 epoch 1129 head B head_i_epoch 1 batch 0: avg loss -1.911320 avg loss no lamb -1.911320 time 2019-03-03 02:53:06.755207
Model ind 640 epoch 1129 head B head_i_epoch 1 batch 100: avg loss -1.845818 avg loss no lamb -1.845818 time 2019-03-03 02:55:46.240397
Model ind 640 epoch 1129 head B head_i_epoch 1 batch 200: avg loss -1.990881 avg loss no lamb -1.990881 time 2019-03-03 02:59:20.219492
last batch sz 160
Pre: time 2019-03-03 03:02:33.250626: 
 	std: 0.051086932
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50991666, 0.6144, 0.61425, 0.61436665, 0.5102]
	train_accs: [0.50991666, 0.6144, 0.61425, 0.61436665, 0.5102]
	best_train_sub_head: 1
	worst: 0.50991666
	avg: 0.5726267
	best: 0.6144

Starting e_i: 1130
Model ind 640 epoch 1130 head A head_i_epoch 0 batch 0: avg loss -3.271416 avg loss no lamb -3.271416 time 2019-03-03 03:02:36.798997
Model ind 640 epoch 1130 head A head_i_epoch 0 batch 100: avg loss -3.212978 avg loss no lamb -3.212978 time 2019-03-03 03:06:10.489241
Model ind 640 epoch 1130 head A head_i_epoch 0 batch 200: avg loss -3.353429 avg loss no lamb -3.353429 time 2019-03-03 03:08:52.474539
last batch sz 160
Model ind 640 epoch 1130 head B head_i_epoch 0 batch 0: avg loss -1.985152 avg loss no lamb -1.985152 time 2019-03-03 03:11:27.021216
Model ind 640 epoch 1130 head B head_i_epoch 0 batch 100: avg loss -1.853304 avg loss no lamb -1.853304 time 2019-03-03 03:15:01.662634
Model ind 640 epoch 1130 head B head_i_epoch 0 batch 200: avg loss -1.933590 avg loss no lamb -1.933590 time 2019-03-03 03:18:38.413904
last batch sz 160
Model ind 640 epoch 1130 head B head_i_epoch 1 batch 0: avg loss -1.916317 avg loss no lamb -1.916317 time 2019-03-03 03:20:33.368764
Model ind 640 epoch 1130 head B head_i_epoch 1 batch 100: avg loss -1.847541 avg loss no lamb -1.847541 time 2019-03-03 03:23:56.808179
Model ind 640 epoch 1130 head B head_i_epoch 1 batch 200: avg loss -2.000311 avg loss no lamb -2.000311 time 2019-03-03 03:27:31.179112
last batch sz 160
Pre: time 2019-03-03 03:30:42.931351: 
 	std: 0.050821748
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51026666, 0.6142833, 0.6142833, 0.61425, 0.5108]
	train_accs: [0.51026666, 0.6142833, 0.6142833, 0.61425, 0.5108]
	best_train_sub_head: 1
	worst: 0.51026666
	avg: 0.5727767
	best: 0.6142833

Starting e_i: 1131
Model ind 640 epoch 1131 head A head_i_epoch 0 batch 0: avg loss -3.260747 avg loss no lamb -3.260747 time 2019-03-03 03:30:51.461422
Model ind 640 epoch 1131 head A head_i_epoch 0 batch 100: avg loss -3.220556 avg loss no lamb -3.220556 time 2019-03-03 03:33:38.138362
Model ind 640 epoch 1131 head A head_i_epoch 0 batch 200: avg loss -3.253505 avg loss no lamb -3.253505 time 2019-03-03 03:37:06.764607
last batch sz 160
Model ind 640 epoch 1131 head B head_i_epoch 0 batch 0: avg loss -1.989842 avg loss no lamb -1.989842 time 2019-03-03 03:39:43.212558
Model ind 640 epoch 1131 head B head_i_epoch 0 batch 100: avg loss -1.810961 avg loss no lamb -1.810961 time 2019-03-03 03:43:18.409860
Model ind 640 epoch 1131 head B head_i_epoch 0 batch 200: avg loss -1.991425 avg loss no lamb -1.991425 time 2019-03-03 03:46:21.334176
last batch sz 160
Model ind 640 epoch 1131 head B head_i_epoch 1 batch 0: avg loss -1.953919 avg loss no lamb -1.953919 time 2019-03-03 03:48:38.074420
Model ind 640 epoch 1131 head B head_i_epoch 1 batch 100: avg loss -1.826025 avg loss no lamb -1.826025 time 2019-03-03 03:52:12.591313
Model ind 640 epoch 1131 head B head_i_epoch 1 batch 200: avg loss -1.975976 avg loss no lamb -1.975976 time 2019-03-03 03:55:47.454106
last batch sz 160
Pre: time 2019-03-03 03:58:42.607194: 
 	std: 0.050450157
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5110667, 0.6143, 0.6142333, 0.6142833, 0.5115167]
	train_accs: [0.5110667, 0.6143, 0.6142333, 0.6142833, 0.5115167]
	best_train_sub_head: 1
	worst: 0.5110667
	avg: 0.57307994
	best: 0.6143

Starting e_i: 1132
Model ind 640 epoch 1132 head A head_i_epoch 0 batch 0: avg loss -3.274905 avg loss no lamb -3.274905 time 2019-03-03 03:58:46.854266
Model ind 640 epoch 1132 head A head_i_epoch 0 batch 100: avg loss -3.283459 avg loss no lamb -3.283459 time 2019-03-03 04:01:54.039364
Model ind 640 epoch 1132 head A head_i_epoch 0 batch 200: avg loss -3.339636 avg loss no lamb -3.339636 time 2019-03-03 04:05:27.649712
last batch sz 160
Model ind 640 epoch 1132 head B head_i_epoch 0 batch 0: avg loss -1.925497 avg loss no lamb -1.925497 time 2019-03-03 04:08:03.785192
Model ind 640 epoch 1132 head B head_i_epoch 0 batch 100: avg loss -1.920548 avg loss no lamb -1.920548 time 2019-03-03 04:11:28.162120
Model ind 640 epoch 1132 head B head_i_epoch 0 batch 200: avg loss -1.948502 avg loss no lamb -1.948502 time 2019-03-03 04:14:23.152177
last batch sz 160
Model ind 640 epoch 1132 head B head_i_epoch 1 batch 0: avg loss -1.863569 avg loss no lamb -1.863569 time 2019-03-03 04:16:57.243422
Model ind 640 epoch 1132 head B head_i_epoch 1 batch 100: avg loss -1.900384 avg loss no lamb -1.900384 time 2019-03-03 04:20:33.270928
Model ind 640 epoch 1132 head B head_i_epoch 1 batch 200: avg loss -2.020377 avg loss no lamb -2.020377 time 2019-03-03 04:24:06.744191
last batch sz 160
Pre: time 2019-03-03 04:26:31.218241: 
 	std: 0.050777946
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107167, 0.6142833, 0.6142667, 0.6143, 0.51055]
	train_accs: [0.5107167, 0.6142833, 0.6142667, 0.6143, 0.51055]
	best_train_sub_head: 3
	worst: 0.51055
	avg: 0.57282335
	best: 0.6143

Starting e_i: 1133
Model ind 640 epoch 1133 head A head_i_epoch 0 batch 0: avg loss -3.328587 avg loss no lamb -3.328587 time 2019-03-03 04:26:34.822907
Model ind 640 epoch 1133 head A head_i_epoch 0 batch 100: avg loss -3.168507 avg loss no lamb -3.168507 time 2019-03-03 04:30:06.148389
Model ind 640 epoch 1133 head A head_i_epoch 0 batch 200: avg loss -3.246163 avg loss no lamb -3.246163 time 2019-03-03 04:33:38.562249
last batch sz 160
Model ind 640 epoch 1133 head B head_i_epoch 0 batch 0: avg loss -1.947115 avg loss no lamb -1.947115 time 2019-03-03 04:36:14.489905
Model ind 640 epoch 1133 head B head_i_epoch 0 batch 100: avg loss -1.903339 avg loss no lamb -1.903339 time 2019-03-03 04:38:53.579267
Model ind 640 epoch 1133 head B head_i_epoch 0 batch 200: avg loss -1.927166 avg loss no lamb -1.927166 time 2019-03-03 04:42:26.074019
last batch sz 160
Model ind 640 epoch 1133 head B head_i_epoch 1 batch 0: avg loss -1.902378 avg loss no lamb -1.902378 time 2019-03-03 04:45:02.183009
Model ind 640 epoch 1133 head B head_i_epoch 1 batch 100: avg loss -1.889699 avg loss no lamb -1.889699 time 2019-03-03 04:48:37.479343
Model ind 640 epoch 1133 head B head_i_epoch 1 batch 200: avg loss -1.976305 avg loss no lamb -1.976305 time 2019-03-03 04:51:28.115085
last batch sz 160
Pre: time 2019-03-03 04:54:27.560421: 
 	std: 0.050656956
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51121664, 0.61455, 0.6145, 0.61433333, 0.5109]
	train_accs: [0.51121664, 0.61455, 0.6145, 0.61433333, 0.5109]
	best_train_sub_head: 1
	worst: 0.5109
	avg: 0.5731
	best: 0.61455

Starting e_i: 1134
Model ind 640 epoch 1134 head A head_i_epoch 0 batch 0: avg loss -3.260339 avg loss no lamb -3.260339 time 2019-03-03 04:54:31.257836
Model ind 640 epoch 1134 head A head_i_epoch 0 batch 100: avg loss -3.238367 avg loss no lamb -3.238367 time 2019-03-03 04:58:05.976654
Model ind 640 epoch 1134 head A head_i_epoch 0 batch 200: avg loss -3.283999 avg loss no lamb -3.283999 time 2019-03-03 05:01:39.870830
last batch sz 160
Model ind 640 epoch 1134 head B head_i_epoch 0 batch 0: avg loss -1.979568 avg loss no lamb -1.979568 time 2019-03-03 05:03:53.136082
Model ind 640 epoch 1134 head B head_i_epoch 0 batch 100: avg loss -1.836038 avg loss no lamb -1.836038 time 2019-03-03 05:07:00.762705
Model ind 640 epoch 1134 head B head_i_epoch 0 batch 200: avg loss -1.961836 avg loss no lamb -1.961836 time 2019-03-03 05:10:33.054166
last batch sz 160
Model ind 640 epoch 1134 head B head_i_epoch 1 batch 0: avg loss -1.916659 avg loss no lamb -1.916659 time 2019-03-03 05:13:09.677313
Model ind 640 epoch 1134 head B head_i_epoch 1 batch 100: avg loss -1.954861 avg loss no lamb -1.954861 time 2019-03-03 05:16:28.604004
Model ind 640 epoch 1134 head B head_i_epoch 1 batch 200: avg loss -1.938080 avg loss no lamb -1.938080 time 2019-03-03 05:19:19.767604
last batch sz 160
Pre: time 2019-03-03 05:22:32.995037: 
 	std: 0.05070855
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108333, 0.61436665, 0.61431664, 0.61446667, 0.51091665]
	train_accs: [0.5108333, 0.61436665, 0.61431664, 0.61446667, 0.51091665]
	best_train_sub_head: 3
	worst: 0.5108333
	avg: 0.57298005
	best: 0.61446667

Starting e_i: 1135
Model ind 640 epoch 1135 head A head_i_epoch 0 batch 0: avg loss -3.271662 avg loss no lamb -3.271662 time 2019-03-03 05:22:36.737716
Model ind 640 epoch 1135 head A head_i_epoch 0 batch 100: avg loss -3.198479 avg loss no lamb -3.198479 time 2019-03-03 05:26:11.078929
Model ind 640 epoch 1135 head A head_i_epoch 0 batch 200: avg loss -3.279965 avg loss no lamb -3.279965 time 2019-03-03 05:29:30.531034
last batch sz 160
Model ind 640 epoch 1135 head B head_i_epoch 0 batch 0: avg loss -1.920642 avg loss no lamb -1.920642 time 2019-03-03 05:31:29.026283
Model ind 640 epoch 1135 head B head_i_epoch 0 batch 100: avg loss -1.900118 avg loss no lamb -1.900118 time 2019-03-03 05:35:04.156907
Model ind 640 epoch 1135 head B head_i_epoch 0 batch 200: avg loss -1.968681 avg loss no lamb -1.968681 time 2019-03-03 05:38:36.832001
last batch sz 160
Model ind 640 epoch 1135 head B head_i_epoch 1 batch 0: avg loss -1.917136 avg loss no lamb -1.917136 time 2019-03-03 05:41:12.818932
Model ind 640 epoch 1135 head B head_i_epoch 1 batch 100: avg loss -1.885244 avg loss no lamb -1.885244 time 2019-03-03 05:43:57.689948
Model ind 640 epoch 1135 head B head_i_epoch 1 batch 200: avg loss -2.006953 avg loss no lamb -2.006953 time 2019-03-03 05:47:29.177648
last batch sz 160
Pre: time 2019-03-03 05:50:40.656600: 
 	std: 0.05063929
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5110667, 0.61475, 0.61448336, 0.61446667, 0.51133335]
	train_accs: [0.5110667, 0.61475, 0.61448336, 0.61446667, 0.51133335]
	best_train_sub_head: 1
	worst: 0.5110667
	avg: 0.5732201
	best: 0.61475

Starting e_i: 1136
Model ind 640 epoch 1136 head A head_i_epoch 0 batch 0: avg loss -3.242081 avg loss no lamb -3.242081 time 2019-03-03 05:50:44.297316
Model ind 640 epoch 1136 head A head_i_epoch 0 batch 100: avg loss -3.201874 avg loss no lamb -3.201874 time 2019-03-03 05:54:16.099772
Model ind 640 epoch 1136 head A head_i_epoch 0 batch 200: avg loss -3.276313 avg loss no lamb -3.276313 time 2019-03-03 05:57:00.552805
last batch sz 160
Model ind 640 epoch 1136 head B head_i_epoch 0 batch 0: avg loss -1.900038 avg loss no lamb -1.900038 time 2019-03-03 05:59:35.220929
Model ind 640 epoch 1136 head B head_i_epoch 0 batch 100: avg loss -1.855880 avg loss no lamb -1.855880 time 2019-03-03 06:03:08.300088
Model ind 640 epoch 1136 head B head_i_epoch 0 batch 200: avg loss -1.900018 avg loss no lamb -1.900018 time 2019-03-03 06:06:43.273033
last batch sz 160
Model ind 640 epoch 1136 head B head_i_epoch 1 batch 0: avg loss -1.910059 avg loss no lamb -1.910059 time 2019-03-03 06:08:53.272458
Model ind 640 epoch 1136 head B head_i_epoch 1 batch 100: avg loss -1.910645 avg loss no lamb -1.910645 time 2019-03-03 06:12:01.617063
Model ind 640 epoch 1136 head B head_i_epoch 1 batch 200: avg loss -2.013715 avg loss no lamb -2.013715 time 2019-03-03 06:15:33.008066
last batch sz 160
Pre: time 2019-03-03 06:18:44.562968: 
 	std: 0.051048763
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5096, 0.6139167, 0.61385, 0.6138667, 0.50975]
	train_accs: [0.5096, 0.6139167, 0.61385, 0.6138667, 0.50975]
	best_train_sub_head: 1
	worst: 0.5096
	avg: 0.57219666
	best: 0.6139167

Starting e_i: 1137
Model ind 640 epoch 1137 head A head_i_epoch 0 batch 0: avg loss -3.302448 avg loss no lamb -3.302448 time 2019-03-03 06:18:49.155045
Model ind 640 epoch 1137 head A head_i_epoch 0 batch 100: avg loss -3.196672 avg loss no lamb -3.196672 time 2019-03-03 06:21:53.596029
Model ind 640 epoch 1137 head A head_i_epoch 0 batch 200: avg loss -3.288796 avg loss no lamb -3.288796 time 2019-03-03 06:25:01.024692
last batch sz 160
Model ind 640 epoch 1137 head B head_i_epoch 0 batch 0: avg loss -1.932081 avg loss no lamb -1.932081 time 2019-03-03 06:27:40.303969
Model ind 640 epoch 1137 head B head_i_epoch 0 batch 100: avg loss -1.917455 avg loss no lamb -1.917455 time 2019-03-03 06:31:14.592616
Model ind 640 epoch 1137 head B head_i_epoch 0 batch 200: avg loss -1.971112 avg loss no lamb -1.971112 time 2019-03-03 06:34:34.743621
last batch sz 160
Model ind 640 epoch 1137 head B head_i_epoch 1 batch 0: avg loss -1.956856 avg loss no lamb -1.956856 time 2019-03-03 06:36:31.943685
Model ind 640 epoch 1137 head B head_i_epoch 1 batch 100: avg loss -1.910735 avg loss no lamb -1.910735 time 2019-03-03 06:40:07.668592
Model ind 640 epoch 1137 head B head_i_epoch 1 batch 200: avg loss -1.951378 avg loss no lamb -1.951378 time 2019-03-03 06:43:38.932298
last batch sz 160
Pre: time 2019-03-03 06:46:51.376657: 
 	std: 0.051159
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50976664, 0.6141, 0.6141667, 0.6142667, 0.5097333]
	train_accs: [0.50976664, 0.6141, 0.6141667, 0.6142667, 0.5097333]
	best_train_sub_head: 3
	worst: 0.5097333
	avg: 0.57240665
	best: 0.6142667

Starting e_i: 1138
Model ind 640 epoch 1138 head A head_i_epoch 0 batch 0: avg loss -3.272130 avg loss no lamb -3.272130 time 2019-03-03 06:46:55.291028
Model ind 640 epoch 1138 head A head_i_epoch 0 batch 100: avg loss -3.268726 avg loss no lamb -3.268726 time 2019-03-03 06:49:40.357939
Model ind 640 epoch 1138 head A head_i_epoch 0 batch 200: avg loss -3.290894 avg loss no lamb -3.290894 time 2019-03-03 06:53:15.248447
last batch sz 160
Model ind 640 epoch 1138 head B head_i_epoch 0 batch 0: avg loss -1.870598 avg loss no lamb -1.870598 time 2019-03-03 06:55:51.070075
Model ind 640 epoch 1138 head B head_i_epoch 0 batch 100: avg loss -1.928790 avg loss no lamb -1.928790 time 2019-03-03 06:59:26.853089
Model ind 640 epoch 1138 head B head_i_epoch 0 batch 200: avg loss -1.978188 avg loss no lamb -1.978188 time 2019-03-03 07:02:10.170815
last batch sz 160
Model ind 640 epoch 1138 head B head_i_epoch 1 batch 0: avg loss -1.961591 avg loss no lamb -1.961591 time 2019-03-03 07:04:45.952232
Model ind 640 epoch 1138 head B head_i_epoch 1 batch 100: avg loss -1.875448 avg loss no lamb -1.875448 time 2019-03-03 07:08:20.403434
Model ind 640 epoch 1138 head B head_i_epoch 1 batch 200: avg loss -1.976084 avg loss no lamb -1.976084 time 2019-03-03 07:11:56.193694
last batch sz 160
Pre: time 2019-03-03 07:14:26.337209: 
 	std: 0.050470483
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51115, 0.61413336, 0.6144, 0.6142333, 0.51131666]
	train_accs: [0.51115, 0.61413336, 0.6144, 0.6142333, 0.51131666]
	best_train_sub_head: 2
	worst: 0.51115
	avg: 0.5730467
	best: 0.6144

Starting e_i: 1139
Model ind 640 epoch 1139 head A head_i_epoch 0 batch 0: avg loss -3.236031 avg loss no lamb -3.236031 time 2019-03-03 07:14:29.396147
Model ind 640 epoch 1139 head A head_i_epoch 0 batch 100: avg loss -3.139045 avg loss no lamb -3.139045 time 2019-03-03 07:17:59.288086
Model ind 640 epoch 1139 head A head_i_epoch 0 batch 200: avg loss -3.244515 avg loss no lamb -3.244515 time 2019-03-03 07:21:32.421951
last batch sz 160
Model ind 640 epoch 1139 head B head_i_epoch 0 batch 0: avg loss -1.913988 avg loss no lamb -1.913988 time 2019-03-03 07:24:09.645341
Model ind 640 epoch 1139 head B head_i_epoch 0 batch 100: avg loss -1.927869 avg loss no lamb -1.927869 time 2019-03-03 07:27:10.876588
Model ind 640 epoch 1139 head B head_i_epoch 0 batch 200: avg loss -1.946214 avg loss no lamb -1.946214 time 2019-03-03 07:30:27.197292
last batch sz 160
Model ind 640 epoch 1139 head B head_i_epoch 1 batch 0: avg loss -1.920945 avg loss no lamb -1.920945 time 2019-03-03 07:33:02.711516
Model ind 640 epoch 1139 head B head_i_epoch 1 batch 100: avg loss -1.846206 avg loss no lamb -1.846206 time 2019-03-03 07:36:37.393204
Model ind 640 epoch 1139 head B head_i_epoch 1 batch 200: avg loss -1.993081 avg loss no lamb -1.993081 time 2019-03-03 07:39:43.112195
last batch sz 160
Pre: time 2019-03-03 07:42:31.031336: 
 	std: 0.050739832
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51058334, 0.61405, 0.61411667, 0.61415, 0.5104833]
	train_accs: [0.51058334, 0.61405, 0.61411667, 0.61415, 0.5104833]
	best_train_sub_head: 3
	worst: 0.5104833
	avg: 0.57267666
	best: 0.61415

Starting e_i: 1140
Model ind 640 epoch 1140 head A head_i_epoch 0 batch 0: avg loss -3.248850 avg loss no lamb -3.248850 time 2019-03-03 07:42:34.580226
Model ind 640 epoch 1140 head A head_i_epoch 0 batch 100: avg loss -3.258454 avg loss no lamb -3.258454 time 2019-03-03 07:46:04.849293
Model ind 640 epoch 1140 head A head_i_epoch 0 batch 200: avg loss -3.238994 avg loss no lamb -3.238994 time 2019-03-03 07:49:39.367432
last batch sz 160
Model ind 640 epoch 1140 head B head_i_epoch 0 batch 0: avg loss -1.940572 avg loss no lamb -1.940572 time 2019-03-03 07:52:11.737036
Model ind 640 epoch 1140 head B head_i_epoch 0 batch 100: avg loss -1.902622 avg loss no lamb -1.902622 time 2019-03-03 07:54:57.484493
Model ind 640 epoch 1140 head B head_i_epoch 0 batch 200: avg loss -1.966034 avg loss no lamb -1.966034 time 2019-03-03 07:58:31.593473
last batch sz 160
Model ind 640 epoch 1140 head B head_i_epoch 1 batch 0: avg loss -1.917911 avg loss no lamb -1.917911 time 2019-03-03 08:01:07.861931
Model ind 640 epoch 1140 head B head_i_epoch 1 batch 100: avg loss -1.872029 avg loss no lamb -1.872029 time 2019-03-03 08:04:42.848645
Model ind 640 epoch 1140 head B head_i_epoch 1 batch 200: avg loss -1.987716 avg loss no lamb -1.987716 time 2019-03-03 08:07:23.850174
last batch sz 160
Pre: time 2019-03-03 08:10:37.110126: 
 	std: 0.05123794
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51056665, 0.61523336, 0.61518335, 0.6153, 0.5107333]
	train_accs: [0.51056665, 0.61523336, 0.61518335, 0.6153, 0.5107333]
	best_train_sub_head: 3
	worst: 0.51056665
	avg: 0.57340336
	best: 0.6153

Starting e_i: 1141
Model ind 640 epoch 1141 head A head_i_epoch 0 batch 0: avg loss -3.332383 avg loss no lamb -3.332383 time 2019-03-03 08:10:45.784864
Model ind 640 epoch 1141 head A head_i_epoch 0 batch 100: avg loss -3.168794 avg loss no lamb -3.168794 time 2019-03-03 08:14:15.030037
Model ind 640 epoch 1141 head A head_i_epoch 0 batch 200: avg loss -3.266162 avg loss no lamb -3.266162 time 2019-03-03 08:17:49.459292
last batch sz 160
Model ind 640 epoch 1141 head B head_i_epoch 0 batch 0: avg loss -1.950989 avg loss no lamb -1.950989 time 2019-03-03 08:19:33.588191
Model ind 640 epoch 1141 head B head_i_epoch 0 batch 100: avg loss -1.921152 avg loss no lamb -1.921152 time 2019-03-03 08:23:08.829600
Model ind 640 epoch 1141 head B head_i_epoch 0 batch 200: avg loss -1.941260 avg loss no lamb -1.941260 time 2019-03-03 08:26:41.526565
last batch sz 160
Model ind 640 epoch 1141 head B head_i_epoch 1 batch 0: avg loss -1.969265 avg loss no lamb -1.969265 time 2019-03-03 08:29:17.343270
Model ind 640 epoch 1141 head B head_i_epoch 1 batch 100: avg loss -1.869189 avg loss no lamb -1.869189 time 2019-03-03 08:32:12.175141
Model ind 640 epoch 1141 head B head_i_epoch 1 batch 200: avg loss -1.955474 avg loss no lamb -1.955474 time 2019-03-03 08:35:32.096994
last batch sz 160
Pre: time 2019-03-03 08:38:44.013715: 
 	std: 0.050611954
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118167, 0.6149667, 0.61515, 0.61516666, 0.51175]
	train_accs: [0.5118167, 0.6149667, 0.61515, 0.61516666, 0.51175]
	best_train_sub_head: 3
	worst: 0.51175
	avg: 0.57377
	best: 0.61516666

Starting e_i: 1142
Model ind 640 epoch 1142 head A head_i_epoch 0 batch 0: avg loss -3.252005 avg loss no lamb -3.252005 time 2019-03-03 08:38:47.806632
Model ind 640 epoch 1142 head A head_i_epoch 0 batch 100: avg loss -3.204196 avg loss no lamb -3.204196 time 2019-03-03 08:42:20.950762
Model ind 640 epoch 1142 head A head_i_epoch 0 batch 200: avg loss -3.278800 avg loss no lamb -3.278800 time 2019-03-03 08:45:14.980660
last batch sz 160
Model ind 640 epoch 1142 head B head_i_epoch 0 batch 0: avg loss -1.946806 avg loss no lamb -1.946806 time 2019-03-03 08:47:43.753240
Model ind 640 epoch 1142 head B head_i_epoch 0 batch 100: avg loss -1.911361 avg loss no lamb -1.911361 time 2019-03-03 08:51:17.874410
Model ind 640 epoch 1142 head B head_i_epoch 0 batch 200: avg loss -2.001217 avg loss no lamb -2.001217 time 2019-03-03 08:54:52.500416
last batch sz 160
Model ind 640 epoch 1142 head B head_i_epoch 1 batch 0: avg loss -1.935869 avg loss no lamb -1.935869 time 2019-03-03 08:57:15.063331
Model ind 640 epoch 1142 head B head_i_epoch 1 batch 100: avg loss -1.906980 avg loss no lamb -1.906980 time 2019-03-03 09:00:10.880853
Model ind 640 epoch 1142 head B head_i_epoch 1 batch 200: avg loss -2.035681 avg loss no lamb -2.035681 time 2019-03-03 09:03:45.027898
last batch sz 160
Pre: time 2019-03-03 09:06:58.328302: 
 	std: 0.050617356
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51055, 0.6139333, 0.6139, 0.6139333, 0.51065]
	train_accs: [0.51055, 0.6139333, 0.6139, 0.6139333, 0.51065]
	best_train_sub_head: 1
	worst: 0.51055
	avg: 0.57259333
	best: 0.6139333

Starting e_i: 1143
Model ind 640 epoch 1143 head A head_i_epoch 0 batch 0: avg loss -3.261272 avg loss no lamb -3.261272 time 2019-03-03 09:07:01.916362
Model ind 640 epoch 1143 head A head_i_epoch 0 batch 100: avg loss -3.334746 avg loss no lamb -3.334746 time 2019-03-03 09:10:17.274183
Model ind 640 epoch 1143 head A head_i_epoch 0 batch 200: avg loss -3.393621 avg loss no lamb -3.393621 time 2019-03-03 09:13:16.461712
last batch sz 160
Model ind 640 epoch 1143 head B head_i_epoch 0 batch 0: avg loss -1.944927 avg loss no lamb -1.944927 time 2019-03-03 09:15:52.452522
Model ind 640 epoch 1143 head B head_i_epoch 0 batch 100: avg loss -1.796796 avg loss no lamb -1.796796 time 2019-03-03 09:19:25.988671
Model ind 640 epoch 1143 head B head_i_epoch 0 batch 200: avg loss -1.986272 avg loss no lamb -1.986272 time 2019-03-03 09:23:00.650857
last batch sz 160
Model ind 640 epoch 1143 head B head_i_epoch 1 batch 0: avg loss -1.906930 avg loss no lamb -1.906930 time 2019-03-03 09:24:47.106659
Model ind 640 epoch 1143 head B head_i_epoch 1 batch 100: avg loss -1.894087 avg loss no lamb -1.894087 time 2019-03-03 09:28:22.243892
Model ind 640 epoch 1143 head B head_i_epoch 1 batch 200: avg loss -2.070971 avg loss no lamb -2.070971 time 2019-03-03 09:31:54.573131
last batch sz 160
Pre: time 2019-03-03 09:35:05.089756: 
 	std: 0.050927702
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51063335, 0.61476666, 0.61448336, 0.6146167, 0.5107]
	train_accs: [0.51063335, 0.61476666, 0.61448336, 0.6146167, 0.5107]
	best_train_sub_head: 1
	worst: 0.51063335
	avg: 0.57304
	best: 0.61476666

Starting e_i: 1144
Model ind 640 epoch 1144 head A head_i_epoch 0 batch 0: avg loss -3.219765 avg loss no lamb -3.219765 time 2019-03-03 09:35:08.842681
Model ind 640 epoch 1144 head A head_i_epoch 0 batch 100: avg loss -3.221339 avg loss no lamb -3.221339 time 2019-03-03 09:37:46.163779
Model ind 640 epoch 1144 head A head_i_epoch 0 batch 200: avg loss -3.354414 avg loss no lamb -3.354414 time 2019-03-03 09:41:21.105693
last batch sz 160
Model ind 640 epoch 1144 head B head_i_epoch 0 batch 0: avg loss -1.926128 avg loss no lamb -1.926128 time 2019-03-03 09:43:56.629461
Model ind 640 epoch 1144 head B head_i_epoch 0 batch 100: avg loss -1.872360 avg loss no lamb -1.872360 time 2019-03-03 09:47:29.363666
Model ind 640 epoch 1144 head B head_i_epoch 0 batch 200: avg loss -1.945262 avg loss no lamb -1.945262 time 2019-03-03 09:50:17.100830
last batch sz 160
Model ind 640 epoch 1144 head B head_i_epoch 1 batch 0: avg loss -1.900669 avg loss no lamb -1.900669 time 2019-03-03 09:52:47.603120
Model ind 640 epoch 1144 head B head_i_epoch 1 batch 100: avg loss -1.901991 avg loss no lamb -1.901991 time 2019-03-03 09:56:21.760407
Model ind 640 epoch 1144 head B head_i_epoch 1 batch 200: avg loss -1.939168 avg loss no lamb -1.939168 time 2019-03-03 09:59:55.392127
last batch sz 160
Pre: time 2019-03-03 10:02:42.852863: 
 	std: 0.050572503
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50981665, 0.6131667, 0.6131667, 0.6131833, 0.5100667]
	train_accs: [0.50981665, 0.6131667, 0.6131667, 0.6131833, 0.5100667]
	best_train_sub_head: 3
	worst: 0.50981665
	avg: 0.57188
	best: 0.6131833

Starting e_i: 1145
Model ind 640 epoch 1145 head A head_i_epoch 0 batch 0: avg loss -3.323555 avg loss no lamb -3.323555 time 2019-03-03 10:02:47.190222
Model ind 640 epoch 1145 head A head_i_epoch 0 batch 100: avg loss -3.212229 avg loss no lamb -3.212229 time 2019-03-03 10:05:57.276335
Model ind 640 epoch 1145 head A head_i_epoch 0 batch 200: avg loss -3.252414 avg loss no lamb -3.252414 time 2019-03-03 10:09:31.388803
last batch sz 160
Model ind 640 epoch 1145 head B head_i_epoch 0 batch 0: avg loss -1.963467 avg loss no lamb -1.963467 time 2019-03-03 10:12:08.243757
Model ind 640 epoch 1145 head B head_i_epoch 0 batch 100: avg loss -1.813703 avg loss no lamb -1.813703 time 2019-03-03 10:15:21.858855
Model ind 640 epoch 1145 head B head_i_epoch 0 batch 200: avg loss -1.949012 avg loss no lamb -1.949012 time 2019-03-03 10:18:23.711890
last batch sz 160
Model ind 640 epoch 1145 head B head_i_epoch 1 batch 0: avg loss -1.918297 avg loss no lamb -1.918297 time 2019-03-03 10:20:58.715469
Model ind 640 epoch 1145 head B head_i_epoch 1 batch 100: avg loss -1.872409 avg loss no lamb -1.872409 time 2019-03-03 10:24:36.384825
Model ind 640 epoch 1145 head B head_i_epoch 1 batch 200: avg loss -2.024709 avg loss no lamb -2.024709 time 2019-03-03 10:28:09.815497
last batch sz 160
Pre: time 2019-03-03 10:30:34.815195: 
 	std: 0.050394196
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5122167, 0.61508334, 0.61513335, 0.6149833, 0.5121833]
	train_accs: [0.5122167, 0.61508334, 0.61513335, 0.6149833, 0.5121833]
	best_train_sub_head: 2
	worst: 0.5121833
	avg: 0.57391995
	best: 0.61513335

Starting e_i: 1146
Model ind 640 epoch 1146 head A head_i_epoch 0 batch 0: avg loss -3.301478 avg loss no lamb -3.301478 time 2019-03-03 10:30:38.224338
Model ind 640 epoch 1146 head A head_i_epoch 0 batch 100: avg loss -3.111664 avg loss no lamb -3.111664 time 2019-03-03 10:34:09.888504
Model ind 640 epoch 1146 head A head_i_epoch 0 batch 200: avg loss -3.220825 avg loss no lamb -3.220825 time 2019-03-03 10:37:43.625772
last batch sz 160
Model ind 640 epoch 1146 head B head_i_epoch 0 batch 0: avg loss -1.947499 avg loss no lamb -1.947499 time 2019-03-03 10:40:20.690511
Model ind 640 epoch 1146 head B head_i_epoch 0 batch 100: avg loss -1.880196 avg loss no lamb -1.880196 time 2019-03-03 10:43:05.108365
Model ind 640 epoch 1146 head B head_i_epoch 0 batch 200: avg loss -1.955292 avg loss no lamb -1.955292 time 2019-03-03 10:46:40.351385
last batch sz 160
Model ind 640 epoch 1146 head B head_i_epoch 1 batch 0: avg loss -1.893888 avg loss no lamb -1.893888 time 2019-03-03 10:49:13.295264
Model ind 640 epoch 1146 head B head_i_epoch 1 batch 100: avg loss -1.866783 avg loss no lamb -1.866783 time 2019-03-03 10:52:46.535165
Model ind 640 epoch 1146 head B head_i_epoch 1 batch 200: avg loss -2.005535 avg loss no lamb -2.005535 time 2019-03-03 10:55:41.572725
last batch sz 160
Pre: time 2019-03-03 10:58:41.179285: 
 	std: 0.050524868
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118167, 0.61505, 0.61508334, 0.61506665, 0.51205]
	train_accs: [0.5118167, 0.61505, 0.61508334, 0.61506665, 0.51205]
	best_train_sub_head: 2
	worst: 0.5118167
	avg: 0.5738133
	best: 0.61508334

Starting e_i: 1147
Model ind 640 epoch 1147 head A head_i_epoch 0 batch 0: avg loss -3.195654 avg loss no lamb -3.195654 time 2019-03-03 10:58:45.073131
Model ind 640 epoch 1147 head A head_i_epoch 0 batch 100: avg loss -3.237661 avg loss no lamb -3.237661 time 2019-03-03 11:02:19.367308
Model ind 640 epoch 1147 head A head_i_epoch 0 batch 200: avg loss -3.272865 avg loss no lamb -3.272865 time 2019-03-03 11:05:51.812550
last batch sz 160
Model ind 640 epoch 1147 head B head_i_epoch 0 batch 0: avg loss -1.903200 avg loss no lamb -1.903200 time 2019-03-03 11:08:07.213572
Model ind 640 epoch 1147 head B head_i_epoch 0 batch 100: avg loss -1.861230 avg loss no lamb -1.861230 time 2019-03-03 11:11:13.405865
Model ind 640 epoch 1147 head B head_i_epoch 0 batch 200: avg loss -2.022236 avg loss no lamb -2.022236 time 2019-03-03 11:14:48.596275
last batch sz 160
Model ind 640 epoch 1147 head B head_i_epoch 1 batch 0: avg loss -1.948765 avg loss no lamb -1.948765 time 2019-03-03 11:17:24.563306
Model ind 640 epoch 1147 head B head_i_epoch 1 batch 100: avg loss -1.811415 avg loss no lamb -1.811415 time 2019-03-03 11:20:55.916413
Model ind 640 epoch 1147 head B head_i_epoch 1 batch 200: avg loss -1.946272 avg loss no lamb -1.946272 time 2019-03-03 11:23:40.743489
last batch sz 160
Pre: time 2019-03-03 11:26:54.755397: 
 	std: 0.051052853
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50981665, 0.61396664, 0.61415, 0.61396664, 0.50981665]
	train_accs: [0.50981665, 0.61396664, 0.61415, 0.61396664, 0.50981665]
	best_train_sub_head: 2
	worst: 0.50981665
	avg: 0.57234335
	best: 0.61415

Starting e_i: 1148
Model ind 640 epoch 1148 head A head_i_epoch 0 batch 0: avg loss -3.342388 avg loss no lamb -3.342388 time 2019-03-03 11:26:58.593928
Model ind 640 epoch 1148 head A head_i_epoch 0 batch 100: avg loss -3.204098 avg loss no lamb -3.204098 time 2019-03-03 11:30:31.625239
Model ind 640 epoch 1148 head A head_i_epoch 0 batch 200: avg loss -3.318661 avg loss no lamb -3.318661 time 2019-03-03 11:34:05.873771
last batch sz 160
Model ind 640 epoch 1148 head B head_i_epoch 0 batch 0: avg loss -2.001642 avg loss no lamb -2.001642 time 2019-03-03 11:35:52.720538
Model ind 640 epoch 1148 head B head_i_epoch 0 batch 100: avg loss -1.832863 avg loss no lamb -1.832863 time 2019-03-03 11:39:26.845831
Model ind 640 epoch 1148 head B head_i_epoch 0 batch 200: avg loss -1.917593 avg loss no lamb -1.917593 time 2019-03-03 11:43:01.271345
last batch sz 160
Model ind 640 epoch 1148 head B head_i_epoch 1 batch 0: avg loss -1.911586 avg loss no lamb -1.911586 time 2019-03-03 11:45:38.131730
Model ind 640 epoch 1148 head B head_i_epoch 1 batch 100: avg loss -1.871925 avg loss no lamb -1.871925 time 2019-03-03 11:48:30.409480
Model ind 640 epoch 1148 head B head_i_epoch 1 batch 200: avg loss -1.994618 avg loss no lamb -1.994618 time 2019-03-03 11:51:56.394133
last batch sz 160
Pre: time 2019-03-03 11:55:04.765171: 
 	std: 0.050371066
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51068336, 0.61345, 0.6134, 0.61338335, 0.5105]
	train_accs: [0.51068336, 0.61345, 0.6134, 0.61338335, 0.5105]
	best_train_sub_head: 1
	worst: 0.5105
	avg: 0.5722833
	best: 0.61345

Starting e_i: 1149
Model ind 640 epoch 1149 head A head_i_epoch 0 batch 0: avg loss -3.268339 avg loss no lamb -3.268339 time 2019-03-03 11:55:07.907842
Model ind 640 epoch 1149 head A head_i_epoch 0 batch 100: avg loss -3.301553 avg loss no lamb -3.301553 time 2019-03-03 11:58:41.789870
Model ind 640 epoch 1149 head A head_i_epoch 0 batch 200: avg loss -3.322779 avg loss no lamb -3.322779 time 2019-03-03 12:01:39.583239
last batch sz 160
Model ind 640 epoch 1149 head B head_i_epoch 0 batch 0: avg loss -1.889846 avg loss no lamb -1.889846 time 2019-03-03 12:03:59.638091
Model ind 640 epoch 1149 head B head_i_epoch 0 batch 100: avg loss -1.893971 avg loss no lamb -1.893971 time 2019-03-03 12:07:32.216677
Model ind 640 epoch 1149 head B head_i_epoch 0 batch 200: avg loss -1.975707 avg loss no lamb -1.975707 time 2019-03-03 12:11:06.495293
last batch sz 160
Model ind 640 epoch 1149 head B head_i_epoch 1 batch 0: avg loss -1.994668 avg loss no lamb -1.994668 time 2019-03-03 12:13:42.878895
Model ind 640 epoch 1149 head B head_i_epoch 1 batch 100: avg loss -1.941303 avg loss no lamb -1.941303 time 2019-03-03 12:16:25.899203
Model ind 640 epoch 1149 head B head_i_epoch 1 batch 200: avg loss -1.964295 avg loss no lamb -1.964295 time 2019-03-03 12:20:00.353093
last batch sz 160
Pre: time 2019-03-03 12:23:13.357643: 
 	std: 0.050432324
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51215, 0.6149333, 0.615, 0.61505, 0.51195]
	train_accs: [0.51215, 0.6149333, 0.615, 0.61505, 0.51195]
	best_train_sub_head: 3
	worst: 0.51195
	avg: 0.57381666
	best: 0.61505

Starting e_i: 1150
Model ind 640 epoch 1150 head A head_i_epoch 0 batch 0: avg loss -3.252587 avg loss no lamb -3.252587 time 2019-03-03 12:23:17.850932
Model ind 640 epoch 1150 head A head_i_epoch 0 batch 100: avg loss -3.224317 avg loss no lamb -3.224317 time 2019-03-03 12:26:47.564527
Model ind 640 epoch 1150 head A head_i_epoch 0 batch 200: avg loss -3.286849 avg loss no lamb -3.286849 time 2019-03-03 12:29:34.577567
last batch sz 160
Model ind 640 epoch 1150 head B head_i_epoch 0 batch 0: avg loss -1.970292 avg loss no lamb -1.970292 time 2019-03-03 12:32:11.309345
Model ind 640 epoch 1150 head B head_i_epoch 0 batch 100: avg loss -1.919120 avg loss no lamb -1.919120 time 2019-03-03 12:35:49.108873
Model ind 640 epoch 1150 head B head_i_epoch 0 batch 200: avg loss -1.974409 avg loss no lamb -1.974409 time 2019-03-03 12:39:25.496456
last batch sz 160
Model ind 640 epoch 1150 head B head_i_epoch 1 batch 0: avg loss -1.920716 avg loss no lamb -1.920716 time 2019-03-03 12:41:16.921078
Model ind 640 epoch 1150 head B head_i_epoch 1 batch 100: avg loss -1.900757 avg loss no lamb -1.900757 time 2019-03-03 12:44:46.143504
Model ind 640 epoch 1150 head B head_i_epoch 1 batch 200: avg loss -1.993520 avg loss no lamb -1.993520 time 2019-03-03 12:48:19.999605
last batch sz 160
Pre: time 2019-03-03 12:51:34.512021: 
 	std: 0.05102699
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5099, 0.61395, 0.61398333, 0.61396664, 0.5097167]
	train_accs: [0.5099, 0.61395, 0.61398333, 0.61396664, 0.5097167]
	best_train_sub_head: 2
	worst: 0.5097167
	avg: 0.57230335
	best: 0.61398333

Starting e_i: 1151
Model ind 640 epoch 1151 head A head_i_epoch 0 batch 0: avg loss -3.306000 avg loss no lamb -3.306000 time 2019-03-03 12:51:45.622021
Model ind 640 epoch 1151 head A head_i_epoch 0 batch 100: avg loss -3.203754 avg loss no lamb -3.203754 time 2019-03-03 12:54:29.693026
Model ind 640 epoch 1151 head A head_i_epoch 0 batch 200: avg loss -3.320293 avg loss no lamb -3.320293 time 2019-03-03 12:58:06.407441
last batch sz 160
Model ind 640 epoch 1151 head B head_i_epoch 0 batch 0: avg loss -1.936285 avg loss no lamb -1.936285 time 2019-03-03 13:00:39.335217
Model ind 640 epoch 1151 head B head_i_epoch 0 batch 100: avg loss -1.808953 avg loss no lamb -1.808953 time 2019-03-03 13:04:13.269157
Model ind 640 epoch 1151 head B head_i_epoch 0 batch 200: avg loss -1.961249 avg loss no lamb -1.961249 time 2019-03-03 13:07:19.555500
last batch sz 160
Model ind 640 epoch 1151 head B head_i_epoch 1 batch 0: avg loss -1.912186 avg loss no lamb -1.912186 time 2019-03-03 13:09:30.371373
Model ind 640 epoch 1151 head B head_i_epoch 1 batch 100: avg loss -1.832769 avg loss no lamb -1.832769 time 2019-03-03 13:13:04.795011
Model ind 640 epoch 1151 head B head_i_epoch 1 batch 200: avg loss -2.010524 avg loss no lamb -2.010524 time 2019-03-03 13:16:40.105739
last batch sz 160
Pre: time 2019-03-03 13:19:44.661482: 
 	std: 0.050941236
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104167, 0.61443335, 0.6144, 0.61446667, 0.5104833]
	train_accs: [0.5104167, 0.61443335, 0.6144, 0.61446667, 0.5104833]
	best_train_sub_head: 3
	worst: 0.5104167
	avg: 0.57284003
	best: 0.61446667

Starting e_i: 1152
Model ind 640 epoch 1152 head A head_i_epoch 0 batch 0: avg loss -3.295931 avg loss no lamb -3.295931 time 2019-03-03 13:19:47.808302
Model ind 640 epoch 1152 head A head_i_epoch 0 batch 100: avg loss -3.218421 avg loss no lamb -3.218421 time 2019-03-03 13:22:44.971262
Model ind 640 epoch 1152 head A head_i_epoch 0 batch 200: avg loss -3.213412 avg loss no lamb -3.213412 time 2019-03-03 13:26:20.721807
last batch sz 160
Model ind 640 epoch 1152 head B head_i_epoch 0 batch 0: avg loss -1.960149 avg loss no lamb -1.960149 time 2019-03-03 13:28:57.977001
Model ind 640 epoch 1152 head B head_i_epoch 0 batch 100: avg loss -1.883129 avg loss no lamb -1.883129 time 2019-03-03 13:32:29.557107
Model ind 640 epoch 1152 head B head_i_epoch 0 batch 200: avg loss -1.968601 avg loss no lamb -1.968601 time 2019-03-03 13:35:11.434218
last batch sz 160
Model ind 640 epoch 1152 head B head_i_epoch 1 batch 0: avg loss -1.964111 avg loss no lamb -1.964111 time 2019-03-03 13:37:46.545289
Model ind 640 epoch 1152 head B head_i_epoch 1 batch 100: avg loss -1.859888 avg loss no lamb -1.859888 time 2019-03-03 13:41:22.580623
Model ind 640 epoch 1152 head B head_i_epoch 1 batch 200: avg loss -1.962957 avg loss no lamb -1.962957 time 2019-03-03 13:44:57.157169
last batch sz 160
Pre: time 2019-03-03 13:47:27.183202: 
 	std: 0.050607935
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118333, 0.6152667, 0.6153167, 0.6153, 0.51215]
	train_accs: [0.5118333, 0.6152667, 0.6153167, 0.6153, 0.51215]
	best_train_sub_head: 2
	worst: 0.5118333
	avg: 0.5739733
	best: 0.6153167

Starting e_i: 1153
Model ind 640 epoch 1153 head A head_i_epoch 0 batch 0: avg loss -3.256403 avg loss no lamb -3.256403 time 2019-03-03 13:47:34.130644
Model ind 640 epoch 1153 head A head_i_epoch 0 batch 100: avg loss -3.219692 avg loss no lamb -3.219692 time 2019-03-03 13:51:03.776919
Model ind 640 epoch 1153 head A head_i_epoch 0 batch 200: avg loss -3.314997 avg loss no lamb -3.314997 time 2019-03-03 13:54:40.884849
last batch sz 160
Model ind 640 epoch 1153 head B head_i_epoch 0 batch 0: avg loss -1.983496 avg loss no lamb -1.983496 time 2019-03-03 13:57:17.445156
Model ind 640 epoch 1153 head B head_i_epoch 0 batch 100: avg loss -1.918859 avg loss no lamb -1.918859 time 2019-03-03 14:00:15.870642
Model ind 640 epoch 1153 head B head_i_epoch 0 batch 200: avg loss -1.980551 avg loss no lamb -1.980551 time 2019-03-03 14:03:37.431520
last batch sz 160
Model ind 640 epoch 1153 head B head_i_epoch 1 batch 0: avg loss -1.951356 avg loss no lamb -1.951356 time 2019-03-03 14:06:12.382840
Model ind 640 epoch 1153 head B head_i_epoch 1 batch 100: avg loss -1.886505 avg loss no lamb -1.886505 time 2019-03-03 14:09:45.989281
Model ind 640 epoch 1153 head B head_i_epoch 1 batch 200: avg loss -1.995452 avg loss no lamb -1.995452 time 2019-03-03 14:12:53.633821
last batch sz 160
Pre: time 2019-03-03 14:15:38.336664: 
 	std: 0.05100525
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51093334, 0.61513335, 0.6152167, 0.61516666, 0.5111833]
	train_accs: [0.51093334, 0.61513335, 0.6152167, 0.61516666, 0.5111833]
	best_train_sub_head: 2
	worst: 0.51093334
	avg: 0.5735267
	best: 0.6152167

Starting e_i: 1154
Model ind 640 epoch 1154 head A head_i_epoch 0 batch 0: avg loss -3.296175 avg loss no lamb -3.296175 time 2019-03-03 14:15:43.463429
Model ind 640 epoch 1154 head A head_i_epoch 0 batch 100: avg loss -3.240067 avg loss no lamb -3.240067 time 2019-03-03 14:19:18.378184
Model ind 640 epoch 1154 head A head_i_epoch 0 batch 200: avg loss -3.264155 avg loss no lamb -3.264155 time 2019-03-03 14:22:48.607536
last batch sz 160
Model ind 640 epoch 1154 head B head_i_epoch 0 batch 0: avg loss -2.001838 avg loss no lamb -2.001838 time 2019-03-03 14:25:23.379561
Model ind 640 epoch 1154 head B head_i_epoch 0 batch 100: avg loss -1.900339 avg loss no lamb -1.900339 time 2019-03-03 14:28:05.217415
Model ind 640 epoch 1154 head B head_i_epoch 0 batch 200: avg loss -2.038244 avg loss no lamb -2.038244 time 2019-03-03 14:31:42.758479
last batch sz 160
Model ind 640 epoch 1154 head B head_i_epoch 1 batch 0: avg loss -1.925283 avg loss no lamb -1.925283 time 2019-03-03 14:34:19.152631
Model ind 640 epoch 1154 head B head_i_epoch 1 batch 100: avg loss -1.853007 avg loss no lamb -1.853007 time 2019-03-03 14:37:50.580949
Model ind 640 epoch 1154 head B head_i_epoch 1 batch 200: avg loss -2.035285 avg loss no lamb -2.035285 time 2019-03-03 14:40:33.256550
last batch sz 160
Pre: time 2019-03-03 14:43:46.360475: 
 	std: 0.051406674
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5096667, 0.61446667, 0.61455, 0.61448336, 0.50946665]
	train_accs: [0.5096667, 0.61446667, 0.61455, 0.61448336, 0.50946665]
	best_train_sub_head: 2
	worst: 0.50946665
	avg: 0.57252663
	best: 0.61455

Starting e_i: 1155
Model ind 640 epoch 1155 head A head_i_epoch 0 batch 0: avg loss -3.301317 avg loss no lamb -3.301317 time 2019-03-03 14:43:50.471633
Model ind 640 epoch 1155 head A head_i_epoch 0 batch 100: avg loss -3.126542 avg loss no lamb -3.126542 time 2019-03-03 14:47:24.021983
Model ind 640 epoch 1155 head A head_i_epoch 0 batch 200: avg loss -3.331097 avg loss no lamb -3.331097 time 2019-03-03 14:50:59.333275
last batch sz 160
Model ind 640 epoch 1155 head B head_i_epoch 0 batch 0: avg loss -1.929439 avg loss no lamb -1.929439 time 2019-03-03 14:53:02.107488
Model ind 640 epoch 1155 head B head_i_epoch 0 batch 100: avg loss -1.902963 avg loss no lamb -1.902963 time 2019-03-03 14:56:20.981294
Model ind 640 epoch 1155 head B head_i_epoch 0 batch 200: avg loss -1.959227 avg loss no lamb -1.959227 time 2019-03-03 14:59:58.471689
last batch sz 160
Model ind 640 epoch 1155 head B head_i_epoch 1 batch 0: avg loss -1.906009 avg loss no lamb -1.906009 time 2019-03-03 15:02:35.487911
Model ind 640 epoch 1155 head B head_i_epoch 1 batch 100: avg loss -1.864910 avg loss no lamb -1.864910 time 2019-03-03 15:05:46.839463
Model ind 640 epoch 1155 head B head_i_epoch 1 batch 200: avg loss -1.993719 avg loss no lamb -1.993719 time 2019-03-03 15:08:58.141145
last batch sz 160
Pre: time 2019-03-03 15:12:08.354469: 
 	std: 0.0510311
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51086664, 0.6149167, 0.61505, 0.61513335, 0.51086664]
	train_accs: [0.51086664, 0.6149167, 0.61505, 0.61513335, 0.51086664]
	best_train_sub_head: 3
	worst: 0.51086664
	avg: 0.57336664
	best: 0.61513335

Starting e_i: 1156
Model ind 640 epoch 1156 head A head_i_epoch 0 batch 0: avg loss -3.249143 avg loss no lamb -3.249143 time 2019-03-03 15:12:12.304846
Model ind 640 epoch 1156 head A head_i_epoch 0 batch 100: avg loss -3.207237 avg loss no lamb -3.207237 time 2019-03-03 15:15:44.582632
Model ind 640 epoch 1156 head A head_i_epoch 0 batch 200: avg loss -3.234494 avg loss no lamb -3.234494 time 2019-03-03 15:18:54.552498
last batch sz 160
Model ind 640 epoch 1156 head B head_i_epoch 0 batch 0: avg loss -1.951064 avg loss no lamb -1.951064 time 2019-03-03 15:21:04.848755
Model ind 640 epoch 1156 head B head_i_epoch 0 batch 100: avg loss -1.835672 avg loss no lamb -1.835672 time 2019-03-03 15:24:37.925573
Model ind 640 epoch 1156 head B head_i_epoch 0 batch 200: avg loss -1.999456 avg loss no lamb -1.999456 time 2019-03-03 15:28:09.171281
last batch sz 160
Model ind 640 epoch 1156 head B head_i_epoch 1 batch 0: avg loss -1.945004 avg loss no lamb -1.945004 time 2019-03-03 15:30:45.110780
Model ind 640 epoch 1156 head B head_i_epoch 1 batch 100: avg loss -1.916289 avg loss no lamb -1.916289 time 2019-03-03 15:33:24.310774
Model ind 640 epoch 1156 head B head_i_epoch 1 batch 200: avg loss -1.982999 avg loss no lamb -1.982999 time 2019-03-03 15:36:59.827789
last batch sz 160
Pre: time 2019-03-03 15:40:12.166590: 
 	std: 0.050968528
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107167, 0.61481667, 0.61471665, 0.6149833, 0.51088333]
	train_accs: [0.5107167, 0.61481667, 0.61471665, 0.6149833, 0.51088333]
	best_train_sub_head: 3
	worst: 0.5107167
	avg: 0.5732233
	best: 0.6149833

Starting e_i: 1157
Model ind 640 epoch 1157 head A head_i_epoch 0 batch 0: avg loss -3.261773 avg loss no lamb -3.261773 time 2019-03-03 15:40:15.933145
Model ind 640 epoch 1157 head A head_i_epoch 0 batch 100: avg loss -3.225597 avg loss no lamb -3.225597 time 2019-03-03 15:43:49.845731
Model ind 640 epoch 1157 head A head_i_epoch 0 batch 200: avg loss -3.283061 avg loss no lamb -3.283061 time 2019-03-03 15:46:32.203740
last batch sz 160
Model ind 640 epoch 1157 head B head_i_epoch 0 batch 0: avg loss -1.913853 avg loss no lamb -1.913853 time 2019-03-03 15:49:07.502144
Model ind 640 epoch 1157 head B head_i_epoch 0 batch 100: avg loss -1.934960 avg loss no lamb -1.934960 time 2019-03-03 15:52:40.650868
Model ind 640 epoch 1157 head B head_i_epoch 0 batch 200: avg loss -1.945309 avg loss no lamb -1.945309 time 2019-03-03 15:56:15.126972
last batch sz 160
Model ind 640 epoch 1157 head B head_i_epoch 1 batch 0: avg loss -1.934552 avg loss no lamb -1.934552 time 2019-03-03 15:58:25.912415
Model ind 640 epoch 1157 head B head_i_epoch 1 batch 100: avg loss -1.868681 avg loss no lamb -1.868681 time 2019-03-03 16:01:33.814039
Model ind 640 epoch 1157 head B head_i_epoch 1 batch 200: avg loss -2.032541 avg loss no lamb -2.032541 time 2019-03-03 16:05:07.233899
last batch sz 160
Pre: time 2019-03-03 16:08:18.076596: 
 	std: 0.05080246
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51085, 0.61466664, 0.61455, 0.61443335, 0.51085]
	train_accs: [0.51085, 0.61466664, 0.61455, 0.61443335, 0.51085]
	best_train_sub_head: 1
	worst: 0.51085
	avg: 0.57307
	best: 0.61466664

Starting e_i: 1158
Model ind 640 epoch 1158 head A head_i_epoch 0 batch 0: avg loss -3.269075 avg loss no lamb -3.269075 time 2019-03-03 16:08:22.305506
Model ind 640 epoch 1158 head A head_i_epoch 0 batch 100: avg loss -3.230927 avg loss no lamb -3.230927 time 2019-03-03 16:11:34.913671
Model ind 640 epoch 1158 head A head_i_epoch 0 batch 200: avg loss -3.289063 avg loss no lamb -3.289063 time 2019-03-03 16:14:39.903060
last batch sz 160
Model ind 640 epoch 1158 head B head_i_epoch 0 batch 0: avg loss -1.971879 avg loss no lamb -1.971879 time 2019-03-03 16:17:15.484461
Model ind 640 epoch 1158 head B head_i_epoch 0 batch 100: avg loss -1.910064 avg loss no lamb -1.910064 time 2019-03-03 16:20:49.646778
Model ind 640 epoch 1158 head B head_i_epoch 0 batch 200: avg loss -2.006634 avg loss no lamb -2.006634 time 2019-03-03 16:24:24.301413
last batch sz 160
Model ind 640 epoch 1158 head B head_i_epoch 1 batch 0: avg loss -1.951463 avg loss no lamb -1.951463 time 2019-03-03 16:26:09.460512
Model ind 640 epoch 1158 head B head_i_epoch 1 batch 100: avg loss -1.910990 avg loss no lamb -1.910990 time 2019-03-03 16:29:46.184660
Model ind 640 epoch 1158 head B head_i_epoch 1 batch 200: avg loss -1.966802 avg loss no lamb -1.966802 time 2019-03-03 16:33:17.998689
last batch sz 160
Pre: time 2019-03-03 16:36:30.682122: 
 	std: 0.05053029
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51165, 0.61485, 0.6149167, 0.61471665, 0.51171666]
	train_accs: [0.51165, 0.61485, 0.6149167, 0.61471665, 0.51171666]
	best_train_sub_head: 2
	worst: 0.51165
	avg: 0.57357
	best: 0.6149167

Starting e_i: 1159
Model ind 640 epoch 1159 head A head_i_epoch 0 batch 0: avg loss -3.277431 avg loss no lamb -3.277431 time 2019-03-03 16:36:34.306148
Model ind 640 epoch 1159 head A head_i_epoch 0 batch 100: avg loss -3.278407 avg loss no lamb -3.278407 time 2019-03-03 16:39:17.954289
Model ind 640 epoch 1159 head A head_i_epoch 0 batch 200: avg loss -3.369663 avg loss no lamb -3.369663 time 2019-03-03 16:42:52.486317
last batch sz 160
Model ind 640 epoch 1159 head B head_i_epoch 0 batch 0: avg loss -1.869748 avg loss no lamb -1.869748 time 2019-03-03 16:45:28.627950
Model ind 640 epoch 1159 head B head_i_epoch 0 batch 100: avg loss -1.871203 avg loss no lamb -1.871203 time 2019-03-03 16:49:01.208972
Model ind 640 epoch 1159 head B head_i_epoch 0 batch 200: avg loss -1.974699 avg loss no lamb -1.974699 time 2019-03-03 16:51:55.679566
last batch sz 160
Model ind 640 epoch 1159 head B head_i_epoch 1 batch 0: avg loss -2.024593 avg loss no lamb -2.024593 time 2019-03-03 16:54:22.716952
Model ind 640 epoch 1159 head B head_i_epoch 1 batch 100: avg loss -1.875675 avg loss no lamb -1.875675 time 2019-03-03 16:57:56.248315
Model ind 640 epoch 1159 head B head_i_epoch 1 batch 200: avg loss -1.955491 avg loss no lamb -1.955491 time 2019-03-03 17:01:31.628559
last batch sz 160
Pre: time 2019-03-03 17:04:26.400417: 
 	std: 0.05054136
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51178336, 0.6152167, 0.61511666, 0.6152167, 0.51225]
	train_accs: [0.51178336, 0.6152167, 0.61511666, 0.6152167, 0.51225]
	best_train_sub_head: 1
	worst: 0.51178336
	avg: 0.5739167
	best: 0.6152167

Starting e_i: 1160
Model ind 640 epoch 1160 head A head_i_epoch 0 batch 0: avg loss -3.313668 avg loss no lamb -3.313668 time 2019-03-03 17:04:31.199715
Model ind 640 epoch 1160 head A head_i_epoch 0 batch 100: avg loss -3.258629 avg loss no lamb -3.258629 time 2019-03-03 17:07:38.045116
Model ind 640 epoch 1160 head A head_i_epoch 0 batch 200: avg loss -3.248300 avg loss no lamb -3.248300 time 2019-03-03 17:11:14.581984
last batch sz 160
Model ind 640 epoch 1160 head B head_i_epoch 0 batch 0: avg loss -1.978440 avg loss no lamb -1.978440 time 2019-03-03 17:13:51.083410
Model ind 640 epoch 1160 head B head_i_epoch 0 batch 100: avg loss -1.874493 avg loss no lamb -1.874493 time 2019-03-03 17:17:13.386305
Model ind 640 epoch 1160 head B head_i_epoch 0 batch 200: avg loss -2.030418 avg loss no lamb -2.030418 time 2019-03-03 17:20:11.042395
last batch sz 160
Model ind 640 epoch 1160 head B head_i_epoch 1 batch 0: avg loss -1.970897 avg loss no lamb -1.970897 time 2019-03-03 17:22:45.129682
Model ind 640 epoch 1160 head B head_i_epoch 1 batch 100: avg loss -1.909353 avg loss no lamb -1.909353 time 2019-03-03 17:26:20.117178
Model ind 640 epoch 1160 head B head_i_epoch 1 batch 200: avg loss -1.975191 avg loss no lamb -1.975191 time 2019-03-03 17:29:54.231380
last batch sz 160
Pre: time 2019-03-03 17:32:14.869714: 
 	std: 0.05057263
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51096666, 0.61435, 0.6144, 0.61446667, 0.51138335]
	train_accs: [0.51096666, 0.61435, 0.6144, 0.61446667, 0.51138335]
	best_train_sub_head: 3
	worst: 0.51096666
	avg: 0.5731133
	best: 0.61446667

Starting e_i: 1161
Model ind 640 epoch 1161 head A head_i_epoch 0 batch 0: avg loss -3.321358 avg loss no lamb -3.321358 time 2019-03-03 17:32:23.063974
Model ind 640 epoch 1161 head A head_i_epoch 0 batch 100: avg loss -3.185276 avg loss no lamb -3.185276 time 2019-03-03 17:35:57.750501
Model ind 640 epoch 1161 head A head_i_epoch 0 batch 200: avg loss -3.344090 avg loss no lamb -3.344090 time 2019-03-03 17:39:29.006336
last batch sz 160
Model ind 640 epoch 1161 head B head_i_epoch 0 batch 0: avg loss -1.962437 avg loss no lamb -1.962437 time 2019-03-03 17:42:06.588718
Model ind 640 epoch 1161 head B head_i_epoch 0 batch 100: avg loss -1.834877 avg loss no lamb -1.834877 time 2019-03-03 17:44:50.373931
Model ind 640 epoch 1161 head B head_i_epoch 0 batch 200: avg loss -1.959074 avg loss no lamb -1.959074 time 2019-03-03 17:48:22.744760
last batch sz 160
Model ind 640 epoch 1161 head B head_i_epoch 1 batch 0: avg loss -1.889945 avg loss no lamb -1.889945 time 2019-03-03 17:50:58.531215
Model ind 640 epoch 1161 head B head_i_epoch 1 batch 100: avg loss -1.899737 avg loss no lamb -1.899737 time 2019-03-03 17:54:32.407907
Model ind 640 epoch 1161 head B head_i_epoch 1 batch 200: avg loss -2.049259 avg loss no lamb -2.049259 time 2019-03-03 17:57:35.185984
last batch sz 160
Pre: time 2019-03-03 18:00:29.608057: 
 	std: 0.05049897
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51155, 0.6146167, 0.61465, 0.61455, 0.5115]
	train_accs: [0.51155, 0.6146167, 0.61465, 0.61455, 0.5115]
	best_train_sub_head: 2
	worst: 0.5115
	avg: 0.5733733
	best: 0.61465

Starting e_i: 1162
Model ind 640 epoch 1162 head A head_i_epoch 0 batch 0: avg loss -3.273832 avg loss no lamb -3.273832 time 2019-03-03 18:00:33.170108
Model ind 640 epoch 1162 head A head_i_epoch 0 batch 100: avg loss -3.233218 avg loss no lamb -3.233218 time 2019-03-03 18:04:08.406754
Model ind 640 epoch 1162 head A head_i_epoch 0 batch 200: avg loss -3.385334 avg loss no lamb -3.385334 time 2019-03-03 18:07:44.996324
last batch sz 160
Model ind 640 epoch 1162 head B head_i_epoch 0 batch 0: avg loss -1.902308 avg loss no lamb -1.902308 time 2019-03-03 18:10:01.628846
Model ind 640 epoch 1162 head B head_i_epoch 0 batch 100: avg loss -1.920744 avg loss no lamb -1.920744 time 2019-03-03 18:13:03.749824
Model ind 640 epoch 1162 head B head_i_epoch 0 batch 200: avg loss -1.995605 avg loss no lamb -1.995605 time 2019-03-03 18:16:39.521969
last batch sz 160
Model ind 640 epoch 1162 head B head_i_epoch 1 batch 0: avg loss -1.956033 avg loss no lamb -1.956033 time 2019-03-03 18:19:17.090449
Model ind 640 epoch 1162 head B head_i_epoch 1 batch 100: avg loss -1.850086 avg loss no lamb -1.850086 time 2019-03-03 18:22:49.663084
Model ind 640 epoch 1162 head B head_i_epoch 1 batch 200: avg loss -2.008885 avg loss no lamb -2.008885 time 2019-03-03 18:25:33.035321
last batch sz 160
Pre: time 2019-03-03 18:28:42.288932: 
 	std: 0.050756227
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107333, 0.61438334, 0.61433333, 0.61455, 0.5109]
	train_accs: [0.5107333, 0.61438334, 0.61433333, 0.61455, 0.5109]
	best_train_sub_head: 3
	worst: 0.5107333
	avg: 0.57298005
	best: 0.61455

Starting e_i: 1163
Model ind 640 epoch 1163 head A head_i_epoch 0 batch 0: avg loss -3.219625 avg loss no lamb -3.219625 time 2019-03-03 18:28:46.883761
Model ind 640 epoch 1163 head A head_i_epoch 0 batch 100: avg loss -3.133384 avg loss no lamb -3.133384 time 2019-03-03 18:32:21.900728
Model ind 640 epoch 1163 head A head_i_epoch 0 batch 200: avg loss -3.334731 avg loss no lamb -3.334731 time 2019-03-03 18:35:55.783553
last batch sz 160
Model ind 640 epoch 1163 head B head_i_epoch 0 batch 0: avg loss -1.945778 avg loss no lamb -1.945778 time 2019-03-03 18:37:45.964024
Model ind 640 epoch 1163 head B head_i_epoch 0 batch 100: avg loss -1.948692 avg loss no lamb -1.948692 time 2019-03-03 18:41:21.932135
Model ind 640 epoch 1163 head B head_i_epoch 0 batch 200: avg loss -1.987470 avg loss no lamb -1.987470 time 2019-03-03 18:44:52.965903
last batch sz 160
Model ind 640 epoch 1163 head B head_i_epoch 1 batch 0: avg loss -1.901156 avg loss no lamb -1.901156 time 2019-03-03 18:47:29.272873
Model ind 640 epoch 1163 head B head_i_epoch 1 batch 100: avg loss -1.792080 avg loss no lamb -1.792080 time 2019-03-03 18:50:22.425449
Model ind 640 epoch 1163 head B head_i_epoch 1 batch 200: avg loss -1.942075 avg loss no lamb -1.942075 time 2019-03-03 18:53:49.046629
last batch sz 160
Pre: time 2019-03-03 18:57:02.127049: 
 	std: 0.05073443
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51238334, 0.61585, 0.61576664, 0.61591667, 0.5121833]
	train_accs: [0.51238334, 0.61585, 0.61576664, 0.61591667, 0.5121833]
	best_train_sub_head: 3
	worst: 0.5121833
	avg: 0.57442
	best: 0.61591667

Starting e_i: 1164
Model ind 640 epoch 1164 head A head_i_epoch 0 batch 0: avg loss -3.336807 avg loss no lamb -3.336807 time 2019-03-03 18:57:05.684709
Model ind 640 epoch 1164 head A head_i_epoch 0 batch 100: avg loss -3.250170 avg loss no lamb -3.250170 time 2019-03-03 19:00:35.627409
Model ind 640 epoch 1164 head A head_i_epoch 0 batch 200: avg loss -3.286143 avg loss no lamb -3.286143 time 2019-03-03 19:03:30.352252
last batch sz 160
Model ind 640 epoch 1164 head B head_i_epoch 0 batch 0: avg loss -1.993846 avg loss no lamb -1.993846 time 2019-03-03 19:05:55.224750
Model ind 640 epoch 1164 head B head_i_epoch 0 batch 100: avg loss -1.902012 avg loss no lamb -1.902012 time 2019-03-03 19:09:27.246869
Model ind 640 epoch 1164 head B head_i_epoch 0 batch 200: avg loss -1.954310 avg loss no lamb -1.954310 time 2019-03-03 19:13:00.857884
last batch sz 160
Model ind 640 epoch 1164 head B head_i_epoch 1 batch 0: avg loss -1.989134 avg loss no lamb -1.989134 time 2019-03-03 19:15:34.912126
Model ind 640 epoch 1164 head B head_i_epoch 1 batch 100: avg loss -1.817270 avg loss no lamb -1.817270 time 2019-03-03 19:18:20.391365
Model ind 640 epoch 1164 head B head_i_epoch 1 batch 200: avg loss -2.001599 avg loss no lamb -2.001599 time 2019-03-03 19:21:55.296541
last batch sz 160
Pre: time 2019-03-03 19:25:07.881758: 
 	std: 0.05071135
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51195, 0.6152667, 0.6152833, 0.61546665, 0.5117]
	train_accs: [0.51195, 0.6152667, 0.6152833, 0.61546665, 0.5117]
	best_train_sub_head: 3
	worst: 0.5117
	avg: 0.5739333
	best: 0.61546665

Starting e_i: 1165
Model ind 640 epoch 1165 head A head_i_epoch 0 batch 0: avg loss -3.365496 avg loss no lamb -3.365496 time 2019-03-03 19:25:11.544131
Model ind 640 epoch 1165 head A head_i_epoch 0 batch 100: avg loss -3.192228 avg loss no lamb -3.192228 time 2019-03-03 19:28:41.399436
Model ind 640 epoch 1165 head A head_i_epoch 0 batch 200: avg loss -3.349251 avg loss no lamb -3.349251 time 2019-03-03 19:31:29.940418
last batch sz 160
Model ind 640 epoch 1165 head B head_i_epoch 0 batch 0: avg loss -1.923488 avg loss no lamb -1.923488 time 2019-03-03 19:34:07.230924
Model ind 640 epoch 1165 head B head_i_epoch 0 batch 100: avg loss -1.887963 avg loss no lamb -1.887963 time 2019-03-03 19:37:41.845720
Model ind 640 epoch 1165 head B head_i_epoch 0 batch 200: avg loss -1.947311 avg loss no lamb -1.947311 time 2019-03-03 19:41:16.354357
last batch sz 160
Model ind 640 epoch 1165 head B head_i_epoch 1 batch 0: avg loss -1.901444 avg loss no lamb -1.901444 time 2019-03-03 19:43:05.404011
Model ind 640 epoch 1165 head B head_i_epoch 1 batch 100: avg loss -1.890846 avg loss no lamb -1.890846 time 2019-03-03 19:46:34.011516
Model ind 640 epoch 1165 head B head_i_epoch 1 batch 200: avg loss -1.928754 avg loss no lamb -1.928754 time 2019-03-03 19:50:06.006124
last batch sz 160
Pre: time 2019-03-03 19:53:18.892070: 
 	std: 0.050843246
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108167, 0.61455, 0.61455, 0.6146, 0.51075]
	train_accs: [0.5108167, 0.61455, 0.61455, 0.6146, 0.51075]
	best_train_sub_head: 3
	worst: 0.51075
	avg: 0.57305336
	best: 0.6146

Starting e_i: 1166
Model ind 640 epoch 1166 head A head_i_epoch 0 batch 0: avg loss -3.287400 avg loss no lamb -3.287400 time 2019-03-03 19:53:22.686952
Model ind 640 epoch 1166 head A head_i_epoch 0 batch 100: avg loss -3.119791 avg loss no lamb -3.119791 time 2019-03-03 19:56:13.201811
Model ind 640 epoch 1166 head A head_i_epoch 0 batch 200: avg loss -3.315011 avg loss no lamb -3.315011 time 2019-03-03 19:59:44.673182
last batch sz 160
Model ind 640 epoch 1166 head B head_i_epoch 0 batch 0: avg loss -1.975447 avg loss no lamb -1.975447 time 2019-03-03 20:02:19.494966
Model ind 640 epoch 1166 head B head_i_epoch 0 batch 100: avg loss -1.917033 avg loss no lamb -1.917033 time 2019-03-03 20:05:52.373275
Model ind 640 epoch 1166 head B head_i_epoch 0 batch 200: avg loss -1.936308 avg loss no lamb -1.936308 time 2019-03-03 20:09:02.700248
last batch sz 160
Model ind 640 epoch 1166 head B head_i_epoch 1 batch 0: avg loss -1.939347 avg loss no lamb -1.939347 time 2019-03-03 20:11:12.338514
Model ind 640 epoch 1166 head B head_i_epoch 1 batch 100: avg loss -1.873804 avg loss no lamb -1.873804 time 2019-03-03 20:14:48.621094
Model ind 640 epoch 1166 head B head_i_epoch 1 batch 200: avg loss -1.969818 avg loss no lamb -1.969818 time 2019-03-03 20:18:22.310128
last batch sz 160
Pre: time 2019-03-03 20:21:28.331012: 
 	std: 0.05085968
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104167, 0.61441666, 0.6143, 0.61443335, 0.5107167]
	train_accs: [0.5104167, 0.61441666, 0.6143, 0.61443335, 0.5107167]
	best_train_sub_head: 3
	worst: 0.5104167
	avg: 0.57285666
	best: 0.61443335

Starting e_i: 1167
Model ind 640 epoch 1167 head A head_i_epoch 0 batch 0: avg loss -3.319651 avg loss no lamb -3.319651 time 2019-03-03 20:21:31.347799
Model ind 640 epoch 1167 head A head_i_epoch 0 batch 100: avg loss -3.130572 avg loss no lamb -3.130572 time 2019-03-03 20:24:25.092165
Model ind 640 epoch 1167 head A head_i_epoch 0 batch 200: avg loss -3.391475 avg loss no lamb -3.391475 time 2019-03-03 20:27:59.602014
last batch sz 160
Model ind 640 epoch 1167 head B head_i_epoch 0 batch 0: avg loss -2.012020 avg loss no lamb -2.012020 time 2019-03-03 20:30:36.565207
Model ind 640 epoch 1167 head B head_i_epoch 0 batch 100: avg loss -1.803815 avg loss no lamb -1.803815 time 2019-03-03 20:34:11.910192
Model ind 640 epoch 1167 head B head_i_epoch 0 batch 200: avg loss -2.009352 avg loss no lamb -2.009352 time 2019-03-03 20:36:55.324098
last batch sz 160
Model ind 640 epoch 1167 head B head_i_epoch 1 batch 0: avg loss -1.911056 avg loss no lamb -1.911056 time 2019-03-03 20:39:26.984307
Model ind 640 epoch 1167 head B head_i_epoch 1 batch 100: avg loss -1.857036 avg loss no lamb -1.857036 time 2019-03-03 20:43:01.247106
Model ind 640 epoch 1167 head B head_i_epoch 1 batch 200: avg loss -2.044063 avg loss no lamb -2.044063 time 2019-03-03 20:46:35.044253
last batch sz 160
Pre: time 2019-03-03 20:49:02.336749: 
 	std: 0.050685395
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5104333, 0.6139333, 0.6139167, 0.6139333, 0.5105]
	train_accs: [0.5104333, 0.6139333, 0.6139167, 0.6139333, 0.5105]
	best_train_sub_head: 1
	worst: 0.5104333
	avg: 0.5725433
	best: 0.6139333

Starting e_i: 1168
Model ind 640 epoch 1168 head A head_i_epoch 0 batch 0: avg loss -3.296395 avg loss no lamb -3.296395 time 2019-03-03 20:49:06.292292
Model ind 640 epoch 1168 head A head_i_epoch 0 batch 100: avg loss -3.228334 avg loss no lamb -3.228334 time 2019-03-03 20:52:42.149312
Model ind 640 epoch 1168 head A head_i_epoch 0 batch 200: avg loss -3.281244 avg loss no lamb -3.281244 time 2019-03-03 20:56:13.093832
last batch sz 160
Model ind 640 epoch 1168 head B head_i_epoch 0 batch 0: avg loss -1.894583 avg loss no lamb -1.894583 time 2019-03-03 20:58:50.238557
Model ind 640 epoch 1168 head B head_i_epoch 0 batch 100: avg loss -1.971485 avg loss no lamb -1.971485 time 2019-03-03 21:01:54.553152
Model ind 640 epoch 1168 head B head_i_epoch 0 batch 200: avg loss -1.943861 avg loss no lamb -1.943861 time 2019-03-03 21:03:44.116841
last batch sz 160
Model ind 640 epoch 1168 head B head_i_epoch 1 batch 0: avg loss -1.978641 avg loss no lamb -1.978641 time 2019-03-03 21:05:03.833193
Model ind 640 epoch 1168 head B head_i_epoch 1 batch 100: avg loss -1.878910 avg loss no lamb -1.878910 time 2019-03-03 21:06:53.050680
Model ind 640 epoch 1168 head B head_i_epoch 1 batch 200: avg loss -1.966159 avg loss no lamb -1.966159 time 2019-03-03 21:08:41.269582
last batch sz 160
Pre: time 2019-03-03 21:10:23.960560: 
 	std: 0.05070447
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51231664, 0.61581665, 0.61586666, 0.6157167, 0.5122833]
	train_accs: [0.51231664, 0.61581665, 0.61586666, 0.6157167, 0.5122833]
	best_train_sub_head: 2
	worst: 0.5122833
	avg: 0.5744
	best: 0.61586666

Starting e_i: 1169
Model ind 640 epoch 1169 head A head_i_epoch 0 batch 0: avg loss -3.249694 avg loss no lamb -3.249694 time 2019-03-03 21:10:26.784115
Model ind 640 epoch 1169 head A head_i_epoch 0 batch 100: avg loss -3.157452 avg loss no lamb -3.157452 time 2019-03-03 21:12:16.935272
Model ind 640 epoch 1169 head A head_i_epoch 0 batch 200: avg loss -3.276092 avg loss no lamb -3.276092 time 2019-03-03 21:14:06.638203
last batch sz 160
Model ind 640 epoch 1169 head B head_i_epoch 0 batch 0: avg loss -1.947074 avg loss no lamb -1.947074 time 2019-03-03 21:15:27.281006
Model ind 640 epoch 1169 head B head_i_epoch 0 batch 100: avg loss -1.948813 avg loss no lamb -1.948813 time 2019-03-03 21:17:14.301010
Model ind 640 epoch 1169 head B head_i_epoch 0 batch 200: avg loss -1.887111 avg loss no lamb -1.887111 time 2019-03-03 21:19:04.108681
last batch sz 160
Model ind 640 epoch 1169 head B head_i_epoch 1 batch 0: avg loss -1.939947 avg loss no lamb -1.939947 time 2019-03-03 21:20:24.191346
Model ind 640 epoch 1169 head B head_i_epoch 1 batch 100: avg loss -1.825035 avg loss no lamb -1.825035 time 2019-03-03 21:22:14.194244
Model ind 640 epoch 1169 head B head_i_epoch 1 batch 200: avg loss -1.958904 avg loss no lamb -1.958904 time 2019-03-03 21:24:03.178410
last batch sz 160
Pre: time 2019-03-03 21:25:44.420916: 
 	std: 0.05074938
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118, 0.6153333, 0.6153167, 0.61545, 0.51175]
	train_accs: [0.5118, 0.6153333, 0.6153167, 0.61545, 0.51175]
	best_train_sub_head: 3
	worst: 0.51175
	avg: 0.57393
	best: 0.61545

Starting e_i: 1170
Model ind 640 epoch 1170 head A head_i_epoch 0 batch 0: avg loss -3.320669 avg loss no lamb -3.320669 time 2019-03-03 21:25:47.485219
Model ind 640 epoch 1170 head A head_i_epoch 0 batch 100: avg loss -3.182418 avg loss no lamb -3.182418 time 2019-03-03 21:27:38.451320
Model ind 640 epoch 1170 head A head_i_epoch 0 batch 200: avg loss -3.271956 avg loss no lamb -3.271956 time 2019-03-03 21:29:28.339830
last batch sz 160
Model ind 640 epoch 1170 head B head_i_epoch 0 batch 0: avg loss -1.953028 avg loss no lamb -1.953028 time 2019-03-03 21:30:48.282614
Model ind 640 epoch 1170 head B head_i_epoch 0 batch 100: avg loss -1.890914 avg loss no lamb -1.890914 time 2019-03-03 21:32:37.819566
Model ind 640 epoch 1170 head B head_i_epoch 0 batch 200: avg loss -1.929181 avg loss no lamb -1.929181 time 2019-03-03 21:34:26.755231
last batch sz 160
Model ind 640 epoch 1170 head B head_i_epoch 1 batch 0: avg loss -1.962133 avg loss no lamb -1.962133 time 2019-03-03 21:35:45.956353
Model ind 640 epoch 1170 head B head_i_epoch 1 batch 100: avg loss -1.893591 avg loss no lamb -1.893591 time 2019-03-03 21:37:34.117822
Model ind 640 epoch 1170 head B head_i_epoch 1 batch 200: avg loss -1.951280 avg loss no lamb -1.951280 time 2019-03-03 21:39:22.653629
last batch sz 160
Pre: time 2019-03-03 21:41:03.561879: 
 	std: 0.050622825
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51205, 0.61541665, 0.61541665, 0.61551666, 0.5121833]
	train_accs: [0.51205, 0.61541665, 0.61541665, 0.61551666, 0.5121833]
	best_train_sub_head: 3
	worst: 0.51205
	avg: 0.5741166
	best: 0.61551666

Starting e_i: 1171
Model ind 640 epoch 1171 head A head_i_epoch 0 batch 0: avg loss -3.327850 avg loss no lamb -3.327850 time 2019-03-03 21:41:10.481762
Model ind 640 epoch 1171 head A head_i_epoch 0 batch 100: avg loss -3.211277 avg loss no lamb -3.211277 time 2019-03-03 21:43:02.597111
Model ind 640 epoch 1171 head A head_i_epoch 0 batch 200: avg loss -3.280745 avg loss no lamb -3.280745 time 2019-03-03 21:44:53.631491
last batch sz 160
Model ind 640 epoch 1171 head B head_i_epoch 0 batch 0: avg loss -1.974862 avg loss no lamb -1.974862 time 2019-03-03 21:46:13.981999
Model ind 640 epoch 1171 head B head_i_epoch 0 batch 100: avg loss -1.887159 avg loss no lamb -1.887159 time 2019-03-03 21:48:03.588641
Model ind 640 epoch 1171 head B head_i_epoch 0 batch 200: avg loss -1.992025 avg loss no lamb -1.992025 time 2019-03-03 21:49:53.454991
last batch sz 160
Model ind 640 epoch 1171 head B head_i_epoch 1 batch 0: avg loss -1.919112 avg loss no lamb -1.919112 time 2019-03-03 21:51:13.068649
Model ind 640 epoch 1171 head B head_i_epoch 1 batch 100: avg loss -1.891858 avg loss no lamb -1.891858 time 2019-03-03 21:53:03.168334
Model ind 640 epoch 1171 head B head_i_epoch 1 batch 200: avg loss -1.960446 avg loss no lamb -1.960446 time 2019-03-03 21:54:53.602451
last batch sz 160
Pre: time 2019-03-03 21:56:33.863757: 
 	std: 0.050262272
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5128667, 0.6156667, 0.6155667, 0.61558336, 0.51315]
	train_accs: [0.5128667, 0.6156667, 0.6155667, 0.61558336, 0.51315]
	best_train_sub_head: 1
	worst: 0.5128667
	avg: 0.5745667
	best: 0.6156667

Starting e_i: 1172
Model ind 640 epoch 1172 head A head_i_epoch 0 batch 0: avg loss -3.210469 avg loss no lamb -3.210469 time 2019-03-03 21:56:36.696539
Model ind 640 epoch 1172 head A head_i_epoch 0 batch 100: avg loss -3.252384 avg loss no lamb -3.252384 time 2019-03-03 21:58:25.332625
Model ind 640 epoch 1172 head A head_i_epoch 0 batch 200: avg loss -3.298798 avg loss no lamb -3.298798 time 2019-03-03 22:00:15.102456
last batch sz 160
Model ind 640 epoch 1172 head B head_i_epoch 0 batch 0: avg loss -1.998408 avg loss no lamb -1.998408 time 2019-03-03 22:01:35.075884
Model ind 640 epoch 1172 head B head_i_epoch 0 batch 100: avg loss -1.894668 avg loss no lamb -1.894668 time 2019-03-03 22:03:24.807376
Model ind 640 epoch 1172 head B head_i_epoch 0 batch 200: avg loss -1.960711 avg loss no lamb -1.960711 time 2019-03-03 22:05:14.460155
last batch sz 160
Model ind 640 epoch 1172 head B head_i_epoch 1 batch 0: avg loss -1.902344 avg loss no lamb -1.902344 time 2019-03-03 22:06:33.669615
Model ind 640 epoch 1172 head B head_i_epoch 1 batch 100: avg loss -1.898207 avg loss no lamb -1.898207 time 2019-03-03 22:08:23.201069
Model ind 640 epoch 1172 head B head_i_epoch 1 batch 200: avg loss -2.015469 avg loss no lamb -2.015469 time 2019-03-03 22:10:12.422405
last batch sz 160
Pre: time 2019-03-03 22:11:52.439684: 
 	std: 0.050573863
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51095, 0.61411667, 0.61413336, 0.61435, 0.51098335]
	train_accs: [0.51095, 0.61411667, 0.61413336, 0.61435, 0.51098335]
	best_train_sub_head: 3
	worst: 0.51095
	avg: 0.5729067
	best: 0.61435

Starting e_i: 1173
Model ind 640 epoch 1173 head A head_i_epoch 0 batch 0: avg loss -3.338669 avg loss no lamb -3.338669 time 2019-03-03 22:11:56.056769
Model ind 640 epoch 1173 head A head_i_epoch 0 batch 100: avg loss -3.224646 avg loss no lamb -3.224646 time 2019-03-03 22:13:44.892260
Model ind 640 epoch 1173 head A head_i_epoch 0 batch 200: avg loss -3.332116 avg loss no lamb -3.332116 time 2019-03-03 22:15:36.306957
last batch sz 160
Model ind 640 epoch 1173 head B head_i_epoch 0 batch 0: avg loss -1.995133 avg loss no lamb -1.995133 time 2019-03-03 22:16:56.237789
Model ind 640 epoch 1173 head B head_i_epoch 0 batch 100: avg loss -1.894317 avg loss no lamb -1.894317 time 2019-03-03 22:18:46.070054
Model ind 640 epoch 1173 head B head_i_epoch 0 batch 200: avg loss -1.962146 avg loss no lamb -1.962146 time 2019-03-03 22:20:33.954179
last batch sz 160
Model ind 640 epoch 1173 head B head_i_epoch 1 batch 0: avg loss -1.973074 avg loss no lamb -1.973074 time 2019-03-03 22:21:53.840067
Model ind 640 epoch 1173 head B head_i_epoch 1 batch 100: avg loss -1.840498 avg loss no lamb -1.840498 time 2019-03-03 22:23:44.259204
Model ind 640 epoch 1173 head B head_i_epoch 1 batch 200: avg loss -1.985785 avg loss no lamb -1.985785 time 2019-03-03 22:25:35.747294
last batch sz 160
Pre: time 2019-03-03 22:27:18.903902: 
 	std: 0.05082422
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5113, 0.61505, 0.61508334, 0.6152, 0.51143336]
	train_accs: [0.5113, 0.61505, 0.61508334, 0.6152, 0.51143336]
	best_train_sub_head: 3
	worst: 0.5113
	avg: 0.57361335
	best: 0.6152

Starting e_i: 1174
Model ind 640 epoch 1174 head A head_i_epoch 0 batch 0: avg loss -3.255856 avg loss no lamb -3.255856 time 2019-03-03 22:27:21.814377
Model ind 640 epoch 1174 head A head_i_epoch 0 batch 100: avg loss -3.257217 avg loss no lamb -3.257217 time 2019-03-03 22:29:11.865812
Model ind 640 epoch 1174 head A head_i_epoch 0 batch 200: avg loss -3.323336 avg loss no lamb -3.323336 time 2019-03-03 22:31:03.443358
last batch sz 160
Model ind 640 epoch 1174 head B head_i_epoch 0 batch 0: avg loss -1.921060 avg loss no lamb -1.921060 time 2019-03-03 22:32:24.432313
Model ind 640 epoch 1174 head B head_i_epoch 0 batch 100: avg loss -1.917754 avg loss no lamb -1.917754 time 2019-03-03 22:34:14.054630
Model ind 640 epoch 1174 head B head_i_epoch 0 batch 200: avg loss -2.017592 avg loss no lamb -2.017592 time 2019-03-03 22:36:03.444040
last batch sz 160
Model ind 640 epoch 1174 head B head_i_epoch 1 batch 0: avg loss -1.979622 avg loss no lamb -1.979622 time 2019-03-03 22:37:23.970745
Model ind 640 epoch 1174 head B head_i_epoch 1 batch 100: avg loss -1.862218 avg loss no lamb -1.862218 time 2019-03-03 22:39:14.483674
Model ind 640 epoch 1174 head B head_i_epoch 1 batch 200: avg loss -2.037887 avg loss no lamb -2.037887 time 2019-03-03 22:41:04.267886
last batch sz 160
Pre: time 2019-03-03 22:42:43.832905: 
 	std: 0.050912667
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108, 0.61476666, 0.6146167, 0.61466664, 0.5107167]
	train_accs: [0.5108, 0.61476666, 0.6146167, 0.61466664, 0.5107167]
	best_train_sub_head: 1
	worst: 0.5107167
	avg: 0.5731133
	best: 0.61476666

Starting e_i: 1175
Model ind 640 epoch 1175 head A head_i_epoch 0 batch 0: avg loss -3.302690 avg loss no lamb -3.302690 time 2019-03-03 22:42:46.787613
Model ind 640 epoch 1175 head A head_i_epoch 0 batch 100: avg loss -3.214937 avg loss no lamb -3.214937 time 2019-03-03 22:44:36.102249
Model ind 640 epoch 1175 head A head_i_epoch 0 batch 200: avg loss -3.340959 avg loss no lamb -3.340959 time 2019-03-03 22:46:27.076457
last batch sz 160
Model ind 640 epoch 1175 head B head_i_epoch 0 batch 0: avg loss -1.960431 avg loss no lamb -1.960431 time 2019-03-03 22:47:50.415553
Model ind 640 epoch 1175 head B head_i_epoch 0 batch 100: avg loss -1.827538 avg loss no lamb -1.827538 time 2019-03-03 22:49:41.443355
Model ind 640 epoch 1175 head B head_i_epoch 0 batch 200: avg loss -1.984865 avg loss no lamb -1.984865 time 2019-03-03 22:51:32.765490
last batch sz 160
Model ind 640 epoch 1175 head B head_i_epoch 1 batch 0: avg loss -1.848772 avg loss no lamb -1.848772 time 2019-03-03 22:52:52.453259
Model ind 640 epoch 1175 head B head_i_epoch 1 batch 100: avg loss -1.916861 avg loss no lamb -1.916861 time 2019-03-03 22:54:42.141554
Model ind 640 epoch 1175 head B head_i_epoch 1 batch 200: avg loss -1.957533 avg loss no lamb -1.957533 time 2019-03-03 22:56:31.780773
last batch sz 160
Pre: time 2019-03-03 22:58:12.378240: 
 	std: 0.051082756
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101167, 0.61445, 0.61446667, 0.61445, 0.51025]
	train_accs: [0.5101167, 0.61445, 0.61446667, 0.61445, 0.51025]
	best_train_sub_head: 2
	worst: 0.5101167
	avg: 0.57274663
	best: 0.61446667

Starting e_i: 1176
Model ind 640 epoch 1176 head A head_i_epoch 0 batch 0: avg loss -3.312428 avg loss no lamb -3.312428 time 2019-03-03 22:58:16.026651
Model ind 640 epoch 1176 head A head_i_epoch 0 batch 100: avg loss -3.198107 avg loss no lamb -3.198107 time 2019-03-03 23:00:06.865221
Model ind 640 epoch 1176 head A head_i_epoch 0 batch 200: avg loss -3.350159 avg loss no lamb -3.350159 time 2019-03-03 23:01:55.839301
last batch sz 160
Model ind 640 epoch 1176 head B head_i_epoch 0 batch 0: avg loss -1.886958 avg loss no lamb -1.886958 time 2019-03-03 23:03:13.893616
Model ind 640 epoch 1176 head B head_i_epoch 0 batch 100: avg loss -1.916025 avg loss no lamb -1.916025 time 2019-03-03 23:05:06.638277
Model ind 640 epoch 1176 head B head_i_epoch 0 batch 200: avg loss -1.964695 avg loss no lamb -1.964695 time 2019-03-03 23:06:57.602940
last batch sz 160
Model ind 640 epoch 1176 head B head_i_epoch 1 batch 0: avg loss -1.949306 avg loss no lamb -1.949306 time 2019-03-03 23:08:18.165381
Model ind 640 epoch 1176 head B head_i_epoch 1 batch 100: avg loss -1.909725 avg loss no lamb -1.909725 time 2019-03-03 23:10:08.991746
Model ind 640 epoch 1176 head B head_i_epoch 1 batch 200: avg loss -1.955408 avg loss no lamb -1.955408 time 2019-03-03 23:12:00.407723
last batch sz 160
Pre: time 2019-03-03 23:13:42.439717: 
 	std: 0.050448626
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51068336, 0.6137, 0.61365, 0.6135333, 0.51061666]
	train_accs: [0.51068336, 0.6137, 0.61365, 0.6135333, 0.51061666]
	best_train_sub_head: 1
	worst: 0.51061666
	avg: 0.57243663
	best: 0.6137

Starting e_i: 1177
Model ind 640 epoch 1177 head A head_i_epoch 0 batch 0: avg loss -3.243567 avg loss no lamb -3.243567 time 2019-03-03 23:13:45.364578
Model ind 640 epoch 1177 head A head_i_epoch 0 batch 100: avg loss -3.184342 avg loss no lamb -3.184342 time 2019-03-03 23:15:35.963119
Model ind 640 epoch 1177 head A head_i_epoch 0 batch 200: avg loss -3.304523 avg loss no lamb -3.304523 time 2019-03-03 23:17:27.862543
last batch sz 160
Model ind 640 epoch 1177 head B head_i_epoch 0 batch 0: avg loss -1.999044 avg loss no lamb -1.999044 time 2019-03-03 23:18:48.732967
Model ind 640 epoch 1177 head B head_i_epoch 0 batch 100: avg loss -1.876447 avg loss no lamb -1.876447 time 2019-03-03 23:20:42.428806
Model ind 640 epoch 1177 head B head_i_epoch 0 batch 200: avg loss -1.961182 avg loss no lamb -1.961182 time 2019-03-03 23:22:32.945303
last batch sz 160
Model ind 640 epoch 1177 head B head_i_epoch 1 batch 0: avg loss -1.904279 avg loss no lamb -1.904279 time 2019-03-03 23:23:50.868310
Model ind 640 epoch 1177 head B head_i_epoch 1 batch 100: avg loss -1.914873 avg loss no lamb -1.914873 time 2019-03-03 23:25:40.790484
Model ind 640 epoch 1177 head B head_i_epoch 1 batch 200: avg loss -1.942201 avg loss no lamb -1.942201 time 2019-03-03 23:27:30.747429
last batch sz 160
Pre: time 2019-03-03 23:29:12.216565: 
 	std: 0.050690874
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114833, 0.61505, 0.61506665, 0.61495, 0.51161665]
	train_accs: [0.5114833, 0.61505, 0.61506665, 0.61495, 0.51161665]
	best_train_sub_head: 2
	worst: 0.5114833
	avg: 0.5736333
	best: 0.61506665

Starting e_i: 1178
Model ind 640 epoch 1178 head A head_i_epoch 0 batch 0: avg loss -3.253713 avg loss no lamb -3.253713 time 2019-03-03 23:29:15.145526
Model ind 640 epoch 1178 head A head_i_epoch 0 batch 100: avg loss -3.274010 avg loss no lamb -3.274010 time 2019-03-03 23:31:06.016085
Model ind 640 epoch 1178 head A head_i_epoch 0 batch 200: avg loss -3.410960 avg loss no lamb -3.410960 time 2019-03-03 23:32:56.261242
last batch sz 160
Model ind 640 epoch 1178 head B head_i_epoch 0 batch 0: avg loss -1.964104 avg loss no lamb -1.964104 time 2019-03-03 23:34:16.338177
Model ind 640 epoch 1178 head B head_i_epoch 0 batch 100: avg loss -1.883851 avg loss no lamb -1.883851 time 2019-03-03 23:36:06.317139
Model ind 640 epoch 1178 head B head_i_epoch 0 batch 200: avg loss -2.034653 avg loss no lamb -2.034653 time 2019-03-03 23:37:57.470887
last batch sz 160
Model ind 640 epoch 1178 head B head_i_epoch 1 batch 0: avg loss -1.931090 avg loss no lamb -1.931090 time 2019-03-03 23:39:17.174735
Model ind 640 epoch 1178 head B head_i_epoch 1 batch 100: avg loss -1.945530 avg loss no lamb -1.945530 time 2019-03-03 23:41:06.636467
Model ind 640 epoch 1178 head B head_i_epoch 1 batch 200: avg loss -1.979203 avg loss no lamb -1.979203 time 2019-03-03 23:42:57.374197
last batch sz 160
Pre: time 2019-03-03 23:44:36.487134: 
 	std: 0.050596967
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51091665, 0.6142167, 0.61415, 0.6143, 0.51096666]
	train_accs: [0.51091665, 0.6142167, 0.61415, 0.6143, 0.51096666]
	best_train_sub_head: 3
	worst: 0.51091665
	avg: 0.57291
	best: 0.6143

Starting e_i: 1179
Model ind 640 epoch 1179 head A head_i_epoch 0 batch 0: avg loss -3.278969 avg loss no lamb -3.278969 time 2019-03-03 23:44:39.655022
Model ind 640 epoch 1179 head A head_i_epoch 0 batch 100: avg loss -3.280078 avg loss no lamb -3.280078 time 2019-03-03 23:46:29.868448
Model ind 640 epoch 1179 head A head_i_epoch 0 batch 200: avg loss -3.297546 avg loss no lamb -3.297546 time 2019-03-03 23:48:19.898927
last batch sz 160
Model ind 640 epoch 1179 head B head_i_epoch 0 batch 0: avg loss -1.957040 avg loss no lamb -1.957040 time 2019-03-03 23:49:40.455681
Model ind 640 epoch 1179 head B head_i_epoch 0 batch 100: avg loss -1.903601 avg loss no lamb -1.903601 time 2019-03-03 23:51:31.205898
Model ind 640 epoch 1179 head B head_i_epoch 0 batch 200: avg loss -2.021291 avg loss no lamb -2.021291 time 2019-03-03 23:53:23.267598
last batch sz 160
Model ind 640 epoch 1179 head B head_i_epoch 1 batch 0: avg loss -1.961539 avg loss no lamb -1.961539 time 2019-03-03 23:54:43.527632
Model ind 640 epoch 1179 head B head_i_epoch 1 batch 100: avg loss -1.923255 avg loss no lamb -1.923255 time 2019-03-03 23:56:33.856844
Model ind 640 epoch 1179 head B head_i_epoch 1 batch 200: avg loss -1.964487 avg loss no lamb -1.964487 time 2019-03-03 23:58:23.905293
last batch sz 160
Pre: time 2019-03-04 00:00:04.840109: 
 	std: 0.05057801
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51143336, 0.6148667, 0.61476666, 0.6149167, 0.51178336]
	train_accs: [0.51143336, 0.6148667, 0.61476666, 0.6149167, 0.51178336]
	best_train_sub_head: 3
	worst: 0.51143336
	avg: 0.5735533
	best: 0.6149167

Starting e_i: 1180
Model ind 640 epoch 1180 head A head_i_epoch 0 batch 0: avg loss -3.264300 avg loss no lamb -3.264300 time 2019-03-04 00:00:07.740287
Model ind 640 epoch 1180 head A head_i_epoch 0 batch 100: avg loss -3.178158 avg loss no lamb -3.178158 time 2019-03-04 00:01:58.253407
Model ind 640 epoch 1180 head A head_i_epoch 0 batch 200: avg loss -3.274046 avg loss no lamb -3.274046 time 2019-03-04 00:03:48.961193
last batch sz 160
Model ind 640 epoch 1180 head B head_i_epoch 0 batch 0: avg loss -1.929596 avg loss no lamb -1.929596 time 2019-03-04 00:05:08.069343
Model ind 640 epoch 1180 head B head_i_epoch 0 batch 100: avg loss -1.895539 avg loss no lamb -1.895539 time 2019-03-04 00:06:56.081207
Model ind 640 epoch 1180 head B head_i_epoch 0 batch 200: avg loss -1.956467 avg loss no lamb -1.956467 time 2019-03-04 00:08:46.518656
last batch sz 160
Model ind 640 epoch 1180 head B head_i_epoch 1 batch 0: avg loss -2.013149 avg loss no lamb -2.013149 time 2019-03-04 00:10:09.081939
Model ind 640 epoch 1180 head B head_i_epoch 1 batch 100: avg loss -1.897714 avg loss no lamb -1.897714 time 2019-03-04 00:11:59.214763
Model ind 640 epoch 1180 head B head_i_epoch 1 batch 200: avg loss -2.031595 avg loss no lamb -2.031595 time 2019-03-04 00:13:50.131557
last batch sz 160
Pre: time 2019-03-04 00:15:33.926040: 
 	std: 0.050501756
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51066667, 0.61371666, 0.61371666, 0.61395, 0.51075]
	train_accs: [0.51066667, 0.61371666, 0.61371666, 0.61395, 0.51075]
	best_train_sub_head: 3
	worst: 0.51066667
	avg: 0.57256
	best: 0.61395

Starting e_i: 1181
Model ind 640 epoch 1181 head A head_i_epoch 0 batch 0: avg loss -3.274950 avg loss no lamb -3.274950 time 2019-03-04 00:15:44.425746
Model ind 640 epoch 1181 head A head_i_epoch 0 batch 100: avg loss -3.147860 avg loss no lamb -3.147860 time 2019-03-04 00:17:36.353869
Model ind 640 epoch 1181 head A head_i_epoch 0 batch 200: avg loss -3.237059 avg loss no lamb -3.237059 time 2019-03-04 00:19:29.325943
last batch sz 160
Model ind 640 epoch 1181 head B head_i_epoch 0 batch 0: avg loss -1.972136 avg loss no lamb -1.972136 time 2019-03-04 00:20:50.501025
Model ind 640 epoch 1181 head B head_i_epoch 0 batch 100: avg loss -1.906888 avg loss no lamb -1.906888 time 2019-03-04 00:22:41.860134
Model ind 640 epoch 1181 head B head_i_epoch 0 batch 200: avg loss -1.989225 avg loss no lamb -1.989225 time 2019-03-04 00:24:33.696403
last batch sz 160
Model ind 640 epoch 1181 head B head_i_epoch 1 batch 0: avg loss -1.933744 avg loss no lamb -1.933744 time 2019-03-04 00:25:55.031573
Model ind 640 epoch 1181 head B head_i_epoch 1 batch 100: avg loss -1.891433 avg loss no lamb -1.891433 time 2019-03-04 00:27:42.323099
Model ind 640 epoch 1181 head B head_i_epoch 1 batch 200: avg loss -1.994370 avg loss no lamb -1.994370 time 2019-03-04 00:29:32.513234
last batch sz 160
Pre: time 2019-03-04 00:31:15.114045: 
 	std: 0.05062009
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51166666, 0.61501664, 0.6149667, 0.6151, 0.51173335]
	train_accs: [0.51166666, 0.61501664, 0.6149667, 0.6151, 0.51173335]
	best_train_sub_head: 3
	worst: 0.51166666
	avg: 0.5736967
	best: 0.6151

Starting e_i: 1182
Model ind 640 epoch 1182 head A head_i_epoch 0 batch 0: avg loss -3.186774 avg loss no lamb -3.186774 time 2019-03-04 00:31:17.980568
Model ind 640 epoch 1182 head A head_i_epoch 0 batch 100: avg loss -3.241621 avg loss no lamb -3.241621 time 2019-03-04 00:33:10.787067
Model ind 640 epoch 1182 head A head_i_epoch 0 batch 200: avg loss -3.322877 avg loss no lamb -3.322877 time 2019-03-04 00:35:01.836736
last batch sz 160
Model ind 640 epoch 1182 head B head_i_epoch 0 batch 0: avg loss -1.853816 avg loss no lamb -1.853816 time 2019-03-04 00:36:24.157492
Model ind 640 epoch 1182 head B head_i_epoch 0 batch 100: avg loss -1.893118 avg loss no lamb -1.893118 time 2019-03-04 00:38:15.194042
Model ind 640 epoch 1182 head B head_i_epoch 0 batch 200: avg loss -1.986795 avg loss no lamb -1.986795 time 2019-03-04 00:40:07.565572
last batch sz 160
Model ind 640 epoch 1182 head B head_i_epoch 1 batch 0: avg loss -1.945400 avg loss no lamb -1.945400 time 2019-03-04 00:41:31.279806
Model ind 640 epoch 1182 head B head_i_epoch 1 batch 100: avg loss -1.948324 avg loss no lamb -1.948324 time 2019-03-04 00:43:23.902459
Model ind 640 epoch 1182 head B head_i_epoch 1 batch 200: avg loss -1.951313 avg loss no lamb -1.951313 time 2019-03-04 00:45:15.725984
last batch sz 160
Pre: time 2019-03-04 00:46:58.977972: 
 	std: 0.050773874
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118167, 0.61548334, 0.61548334, 0.61558336, 0.5119333]
	train_accs: [0.5118167, 0.61548334, 0.61548334, 0.61558336, 0.5119333]
	best_train_sub_head: 3
	worst: 0.5118167
	avg: 0.57406
	best: 0.61558336

Starting e_i: 1183
Model ind 640 epoch 1183 head A head_i_epoch 0 batch 0: avg loss -3.380014 avg loss no lamb -3.380014 time 2019-03-04 00:47:01.729410
Model ind 640 epoch 1183 head A head_i_epoch 0 batch 100: avg loss -3.181306 avg loss no lamb -3.181306 time 2019-03-04 00:48:49.063121
Model ind 640 epoch 1183 head A head_i_epoch 0 batch 200: avg loss -3.269684 avg loss no lamb -3.269684 time 2019-03-04 00:50:39.643974
last batch sz 160
Model ind 640 epoch 1183 head B head_i_epoch 0 batch 0: avg loss -1.958160 avg loss no lamb -1.958160 time 2019-03-04 00:52:00.805808
Model ind 640 epoch 1183 head B head_i_epoch 0 batch 100: avg loss -1.968480 avg loss no lamb -1.968480 time 2019-03-04 00:53:52.176790
Model ind 640 epoch 1183 head B head_i_epoch 0 batch 200: avg loss -2.024982 avg loss no lamb -2.024982 time 2019-03-04 00:55:45.205408
last batch sz 160
Model ind 640 epoch 1183 head B head_i_epoch 1 batch 0: avg loss -1.947179 avg loss no lamb -1.947179 time 2019-03-04 00:57:07.732804
Model ind 640 epoch 1183 head B head_i_epoch 1 batch 100: avg loss -1.929587 avg loss no lamb -1.929587 time 2019-03-04 00:59:00.927058
Model ind 640 epoch 1183 head B head_i_epoch 1 batch 200: avg loss -2.011188 avg loss no lamb -2.011188 time 2019-03-04 01:00:51.894962
last batch sz 160
Pre: time 2019-03-04 01:02:36.152695: 
 	std: 0.050603762
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51203334, 0.6152667, 0.6152667, 0.61535, 0.51196665]
	train_accs: [0.51203334, 0.6152667, 0.6152667, 0.61535, 0.51196665]
	best_train_sub_head: 3
	worst: 0.51196665
	avg: 0.5739767
	best: 0.61535

Starting e_i: 1184
Model ind 640 epoch 1184 head A head_i_epoch 0 batch 0: avg loss -3.321995 avg loss no lamb -3.321995 time 2019-03-04 01:02:38.958523
Model ind 640 epoch 1184 head A head_i_epoch 0 batch 100: avg loss -3.278349 avg loss no lamb -3.278349 time 2019-03-04 01:04:30.443309
Model ind 640 epoch 1184 head A head_i_epoch 0 batch 200: avg loss -3.358282 avg loss no lamb -3.358282 time 2019-03-04 01:06:21.640466
last batch sz 160
Model ind 640 epoch 1184 head B head_i_epoch 0 batch 0: avg loss -1.925151 avg loss no lamb -1.925151 time 2019-03-04 01:07:42.634306
Model ind 640 epoch 1184 head B head_i_epoch 0 batch 100: avg loss -1.850366 avg loss no lamb -1.850366 time 2019-03-04 01:09:32.045422
Model ind 640 epoch 1184 head B head_i_epoch 0 batch 200: avg loss -2.029868 avg loss no lamb -2.029868 time 2019-03-04 01:11:21.773347
last batch sz 160
Model ind 640 epoch 1184 head B head_i_epoch 1 batch 0: avg loss -1.994974 avg loss no lamb -1.994974 time 2019-03-04 01:12:40.898304
Model ind 640 epoch 1184 head B head_i_epoch 1 batch 100: avg loss -1.831747 avg loss no lamb -1.831747 time 2019-03-04 01:14:33.319300
Model ind 640 epoch 1184 head B head_i_epoch 1 batch 200: avg loss -1.948944 avg loss no lamb -1.948944 time 2019-03-04 01:16:23.276524
last batch sz 160
Pre: time 2019-03-04 01:18:03.366271: 
 	std: 0.050718233
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51165, 0.61495, 0.6149833, 0.61495, 0.51121664]
	train_accs: [0.51165, 0.61495, 0.6149833, 0.61495, 0.51121664]
	best_train_sub_head: 2
	worst: 0.51121664
	avg: 0.57355
	best: 0.6149833

Starting e_i: 1185
Model ind 640 epoch 1185 head A head_i_epoch 0 batch 0: avg loss -3.311411 avg loss no lamb -3.311411 time 2019-03-04 01:18:06.769509
Model ind 640 epoch 1185 head A head_i_epoch 0 batch 100: avg loss -3.316313 avg loss no lamb -3.316313 time 2019-03-04 01:19:56.341221
Model ind 640 epoch 1185 head A head_i_epoch 0 batch 200: avg loss -3.302205 avg loss no lamb -3.302205 time 2019-03-04 01:21:46.115532
last batch sz 160
Model ind 640 epoch 1185 head B head_i_epoch 0 batch 0: avg loss -1.921825 avg loss no lamb -1.921825 time 2019-03-04 01:23:06.534677
Model ind 640 epoch 1185 head B head_i_epoch 0 batch 100: avg loss -1.932964 avg loss no lamb -1.932964 time 2019-03-04 01:24:55.898186
Model ind 640 epoch 1185 head B head_i_epoch 0 batch 200: avg loss -1.925457 avg loss no lamb -1.925457 time 2019-03-04 01:26:46.209901
last batch sz 160
Model ind 640 epoch 1185 head B head_i_epoch 1 batch 0: avg loss -1.891329 avg loss no lamb -1.891329 time 2019-03-04 01:28:05.263168
Model ind 640 epoch 1185 head B head_i_epoch 1 batch 100: avg loss -1.939422 avg loss no lamb -1.939422 time 2019-03-04 01:29:52.811898
Model ind 640 epoch 1185 head B head_i_epoch 1 batch 200: avg loss -1.938172 avg loss no lamb -1.938172 time 2019-03-04 01:31:44.010838
last batch sz 160
Pre: time 2019-03-04 01:33:24.516834: 
 	std: 0.05041083
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5126167, 0.6152667, 0.61518335, 0.61525, 0.51205]
	train_accs: [0.5126167, 0.6152667, 0.61518335, 0.61525, 0.51205]
	best_train_sub_head: 1
	worst: 0.51205
	avg: 0.5740734
	best: 0.6152667

Starting e_i: 1186
Model ind 640 epoch 1186 head A head_i_epoch 0 batch 0: avg loss -3.303838 avg loss no lamb -3.303838 time 2019-03-04 01:33:28.135537
Model ind 640 epoch 1186 head A head_i_epoch 0 batch 100: avg loss -3.296064 avg loss no lamb -3.296064 time 2019-03-04 01:35:18.057354
Model ind 640 epoch 1186 head A head_i_epoch 0 batch 200: avg loss -3.320184 avg loss no lamb -3.320184 time 2019-03-04 01:37:08.336224
last batch sz 160
Model ind 640 epoch 1186 head B head_i_epoch 0 batch 0: avg loss -1.927455 avg loss no lamb -1.927455 time 2019-03-04 01:38:28.298808
Model ind 640 epoch 1186 head B head_i_epoch 0 batch 100: avg loss -1.931373 avg loss no lamb -1.931373 time 2019-03-04 01:40:18.601634
Model ind 640 epoch 1186 head B head_i_epoch 0 batch 200: avg loss -2.021800 avg loss no lamb -2.021800 time 2019-03-04 01:42:09.108614
last batch sz 160
Model ind 640 epoch 1186 head B head_i_epoch 1 batch 0: avg loss -1.899013 avg loss no lamb -1.899013 time 2019-03-04 01:43:28.620136
Model ind 640 epoch 1186 head B head_i_epoch 1 batch 100: avg loss -1.880999 avg loss no lamb -1.880999 time 2019-03-04 01:45:18.075920
Model ind 640 epoch 1186 head B head_i_epoch 1 batch 200: avg loss -2.029472 avg loss no lamb -2.029472 time 2019-03-04 01:47:10.385649
last batch sz 160
Pre: time 2019-03-04 01:48:51.151748: 
 	std: 0.0508127
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51131666, 0.61473334, 0.6145167, 0.6145833, 0.5104667]
	train_accs: [0.51131666, 0.61473334, 0.6145167, 0.6145833, 0.5104667]
	best_train_sub_head: 1
	worst: 0.5104667
	avg: 0.57312334
	best: 0.61473334

Starting e_i: 1187
Model ind 640 epoch 1187 head A head_i_epoch 0 batch 0: avg loss -3.215947 avg loss no lamb -3.215947 time 2019-03-04 01:48:54.012694
Model ind 640 epoch 1187 head A head_i_epoch 0 batch 100: avg loss -3.162499 avg loss no lamb -3.162499 time 2019-03-04 01:50:42.342416
Model ind 640 epoch 1187 head A head_i_epoch 0 batch 200: avg loss -3.359303 avg loss no lamb -3.359303 time 2019-03-04 01:52:30.985203
last batch sz 160
Model ind 640 epoch 1187 head B head_i_epoch 0 batch 0: avg loss -1.899593 avg loss no lamb -1.899593 time 2019-03-04 01:53:50.901211
Model ind 640 epoch 1187 head B head_i_epoch 0 batch 100: avg loss -1.898717 avg loss no lamb -1.898717 time 2019-03-04 01:55:40.364959
Model ind 640 epoch 1187 head B head_i_epoch 0 batch 200: avg loss -1.928123 avg loss no lamb -1.928123 time 2019-03-04 01:57:30.785828
last batch sz 160
Model ind 640 epoch 1187 head B head_i_epoch 1 batch 0: avg loss -1.905785 avg loss no lamb -1.905785 time 2019-03-04 01:58:51.191149
Model ind 640 epoch 1187 head B head_i_epoch 1 batch 100: avg loss -1.938968 avg loss no lamb -1.938968 time 2019-03-04 02:00:42.123574
Model ind 640 epoch 1187 head B head_i_epoch 1 batch 200: avg loss -1.928914 avg loss no lamb -1.928914 time 2019-03-04 02:02:34.210980
last batch sz 160
Pre: time 2019-03-04 02:04:17.568708: 
 	std: 0.050746784
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118, 0.61516666, 0.61523336, 0.61518335, 0.5114167]
	train_accs: [0.5118, 0.61516666, 0.61523336, 0.61518335, 0.5114167]
	best_train_sub_head: 2
	worst: 0.5114167
	avg: 0.57376003
	best: 0.61523336

Starting e_i: 1188
Model ind 640 epoch 1188 head A head_i_epoch 0 batch 0: avg loss -3.323624 avg loss no lamb -3.323624 time 2019-03-04 02:04:20.390833
Model ind 640 epoch 1188 head A head_i_epoch 0 batch 100: avg loss -3.190693 avg loss no lamb -3.190693 time 2019-03-04 02:06:11.752659
Model ind 640 epoch 1188 head A head_i_epoch 0 batch 200: avg loss -3.291079 avg loss no lamb -3.291079 time 2019-03-04 02:08:02.399933
last batch sz 160
Model ind 640 epoch 1188 head B head_i_epoch 0 batch 0: avg loss -2.000141 avg loss no lamb -2.000141 time 2019-03-04 02:09:23.509516
Model ind 640 epoch 1188 head B head_i_epoch 0 batch 100: avg loss -1.851777 avg loss no lamb -1.851777 time 2019-03-04 02:11:14.623572
Model ind 640 epoch 1188 head B head_i_epoch 0 batch 200: avg loss -2.010587 avg loss no lamb -2.010587 time 2019-03-04 02:13:02.282512
last batch sz 160
Model ind 640 epoch 1188 head B head_i_epoch 1 batch 0: avg loss -1.976358 avg loss no lamb -1.976358 time 2019-03-04 02:14:22.637208
Model ind 640 epoch 1188 head B head_i_epoch 1 batch 100: avg loss -1.942286 avg loss no lamb -1.942286 time 2019-03-04 02:16:14.216779
Model ind 640 epoch 1188 head B head_i_epoch 1 batch 200: avg loss -2.012980 avg loss no lamb -2.012980 time 2019-03-04 02:18:08.604627
last batch sz 160
Pre: time 2019-03-04 02:19:51.798724: 
 	std: 0.05057804
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51086664, 0.6138667, 0.61408335, 0.6139, 0.51055]
	train_accs: [0.51086664, 0.6138667, 0.61408335, 0.6139, 0.51055]
	best_train_sub_head: 2
	worst: 0.51055
	avg: 0.57265335
	best: 0.61408335

Starting e_i: 1189
Model ind 640 epoch 1189 head A head_i_epoch 0 batch 0: avg loss -3.288166 avg loss no lamb -3.288166 time 2019-03-04 02:19:55.698053
Model ind 640 epoch 1189 head A head_i_epoch 0 batch 100: avg loss -3.232021 avg loss no lamb -3.232021 time 2019-03-04 02:21:47.574225
Model ind 640 epoch 1189 head A head_i_epoch 0 batch 200: avg loss -3.317426 avg loss no lamb -3.317426 time 2019-03-04 02:23:39.726974
last batch sz 160
Model ind 640 epoch 1189 head B head_i_epoch 0 batch 0: avg loss -1.903182 avg loss no lamb -1.903182 time 2019-03-04 02:25:02.417844
Model ind 640 epoch 1189 head B head_i_epoch 0 batch 100: avg loss -1.920301 avg loss no lamb -1.920301 time 2019-03-04 02:26:54.366643
Model ind 640 epoch 1189 head B head_i_epoch 0 batch 200: avg loss -1.840059 avg loss no lamb -1.840059 time 2019-03-04 02:28:46.011185
last batch sz 160
Model ind 640 epoch 1189 head B head_i_epoch 1 batch 0: avg loss -1.891138 avg loss no lamb -1.891138 time 2019-03-04 02:30:07.521373
Model ind 640 epoch 1189 head B head_i_epoch 1 batch 100: avg loss -1.919178 avg loss no lamb -1.919178 time 2019-03-04 02:31:59.713180
Model ind 640 epoch 1189 head B head_i_epoch 1 batch 200: avg loss -1.998144 avg loss no lamb -1.998144 time 2019-03-04 02:33:46.443632
last batch sz 160
Pre: time 2019-03-04 02:35:29.479994: 
 	std: 0.05055889
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51168334, 0.61473334, 0.61473334, 0.61481667, 0.51143336]
	train_accs: [0.51168334, 0.61473334, 0.61473334, 0.61481667, 0.51143336]
	best_train_sub_head: 3
	worst: 0.51143336
	avg: 0.57348
	best: 0.61481667

Starting e_i: 1190
Model ind 640 epoch 1190 head A head_i_epoch 0 batch 0: avg loss -3.250910 avg loss no lamb -3.250910 time 2019-03-04 02:35:32.346400
Model ind 640 epoch 1190 head A head_i_epoch 0 batch 100: avg loss -3.211211 avg loss no lamb -3.211211 time 2019-03-04 02:37:22.257986
Model ind 640 epoch 1190 head A head_i_epoch 0 batch 200: avg loss -3.303998 avg loss no lamb -3.303998 time 2019-03-04 02:39:12.438725
last batch sz 160
Model ind 640 epoch 1190 head B head_i_epoch 0 batch 0: avg loss -1.926734 avg loss no lamb -1.926734 time 2019-03-04 02:40:33.054223
Model ind 640 epoch 1190 head B head_i_epoch 0 batch 100: avg loss -1.888880 avg loss no lamb -1.888880 time 2019-03-04 02:42:22.427757
Model ind 640 epoch 1190 head B head_i_epoch 0 batch 200: avg loss -1.991718 avg loss no lamb -1.991718 time 2019-03-04 02:44:13.006924
last batch sz 160
Model ind 640 epoch 1190 head B head_i_epoch 1 batch 0: avg loss -1.970734 avg loss no lamb -1.970734 time 2019-03-04 02:45:33.238937
Model ind 640 epoch 1190 head B head_i_epoch 1 batch 100: avg loss -1.877817 avg loss no lamb -1.877817 time 2019-03-04 02:47:23.042829
Model ind 640 epoch 1190 head B head_i_epoch 1 batch 200: avg loss -1.987466 avg loss no lamb -1.987466 time 2019-03-04 02:49:12.283357
last batch sz 160
Pre: time 2019-03-04 02:50:53.537895: 
 	std: 0.05059693
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108, 0.61411667, 0.61408335, 0.61406666, 0.5108167]
	train_accs: [0.5108, 0.61411667, 0.61408335, 0.61406666, 0.5108167]
	best_train_sub_head: 1
	worst: 0.5108
	avg: 0.5727767
	best: 0.61411667

Starting e_i: 1191
Model ind 640 epoch 1191 head A head_i_epoch 0 batch 0: avg loss -3.335309 avg loss no lamb -3.335309 time 2019-03-04 02:50:59.766993
Model ind 640 epoch 1191 head A head_i_epoch 0 batch 100: avg loss -3.259937 avg loss no lamb -3.259937 time 2019-03-04 02:52:51.527060
Model ind 640 epoch 1191 head A head_i_epoch 0 batch 200: avg loss -3.304567 avg loss no lamb -3.304567 time 2019-03-04 02:54:38.482342
last batch sz 160
Model ind 640 epoch 1191 head B head_i_epoch 0 batch 0: avg loss -1.901469 avg loss no lamb -1.901469 time 2019-03-04 02:55:58.322938
Model ind 640 epoch 1191 head B head_i_epoch 0 batch 100: avg loss -1.942917 avg loss no lamb -1.942917 time 2019-03-04 02:57:48.599614
Model ind 640 epoch 1191 head B head_i_epoch 0 batch 200: avg loss -1.966658 avg loss no lamb -1.966658 time 2019-03-04 02:59:39.447976
last batch sz 160
Model ind 640 epoch 1191 head B head_i_epoch 1 batch 0: avg loss -1.941596 avg loss no lamb -1.941596 time 2019-03-04 03:01:00.735269
Model ind 640 epoch 1191 head B head_i_epoch 1 batch 100: avg loss -1.910813 avg loss no lamb -1.910813 time 2019-03-04 03:02:52.678329
Model ind 640 epoch 1191 head B head_i_epoch 1 batch 200: avg loss -1.934538 avg loss no lamb -1.934538 time 2019-03-04 03:04:42.797751
last batch sz 160
Pre: time 2019-03-04 03:06:26.165013: 
 	std: 0.050439086
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51173335, 0.61465, 0.61478335, 0.61471665, 0.51178336]
	train_accs: [0.51173335, 0.61465, 0.61478335, 0.61471665, 0.51178336]
	best_train_sub_head: 2
	worst: 0.51173335
	avg: 0.5735333
	best: 0.61478335

Starting e_i: 1192
Model ind 640 epoch 1192 head A head_i_epoch 0 batch 0: avg loss -3.293566 avg loss no lamb -3.293566 time 2019-03-04 03:06:29.002288
Model ind 640 epoch 1192 head A head_i_epoch 0 batch 100: avg loss -3.263561 avg loss no lamb -3.263561 time 2019-03-04 03:08:21.483926
Model ind 640 epoch 1192 head A head_i_epoch 0 batch 200: avg loss -3.395411 avg loss no lamb -3.395411 time 2019-03-04 03:10:12.142186
last batch sz 160
Model ind 640 epoch 1192 head B head_i_epoch 0 batch 0: avg loss -1.930469 avg loss no lamb -1.930469 time 2019-03-04 03:11:33.670214
Model ind 640 epoch 1192 head B head_i_epoch 0 batch 100: avg loss -1.983296 avg loss no lamb -1.983296 time 2019-03-04 03:13:25.444526
Model ind 640 epoch 1192 head B head_i_epoch 0 batch 200: avg loss -2.013240 avg loss no lamb -2.013240 time 2019-03-04 03:15:15.203608
last batch sz 160
Model ind 640 epoch 1192 head B head_i_epoch 1 batch 0: avg loss -1.964215 avg loss no lamb -1.964215 time 2019-03-04 03:16:34.276533
Model ind 640 epoch 1192 head B head_i_epoch 1 batch 100: avg loss -1.822275 avg loss no lamb -1.822275 time 2019-03-04 03:18:25.815104
Model ind 640 epoch 1192 head B head_i_epoch 1 batch 200: avg loss -1.957590 avg loss no lamb -1.957590 time 2019-03-04 03:20:18.898643
last batch sz 160
Pre: time 2019-03-04 03:22:02.921560: 
 	std: 0.05071807
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51126665, 0.61471665, 0.61481667, 0.61485, 0.51126665]
	train_accs: [0.51126665, 0.61471665, 0.61481667, 0.61485, 0.51126665]
	best_train_sub_head: 3
	worst: 0.51126665
	avg: 0.57338333
	best: 0.61485

Starting e_i: 1193
Model ind 640 epoch 1193 head A head_i_epoch 0 batch 0: avg loss -3.304664 avg loss no lamb -3.304664 time 2019-03-04 03:22:05.920312
Model ind 640 epoch 1193 head A head_i_epoch 0 batch 100: avg loss -3.216021 avg loss no lamb -3.216021 time 2019-03-04 03:23:58.952352
Model ind 640 epoch 1193 head A head_i_epoch 0 batch 200: avg loss -3.292085 avg loss no lamb -3.292085 time 2019-03-04 03:25:51.221900
last batch sz 160
Model ind 640 epoch 1193 head B head_i_epoch 0 batch 0: avg loss -1.954898 avg loss no lamb -1.954898 time 2019-03-04 03:27:12.016725
Model ind 640 epoch 1193 head B head_i_epoch 0 batch 100: avg loss -1.883890 avg loss no lamb -1.883890 time 2019-03-04 03:29:03.207687
Model ind 640 epoch 1193 head B head_i_epoch 0 batch 200: avg loss -1.956715 avg loss no lamb -1.956715 time 2019-03-04 03:30:56.383224
last batch sz 160
Model ind 640 epoch 1193 head B head_i_epoch 1 batch 0: avg loss -1.933743 avg loss no lamb -1.933743 time 2019-03-04 03:32:18.953549
Model ind 640 epoch 1193 head B head_i_epoch 1 batch 100: avg loss -1.788873 avg loss no lamb -1.788873 time 2019-03-04 03:34:12.283957
Model ind 640 epoch 1193 head B head_i_epoch 1 batch 200: avg loss -1.972314 avg loss no lamb -1.972314 time 2019-03-04 03:36:04.043875
last batch sz 160
Pre: time 2019-03-04 03:37:43.781384: 
 	std: 0.050542526
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51136667, 0.6145333, 0.6146167, 0.6146333, 0.5114833]
	train_accs: [0.51136667, 0.6145333, 0.6146167, 0.6146333, 0.5114833]
	best_train_sub_head: 3
	worst: 0.51136667
	avg: 0.5733267
	best: 0.6146333

Starting e_i: 1194
Model ind 640 epoch 1194 head A head_i_epoch 0 batch 0: avg loss -3.317664 avg loss no lamb -3.317664 time 2019-03-04 03:37:47.903019
Model ind 640 epoch 1194 head A head_i_epoch 0 batch 100: avg loss -3.257052 avg loss no lamb -3.257052 time 2019-03-04 03:39:39.312168
Model ind 640 epoch 1194 head A head_i_epoch 0 batch 200: avg loss -3.314971 avg loss no lamb -3.314971 time 2019-03-04 03:41:30.738918
last batch sz 160
Model ind 640 epoch 1194 head B head_i_epoch 0 batch 0: avg loss -1.945684 avg loss no lamb -1.945684 time 2019-03-04 03:42:50.938296
Model ind 640 epoch 1194 head B head_i_epoch 0 batch 100: avg loss -1.979245 avg loss no lamb -1.979245 time 2019-03-04 03:44:41.020937
Model ind 640 epoch 1194 head B head_i_epoch 0 batch 200: avg loss -1.990146 avg loss no lamb -1.990146 time 2019-03-04 03:46:30.812343
last batch sz 160
Model ind 640 epoch 1194 head B head_i_epoch 1 batch 0: avg loss -1.947368 avg loss no lamb -1.947368 time 2019-03-04 03:47:52.065836
Model ind 640 epoch 1194 head B head_i_epoch 1 batch 100: avg loss -1.969621 avg loss no lamb -1.969621 time 2019-03-04 03:49:42.044493
Model ind 640 epoch 1194 head B head_i_epoch 1 batch 200: avg loss -1.937110 avg loss no lamb -1.937110 time 2019-03-04 03:51:32.086275
last batch sz 160
Pre: time 2019-03-04 03:53:12.893734: 
 	std: 0.050588794
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51045, 0.61378336, 0.6138, 0.61373335, 0.51056665]
	train_accs: [0.51045, 0.61378336, 0.6138, 0.61373335, 0.51056665]
	best_train_sub_head: 2
	worst: 0.51045
	avg: 0.5724667
	best: 0.6138

Starting e_i: 1195
Model ind 640 epoch 1195 head A head_i_epoch 0 batch 0: avg loss -3.254880 avg loss no lamb -3.254880 time 2019-03-04 03:53:15.763466
Model ind 640 epoch 1195 head A head_i_epoch 0 batch 100: avg loss -3.219400 avg loss no lamb -3.219400 time 2019-03-04 03:55:05.045010
Model ind 640 epoch 1195 head A head_i_epoch 0 batch 200: avg loss -3.233127 avg loss no lamb -3.233127 time 2019-03-04 03:56:56.307140
last batch sz 160
Model ind 640 epoch 1195 head B head_i_epoch 0 batch 0: avg loss -1.902955 avg loss no lamb -1.902955 time 2019-03-04 03:58:14.955903
Model ind 640 epoch 1195 head B head_i_epoch 0 batch 100: avg loss -1.879997 avg loss no lamb -1.879997 time 2019-03-04 04:00:05.585662
Model ind 640 epoch 1195 head B head_i_epoch 0 batch 200: avg loss -1.963773 avg loss no lamb -1.963773 time 2019-03-04 04:01:54.965499
last batch sz 160
Model ind 640 epoch 1195 head B head_i_epoch 1 batch 0: avg loss -1.963564 avg loss no lamb -1.963564 time 2019-03-04 04:03:14.738578
Model ind 640 epoch 1195 head B head_i_epoch 1 batch 100: avg loss -1.904892 avg loss no lamb -1.904892 time 2019-03-04 04:05:04.307497
Model ind 640 epoch 1195 head B head_i_epoch 1 batch 200: avg loss -1.959991 avg loss no lamb -1.959991 time 2019-03-04 04:06:53.576395
last batch sz 160
Pre: time 2019-03-04 04:08:33.907921: 
 	std: 0.050704487
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5107333, 0.61415, 0.61411667, 0.61413336, 0.51053333]
	train_accs: [0.5107333, 0.61415, 0.61411667, 0.61413336, 0.51053333]
	best_train_sub_head: 1
	worst: 0.51053333
	avg: 0.5727333
	best: 0.61415

Starting e_i: 1196
Model ind 640 epoch 1196 head A head_i_epoch 0 batch 0: avg loss -3.352194 avg loss no lamb -3.352194 time 2019-03-04 04:08:36.749357
Model ind 640 epoch 1196 head A head_i_epoch 0 batch 100: avg loss -3.133482 avg loss no lamb -3.133482 time 2019-03-04 04:10:25.575915
Model ind 640 epoch 1196 head A head_i_epoch 0 batch 200: avg loss -3.333678 avg loss no lamb -3.333678 time 2019-03-04 04:12:15.222996
last batch sz 160
Model ind 640 epoch 1196 head B head_i_epoch 0 batch 0: avg loss -1.938344 avg loss no lamb -1.938344 time 2019-03-04 04:13:36.184582
Model ind 640 epoch 1196 head B head_i_epoch 0 batch 100: avg loss -1.958553 avg loss no lamb -1.958553 time 2019-03-04 04:15:25.730336
Model ind 640 epoch 1196 head B head_i_epoch 0 batch 200: avg loss -1.966142 avg loss no lamb -1.966142 time 2019-03-04 04:17:14.880151
last batch sz 160
Model ind 640 epoch 1196 head B head_i_epoch 1 batch 0: avg loss -2.000026 avg loss no lamb -2.000026 time 2019-03-04 04:18:33.308494
Model ind 640 epoch 1196 head B head_i_epoch 1 batch 100: avg loss -1.909104 avg loss no lamb -1.909104 time 2019-03-04 04:20:22.190349
Model ind 640 epoch 1196 head B head_i_epoch 1 batch 200: avg loss -1.948803 avg loss no lamb -1.948803 time 2019-03-04 04:22:11.288568
last batch sz 160
Pre: time 2019-03-04 04:23:52.956070: 
 	std: 0.050919455
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5104667, 0.61435, 0.61436665, 0.61445, 0.5104333]
	train_accs: [0.5104667, 0.61435, 0.61436665, 0.61445, 0.5104333]
	best_train_sub_head: 3
	worst: 0.5104333
	avg: 0.57281333
	best: 0.61445

Starting e_i: 1197
Model ind 640 epoch 1197 head A head_i_epoch 0 batch 0: avg loss -3.255379 avg loss no lamb -3.255379 time 2019-03-04 04:23:55.774997
Model ind 640 epoch 1197 head A head_i_epoch 0 batch 100: avg loss -3.259756 avg loss no lamb -3.259756 time 2019-03-04 04:25:46.025799
Model ind 640 epoch 1197 head A head_i_epoch 0 batch 200: avg loss -3.305470 avg loss no lamb -3.305470 time 2019-03-04 04:27:34.153929
last batch sz 160
Model ind 640 epoch 1197 head B head_i_epoch 0 batch 0: avg loss -1.945704 avg loss no lamb -1.945704 time 2019-03-04 04:28:54.487036
Model ind 640 epoch 1197 head B head_i_epoch 0 batch 100: avg loss -1.897908 avg loss no lamb -1.897908 time 2019-03-04 04:30:44.209672
Model ind 640 epoch 1197 head B head_i_epoch 0 batch 200: avg loss -1.961972 avg loss no lamb -1.961972 time 2019-03-04 04:32:33.414258
last batch sz 160
Model ind 640 epoch 1197 head B head_i_epoch 1 batch 0: avg loss -1.924563 avg loss no lamb -1.924563 time 2019-03-04 04:33:53.537726
Model ind 640 epoch 1197 head B head_i_epoch 1 batch 100: avg loss -1.912081 avg loss no lamb -1.912081 time 2019-03-04 04:35:42.856204
Model ind 640 epoch 1197 head B head_i_epoch 1 batch 200: avg loss -1.945172 avg loss no lamb -1.945172 time 2019-03-04 04:37:31.740547
last batch sz 160
Pre: time 2019-03-04 04:39:11.858510: 
 	std: 0.050605103
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.61448336, 0.6145, 0.6145333, 0.51121664]
	train_accs: [0.5112, 0.61448336, 0.6145, 0.6145333, 0.51121664]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.5731867
	best: 0.6145333

Starting e_i: 1198
Model ind 640 epoch 1198 head A head_i_epoch 0 batch 0: avg loss -3.277381 avg loss no lamb -3.277381 time 2019-03-04 04:39:14.546508
Model ind 640 epoch 1198 head A head_i_epoch 0 batch 100: avg loss -3.243226 avg loss no lamb -3.243226 time 2019-03-04 04:41:01.988362
Model ind 640 epoch 1198 head A head_i_epoch 0 batch 200: avg loss -3.306839 avg loss no lamb -3.306839 time 2019-03-04 04:42:51.122564
last batch sz 160
Model ind 640 epoch 1198 head B head_i_epoch 0 batch 0: avg loss -1.991694 avg loss no lamb -1.991694 time 2019-03-04 04:44:10.313152
Model ind 640 epoch 1198 head B head_i_epoch 0 batch 100: avg loss -1.891886 avg loss no lamb -1.891886 time 2019-03-04 04:46:01.472394
Model ind 640 epoch 1198 head B head_i_epoch 0 batch 200: avg loss -1.993232 avg loss no lamb -1.993232 time 2019-03-04 04:47:51.369298
last batch sz 160
Model ind 640 epoch 1198 head B head_i_epoch 1 batch 0: avg loss -1.930768 avg loss no lamb -1.930768 time 2019-03-04 04:49:11.198817
Model ind 640 epoch 1198 head B head_i_epoch 1 batch 100: avg loss -1.952462 avg loss no lamb -1.952462 time 2019-03-04 04:51:00.726772
Model ind 640 epoch 1198 head B head_i_epoch 1 batch 200: avg loss -1.874848 avg loss no lamb -1.874848 time 2019-03-04 04:52:50.113851
last batch sz 160
Pre: time 2019-03-04 04:54:30.528809: 
 	std: 0.050432302
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51101667, 0.61403334, 0.6139, 0.614, 0.51105]
	train_accs: [0.51101667, 0.61403334, 0.6139, 0.614, 0.51105]
	best_train_sub_head: 1
	worst: 0.51101667
	avg: 0.57280004
	best: 0.61403334

Starting e_i: 1199
Model ind 640 epoch 1199 head A head_i_epoch 0 batch 0: avg loss -3.261358 avg loss no lamb -3.261358 time 2019-03-04 04:54:34.227795
Model ind 640 epoch 1199 head A head_i_epoch 0 batch 100: avg loss -3.225475 avg loss no lamb -3.225475 time 2019-03-04 04:56:23.872543
Model ind 640 epoch 1199 head A head_i_epoch 0 batch 200: avg loss -3.294128 avg loss no lamb -3.294128 time 2019-03-04 04:58:13.574659
last batch sz 160
Model ind 640 epoch 1199 head B head_i_epoch 0 batch 0: avg loss -1.888820 avg loss no lamb -1.888820 time 2019-03-04 04:59:32.761274
Model ind 640 epoch 1199 head B head_i_epoch 0 batch 100: avg loss -1.893021 avg loss no lamb -1.893021 time 2019-03-04 05:01:21.647040
Model ind 640 epoch 1199 head B head_i_epoch 0 batch 200: avg loss -2.020248 avg loss no lamb -2.020248 time 2019-03-04 05:03:12.730753
last batch sz 160
Model ind 640 epoch 1199 head B head_i_epoch 1 batch 0: avg loss -1.908690 avg loss no lamb -1.908690 time 2019-03-04 05:04:33.099488
Model ind 640 epoch 1199 head B head_i_epoch 1 batch 100: avg loss -1.907796 avg loss no lamb -1.907796 time 2019-03-04 05:06:23.390553
Model ind 640 epoch 1199 head B head_i_epoch 1 batch 200: avg loss -1.970089 avg loss no lamb -1.970089 time 2019-03-04 05:08:13.312954
last batch sz 160
Pre: time 2019-03-04 05:09:54.366778: 
 	std: 0.05040372
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5107833, 0.61375, 0.6135833, 0.61365, 0.5107667]
	train_accs: [0.5107833, 0.61375, 0.6135833, 0.61365, 0.5107667]
	best_train_sub_head: 1
	worst: 0.5107667
	avg: 0.57250667
	best: 0.61375

Starting e_i: 1200
Model ind 640 epoch 1200 head A head_i_epoch 0 batch 0: avg loss -3.283997 avg loss no lamb -3.283997 time 2019-03-04 05:09:57.212979
Model ind 640 epoch 1200 head A head_i_epoch 0 batch 100: avg loss -3.294748 avg loss no lamb -3.294748 time 2019-03-04 05:11:47.339342
Model ind 640 epoch 1200 head A head_i_epoch 0 batch 200: avg loss -3.261708 avg loss no lamb -3.261708 time 2019-03-04 05:13:37.041920
last batch sz 160
Model ind 640 epoch 1200 head B head_i_epoch 0 batch 0: avg loss -2.017932 avg loss no lamb -2.017932 time 2019-03-04 05:14:56.317946
Model ind 640 epoch 1200 head B head_i_epoch 0 batch 100: avg loss -1.909608 avg loss no lamb -1.909608 time 2019-03-04 05:16:46.351845
Model ind 640 epoch 1200 head B head_i_epoch 0 batch 200: avg loss -1.919891 avg loss no lamb -1.919891 time 2019-03-04 05:18:36.771123
last batch sz 160
Model ind 640 epoch 1200 head B head_i_epoch 1 batch 0: avg loss -1.954622 avg loss no lamb -1.954622 time 2019-03-04 05:19:56.652117
Model ind 640 epoch 1200 head B head_i_epoch 1 batch 100: avg loss -1.933295 avg loss no lamb -1.933295 time 2019-03-04 05:21:45.106726
Model ind 640 epoch 1200 head B head_i_epoch 1 batch 200: avg loss -1.966828 avg loss no lamb -1.966828 time 2019-03-04 05:23:34.057422
last batch sz 160
Pre: time 2019-03-04 05:25:14.618574: 
 	std: 0.05057245
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51195, 0.61511666, 0.61513335, 0.61511666, 0.5118333]
	train_accs: [0.51195, 0.61511666, 0.61513335, 0.61511666, 0.5118333]
	best_train_sub_head: 2
	worst: 0.5118333
	avg: 0.57382995
	best: 0.61513335

Starting e_i: 1201
Model ind 640 epoch 1201 head A head_i_epoch 0 batch 0: avg loss -3.279091 avg loss no lamb -3.279091 time 2019-03-04 05:25:21.348475
Model ind 640 epoch 1201 head A head_i_epoch 0 batch 100: avg loss -3.242879 avg loss no lamb -3.242879 time 2019-03-04 05:27:11.790915
Model ind 640 epoch 1201 head A head_i_epoch 0 batch 200: avg loss -3.328167 avg loss no lamb -3.328167 time 2019-03-04 05:29:01.994142
last batch sz 160
Model ind 640 epoch 1201 head B head_i_epoch 0 batch 0: avg loss -1.899163 avg loss no lamb -1.899163 time 2019-03-04 05:30:21.700811
Model ind 640 epoch 1201 head B head_i_epoch 0 batch 100: avg loss -1.970732 avg loss no lamb -1.970732 time 2019-03-04 05:32:10.352504
Model ind 640 epoch 1201 head B head_i_epoch 0 batch 200: avg loss -1.971264 avg loss no lamb -1.971264 time 2019-03-04 05:34:01.671027
last batch sz 160
Model ind 640 epoch 1201 head B head_i_epoch 1 batch 0: avg loss -1.927511 avg loss no lamb -1.927511 time 2019-03-04 05:35:21.293008
Model ind 640 epoch 1201 head B head_i_epoch 1 batch 100: avg loss -1.869148 avg loss no lamb -1.869148 time 2019-03-04 05:37:10.715749
Model ind 640 epoch 1201 head B head_i_epoch 1 batch 200: avg loss -1.926388 avg loss no lamb -1.926388 time 2019-03-04 05:39:00.618198
last batch sz 160
Pre: time 2019-03-04 05:40:41.104024: 
 	std: 0.050872266
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107333, 0.6141667, 0.6142833, 0.6143, 0.5100833]
	train_accs: [0.5107333, 0.6141667, 0.6142833, 0.6143, 0.5100833]
	best_train_sub_head: 3
	worst: 0.5100833
	avg: 0.57271326
	best: 0.6143

Starting e_i: 1202
Model ind 640 epoch 1202 head A head_i_epoch 0 batch 0: avg loss -3.283803 avg loss no lamb -3.283803 time 2019-03-04 05:40:44.758047
Model ind 640 epoch 1202 head A head_i_epoch 0 batch 100: avg loss -3.220046 avg loss no lamb -3.220046 time 2019-03-04 05:42:34.337575
Model ind 640 epoch 1202 head A head_i_epoch 0 batch 200: avg loss -3.432406 avg loss no lamb -3.432406 time 2019-03-04 05:44:22.127356
last batch sz 160
Model ind 640 epoch 1202 head B head_i_epoch 0 batch 0: avg loss -1.906440 avg loss no lamb -1.906440 time 2019-03-04 05:45:42.793763
Model ind 640 epoch 1202 head B head_i_epoch 0 batch 100: avg loss -1.968338 avg loss no lamb -1.968338 time 2019-03-04 05:47:33.096791
Model ind 640 epoch 1202 head B head_i_epoch 0 batch 200: avg loss -1.993345 avg loss no lamb -1.993345 time 2019-03-04 05:49:24.797383
last batch sz 160
Model ind 640 epoch 1202 head B head_i_epoch 1 batch 0: avg loss -2.018021 avg loss no lamb -2.018021 time 2019-03-04 05:50:47.078444
Model ind 640 epoch 1202 head B head_i_epoch 1 batch 100: avg loss -1.919284 avg loss no lamb -1.919284 time 2019-03-04 05:52:37.088515
Model ind 640 epoch 1202 head B head_i_epoch 1 batch 200: avg loss -1.950531 avg loss no lamb -1.950531 time 2019-03-04 05:54:27.425058
last batch sz 160
Pre: time 2019-03-04 05:56:09.747165: 
 	std: 0.050757594
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101, 0.6134833, 0.6136, 0.61366665, 0.50985]
	train_accs: [0.5101, 0.6134833, 0.6136, 0.61366665, 0.50985]
	best_train_sub_head: 3
	worst: 0.50985
	avg: 0.57214004
	best: 0.61366665

Starting e_i: 1203
Model ind 640 epoch 1203 head A head_i_epoch 0 batch 0: avg loss -3.272762 avg loss no lamb -3.272762 time 2019-03-04 05:56:12.674013
Model ind 640 epoch 1203 head A head_i_epoch 0 batch 100: avg loss -3.221865 avg loss no lamb -3.221865 time 2019-03-04 05:58:02.563111
Model ind 640 epoch 1203 head A head_i_epoch 0 batch 200: avg loss -3.328452 avg loss no lamb -3.328452 time 2019-03-04 05:59:53.451755
last batch sz 160
Model ind 640 epoch 1203 head B head_i_epoch 0 batch 0: avg loss -1.952418 avg loss no lamb -1.952418 time 2019-03-04 06:01:14.163210
Model ind 640 epoch 1203 head B head_i_epoch 0 batch 100: avg loss -1.877873 avg loss no lamb -1.877873 time 2019-03-04 06:03:06.087467
Model ind 640 epoch 1203 head B head_i_epoch 0 batch 200: avg loss -1.916665 avg loss no lamb -1.916665 time 2019-03-04 06:04:53.057607
last batch sz 160
Model ind 640 epoch 1203 head B head_i_epoch 1 batch 0: avg loss -1.939176 avg loss no lamb -1.939176 time 2019-03-04 06:06:12.981246
Model ind 640 epoch 1203 head B head_i_epoch 1 batch 100: avg loss -1.979707 avg loss no lamb -1.979707 time 2019-03-04 06:08:04.642226
Model ind 640 epoch 1203 head B head_i_epoch 1 batch 200: avg loss -1.964760 avg loss no lamb -1.964760 time 2019-03-04 06:09:54.352465
last batch sz 160
Pre: time 2019-03-04 06:11:35.565129: 
 	std: 0.05100802
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5101333, 0.61406666, 0.6142667, 0.61405, 0.50988334]
	train_accs: [0.5101333, 0.61406666, 0.6142667, 0.61405, 0.50988334]
	best_train_sub_head: 2
	worst: 0.50988334
	avg: 0.57248
	best: 0.6142667

Starting e_i: 1204
Model ind 640 epoch 1204 head A head_i_epoch 0 batch 0: avg loss -3.327480 avg loss no lamb -3.327480 time 2019-03-04 06:11:38.402521
Model ind 640 epoch 1204 head A head_i_epoch 0 batch 100: avg loss -3.200398 avg loss no lamb -3.200398 time 2019-03-04 06:13:28.499246
Model ind 640 epoch 1204 head A head_i_epoch 0 batch 200: avg loss -3.319257 avg loss no lamb -3.319257 time 2019-03-04 06:15:18.454279
last batch sz 160
Model ind 640 epoch 1204 head B head_i_epoch 0 batch 0: avg loss -1.927766 avg loss no lamb -1.927766 time 2019-03-04 06:16:39.334846
Model ind 640 epoch 1204 head B head_i_epoch 0 batch 100: avg loss -1.866436 avg loss no lamb -1.866436 time 2019-03-04 06:18:29.585190
Model ind 640 epoch 1204 head B head_i_epoch 0 batch 200: avg loss -1.876234 avg loss no lamb -1.876234 time 2019-03-04 06:20:19.166443
last batch sz 160
Model ind 640 epoch 1204 head B head_i_epoch 1 batch 0: avg loss -2.001340 avg loss no lamb -2.001340 time 2019-03-04 06:21:39.177440
Model ind 640 epoch 1204 head B head_i_epoch 1 batch 100: avg loss -1.932305 avg loss no lamb -1.932305 time 2019-03-04 06:23:31.511137
Model ind 640 epoch 1204 head B head_i_epoch 1 batch 200: avg loss -2.006042 avg loss no lamb -2.006042 time 2019-03-04 06:25:19.138889
last batch sz 160
Pre: time 2019-03-04 06:27:00.940439: 
 	std: 0.050965823
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51101667, 0.6149333, 0.61485, 0.6148667, 0.51068336]
	train_accs: [0.51101667, 0.6149333, 0.61485, 0.6148667, 0.51068336]
	best_train_sub_head: 1
	worst: 0.51068336
	avg: 0.57326996
	best: 0.6149333

Starting e_i: 1205
Model ind 640 epoch 1205 head A head_i_epoch 0 batch 0: avg loss -3.239592 avg loss no lamb -3.239592 time 2019-03-04 06:27:03.826081
Model ind 640 epoch 1205 head A head_i_epoch 0 batch 100: avg loss -3.240400 avg loss no lamb -3.240400 time 2019-03-04 06:28:54.548797
Model ind 640 epoch 1205 head A head_i_epoch 0 batch 200: avg loss -3.276629 avg loss no lamb -3.276629 time 2019-03-04 06:30:44.529988
last batch sz 160
Model ind 640 epoch 1205 head B head_i_epoch 0 batch 0: avg loss -2.039774 avg loss no lamb -2.039774 time 2019-03-04 06:32:04.375966
Model ind 640 epoch 1205 head B head_i_epoch 0 batch 100: avg loss -1.879640 avg loss no lamb -1.879640 time 2019-03-04 06:33:54.505062
Model ind 640 epoch 1205 head B head_i_epoch 0 batch 200: avg loss -1.996909 avg loss no lamb -1.996909 time 2019-03-04 06:35:45.759002
last batch sz 160
Model ind 640 epoch 1205 head B head_i_epoch 1 batch 0: avg loss -1.924058 avg loss no lamb -1.924058 time 2019-03-04 06:37:04.706051
Model ind 640 epoch 1205 head B head_i_epoch 1 batch 100: avg loss -1.943834 avg loss no lamb -1.943834 time 2019-03-04 06:38:56.751968
Model ind 640 epoch 1205 head B head_i_epoch 1 batch 200: avg loss -2.025464 avg loss no lamb -2.025464 time 2019-03-04 06:40:47.598548
last batch sz 160
Pre: time 2019-03-04 06:42:30.236408: 
 	std: 0.05070597
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51138335, 0.61473334, 0.61473334, 0.6146167, 0.511]
	train_accs: [0.51138335, 0.61473334, 0.61473334, 0.6146167, 0.511]
	best_train_sub_head: 1
	worst: 0.511
	avg: 0.5732933
	best: 0.61473334

Starting e_i: 1206
Model ind 640 epoch 1206 head A head_i_epoch 0 batch 0: avg loss -3.304683 avg loss no lamb -3.304683 time 2019-03-04 06:42:33.914211
Model ind 640 epoch 1206 head A head_i_epoch 0 batch 100: avg loss -3.250228 avg loss no lamb -3.250228 time 2019-03-04 06:44:25.243238
Model ind 640 epoch 1206 head A head_i_epoch 0 batch 200: avg loss -3.316765 avg loss no lamb -3.316765 time 2019-03-04 06:46:15.314475
last batch sz 160
Model ind 640 epoch 1206 head B head_i_epoch 0 batch 0: avg loss -1.977426 avg loss no lamb -1.977426 time 2019-03-04 06:47:34.158153
Model ind 640 epoch 1206 head B head_i_epoch 0 batch 100: avg loss -1.896398 avg loss no lamb -1.896398 time 2019-03-04 06:49:24.657023
Model ind 640 epoch 1206 head B head_i_epoch 0 batch 200: avg loss -1.967039 avg loss no lamb -1.967039 time 2019-03-04 06:51:14.459362
last batch sz 160
Model ind 640 epoch 1206 head B head_i_epoch 1 batch 0: avg loss -1.829941 avg loss no lamb -1.829941 time 2019-03-04 06:52:34.770087
Model ind 640 epoch 1206 head B head_i_epoch 1 batch 100: avg loss -1.978146 avg loss no lamb -1.978146 time 2019-03-04 06:54:26.145308
Model ind 640 epoch 1206 head B head_i_epoch 1 batch 200: avg loss -1.965252 avg loss no lamb -1.965252 time 2019-03-04 06:56:17.722485
last batch sz 160
Pre: time 2019-03-04 06:57:58.266010: 
 	std: 0.05098081
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5112, 0.61511666, 0.6151, 0.61505, 0.51085]
	train_accs: [0.5112, 0.61511666, 0.6151, 0.61505, 0.51085]
	best_train_sub_head: 1
	worst: 0.51085
	avg: 0.5734633
	best: 0.61511666

Starting e_i: 1207
Model ind 640 epoch 1207 head A head_i_epoch 0 batch 0: avg loss -3.334837 avg loss no lamb -3.334837 time 2019-03-04 06:58:02.018673
Model ind 640 epoch 1207 head A head_i_epoch 0 batch 100: avg loss -3.245863 avg loss no lamb -3.245863 time 2019-03-04 06:59:52.181986
Model ind 640 epoch 1207 head A head_i_epoch 0 batch 200: avg loss -3.389838 avg loss no lamb -3.389838 time 2019-03-04 07:01:43.268186
last batch sz 160
Model ind 640 epoch 1207 head B head_i_epoch 0 batch 0: avg loss -1.991200 avg loss no lamb -1.991200 time 2019-03-04 07:03:03.462046
Model ind 640 epoch 1207 head B head_i_epoch 0 batch 100: avg loss -1.935278 avg loss no lamb -1.935278 time 2019-03-04 07:04:54.895632
Model ind 640 epoch 1207 head B head_i_epoch 0 batch 200: avg loss -2.086006 avg loss no lamb -2.086006 time 2019-03-04 07:06:46.237507
last batch sz 160
Model ind 640 epoch 1207 head B head_i_epoch 1 batch 0: avg loss -1.973274 avg loss no lamb -1.973274 time 2019-03-04 07:08:03.872171
Model ind 640 epoch 1207 head B head_i_epoch 1 batch 100: avg loss -1.907525 avg loss no lamb -1.907525 time 2019-03-04 07:09:54.109384
Model ind 640 epoch 1207 head B head_i_epoch 1 batch 200: avg loss -2.051726 avg loss no lamb -2.051726 time 2019-03-04 07:11:46.321655
last batch sz 160
Pre: time 2019-03-04 07:13:26.954912: 
 	std: 0.05079169
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5104833, 0.6142667, 0.61438334, 0.61438334, 0.51085]
	train_accs: [0.5104833, 0.6142667, 0.61438334, 0.61438334, 0.51085]
	best_train_sub_head: 2
	worst: 0.5104833
	avg: 0.5728733
	best: 0.61438334

Starting e_i: 1208
Model ind 640 epoch 1208 head A head_i_epoch 0 batch 0: avg loss -3.293916 avg loss no lamb -3.293916 time 2019-03-04 07:13:29.748986
Model ind 640 epoch 1208 head A head_i_epoch 0 batch 100: avg loss -3.221360 avg loss no lamb -3.221360 time 2019-03-04 07:15:20.135771
Model ind 640 epoch 1208 head A head_i_epoch 0 batch 200: avg loss -3.197726 avg loss no lamb -3.197726 time 2019-03-04 07:17:10.007103
last batch sz 160
Model ind 640 epoch 1208 head B head_i_epoch 0 batch 0: avg loss -1.999716 avg loss no lamb -1.999716 time 2019-03-04 07:18:29.724079
Model ind 640 epoch 1208 head B head_i_epoch 0 batch 100: avg loss -1.916833 avg loss no lamb -1.916833 time 2019-03-04 07:20:19.014950
Model ind 640 epoch 1208 head B head_i_epoch 0 batch 200: avg loss -2.013569 avg loss no lamb -2.013569 time 2019-03-04 07:22:08.453092
last batch sz 160
Model ind 640 epoch 1208 head B head_i_epoch 1 batch 0: avg loss -1.993903 avg loss no lamb -1.993903 time 2019-03-04 07:23:28.219277
Model ind 640 epoch 1208 head B head_i_epoch 1 batch 100: avg loss -1.974525 avg loss no lamb -1.974525 time 2019-03-04 07:25:17.536634
Model ind 640 epoch 1208 head B head_i_epoch 1 batch 200: avg loss -1.993284 avg loss no lamb -1.993284 time 2019-03-04 07:27:06.817502
last batch sz 160
Pre: time 2019-03-04 07:28:46.260420: 
 	std: 0.0506829
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51105, 0.61476666, 0.6148, 0.61465, 0.5115167]
	train_accs: [0.51105, 0.61476666, 0.6148, 0.61465, 0.5115167]
	best_train_sub_head: 2
	worst: 0.51105
	avg: 0.5733566
	best: 0.6148

Starting e_i: 1209
Model ind 640 epoch 1209 head A head_i_epoch 0 batch 0: avg loss -3.273508 avg loss no lamb -3.273508 time 2019-03-04 07:28:48.981540
Model ind 640 epoch 1209 head A head_i_epoch 0 batch 100: avg loss -3.270784 avg loss no lamb -3.270784 time 2019-03-04 07:30:39.521487
Model ind 640 epoch 1209 head A head_i_epoch 0 batch 200: avg loss -3.326688 avg loss no lamb -3.326688 time 2019-03-04 07:32:28.629909
last batch sz 160
Model ind 640 epoch 1209 head B head_i_epoch 0 batch 0: avg loss -1.936524 avg loss no lamb -1.936524 time 2019-03-04 07:33:47.800172
Model ind 640 epoch 1209 head B head_i_epoch 0 batch 100: avg loss -1.891541 avg loss no lamb -1.891541 time 2019-03-04 07:35:36.412311
Model ind 640 epoch 1209 head B head_i_epoch 0 batch 200: avg loss -1.931260 avg loss no lamb -1.931260 time 2019-03-04 07:37:25.855406
last batch sz 160
Model ind 640 epoch 1209 head B head_i_epoch 1 batch 0: avg loss -1.947186 avg loss no lamb -1.947186 time 2019-03-04 07:38:45.320832
Model ind 640 epoch 1209 head B head_i_epoch 1 batch 100: avg loss -1.962004 avg loss no lamb -1.962004 time 2019-03-04 07:40:35.065532
Model ind 640 epoch 1209 head B head_i_epoch 1 batch 200: avg loss -1.984351 avg loss no lamb -1.984351 time 2019-03-04 07:42:24.473940
last batch sz 160
Pre: time 2019-03-04 07:44:08.126127: 
 	std: 0.05080927
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51091665, 0.61476666, 0.6146167, 0.61473334, 0.5110667]
	train_accs: [0.51091665, 0.61476666, 0.6146167, 0.61473334, 0.5110667]
	best_train_sub_head: 1
	worst: 0.51091665
	avg: 0.57322
	best: 0.61476666

Starting e_i: 1210
Model ind 640 epoch 1210 head A head_i_epoch 0 batch 0: avg loss -3.322113 avg loss no lamb -3.322113 time 2019-03-04 07:44:11.883355
Model ind 640 epoch 1210 head A head_i_epoch 0 batch 100: avg loss -3.300853 avg loss no lamb -3.300853 time 2019-03-04 07:46:03.106497
Model ind 640 epoch 1210 head A head_i_epoch 0 batch 200: avg loss -3.357136 avg loss no lamb -3.357136 time 2019-03-04 07:47:54.899653
last batch sz 160
Model ind 640 epoch 1210 head B head_i_epoch 0 batch 0: avg loss -1.888853 avg loss no lamb -1.888853 time 2019-03-04 07:49:15.618272
Model ind 640 epoch 1210 head B head_i_epoch 0 batch 100: avg loss -1.932083 avg loss no lamb -1.932083 time 2019-03-04 07:51:04.355856
Model ind 640 epoch 1210 head B head_i_epoch 0 batch 200: avg loss -1.980467 avg loss no lamb -1.980467 time 2019-03-04 07:52:55.932729
last batch sz 160
Model ind 640 epoch 1210 head B head_i_epoch 1 batch 0: avg loss -2.032469 avg loss no lamb -2.032469 time 2019-03-04 07:54:16.228177
Model ind 640 epoch 1210 head B head_i_epoch 1 batch 100: avg loss -1.843976 avg loss no lamb -1.843976 time 2019-03-04 07:56:06.922280
Model ind 640 epoch 1210 head B head_i_epoch 1 batch 200: avg loss -1.948596 avg loss no lamb -1.948596 time 2019-03-04 07:57:57.219249
last batch sz 160
Pre: time 2019-03-04 07:59:39.928357: 
 	std: 0.04993423
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51276666, 0.61465, 0.61468333, 0.61475, 0.51276666]
	train_accs: [0.51276666, 0.61465, 0.61468333, 0.61475, 0.51276666]
	best_train_sub_head: 3
	worst: 0.51276666
	avg: 0.57392335
	best: 0.61475

Starting e_i: 1211
Model ind 640 epoch 1211 head A head_i_epoch 0 batch 0: avg loss -3.279035 avg loss no lamb -3.279035 time 2019-03-04 07:59:46.449624
Model ind 640 epoch 1211 head A head_i_epoch 0 batch 100: avg loss -3.228587 avg loss no lamb -3.228587 time 2019-03-04 08:01:38.530690
Model ind 640 epoch 1211 head A head_i_epoch 0 batch 200: avg loss -3.318655 avg loss no lamb -3.318655 time 2019-03-04 08:03:30.107859
last batch sz 160
Model ind 640 epoch 1211 head B head_i_epoch 0 batch 0: avg loss -1.984722 avg loss no lamb -1.984722 time 2019-03-04 08:04:50.971678
Model ind 640 epoch 1211 head B head_i_epoch 0 batch 100: avg loss -1.906611 avg loss no lamb -1.906611 time 2019-03-04 08:06:42.166911
Model ind 640 epoch 1211 head B head_i_epoch 0 batch 200: avg loss -2.009671 avg loss no lamb -2.009671 time 2019-03-04 08:08:32.104747
last batch sz 160
Model ind 640 epoch 1211 head B head_i_epoch 1 batch 0: avg loss -1.929796 avg loss no lamb -1.929796 time 2019-03-04 08:09:53.994160
Model ind 640 epoch 1211 head B head_i_epoch 1 batch 100: avg loss -1.991920 avg loss no lamb -1.991920 time 2019-03-04 08:11:42.468866
Model ind 640 epoch 1211 head B head_i_epoch 1 batch 200: avg loss -2.042933 avg loss no lamb -2.042933 time 2019-03-04 08:13:32.795706
last batch sz 160
Pre: time 2019-03-04 08:15:14.209821: 
 	std: 0.05067727
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5113, 0.61465, 0.6146333, 0.6146, 0.5110667]
	train_accs: [0.5113, 0.61465, 0.6146333, 0.6146, 0.5110667]
	best_train_sub_head: 1
	worst: 0.5110667
	avg: 0.57325
	best: 0.61465

Starting e_i: 1212
Model ind 640 epoch 1212 head A head_i_epoch 0 batch 0: avg loss -3.340860 avg loss no lamb -3.340860 time 2019-03-04 08:15:17.004886
Model ind 640 epoch 1212 head A head_i_epoch 0 batch 100: avg loss -3.236523 avg loss no lamb -3.236523 time 2019-03-04 08:17:09.756352
Model ind 640 epoch 1212 head A head_i_epoch 0 batch 200: avg loss -3.371705 avg loss no lamb -3.371705 time 2019-03-04 08:19:00.707708
last batch sz 160
Model ind 640 epoch 1212 head B head_i_epoch 0 batch 0: avg loss -1.960569 avg loss no lamb -1.960569 time 2019-03-04 08:20:21.485948
Model ind 640 epoch 1212 head B head_i_epoch 0 batch 100: avg loss -1.840321 avg loss no lamb -1.840321 time 2019-03-04 08:22:12.726020
Model ind 640 epoch 1212 head B head_i_epoch 0 batch 200: avg loss -1.983613 avg loss no lamb -1.983613 time 2019-03-04 08:24:04.756961
last batch sz 160
Model ind 640 epoch 1212 head B head_i_epoch 1 batch 0: avg loss -1.920527 avg loss no lamb -1.920527 time 2019-03-04 08:25:26.130387
Model ind 640 epoch 1212 head B head_i_epoch 1 batch 100: avg loss -1.970963 avg loss no lamb -1.970963 time 2019-03-04 08:27:18.208157
Model ind 640 epoch 1212 head B head_i_epoch 1 batch 200: avg loss -1.967908 avg loss no lamb -1.967908 time 2019-03-04 08:29:09.376957
last batch sz 160
Pre: time 2019-03-04 08:30:51.795440: 
 	std: 0.051110458
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.50983334, 0.6145167, 0.61445, 0.6145667, 0.51053333]
	train_accs: [0.50983334, 0.6145167, 0.61445, 0.6145667, 0.51053333]
	best_train_sub_head: 3
	worst: 0.50983334
	avg: 0.57278
	best: 0.6145667

Starting e_i: 1213
Model ind 640 epoch 1213 head A head_i_epoch 0 batch 0: avg loss -3.284833 avg loss no lamb -3.284833 time 2019-03-04 08:30:54.542446
Model ind 640 epoch 1213 head A head_i_epoch 0 batch 100: avg loss -3.220860 avg loss no lamb -3.220860 time 2019-03-04 08:32:43.698340
Model ind 640 epoch 1213 head A head_i_epoch 0 batch 200: avg loss -3.278596 avg loss no lamb -3.278596 time 2019-03-04 08:34:34.380737
last batch sz 160
Model ind 640 epoch 1213 head B head_i_epoch 0 batch 0: avg loss -1.952197 avg loss no lamb -1.952197 time 2019-03-04 08:35:54.721740
Model ind 640 epoch 1213 head B head_i_epoch 0 batch 100: avg loss -1.833904 avg loss no lamb -1.833904 time 2019-03-04 08:37:45.549900
Model ind 640 epoch 1213 head B head_i_epoch 0 batch 200: avg loss -1.880760 avg loss no lamb -1.880760 time 2019-03-04 08:39:36.476207
last batch sz 160
Model ind 640 epoch 1213 head B head_i_epoch 1 batch 0: avg loss -1.877939 avg loss no lamb -1.877939 time 2019-03-04 08:40:57.486758
Model ind 640 epoch 1213 head B head_i_epoch 1 batch 100: avg loss -1.943494 avg loss no lamb -1.943494 time 2019-03-04 08:42:49.309194
Model ind 640 epoch 1213 head B head_i_epoch 1 batch 200: avg loss -2.010691 avg loss no lamb -2.010691 time 2019-03-04 08:44:40.318474
last batch sz 160
Pre: time 2019-03-04 08:46:24.491963: 
 	std: 0.050392833
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115833, 0.61436665, 0.6144, 0.6145, 0.5115333]
	train_accs: [0.5115833, 0.61436665, 0.6144, 0.6145, 0.5115333]
	best_train_sub_head: 3
	worst: 0.5115333
	avg: 0.57327664
	best: 0.6145

Starting e_i: 1214
Model ind 640 epoch 1214 head A head_i_epoch 0 batch 0: avg loss -3.342730 avg loss no lamb -3.342730 time 2019-03-04 08:46:27.300299
Model ind 640 epoch 1214 head A head_i_epoch 0 batch 100: avg loss -3.219451 avg loss no lamb -3.219451 time 2019-03-04 08:48:21.333507
Model ind 640 epoch 1214 head A head_i_epoch 0 batch 200: avg loss -3.300379 avg loss no lamb -3.300379 time 2019-03-04 08:50:13.518692
last batch sz 160
Model ind 640 epoch 1214 head B head_i_epoch 0 batch 0: avg loss -1.930569 avg loss no lamb -1.930569 time 2019-03-04 08:51:35.032659
Model ind 640 epoch 1214 head B head_i_epoch 0 batch 100: avg loss -1.855554 avg loss no lamb -1.855554 time 2019-03-04 08:53:22.218318
Model ind 640 epoch 1214 head B head_i_epoch 0 batch 200: avg loss -1.933469 avg loss no lamb -1.933469 time 2019-03-04 08:55:12.326188
last batch sz 160
Model ind 640 epoch 1214 head B head_i_epoch 1 batch 0: avg loss -1.970490 avg loss no lamb -1.970490 time 2019-03-04 08:56:32.983412
Model ind 640 epoch 1214 head B head_i_epoch 1 batch 100: avg loss -1.889621 avg loss no lamb -1.889621 time 2019-03-04 08:58:23.082296
Model ind 640 epoch 1214 head B head_i_epoch 1 batch 200: avg loss -1.990523 avg loss no lamb -1.990523 time 2019-03-04 09:00:12.946241
last batch sz 160
Pre: time 2019-03-04 09:01:53.778398: 
 	std: 0.050282605
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51175, 0.61436665, 0.61446667, 0.61433333, 0.51175]
	train_accs: [0.51175, 0.61436665, 0.61446667, 0.61433333, 0.51175]
	best_train_sub_head: 2
	worst: 0.51175
	avg: 0.5733333
	best: 0.61446667

Starting e_i: 1215
Model ind 640 epoch 1215 head A head_i_epoch 0 batch 0: avg loss -3.269028 avg loss no lamb -3.269028 time 2019-03-04 09:01:57.412785
Model ind 640 epoch 1215 head A head_i_epoch 0 batch 100: avg loss -3.170502 avg loss no lamb -3.170502 time 2019-03-04 09:03:47.014975
Model ind 640 epoch 1215 head A head_i_epoch 0 batch 200: avg loss -3.242652 avg loss no lamb -3.242652 time 2019-03-04 09:05:39.809834
last batch sz 160
Model ind 640 epoch 1215 head B head_i_epoch 0 batch 0: avg loss -1.885466 avg loss no lamb -1.885466 time 2019-03-04 09:07:00.508627
Model ind 640 epoch 1215 head B head_i_epoch 0 batch 100: avg loss -1.965531 avg loss no lamb -1.965531 time 2019-03-04 09:08:50.986501
Model ind 640 epoch 1215 head B head_i_epoch 0 batch 200: avg loss -1.959143 avg loss no lamb -1.959143 time 2019-03-04 09:10:41.272695
last batch sz 160
Model ind 640 epoch 1215 head B head_i_epoch 1 batch 0: avg loss -1.992378 avg loss no lamb -1.992378 time 2019-03-04 09:12:01.129774
Model ind 640 epoch 1215 head B head_i_epoch 1 batch 100: avg loss -1.986535 avg loss no lamb -1.986535 time 2019-03-04 09:13:49.544482
Model ind 640 epoch 1215 head B head_i_epoch 1 batch 200: avg loss -1.958801 avg loss no lamb -1.958801 time 2019-03-04 09:15:39.539531
last batch sz 160
Pre: time 2019-03-04 09:17:21.356212: 
 	std: 0.050406445
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51163334, 0.6146, 0.6146, 0.61445, 0.51168334]
	train_accs: [0.51163334, 0.6146, 0.6146, 0.61445, 0.51168334]
	best_train_sub_head: 1
	worst: 0.51163334
	avg: 0.57339334
	best: 0.6146

Starting e_i: 1216
Model ind 640 epoch 1216 head A head_i_epoch 0 batch 0: avg loss -3.380738 avg loss no lamb -3.380738 time 2019-03-04 09:17:24.294275
Model ind 640 epoch 1216 head A head_i_epoch 0 batch 100: avg loss -3.210810 avg loss no lamb -3.210810 time 2019-03-04 09:19:14.919852
Model ind 640 epoch 1216 head A head_i_epoch 0 batch 200: avg loss -3.265290 avg loss no lamb -3.265290 time 2019-03-04 09:21:07.016713
last batch sz 160
Model ind 640 epoch 1216 head B head_i_epoch 0 batch 0: avg loss -1.872570 avg loss no lamb -1.872570 time 2019-03-04 09:22:28.379125
Model ind 640 epoch 1216 head B head_i_epoch 0 batch 100: avg loss -1.864114 avg loss no lamb -1.864114 time 2019-03-04 09:24:17.792861
Model ind 640 epoch 1216 head B head_i_epoch 0 batch 200: avg loss -1.993919 avg loss no lamb -1.993919 time 2019-03-04 09:26:08.051374
last batch sz 160
Model ind 640 epoch 1216 head B head_i_epoch 1 batch 0: avg loss -1.953282 avg loss no lamb -1.953282 time 2019-03-04 09:27:28.687813
Model ind 640 epoch 1216 head B head_i_epoch 1 batch 100: avg loss -1.942035 avg loss no lamb -1.942035 time 2019-03-04 09:29:18.962613
Model ind 640 epoch 1216 head B head_i_epoch 1 batch 200: avg loss -1.930217 avg loss no lamb -1.930217 time 2019-03-04 09:31:09.362272
last batch sz 160
Pre: time 2019-03-04 09:32:51.427335: 
 	std: 0.050553408
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51168334, 0.61475, 0.6148833, 0.6148667, 0.5116]
	train_accs: [0.51168334, 0.61475, 0.6148833, 0.6148667, 0.5116]
	best_train_sub_head: 2
	worst: 0.5116
	avg: 0.57355666
	best: 0.6148833

Starting e_i: 1217
Model ind 640 epoch 1217 head A head_i_epoch 0 batch 0: avg loss -3.336000 avg loss no lamb -3.336000 time 2019-03-04 09:32:54.339000
Model ind 640 epoch 1217 head A head_i_epoch 0 batch 100: avg loss -3.176263 avg loss no lamb -3.176263 time 2019-03-04 09:34:43.326945
Model ind 640 epoch 1217 head A head_i_epoch 0 batch 200: avg loss -3.254313 avg loss no lamb -3.254313 time 2019-03-04 09:36:31.869960
last batch sz 160
Model ind 640 epoch 1217 head B head_i_epoch 0 batch 0: avg loss -1.957492 avg loss no lamb -1.957492 time 2019-03-04 09:37:53.079028
Model ind 640 epoch 1217 head B head_i_epoch 0 batch 100: avg loss -1.891974 avg loss no lamb -1.891974 time 2019-03-04 09:39:43.142696
Model ind 640 epoch 1217 head B head_i_epoch 0 batch 200: avg loss -1.947925 avg loss no lamb -1.947925 time 2019-03-04 09:41:35.129063
last batch sz 160
Model ind 640 epoch 1217 head B head_i_epoch 1 batch 0: avg loss -1.935276 avg loss no lamb -1.935276 time 2019-03-04 09:42:55.764736
Model ind 640 epoch 1217 head B head_i_epoch 1 batch 100: avg loss -1.957478 avg loss no lamb -1.957478 time 2019-03-04 09:44:46.595736
Model ind 640 epoch 1217 head B head_i_epoch 1 batch 200: avg loss -1.987926 avg loss no lamb -1.987926 time 2019-03-04 09:46:37.302213
last batch sz 160
Pre: time 2019-03-04 09:48:18.072058: 
 	std: 0.050527543
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5112, 0.61436665, 0.6143, 0.6144, 0.5112333]
	train_accs: [0.5112, 0.61436665, 0.6143, 0.6144, 0.5112333]
	best_train_sub_head: 3
	worst: 0.5112
	avg: 0.5731
	best: 0.6144

Starting e_i: 1218
Model ind 640 epoch 1218 head A head_i_epoch 0 batch 0: avg loss -3.312644 avg loss no lamb -3.312644 time 2019-03-04 09:48:21.862412
Model ind 640 epoch 1218 head A head_i_epoch 0 batch 100: avg loss -3.284472 avg loss no lamb -3.284472 time 2019-03-04 09:50:12.162914
Model ind 640 epoch 1218 head A head_i_epoch 0 batch 200: avg loss -3.316381 avg loss no lamb -3.316381 time 2019-03-04 09:52:03.387658
last batch sz 160
Model ind 640 epoch 1218 head B head_i_epoch 0 batch 0: avg loss -1.953199 avg loss no lamb -1.953199 time 2019-03-04 09:53:24.766037
Model ind 640 epoch 1218 head B head_i_epoch 0 batch 100: avg loss -1.931182 avg loss no lamb -1.931182 time 2019-03-04 09:55:16.694196
Model ind 640 epoch 1218 head B head_i_epoch 0 batch 200: avg loss -1.959735 avg loss no lamb -1.959735 time 2019-03-04 09:57:04.443419
last batch sz 160
Model ind 640 epoch 1218 head B head_i_epoch 1 batch 0: avg loss -1.890096 avg loss no lamb -1.890096 time 2019-03-04 09:58:25.590311
Model ind 640 epoch 1218 head B head_i_epoch 1 batch 100: avg loss -1.915715 avg loss no lamb -1.915715 time 2019-03-04 10:00:15.729009
Model ind 640 epoch 1218 head B head_i_epoch 1 batch 200: avg loss -2.030107 avg loss no lamb -2.030107 time 2019-03-04 10:02:05.505802
last batch sz 160
Pre: time 2019-03-04 10:03:46.265501: 
 	std: 0.050703153
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51053333, 0.61413336, 0.6142167, 0.61406666, 0.51075]
	train_accs: [0.51053333, 0.61413336, 0.6142167, 0.61406666, 0.51075]
	best_train_sub_head: 2
	worst: 0.51053333
	avg: 0.57274
	best: 0.6142167

Starting e_i: 1219
Model ind 640 epoch 1219 head A head_i_epoch 0 batch 0: avg loss -3.356035 avg loss no lamb -3.356035 time 2019-03-04 10:03:49.098174
Model ind 640 epoch 1219 head A head_i_epoch 0 batch 100: avg loss -3.259621 avg loss no lamb -3.259621 time 2019-03-04 10:05:39.085741
Model ind 640 epoch 1219 head A head_i_epoch 0 batch 200: avg loss -3.379189 avg loss no lamb -3.379189 time 2019-03-04 10:07:27.974225
last batch sz 160
Model ind 640 epoch 1219 head B head_i_epoch 0 batch 0: avg loss -2.009455 avg loss no lamb -2.009455 time 2019-03-04 10:08:47.433830
Model ind 640 epoch 1219 head B head_i_epoch 0 batch 100: avg loss -1.922701 avg loss no lamb -1.922701 time 2019-03-04 10:10:39.296174
Model ind 640 epoch 1219 head B head_i_epoch 0 batch 200: avg loss -1.959754 avg loss no lamb -1.959754 time 2019-03-04 10:12:29.698587
last batch sz 160
Model ind 640 epoch 1219 head B head_i_epoch 1 batch 0: avg loss -1.897177 avg loss no lamb -1.897177 time 2019-03-04 10:13:50.240352
Model ind 640 epoch 1219 head B head_i_epoch 1 batch 100: avg loss -1.866951 avg loss no lamb -1.866951 time 2019-03-04 10:15:39.912882
Model ind 640 epoch 1219 head B head_i_epoch 1 batch 200: avg loss -1.927656 avg loss no lamb -1.927656 time 2019-03-04 10:17:27.393632
last batch sz 160
Pre: time 2019-03-04 10:19:09.025256: 
 	std: 0.050330214
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51131666, 0.614, 0.61405, 0.61408335, 0.5113]
	train_accs: [0.51131666, 0.614, 0.61405, 0.61408335, 0.5113]
	best_train_sub_head: 3
	worst: 0.5113
	avg: 0.57295
	best: 0.61408335

Starting e_i: 1220
Model ind 640 epoch 1220 head A head_i_epoch 0 batch 0: avg loss -3.242084 avg loss no lamb -3.242084 time 2019-03-04 10:19:11.824125
Model ind 640 epoch 1220 head A head_i_epoch 0 batch 100: avg loss -3.200881 avg loss no lamb -3.200881 time 2019-03-04 10:21:01.499077
Model ind 640 epoch 1220 head A head_i_epoch 0 batch 200: avg loss -3.304977 avg loss no lamb -3.304977 time 2019-03-04 10:22:51.012570
last batch sz 160
Model ind 640 epoch 1220 head B head_i_epoch 0 batch 0: avg loss -1.903672 avg loss no lamb -1.903672 time 2019-03-04 10:24:10.567401
Model ind 640 epoch 1220 head B head_i_epoch 0 batch 100: avg loss -1.897935 avg loss no lamb -1.897935 time 2019-03-04 10:25:59.690192
Model ind 640 epoch 1220 head B head_i_epoch 0 batch 200: avg loss -2.027114 avg loss no lamb -2.027114 time 2019-03-04 10:27:51.083747
last batch sz 160
Model ind 640 epoch 1220 head B head_i_epoch 1 batch 0: avg loss -1.904386 avg loss no lamb -1.904386 time 2019-03-04 10:29:10.796281
Model ind 640 epoch 1220 head B head_i_epoch 1 batch 100: avg loss -1.870649 avg loss no lamb -1.870649 time 2019-03-04 10:31:00.171521
Model ind 640 epoch 1220 head B head_i_epoch 1 batch 200: avg loss -2.039652 avg loss no lamb -2.039652 time 2019-03-04 10:32:49.871245
last batch sz 160
Pre: time 2019-03-04 10:34:32.310936: 
 	std: 0.050546654
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51121664, 0.61425, 0.6142333, 0.6143, 0.51095]
	train_accs: [0.51121664, 0.61425, 0.6142333, 0.6143, 0.51095]
	best_train_sub_head: 3
	worst: 0.51095
	avg: 0.57299
	best: 0.6143

Starting e_i: 1221
Model ind 640 epoch 1221 head A head_i_epoch 0 batch 0: avg loss -3.191247 avg loss no lamb -3.191247 time 2019-03-04 10:34:39.105919
Model ind 640 epoch 1221 head A head_i_epoch 0 batch 100: avg loss -3.274835 avg loss no lamb -3.274835 time 2019-03-04 10:36:28.750104
Model ind 640 epoch 1221 head A head_i_epoch 0 batch 200: avg loss -3.324486 avg loss no lamb -3.324486 time 2019-03-04 10:38:17.222445
last batch sz 160
Model ind 640 epoch 1221 head B head_i_epoch 0 batch 0: avg loss -2.037581 avg loss no lamb -2.037581 time 2019-03-04 10:39:36.653145
Model ind 640 epoch 1221 head B head_i_epoch 0 batch 100: avg loss -1.920004 avg loss no lamb -1.920004 time 2019-03-04 10:41:26.770468
Model ind 640 epoch 1221 head B head_i_epoch 0 batch 200: avg loss -1.969779 avg loss no lamb -1.969779 time 2019-03-04 10:43:17.895463
last batch sz 160
Model ind 640 epoch 1221 head B head_i_epoch 1 batch 0: avg loss -1.997504 avg loss no lamb -1.997504 time 2019-03-04 10:44:37.767221
Model ind 640 epoch 1221 head B head_i_epoch 1 batch 100: avg loss -1.908667 avg loss no lamb -1.908667 time 2019-03-04 10:46:27.804293
Model ind 640 epoch 1221 head B head_i_epoch 1 batch 200: avg loss -1.966279 avg loss no lamb -1.966279 time 2019-03-04 10:48:17.610404
last batch sz 160
Pre: time 2019-03-04 10:49:58.463728: 
 	std: 0.05036976
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5116, 0.6142333, 0.61445, 0.6145167, 0.51156664]
	train_accs: [0.5116, 0.6142333, 0.61445, 0.6145167, 0.51156664]
	best_train_sub_head: 3
	worst: 0.51156664
	avg: 0.5732733
	best: 0.6145167

Starting e_i: 1222
Model ind 640 epoch 1222 head A head_i_epoch 0 batch 0: avg loss -3.343314 avg loss no lamb -3.343314 time 2019-03-04 10:50:01.229024
Model ind 640 epoch 1222 head A head_i_epoch 0 batch 100: avg loss -3.302541 avg loss no lamb -3.302541 time 2019-03-04 10:51:51.113618
Model ind 640 epoch 1222 head A head_i_epoch 0 batch 200: avg loss -3.267798 avg loss no lamb -3.267798 time 2019-03-04 10:53:40.454833
last batch sz 160
Model ind 640 epoch 1222 head B head_i_epoch 0 batch 0: avg loss -1.864113 avg loss no lamb -1.864113 time 2019-03-04 10:55:00.014344
Model ind 640 epoch 1222 head B head_i_epoch 0 batch 100: avg loss -1.906635 avg loss no lamb -1.906635 time 2019-03-04 10:56:48.894944
Model ind 640 epoch 1222 head B head_i_epoch 0 batch 200: avg loss -1.938274 avg loss no lamb -1.938274 time 2019-03-04 10:58:38.006402
last batch sz 160
Model ind 640 epoch 1222 head B head_i_epoch 1 batch 0: avg loss -1.956864 avg loss no lamb -1.956864 time 2019-03-04 10:59:56.322206
Model ind 640 epoch 1222 head B head_i_epoch 1 batch 100: avg loss -1.806657 avg loss no lamb -1.806657 time 2019-03-04 11:01:47.232571
Model ind 640 epoch 1222 head B head_i_epoch 1 batch 200: avg loss -1.977095 avg loss no lamb -1.977095 time 2019-03-04 11:03:36.446873
last batch sz 160
Pre: time 2019-03-04 11:05:17.358342: 
 	std: 0.050456885
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5119, 0.61478335, 0.6150333, 0.61471665, 0.5118]
	train_accs: [0.5119, 0.61478335, 0.6150333, 0.61471665, 0.5118]
	best_train_sub_head: 2
	worst: 0.5118
	avg: 0.57364666
	best: 0.6150333

Starting e_i: 1223
Model ind 640 epoch 1223 head A head_i_epoch 0 batch 0: avg loss -3.299564 avg loss no lamb -3.299564 time 2019-03-04 11:05:21.145037
Model ind 640 epoch 1223 head A head_i_epoch 0 batch 100: avg loss -3.189720 avg loss no lamb -3.189720 time 2019-03-04 11:07:11.788869
Model ind 640 epoch 1223 head A head_i_epoch 0 batch 200: avg loss -3.326840 avg loss no lamb -3.326840 time 2019-03-04 11:09:01.589916
last batch sz 160
Model ind 640 epoch 1223 head B head_i_epoch 0 batch 0: avg loss -1.949620 avg loss no lamb -1.949620 time 2019-03-04 11:10:21.597650
Model ind 640 epoch 1223 head B head_i_epoch 0 batch 100: avg loss -1.938505 avg loss no lamb -1.938505 time 2019-03-04 11:12:11.432447
Model ind 640 epoch 1223 head B head_i_epoch 0 batch 200: avg loss -2.008582 avg loss no lamb -2.008582 time 2019-03-04 11:14:00.716231
last batch sz 160
Model ind 640 epoch 1223 head B head_i_epoch 1 batch 0: avg loss -1.989635 avg loss no lamb -1.989635 time 2019-03-04 11:15:22.776862
Model ind 640 epoch 1223 head B head_i_epoch 1 batch 100: avg loss -1.899073 avg loss no lamb -1.899073 time 2019-03-04 11:17:12.249320
Model ind 640 epoch 1223 head B head_i_epoch 1 batch 200: avg loss -1.975650 avg loss no lamb -1.975650 time 2019-03-04 11:19:02.164526
last batch sz 160
Pre: time 2019-03-04 11:20:40.868085: 
 	std: 0.050576534
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5122167, 0.61553335, 0.61548334, 0.61545, 0.5122833]
	train_accs: [0.5122167, 0.61553335, 0.61548334, 0.61545, 0.5122833]
	best_train_sub_head: 1
	worst: 0.5122167
	avg: 0.57419336
	best: 0.61553335

Starting e_i: 1224
Model ind 640 epoch 1224 head A head_i_epoch 0 batch 0: avg loss -3.239207 avg loss no lamb -3.239207 time 2019-03-04 11:20:43.409376
Model ind 640 epoch 1224 head A head_i_epoch 0 batch 100: avg loss -3.274573 avg loss no lamb -3.274573 time 2019-03-04 11:22:33.057473
Model ind 640 epoch 1224 head A head_i_epoch 0 batch 200: avg loss -3.314770 avg loss no lamb -3.314770 time 2019-03-04 11:24:23.528042
last batch sz 160
Model ind 640 epoch 1224 head B head_i_epoch 0 batch 0: avg loss -1.901677 avg loss no lamb -1.901677 time 2019-03-04 11:25:43.222819
Model ind 640 epoch 1224 head B head_i_epoch 0 batch 100: avg loss -1.934223 avg loss no lamb -1.934223 time 2019-03-04 11:27:32.938888
Model ind 640 epoch 1224 head B head_i_epoch 0 batch 200: avg loss -1.935131 avg loss no lamb -1.935131 time 2019-03-04 11:29:21.718942
last batch sz 160
Model ind 640 epoch 1224 head B head_i_epoch 1 batch 0: avg loss -1.883449 avg loss no lamb -1.883449 time 2019-03-04 11:30:41.674038
Model ind 640 epoch 1224 head B head_i_epoch 1 batch 100: avg loss -1.963917 avg loss no lamb -1.963917 time 2019-03-04 11:32:32.761686
Model ind 640 epoch 1224 head B head_i_epoch 1 batch 200: avg loss -1.937638 avg loss no lamb -1.937638 time 2019-03-04 11:34:22.217863
last batch sz 160
Pre: time 2019-03-04 11:36:02.692441: 
 	std: 0.050508615
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115, 0.61441666, 0.61445, 0.61438334, 0.5111333]
	train_accs: [0.5115, 0.61441666, 0.61445, 0.61438334, 0.5111333]
	best_train_sub_head: 2
	worst: 0.5111333
	avg: 0.5731767
	best: 0.61445

Starting e_i: 1225
Model ind 640 epoch 1225 head A head_i_epoch 0 batch 0: avg loss -3.300000 avg loss no lamb -3.300000 time 2019-03-04 11:36:05.514671
Model ind 640 epoch 1225 head A head_i_epoch 0 batch 100: avg loss -3.197064 avg loss no lamb -3.197064 time 2019-03-04 11:37:55.719517
Model ind 640 epoch 1225 head A head_i_epoch 0 batch 200: avg loss -3.311107 avg loss no lamb -3.311107 time 2019-03-04 11:39:45.299101
last batch sz 160
Model ind 640 epoch 1225 head B head_i_epoch 0 batch 0: avg loss -2.023630 avg loss no lamb -2.023630 time 2019-03-04 11:41:05.093494
Model ind 640 epoch 1225 head B head_i_epoch 0 batch 100: avg loss -1.910693 avg loss no lamb -1.910693 time 2019-03-04 11:42:52.608609
Model ind 640 epoch 1225 head B head_i_epoch 0 batch 200: avg loss -1.920371 avg loss no lamb -1.920371 time 2019-03-04 11:44:43.030301
last batch sz 160
Model ind 640 epoch 1225 head B head_i_epoch 1 batch 0: avg loss -2.013264 avg loss no lamb -2.013264 time 2019-03-04 11:46:02.045577
Model ind 640 epoch 1225 head B head_i_epoch 1 batch 100: avg loss -1.932733 avg loss no lamb -1.932733 time 2019-03-04 11:47:53.476538
Model ind 640 epoch 1225 head B head_i_epoch 1 batch 200: avg loss -1.978441 avg loss no lamb -1.978441 time 2019-03-04 11:49:43.095886
last batch sz 160
Pre: time 2019-03-04 11:51:23.888712: 
 	std: 0.050765704
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111833, 0.6149, 0.6148667, 0.61478335, 0.51126665]
	train_accs: [0.5111833, 0.6149, 0.6148667, 0.61478335, 0.51126665]
	best_train_sub_head: 1
	worst: 0.5111833
	avg: 0.5734
	best: 0.6149

Starting e_i: 1226
Model ind 640 epoch 1226 head A head_i_epoch 0 batch 0: avg loss -3.288085 avg loss no lamb -3.288085 time 2019-03-04 11:51:28.361862
Model ind 640 epoch 1226 head A head_i_epoch 0 batch 100: avg loss -3.238537 avg loss no lamb -3.238537 time 2019-03-04 11:53:18.132667
Model ind 640 epoch 1226 head A head_i_epoch 0 batch 200: avg loss -3.321274 avg loss no lamb -3.321274 time 2019-03-04 11:55:08.237624
last batch sz 160
Model ind 640 epoch 1226 head B head_i_epoch 0 batch 0: avg loss -1.923674 avg loss no lamb -1.923674 time 2019-03-04 11:56:28.092261
Model ind 640 epoch 1226 head B head_i_epoch 0 batch 100: avg loss -1.937690 avg loss no lamb -1.937690 time 2019-03-04 11:58:18.113358
Model ind 640 epoch 1226 head B head_i_epoch 0 batch 200: avg loss -2.053037 avg loss no lamb -2.053037 time 2019-03-04 12:00:08.307003
last batch sz 160
Model ind 640 epoch 1226 head B head_i_epoch 1 batch 0: avg loss -1.916215 avg loss no lamb -1.916215 time 2019-03-04 12:01:27.820295
Model ind 640 epoch 1226 head B head_i_epoch 1 batch 100: avg loss -1.909262 avg loss no lamb -1.909262 time 2019-03-04 12:03:15.304738
Model ind 640 epoch 1226 head B head_i_epoch 1 batch 200: avg loss -1.978466 avg loss no lamb -1.978466 time 2019-03-04 12:05:06.827905
last batch sz 160
Pre: time 2019-03-04 12:06:47.977394: 
 	std: 0.05040098
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51055, 0.61345, 0.61343336, 0.61343336, 0.51056665]
	train_accs: [0.51055, 0.61345, 0.61343336, 0.61343336, 0.51056665]
	best_train_sub_head: 1
	worst: 0.51055
	avg: 0.5722867
	best: 0.61345

Starting e_i: 1227
Model ind 640 epoch 1227 head A head_i_epoch 0 batch 0: avg loss -3.352945 avg loss no lamb -3.352945 time 2019-03-04 12:06:50.864984
Model ind 640 epoch 1227 head A head_i_epoch 0 batch 100: avg loss -3.224393 avg loss no lamb -3.224393 time 2019-03-04 12:08:40.502621
Model ind 640 epoch 1227 head A head_i_epoch 0 batch 200: avg loss -3.297539 avg loss no lamb -3.297539 time 2019-03-04 12:10:30.178137
last batch sz 160
Model ind 640 epoch 1227 head B head_i_epoch 0 batch 0: avg loss -1.950862 avg loss no lamb -1.950862 time 2019-03-04 12:11:49.995781
Model ind 640 epoch 1227 head B head_i_epoch 0 batch 100: avg loss -1.896647 avg loss no lamb -1.896647 time 2019-03-04 12:13:40.197273
Model ind 640 epoch 1227 head B head_i_epoch 0 batch 200: avg loss -1.897262 avg loss no lamb -1.897262 time 2019-03-04 12:15:30.046565
last batch sz 160
Model ind 640 epoch 1227 head B head_i_epoch 1 batch 0: avg loss -1.903305 avg loss no lamb -1.903305 time 2019-03-04 12:16:49.690309
Model ind 640 epoch 1227 head B head_i_epoch 1 batch 100: avg loss -1.888151 avg loss no lamb -1.888151 time 2019-03-04 12:18:38.910937
Model ind 640 epoch 1227 head B head_i_epoch 1 batch 200: avg loss -1.914639 avg loss no lamb -1.914639 time 2019-03-04 12:20:29.521545
last batch sz 160
Pre: time 2019-03-04 12:22:09.927142: 
 	std: 0.050293524
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51185, 0.61466664, 0.6145667, 0.6146, 0.51205]
	train_accs: [0.51185, 0.61466664, 0.6145667, 0.6146, 0.51205]
	best_train_sub_head: 1
	worst: 0.51185
	avg: 0.57354665
	best: 0.61466664

Starting e_i: 1228
Model ind 640 epoch 1228 head A head_i_epoch 0 batch 0: avg loss -3.317843 avg loss no lamb -3.317843 time 2019-03-04 12:22:12.848964
Model ind 640 epoch 1228 head A head_i_epoch 0 batch 100: avg loss -3.245140 avg loss no lamb -3.245140 time 2019-03-04 12:24:01.638646
Model ind 640 epoch 1228 head A head_i_epoch 0 batch 200: avg loss -3.323771 avg loss no lamb -3.323771 time 2019-03-04 12:25:50.916801
last batch sz 160
Model ind 640 epoch 1228 head B head_i_epoch 0 batch 0: avg loss -1.874601 avg loss no lamb -1.874601 time 2019-03-04 12:27:10.529725
Model ind 640 epoch 1228 head B head_i_epoch 0 batch 100: avg loss -1.972448 avg loss no lamb -1.972448 time 2019-03-04 12:28:59.630971
Model ind 640 epoch 1228 head B head_i_epoch 0 batch 200: avg loss -1.940054 avg loss no lamb -1.940054 time 2019-03-04 12:30:48.999917
last batch sz 160
Model ind 640 epoch 1228 head B head_i_epoch 1 batch 0: avg loss -1.933367 avg loss no lamb -1.933367 time 2019-03-04 12:32:08.841156
Model ind 640 epoch 1228 head B head_i_epoch 1 batch 100: avg loss -1.882525 avg loss no lamb -1.882525 time 2019-03-04 12:33:58.114982
Model ind 640 epoch 1228 head B head_i_epoch 1 batch 200: avg loss -1.994500 avg loss no lamb -1.994500 time 2019-03-04 12:35:47.677883
last batch sz 160
Pre: time 2019-03-04 12:37:30.166290: 
 	std: 0.050526172
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51155, 0.61466664, 0.6146333, 0.6145833, 0.51143336]
	train_accs: [0.51155, 0.61466664, 0.6146333, 0.6145833, 0.51143336]
	best_train_sub_head: 1
	worst: 0.51143336
	avg: 0.5733733
	best: 0.61466664

Starting e_i: 1229
Model ind 640 epoch 1229 head A head_i_epoch 0 batch 0: avg loss -3.269529 avg loss no lamb -3.269529 time 2019-03-04 12:37:33.039060
Model ind 640 epoch 1229 head A head_i_epoch 0 batch 100: avg loss -3.178909 avg loss no lamb -3.178909 time 2019-03-04 12:39:22.610449
Model ind 640 epoch 1229 head A head_i_epoch 0 batch 200: avg loss -3.326759 avg loss no lamb -3.326759 time 2019-03-04 12:41:12.511696
last batch sz 160
Model ind 640 epoch 1229 head B head_i_epoch 0 batch 0: avg loss -1.929189 avg loss no lamb -1.929189 time 2019-03-04 12:42:32.302657
Model ind 640 epoch 1229 head B head_i_epoch 0 batch 100: avg loss -1.968035 avg loss no lamb -1.968035 time 2019-03-04 12:44:22.611453
Model ind 640 epoch 1229 head B head_i_epoch 0 batch 200: avg loss -1.971292 avg loss no lamb -1.971292 time 2019-03-04 12:46:09.592466
last batch sz 160
Model ind 640 epoch 1229 head B head_i_epoch 1 batch 0: avg loss -1.929925 avg loss no lamb -1.929925 time 2019-03-04 12:47:29.531325
Model ind 640 epoch 1229 head B head_i_epoch 1 batch 100: avg loss -1.943558 avg loss no lamb -1.943558 time 2019-03-04 12:49:19.991728
Model ind 640 epoch 1229 head B head_i_epoch 1 batch 200: avg loss -1.908707 avg loss no lamb -1.908707 time 2019-03-04 12:51:16.353038
last batch sz 160
Pre: time 2019-03-04 12:53:50.159960: 
 	std: 0.05061598
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111667, 0.6145, 0.6145833, 0.6145, 0.51125]
	train_accs: [0.5111667, 0.6145, 0.6145833, 0.6145, 0.51125]
	best_train_sub_head: 2
	worst: 0.5111667
	avg: 0.5732
	best: 0.6145833

Starting e_i: 1230
Model ind 640 epoch 1230 head A head_i_epoch 0 batch 0: avg loss -3.300790 avg loss no lamb -3.300790 time 2019-03-04 12:53:53.752674
Model ind 640 epoch 1230 head A head_i_epoch 0 batch 100: avg loss -3.208423 avg loss no lamb -3.208423 time 2019-03-04 12:56:40.303413
Model ind 640 epoch 1230 head A head_i_epoch 0 batch 200: avg loss -3.268538 avg loss no lamb -3.268538 time 2019-03-04 12:59:05.240630
last batch sz 160
Model ind 640 epoch 1230 head B head_i_epoch 0 batch 0: avg loss -1.946993 avg loss no lamb -1.946993 time 2019-03-04 13:00:25.381531
Model ind 640 epoch 1230 head B head_i_epoch 0 batch 100: avg loss -1.952600 avg loss no lamb -1.952600 time 2019-03-04 13:02:16.035862
Model ind 640 epoch 1230 head B head_i_epoch 0 batch 200: avg loss -2.003477 avg loss no lamb -2.003477 time 2019-03-04 13:04:06.186234
last batch sz 160
Model ind 640 epoch 1230 head B head_i_epoch 1 batch 0: avg loss -1.991919 avg loss no lamb -1.991919 time 2019-03-04 13:05:25.504974
Model ind 640 epoch 1230 head B head_i_epoch 1 batch 100: avg loss -1.911592 avg loss no lamb -1.911592 time 2019-03-04 13:07:11.307343
Model ind 640 epoch 1230 head B head_i_epoch 1 batch 200: avg loss -2.009614 avg loss no lamb -2.009614 time 2019-03-04 13:09:20.528200
last batch sz 160
Pre: time 2019-03-04 13:11:24.264094: 
 	std: 0.050584756
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5117667, 0.6148833, 0.6149167, 0.6148667, 0.5115]
	train_accs: [0.5117667, 0.6148833, 0.6149167, 0.6148667, 0.5115]
	best_train_sub_head: 2
	worst: 0.5115
	avg: 0.57358664
	best: 0.6149167

Starting e_i: 1231
Model ind 640 epoch 1231 head A head_i_epoch 0 batch 0: avg loss -3.362993 avg loss no lamb -3.362993 time 2019-03-04 13:11:31.580396
Model ind 640 epoch 1231 head A head_i_epoch 0 batch 100: avg loss -3.283848 avg loss no lamb -3.283848 time 2019-03-04 13:13:43.249317
Model ind 640 epoch 1231 head A head_i_epoch 0 batch 200: avg loss -3.432485 avg loss no lamb -3.432485 time 2019-03-04 13:15:33.129669
last batch sz 160
Model ind 640 epoch 1231 head B head_i_epoch 0 batch 0: avg loss -1.939718 avg loss no lamb -1.939718 time 2019-03-04 13:16:52.558889
Model ind 640 epoch 1231 head B head_i_epoch 0 batch 100: avg loss -1.881145 avg loss no lamb -1.881145 time 2019-03-04 13:18:44.526574
Model ind 640 epoch 1231 head B head_i_epoch 0 batch 200: avg loss -1.987437 avg loss no lamb -1.987437 time 2019-03-04 13:20:37.378039
last batch sz 160
Model ind 640 epoch 1231 head B head_i_epoch 1 batch 0: avg loss -1.995609 avg loss no lamb -1.995609 time 2019-03-04 13:21:56.979673
Model ind 640 epoch 1231 head B head_i_epoch 1 batch 100: avg loss -1.878560 avg loss no lamb -1.878560 time 2019-03-04 13:23:46.793398
Model ind 640 epoch 1231 head B head_i_epoch 1 batch 200: avg loss -1.920363 avg loss no lamb -1.920363 time 2019-03-04 13:25:36.083698
last batch sz 160
Pre: time 2019-03-04 13:27:16.850083: 
 	std: 0.050686784
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51171666, 0.61506665, 0.61508334, 0.61511666, 0.5115333]
	train_accs: [0.51171666, 0.61506665, 0.61508334, 0.61511666, 0.5115333]
	best_train_sub_head: 3
	worst: 0.5115333
	avg: 0.57370335
	best: 0.61511666

Starting e_i: 1232
Model ind 640 epoch 1232 head A head_i_epoch 0 batch 0: avg loss -3.270944 avg loss no lamb -3.270944 time 2019-03-04 13:27:19.285799
Model ind 640 epoch 1232 head A head_i_epoch 0 batch 100: avg loss -3.272989 avg loss no lamb -3.272989 time 2019-03-04 13:29:06.442735
Model ind 640 epoch 1232 head A head_i_epoch 0 batch 200: avg loss -3.346578 avg loss no lamb -3.346578 time 2019-03-04 13:30:54.393761
last batch sz 160
Model ind 640 epoch 1232 head B head_i_epoch 0 batch 0: avg loss -1.990417 avg loss no lamb -1.990417 time 2019-03-04 13:32:13.693818
Model ind 640 epoch 1232 head B head_i_epoch 0 batch 100: avg loss -1.920111 avg loss no lamb -1.920111 time 2019-03-04 13:34:02.377610
Model ind 640 epoch 1232 head B head_i_epoch 0 batch 200: avg loss -1.960306 avg loss no lamb -1.960306 time 2019-03-04 13:35:54.966189
last batch sz 160
Model ind 640 epoch 1232 head B head_i_epoch 1 batch 0: avg loss -1.994819 avg loss no lamb -1.994819 time 2019-03-04 13:37:18.867349
Model ind 640 epoch 1232 head B head_i_epoch 1 batch 100: avg loss -2.049439 avg loss no lamb -2.049439 time 2019-03-04 13:39:07.087057
Model ind 640 epoch 1232 head B head_i_epoch 1 batch 200: avg loss -2.004716 avg loss no lamb -2.004716 time 2019-03-04 13:40:55.062896
last batch sz 160
Pre: time 2019-03-04 13:42:33.525257: 
 	std: 0.05105286
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51031667, 0.61448336, 0.61438334, 0.6145667, 0.51021665]
	train_accs: [0.51031667, 0.61448336, 0.61438334, 0.6145667, 0.51021665]
	best_train_sub_head: 3
	worst: 0.51021665
	avg: 0.57279336
	best: 0.6145667

Starting e_i: 1233
Model ind 640 epoch 1233 head A head_i_epoch 0 batch 0: avg loss -3.320333 avg loss no lamb -3.320333 time 2019-03-04 13:42:36.365964
Model ind 640 epoch 1233 head A head_i_epoch 0 batch 100: avg loss -3.194607 avg loss no lamb -3.194607 time 2019-03-04 13:44:23.966968
Model ind 640 epoch 1233 head A head_i_epoch 0 batch 200: avg loss -3.306415 avg loss no lamb -3.306415 time 2019-03-04 13:46:11.870011
last batch sz 160
Model ind 640 epoch 1233 head B head_i_epoch 0 batch 0: avg loss -1.978747 avg loss no lamb -1.978747 time 2019-03-04 13:47:30.197652
Model ind 640 epoch 1233 head B head_i_epoch 0 batch 100: avg loss -1.949153 avg loss no lamb -1.949153 time 2019-03-04 13:49:15.146605
Model ind 640 epoch 1233 head B head_i_epoch 0 batch 200: avg loss -1.987092 avg loss no lamb -1.987092 time 2019-03-04 13:51:03.152540
last batch sz 160
Model ind 640 epoch 1233 head B head_i_epoch 1 batch 0: avg loss -1.904142 avg loss no lamb -1.904142 time 2019-03-04 13:52:21.198467
Model ind 640 epoch 1233 head B head_i_epoch 1 batch 100: avg loss -1.898691 avg loss no lamb -1.898691 time 2019-03-04 13:54:08.627083
Model ind 640 epoch 1233 head B head_i_epoch 1 batch 200: avg loss -2.040348 avg loss no lamb -2.040348 time 2019-03-04 13:55:55.472073
last batch sz 160
Pre: time 2019-03-04 13:57:33.817603: 
 	std: 0.05095902
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5107833, 0.61475, 0.6146, 0.6146333, 0.5105]
	train_accs: [0.5107833, 0.61475, 0.6146, 0.6146333, 0.5105]
	best_train_sub_head: 1
	worst: 0.5105
	avg: 0.5730533
	best: 0.61475

Starting e_i: 1234
Model ind 640 epoch 1234 head A head_i_epoch 0 batch 0: avg loss -3.290247 avg loss no lamb -3.290247 time 2019-03-04 13:57:37.550979
Model ind 640 epoch 1234 head A head_i_epoch 0 batch 100: avg loss -3.228129 avg loss no lamb -3.228129 time 2019-03-04 13:59:25.634967
Model ind 640 epoch 1234 head A head_i_epoch 0 batch 200: avg loss -3.225166 avg loss no lamb -3.225166 time 2019-03-04 14:01:13.298780
last batch sz 160
Model ind 640 epoch 1234 head B head_i_epoch 0 batch 0: avg loss -1.985552 avg loss no lamb -1.985552 time 2019-03-04 14:02:31.485561
Model ind 640 epoch 1234 head B head_i_epoch 0 batch 100: avg loss -1.866545 avg loss no lamb -1.866545 time 2019-03-04 14:04:19.365357
Model ind 640 epoch 1234 head B head_i_epoch 0 batch 200: avg loss -2.035933 avg loss no lamb -2.035933 time 2019-03-04 14:06:07.099821
last batch sz 160
Model ind 640 epoch 1234 head B head_i_epoch 1 batch 0: avg loss -1.918936 avg loss no lamb -1.918936 time 2019-03-04 14:07:26.198405
Model ind 640 epoch 1234 head B head_i_epoch 1 batch 100: avg loss -1.968045 avg loss no lamb -1.968045 time 2019-03-04 14:09:12.725759
Model ind 640 epoch 1234 head B head_i_epoch 1 batch 200: avg loss -1.973367 avg loss no lamb -1.973367 time 2019-03-04 14:11:00.045343
last batch sz 160
Pre: time 2019-03-04 14:12:39.681871: 
 	std: 0.050707284
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5110833, 0.61438334, 0.61446667, 0.61441666, 0.51075]
	train_accs: [0.5110833, 0.61438334, 0.61446667, 0.61441666, 0.51075]
	best_train_sub_head: 2
	worst: 0.51075
	avg: 0.57302
	best: 0.61446667

Starting e_i: 1235
Model ind 640 epoch 1235 head A head_i_epoch 0 batch 0: avg loss -3.318966 avg loss no lamb -3.318966 time 2019-03-04 14:12:44.702382
Model ind 640 epoch 1235 head A head_i_epoch 0 batch 100: avg loss -3.262027 avg loss no lamb -3.262027 time 2019-03-04 14:14:32.898194
Model ind 640 epoch 1235 head A head_i_epoch 0 batch 200: avg loss -3.244341 avg loss no lamb -3.244341 time 2019-03-04 14:16:20.859168
last batch sz 160
Model ind 640 epoch 1235 head B head_i_epoch 0 batch 0: avg loss -1.948142 avg loss no lamb -1.948142 time 2019-03-04 14:17:39.239971
Model ind 640 epoch 1235 head B head_i_epoch 0 batch 100: avg loss -1.896289 avg loss no lamb -1.896289 time 2019-03-04 14:19:26.688640
Model ind 640 epoch 1235 head B head_i_epoch 0 batch 200: avg loss -2.029219 avg loss no lamb -2.029219 time 2019-03-04 14:21:14.133094
last batch sz 160
Model ind 640 epoch 1235 head B head_i_epoch 1 batch 0: avg loss -1.939705 avg loss no lamb -1.939705 time 2019-03-04 14:22:32.709232
Model ind 640 epoch 1235 head B head_i_epoch 1 batch 100: avg loss -1.895848 avg loss no lamb -1.895848 time 2019-03-04 14:24:20.096371
Model ind 640 epoch 1235 head B head_i_epoch 1 batch 200: avg loss -2.006959 avg loss no lamb -2.006959 time 2019-03-04 14:26:08.124459
last batch sz 160
Pre: time 2019-03-04 14:27:47.493591: 
 	std: 0.050772496
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107667, 0.61433333, 0.61443335, 0.61435, 0.5107]
	train_accs: [0.5107667, 0.61433333, 0.61443335, 0.61435, 0.5107]
	best_train_sub_head: 2
	worst: 0.5107
	avg: 0.5729167
	best: 0.61443335

Starting e_i: 1236
Model ind 640 epoch 1236 head A head_i_epoch 0 batch 0: avg loss -3.243967 avg loss no lamb -3.243967 time 2019-03-04 14:27:50.567517
Model ind 640 epoch 1236 head A head_i_epoch 0 batch 100: avg loss -3.235277 avg loss no lamb -3.235277 time 2019-03-04 14:29:38.037914
Model ind 640 epoch 1236 head A head_i_epoch 0 batch 200: avg loss -3.354451 avg loss no lamb -3.354451 time 2019-03-04 14:31:23.224080
last batch sz 160
Model ind 640 epoch 1236 head B head_i_epoch 0 batch 0: avg loss -2.038067 avg loss no lamb -2.038067 time 2019-03-04 14:32:43.010484
Model ind 640 epoch 1236 head B head_i_epoch 0 batch 100: avg loss -1.907361 avg loss no lamb -1.907361 time 2019-03-04 14:34:34.223678
Model ind 640 epoch 1236 head B head_i_epoch 0 batch 200: avg loss -1.949229 avg loss no lamb -1.949229 time 2019-03-04 14:36:23.337307
last batch sz 160
Model ind 640 epoch 1236 head B head_i_epoch 1 batch 0: avg loss -1.930285 avg loss no lamb -1.930285 time 2019-03-04 14:37:42.694262
Model ind 640 epoch 1236 head B head_i_epoch 1 batch 100: avg loss -1.833300 avg loss no lamb -1.833300 time 2019-03-04 14:39:32.112114
Model ind 640 epoch 1236 head B head_i_epoch 1 batch 200: avg loss -1.960427 avg loss no lamb -1.960427 time 2019-03-04 14:41:20.956484
last batch sz 160
Pre: time 2019-03-04 14:43:01.993957: 
 	std: 0.050573837
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51231664, 0.6156667, 0.61565, 0.61558336, 0.51248336]
	train_accs: [0.51231664, 0.6156667, 0.61565, 0.61558336, 0.51248336]
	best_train_sub_head: 1
	worst: 0.51231664
	avg: 0.57434
	best: 0.6156667

Starting e_i: 1237
Model ind 640 epoch 1237 head A head_i_epoch 0 batch 0: avg loss -3.265774 avg loss no lamb -3.265774 time 2019-03-04 14:43:04.748941
Model ind 640 epoch 1237 head A head_i_epoch 0 batch 100: avg loss -3.195353 avg loss no lamb -3.195353 time 2019-03-04 14:44:53.922160
Model ind 640 epoch 1237 head A head_i_epoch 0 batch 200: avg loss -3.321654 avg loss no lamb -3.321654 time 2019-03-04 14:46:42.928376
last batch sz 160
Model ind 640 epoch 1237 head B head_i_epoch 0 batch 0: avg loss -1.984599 avg loss no lamb -1.984599 time 2019-03-04 14:48:01.797837
Model ind 640 epoch 1237 head B head_i_epoch 0 batch 100: avg loss -1.882474 avg loss no lamb -1.882474 time 2019-03-04 14:49:53.481069
Model ind 640 epoch 1237 head B head_i_epoch 0 batch 200: avg loss -1.960583 avg loss no lamb -1.960583 time 2019-03-04 14:51:41.035346
last batch sz 160
Model ind 640 epoch 1237 head B head_i_epoch 1 batch 0: avg loss -1.904698 avg loss no lamb -1.904698 time 2019-03-04 14:53:00.213888
Model ind 640 epoch 1237 head B head_i_epoch 1 batch 100: avg loss -1.956979 avg loss no lamb -1.956979 time 2019-03-04 14:54:49.815697
Model ind 640 epoch 1237 head B head_i_epoch 1 batch 200: avg loss -1.958552 avg loss no lamb -1.958552 time 2019-03-04 14:56:38.884704
last batch sz 160
Pre: time 2019-03-04 14:58:19.418431: 
 	std: 0.05036335
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114, 0.6146167, 0.6145, 0.6145167, 0.51208335]
	train_accs: [0.5114, 0.6146167, 0.6145, 0.6145167, 0.51208335]
	best_train_sub_head: 1
	worst: 0.5114
	avg: 0.5734233
	best: 0.6146167

Starting e_i: 1238
Model ind 640 epoch 1238 head A head_i_epoch 0 batch 0: avg loss -3.367273 avg loss no lamb -3.367273 time 2019-03-04 14:58:22.283547
Model ind 640 epoch 1238 head A head_i_epoch 0 batch 100: avg loss -3.307369 avg loss no lamb -3.307369 time 2019-03-04 15:00:12.311766
Model ind 640 epoch 1238 head A head_i_epoch 0 batch 200: avg loss -3.344238 avg loss no lamb -3.344238 time 2019-03-04 15:02:02.049853
last batch sz 160
Model ind 640 epoch 1238 head B head_i_epoch 0 batch 0: avg loss -1.990147 avg loss no lamb -1.990147 time 2019-03-04 15:03:21.177891
Model ind 640 epoch 1238 head B head_i_epoch 0 batch 100: avg loss -1.920804 avg loss no lamb -1.920804 time 2019-03-04 15:05:11.089346
Model ind 640 epoch 1238 head B head_i_epoch 0 batch 200: avg loss -1.947675 avg loss no lamb -1.947675 time 2019-03-04 15:07:01.271438
last batch sz 160
Model ind 640 epoch 1238 head B head_i_epoch 1 batch 0: avg loss -1.947969 avg loss no lamb -1.947969 time 2019-03-04 15:08:20.108168
Model ind 640 epoch 1238 head B head_i_epoch 1 batch 100: avg loss -1.881042 avg loss no lamb -1.881042 time 2019-03-04 15:10:14.167643
Model ind 640 epoch 1238 head B head_i_epoch 1 batch 200: avg loss -1.928237 avg loss no lamb -1.928237 time 2019-03-04 15:12:02.915818
last batch sz 160
Pre: time 2019-03-04 15:13:40.975304: 
 	std: 0.050356332
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111833, 0.61425, 0.6142667, 0.61415, 0.51168334]
	train_accs: [0.5111833, 0.61425, 0.6142667, 0.61415, 0.51168334]
	best_train_sub_head: 2
	worst: 0.5111833
	avg: 0.57310665
	best: 0.6142667

Starting e_i: 1239
Model ind 640 epoch 1239 head A head_i_epoch 0 batch 0: avg loss -3.331558 avg loss no lamb -3.331558 time 2019-03-04 15:13:43.932896
Model ind 640 epoch 1239 head A head_i_epoch 0 batch 100: avg loss -3.260087 avg loss no lamb -3.260087 time 2019-03-04 15:15:33.248497
Model ind 640 epoch 1239 head A head_i_epoch 0 batch 200: avg loss -3.381991 avg loss no lamb -3.381991 time 2019-03-04 15:17:22.450490
last batch sz 160
Model ind 640 epoch 1239 head B head_i_epoch 0 batch 0: avg loss -2.012511 avg loss no lamb -2.012511 time 2019-03-04 15:18:41.252920
Model ind 640 epoch 1239 head B head_i_epoch 0 batch 100: avg loss -1.969576 avg loss no lamb -1.969576 time 2019-03-04 15:20:28.634006
Model ind 640 epoch 1239 head B head_i_epoch 0 batch 200: avg loss -1.953190 avg loss no lamb -1.953190 time 2019-03-04 15:22:21.078849
last batch sz 160
Model ind 640 epoch 1239 head B head_i_epoch 1 batch 0: avg loss -1.936731 avg loss no lamb -1.936731 time 2019-03-04 15:23:40.692908
Model ind 640 epoch 1239 head B head_i_epoch 1 batch 100: avg loss -1.875075 avg loss no lamb -1.875075 time 2019-03-04 15:25:29.787479
Model ind 640 epoch 1239 head B head_i_epoch 1 batch 200: avg loss -1.950721 avg loss no lamb -1.950721 time 2019-03-04 15:27:19.802158
last batch sz 160
Pre: time 2019-03-04 15:29:00.210220: 
 	std: 0.050557453
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51165, 0.61485, 0.6148667, 0.6148833, 0.51168334]
	train_accs: [0.51165, 0.61485, 0.6148667, 0.6148833, 0.51168334]
	best_train_sub_head: 3
	worst: 0.51165
	avg: 0.57358664
	best: 0.6148833

Starting e_i: 1240
Model ind 640 epoch 1240 head A head_i_epoch 0 batch 0: avg loss -3.266267 avg loss no lamb -3.266267 time 2019-03-04 15:29:03.457395
Model ind 640 epoch 1240 head A head_i_epoch 0 batch 100: avg loss -3.303133 avg loss no lamb -3.303133 time 2019-03-04 15:30:52.864844
Model ind 640 epoch 1240 head A head_i_epoch 0 batch 200: avg loss -3.325039 avg loss no lamb -3.325039 time 2019-03-04 15:32:42.994581
last batch sz 160
Model ind 640 epoch 1240 head B head_i_epoch 0 batch 0: avg loss -1.891274 avg loss no lamb -1.891274 time 2019-03-04 15:34:07.715957
Model ind 640 epoch 1240 head B head_i_epoch 0 batch 100: avg loss -1.876910 avg loss no lamb -1.876910 time 2019-03-04 15:36:00.005966
Model ind 640 epoch 1240 head B head_i_epoch 0 batch 200: avg loss -1.897925 avg loss no lamb -1.897925 time 2019-03-04 15:37:51.014546
last batch sz 160
Model ind 640 epoch 1240 head B head_i_epoch 1 batch 0: avg loss -1.949510 avg loss no lamb -1.949510 time 2019-03-04 15:39:09.225128
Model ind 640 epoch 1240 head B head_i_epoch 1 batch 100: avg loss -1.903200 avg loss no lamb -1.903200 time 2019-03-04 15:40:53.858845
Model ind 640 epoch 1240 head B head_i_epoch 1 batch 200: avg loss -1.937112 avg loss no lamb -1.937112 time 2019-03-04 15:42:39.635209
last batch sz 160
Pre: time 2019-03-04 15:44:16.710635: 
 	std: 0.050949406
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111667, 0.61506665, 0.6152167, 0.61511666, 0.5111]
	train_accs: [0.5111667, 0.61506665, 0.6152167, 0.61511666, 0.5111]
	best_train_sub_head: 2
	worst: 0.5111
	avg: 0.57353336
	best: 0.6152167

Starting e_i: 1241
Model ind 640 epoch 1241 head A head_i_epoch 0 batch 0: avg loss -3.314003 avg loss no lamb -3.314003 time 2019-03-04 15:44:23.085338
Model ind 640 epoch 1241 head A head_i_epoch 0 batch 100: avg loss -3.276125 avg loss no lamb -3.276125 time 2019-03-04 15:46:07.038233
Model ind 640 epoch 1241 head A head_i_epoch 0 batch 200: avg loss -3.376275 avg loss no lamb -3.376275 time 2019-03-04 15:47:51.175409
last batch sz 160
Model ind 640 epoch 1241 head B head_i_epoch 0 batch 0: avg loss -1.927087 avg loss no lamb -1.927087 time 2019-03-04 15:49:06.817731
Model ind 640 epoch 1241 head B head_i_epoch 0 batch 100: avg loss -2.041831 avg loss no lamb -2.041831 time 2019-03-04 15:50:52.322612
Model ind 640 epoch 1241 head B head_i_epoch 0 batch 200: avg loss -2.006608 avg loss no lamb -2.006608 time 2019-03-04 15:52:40.057385
last batch sz 160
Model ind 640 epoch 1241 head B head_i_epoch 1 batch 0: avg loss -2.000251 avg loss no lamb -2.000251 time 2019-03-04 15:53:56.184319
Model ind 640 epoch 1241 head B head_i_epoch 1 batch 100: avg loss -1.905776 avg loss no lamb -1.905776 time 2019-03-04 15:55:46.339248
Model ind 640 epoch 1241 head B head_i_epoch 1 batch 200: avg loss -1.968108 avg loss no lamb -1.968108 time 2019-03-04 15:57:35.134464
last batch sz 160
Pre: time 2019-03-04 15:59:15.062504: 
 	std: 0.05070043
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108, 0.61413336, 0.61413336, 0.6142333, 0.51055]
	train_accs: [0.5108, 0.61413336, 0.61413336, 0.6142333, 0.51055]
	best_train_sub_head: 3
	worst: 0.51055
	avg: 0.57277
	best: 0.6142333

Starting e_i: 1242
Model ind 640 epoch 1242 head A head_i_epoch 0 batch 0: avg loss -3.258500 avg loss no lamb -3.258500 time 2019-03-04 15:59:18.702671
Model ind 640 epoch 1242 head A head_i_epoch 0 batch 100: avg loss -3.245477 avg loss no lamb -3.245477 time 2019-03-04 16:01:10.292807
Model ind 640 epoch 1242 head A head_i_epoch 0 batch 200: avg loss -3.196137 avg loss no lamb -3.196137 time 2019-03-04 16:02:58.599492
last batch sz 160
Model ind 640 epoch 1242 head B head_i_epoch 0 batch 0: avg loss -1.917102 avg loss no lamb -1.917102 time 2019-03-04 16:04:19.732905
Model ind 640 epoch 1242 head B head_i_epoch 0 batch 100: avg loss -2.010689 avg loss no lamb -2.010689 time 2019-03-04 16:06:08.800640
Model ind 640 epoch 1242 head B head_i_epoch 0 batch 200: avg loss -1.996366 avg loss no lamb -1.996366 time 2019-03-04 16:07:55.890214
last batch sz 160
Model ind 640 epoch 1242 head B head_i_epoch 1 batch 0: avg loss -1.940069 avg loss no lamb -1.940069 time 2019-03-04 16:09:18.075926
Model ind 640 epoch 1242 head B head_i_epoch 1 batch 100: avg loss -2.019109 avg loss no lamb -2.019109 time 2019-03-04 16:11:07.733935
Model ind 640 epoch 1242 head B head_i_epoch 1 batch 200: avg loss -1.996301 avg loss no lamb -1.996301 time 2019-03-04 16:12:57.321744
last batch sz 160
Pre: time 2019-03-04 16:14:38.741553: 
 	std: 0.050803784
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5108167, 0.61445, 0.6145833, 0.61445, 0.5107667]
	train_accs: [0.5108167, 0.61445, 0.6145833, 0.61445, 0.5107667]
	best_train_sub_head: 2
	worst: 0.5107667
	avg: 0.57301337
	best: 0.6145833

Starting e_i: 1243
Model ind 640 epoch 1243 head A head_i_epoch 0 batch 0: avg loss -3.397757 avg loss no lamb -3.397757 time 2019-03-04 16:14:41.823306
Model ind 640 epoch 1243 head A head_i_epoch 0 batch 100: avg loss -3.242649 avg loss no lamb -3.242649 time 2019-03-04 16:16:30.916140
Model ind 640 epoch 1243 head A head_i_epoch 0 batch 200: avg loss -3.280348 avg loss no lamb -3.280348 time 2019-03-04 16:18:20.412814
last batch sz 160
Model ind 640 epoch 1243 head B head_i_epoch 0 batch 0: avg loss -1.924474 avg loss no lamb -1.924474 time 2019-03-04 16:19:39.908138
Model ind 640 epoch 1243 head B head_i_epoch 0 batch 100: avg loss -1.953929 avg loss no lamb -1.953929 time 2019-03-04 16:21:27.889620
Model ind 640 epoch 1243 head B head_i_epoch 0 batch 200: avg loss -2.030425 avg loss no lamb -2.030425 time 2019-03-04 16:23:17.873407
last batch sz 160
Model ind 640 epoch 1243 head B head_i_epoch 1 batch 0: avg loss -1.933242 avg loss no lamb -1.933242 time 2019-03-04 16:24:34.618720
Model ind 640 epoch 1243 head B head_i_epoch 1 batch 100: avg loss -1.899922 avg loss no lamb -1.899922 time 2019-03-04 16:26:27.243517
Model ind 640 epoch 1243 head B head_i_epoch 1 batch 200: avg loss -1.995660 avg loss no lamb -1.995660 time 2019-03-04 16:28:16.876601
last batch sz 160
Pre: time 2019-03-04 16:29:57.333721: 
 	std: 0.05034523
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115, 0.61411667, 0.6142667, 0.6142667, 0.5114]
	train_accs: [0.5115, 0.61411667, 0.6142667, 0.6142667, 0.5114]
	best_train_sub_head: 2
	worst: 0.5114
	avg: 0.57311
	best: 0.6142667

Starting e_i: 1244
Model ind 640 epoch 1244 head A head_i_epoch 0 batch 0: avg loss -3.297651 avg loss no lamb -3.297651 time 2019-03-04 16:30:00.729461
Model ind 640 epoch 1244 head A head_i_epoch 0 batch 100: avg loss -3.229384 avg loss no lamb -3.229384 time 2019-03-04 16:31:50.000477
Model ind 640 epoch 1244 head A head_i_epoch 0 batch 200: avg loss -3.257967 avg loss no lamb -3.257967 time 2019-03-04 16:33:40.763863
last batch sz 160
Model ind 640 epoch 1244 head B head_i_epoch 0 batch 0: avg loss -1.953908 avg loss no lamb -1.953908 time 2019-03-04 16:35:00.987479
Model ind 640 epoch 1244 head B head_i_epoch 0 batch 100: avg loss -1.939744 avg loss no lamb -1.939744 time 2019-03-04 16:36:50.711976
Model ind 640 epoch 1244 head B head_i_epoch 0 batch 200: avg loss -2.015627 avg loss no lamb -2.015627 time 2019-03-04 16:38:40.387189
last batch sz 160
Model ind 640 epoch 1244 head B head_i_epoch 1 batch 0: avg loss -1.949655 avg loss no lamb -1.949655 time 2019-03-04 16:40:00.835970
Model ind 640 epoch 1244 head B head_i_epoch 1 batch 100: avg loss -1.883003 avg loss no lamb -1.883003 time 2019-03-04 16:41:52.940575
Model ind 640 epoch 1244 head B head_i_epoch 1 batch 200: avg loss -1.999397 avg loss no lamb -1.999397 time 2019-03-04 16:43:42.630986
last batch sz 160
Pre: time 2019-03-04 16:45:25.217084: 
 	std: 0.0509249
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51031667, 0.6142833, 0.6143, 0.6142167, 0.51031667]
	train_accs: [0.51031667, 0.6142833, 0.6143, 0.6142167, 0.51031667]
	best_train_sub_head: 2
	worst: 0.51031667
	avg: 0.5726866
	best: 0.6143

Starting e_i: 1245
Model ind 640 epoch 1245 head A head_i_epoch 0 batch 0: avg loss -3.343201 avg loss no lamb -3.343201 time 2019-03-04 16:45:27.780794
Model ind 640 epoch 1245 head A head_i_epoch 0 batch 100: avg loss -3.259699 avg loss no lamb -3.259699 time 2019-03-04 16:47:18.481322
Model ind 640 epoch 1245 head A head_i_epoch 0 batch 200: avg loss -3.338404 avg loss no lamb -3.338404 time 2019-03-04 16:49:08.778462
last batch sz 160
Model ind 640 epoch 1245 head B head_i_epoch 0 batch 0: avg loss -1.971247 avg loss no lamb -1.971247 time 2019-03-04 16:50:29.138090
Model ind 640 epoch 1245 head B head_i_epoch 0 batch 100: avg loss -1.928124 avg loss no lamb -1.928124 time 2019-03-04 16:52:19.820202
Model ind 640 epoch 1245 head B head_i_epoch 0 batch 200: avg loss -2.020313 avg loss no lamb -2.020313 time 2019-03-04 16:54:09.717396
last batch sz 160
Model ind 640 epoch 1245 head B head_i_epoch 1 batch 0: avg loss -2.060515 avg loss no lamb -2.060515 time 2019-03-04 16:55:28.394576
Model ind 640 epoch 1245 head B head_i_epoch 1 batch 100: avg loss -1.934976 avg loss no lamb -1.934976 time 2019-03-04 16:57:20.506092
Model ind 640 epoch 1245 head B head_i_epoch 1 batch 200: avg loss -1.921382 avg loss no lamb -1.921382 time 2019-03-04 16:59:11.560644
last batch sz 160
Pre: time 2019-03-04 17:00:51.214510: 
 	std: 0.05071545
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51093334, 0.61468333, 0.61455, 0.6146333, 0.51126665]
	train_accs: [0.51093334, 0.61468333, 0.61455, 0.6146333, 0.51126665]
	best_train_sub_head: 1
	worst: 0.51093334
	avg: 0.57321334
	best: 0.61468333

Starting e_i: 1246
Model ind 640 epoch 1246 head A head_i_epoch 0 batch 0: avg loss -3.300652 avg loss no lamb -3.300652 time 2019-03-04 17:00:54.331450
Model ind 640 epoch 1246 head A head_i_epoch 0 batch 100: avg loss -3.263713 avg loss no lamb -3.263713 time 2019-03-04 17:02:44.445528
Model ind 640 epoch 1246 head A head_i_epoch 0 batch 200: avg loss -3.322989 avg loss no lamb -3.322989 time 2019-03-04 17:04:35.658404
last batch sz 160
Model ind 640 epoch 1246 head B head_i_epoch 0 batch 0: avg loss -1.993181 avg loss no lamb -1.993181 time 2019-03-04 17:05:55.439847
Model ind 640 epoch 1246 head B head_i_epoch 0 batch 100: avg loss -1.896235 avg loss no lamb -1.896235 time 2019-03-04 17:07:44.553059
Model ind 640 epoch 1246 head B head_i_epoch 0 batch 200: avg loss -1.942877 avg loss no lamb -1.942877 time 2019-03-04 17:09:34.939415
last batch sz 160
Model ind 640 epoch 1246 head B head_i_epoch 1 batch 0: avg loss -1.954429 avg loss no lamb -1.954429 time 2019-03-04 17:10:55.308434
Model ind 640 epoch 1246 head B head_i_epoch 1 batch 100: avg loss -1.939216 avg loss no lamb -1.939216 time 2019-03-04 17:12:43.077574
Model ind 640 epoch 1246 head B head_i_epoch 1 batch 200: avg loss -2.006213 avg loss no lamb -2.006213 time 2019-03-04 17:14:35.614222
last batch sz 160
Pre: time 2019-03-04 17:16:17.176937: 
 	std: 0.05033709
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118167, 0.6145167, 0.61475, 0.6145833, 0.51191664]
	train_accs: [0.5118167, 0.6145167, 0.61475, 0.6145833, 0.51191664]
	best_train_sub_head: 2
	worst: 0.5118167
	avg: 0.57351667
	best: 0.61475

Starting e_i: 1247
Model ind 640 epoch 1247 head A head_i_epoch 0 batch 0: avg loss -3.328662 avg loss no lamb -3.328662 time 2019-03-04 17:16:19.815906
Model ind 640 epoch 1247 head A head_i_epoch 0 batch 100: avg loss -3.333308 avg loss no lamb -3.333308 time 2019-03-04 17:18:09.483316
Model ind 640 epoch 1247 head A head_i_epoch 0 batch 200: avg loss -3.315479 avg loss no lamb -3.315479 time 2019-03-04 17:19:58.295836
last batch sz 160
Model ind 640 epoch 1247 head B head_i_epoch 0 batch 0: avg loss -2.010226 avg loss no lamb -2.010226 time 2019-03-04 17:21:18.438559
Model ind 640 epoch 1247 head B head_i_epoch 0 batch 100: avg loss -1.893442 avg loss no lamb -1.893442 time 2019-03-04 17:23:10.291568
Model ind 640 epoch 1247 head B head_i_epoch 0 batch 200: avg loss -1.972455 avg loss no lamb -1.972455 time 2019-03-04 17:24:59.991913
last batch sz 160
Model ind 640 epoch 1247 head B head_i_epoch 1 batch 0: avg loss -1.947906 avg loss no lamb -1.947906 time 2019-03-04 17:26:19.741168
Model ind 640 epoch 1247 head B head_i_epoch 1 batch 100: avg loss -1.903532 avg loss no lamb -1.903532 time 2019-03-04 17:28:11.449419
Model ind 640 epoch 1247 head B head_i_epoch 1 batch 200: avg loss -1.976546 avg loss no lamb -1.976546 time 2019-03-04 17:30:04.060059
last batch sz 160
Pre: time 2019-03-04 17:31:45.069542: 
 	std: 0.050724883
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51195, 0.6154, 0.61536664, 0.6153833, 0.51173335]
	train_accs: [0.51195, 0.6154, 0.61536664, 0.6153833, 0.51173335]
	best_train_sub_head: 1
	worst: 0.51173335
	avg: 0.5739667
	best: 0.6154

Starting e_i: 1248
Model ind 640 epoch 1248 head A head_i_epoch 0 batch 0: avg loss -3.329921 avg loss no lamb -3.329921 time 2019-03-04 17:31:48.201783
Model ind 640 epoch 1248 head A head_i_epoch 0 batch 100: avg loss -3.243176 avg loss no lamb -3.243176 time 2019-03-04 17:33:38.518905
Model ind 640 epoch 1248 head A head_i_epoch 0 batch 200: avg loss -3.289721 avg loss no lamb -3.289721 time 2019-03-04 17:35:29.892752
last batch sz 160
Model ind 640 epoch 1248 head B head_i_epoch 0 batch 0: avg loss -1.984367 avg loss no lamb -1.984367 time 2019-03-04 17:36:51.092340
Model ind 640 epoch 1248 head B head_i_epoch 0 batch 100: avg loss -1.883166 avg loss no lamb -1.883166 time 2019-03-04 17:38:40.413913
Model ind 640 epoch 1248 head B head_i_epoch 0 batch 200: avg loss -1.985914 avg loss no lamb -1.985914 time 2019-03-04 17:40:30.606162
last batch sz 160
Model ind 640 epoch 1248 head B head_i_epoch 1 batch 0: avg loss -1.989138 avg loss no lamb -1.989138 time 2019-03-04 17:41:51.901260
Model ind 640 epoch 1248 head B head_i_epoch 1 batch 100: avg loss -1.970067 avg loss no lamb -1.970067 time 2019-03-04 17:43:40.604048
Model ind 640 epoch 1248 head B head_i_epoch 1 batch 200: avg loss -1.982390 avg loss no lamb -1.982390 time 2019-03-04 17:45:29.828223
last batch sz 160
Pre: time 2019-03-04 17:47:14.223363: 
 	std: 0.051225647
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5101, 0.61468333, 0.6146333, 0.61475, 0.51015]
	train_accs: [0.5101, 0.61468333, 0.6146333, 0.61475, 0.51015]
	best_train_sub_head: 3
	worst: 0.5101
	avg: 0.5728633
	best: 0.61475

Starting e_i: 1249
Model ind 640 epoch 1249 head A head_i_epoch 0 batch 0: avg loss -3.232197 avg loss no lamb -3.232197 time 2019-03-04 17:47:16.850059
Model ind 640 epoch 1249 head A head_i_epoch 0 batch 100: avg loss -3.261934 avg loss no lamb -3.261934 time 2019-03-04 17:49:08.023503
Model ind 640 epoch 1249 head A head_i_epoch 0 batch 200: avg loss -3.349543 avg loss no lamb -3.349543 time 2019-03-04 17:50:57.637896
last batch sz 160
Model ind 640 epoch 1249 head B head_i_epoch 0 batch 0: avg loss -2.001278 avg loss no lamb -2.001278 time 2019-03-04 17:52:17.604137
Model ind 640 epoch 1249 head B head_i_epoch 0 batch 100: avg loss -1.956914 avg loss no lamb -1.956914 time 2019-03-04 17:54:09.435286
Model ind 640 epoch 1249 head B head_i_epoch 0 batch 200: avg loss -1.958660 avg loss no lamb -1.958660 time 2019-03-04 17:55:58.635619
last batch sz 160
Model ind 640 epoch 1249 head B head_i_epoch 1 batch 0: avg loss -1.907831 avg loss no lamb -1.907831 time 2019-03-04 17:57:17.894947
Model ind 640 epoch 1249 head B head_i_epoch 1 batch 100: avg loss -1.906531 avg loss no lamb -1.906531 time 2019-03-04 17:59:08.651823
Model ind 640 epoch 1249 head B head_i_epoch 1 batch 200: avg loss -1.884584 avg loss no lamb -1.884584 time 2019-03-04 18:00:57.821462
last batch sz 160
Pre: time 2019-03-04 18:02:39.156362: 
 	std: 0.05071124
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51051664, 0.61401665, 0.61398333, 0.61401665, 0.5104667]
	train_accs: [0.51051664, 0.61401665, 0.61398333, 0.61401665, 0.5104667]
	best_train_sub_head: 1
	worst: 0.5104667
	avg: 0.5726
	best: 0.61401665

Starting e_i: 1250
Model ind 640 epoch 1250 head A head_i_epoch 0 batch 0: avg loss -3.291584 avg loss no lamb -3.291584 time 2019-03-04 18:02:43.189225
Model ind 640 epoch 1250 head A head_i_epoch 0 batch 100: avg loss -3.192739 avg loss no lamb -3.192739 time 2019-03-04 18:04:33.214875
Model ind 640 epoch 1250 head A head_i_epoch 0 batch 200: avg loss -3.417451 avg loss no lamb -3.417451 time 2019-03-04 18:06:24.041896
last batch sz 160
Model ind 640 epoch 1250 head B head_i_epoch 0 batch 0: avg loss -1.900386 avg loss no lamb -1.900386 time 2019-03-04 18:07:44.716028
Model ind 640 epoch 1250 head B head_i_epoch 0 batch 100: avg loss -1.851483 avg loss no lamb -1.851483 time 2019-03-04 18:09:34.148475
Model ind 640 epoch 1250 head B head_i_epoch 0 batch 200: avg loss -1.950714 avg loss no lamb -1.950714 time 2019-03-04 18:11:24.839694
last batch sz 160
Model ind 640 epoch 1250 head B head_i_epoch 1 batch 0: avg loss -1.938127 avg loss no lamb -1.938127 time 2019-03-04 18:12:46.479631
Model ind 640 epoch 1250 head B head_i_epoch 1 batch 100: avg loss -1.901558 avg loss no lamb -1.901558 time 2019-03-04 18:14:37.189936
Model ind 640 epoch 1250 head B head_i_epoch 1 batch 200: avg loss -1.996419 avg loss no lamb -1.996419 time 2019-03-04 18:16:24.991736
last batch sz 160
Pre: time 2019-03-04 18:18:08.015414: 
 	std: 0.05062016
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51133335, 0.61448336, 0.61446667, 0.6145833, 0.51103336]
	train_accs: [0.51133335, 0.61448336, 0.61446667, 0.6145833, 0.51103336]
	best_train_sub_head: 3
	worst: 0.51103336
	avg: 0.57318
	best: 0.6145833

Starting e_i: 1251
Model ind 640 epoch 1251 head A head_i_epoch 0 batch 0: avg loss -3.205364 avg loss no lamb -3.205364 time 2019-03-04 18:18:19.148418
Model ind 640 epoch 1251 head A head_i_epoch 0 batch 100: avg loss -3.179807 avg loss no lamb -3.179807 time 2019-03-04 18:20:10.826283
Model ind 640 epoch 1251 head A head_i_epoch 0 batch 200: avg loss -3.285281 avg loss no lamb -3.285281 time 2019-03-04 18:22:00.747913
last batch sz 160
Model ind 640 epoch 1251 head B head_i_epoch 0 batch 0: avg loss -1.922918 avg loss no lamb -1.922918 time 2019-03-04 18:23:20.911629
Model ind 640 epoch 1251 head B head_i_epoch 0 batch 100: avg loss -1.943533 avg loss no lamb -1.943533 time 2019-03-04 18:25:12.661867
Model ind 640 epoch 1251 head B head_i_epoch 0 batch 200: avg loss -2.019302 avg loss no lamb -2.019302 time 2019-03-04 18:27:03.367269
last batch sz 160
Model ind 640 epoch 1251 head B head_i_epoch 1 batch 0: avg loss -1.964567 avg loss no lamb -1.964567 time 2019-03-04 18:28:21.997716
Model ind 640 epoch 1251 head B head_i_epoch 1 batch 100: avg loss -1.973558 avg loss no lamb -1.973558 time 2019-03-04 18:30:12.547236
Model ind 640 epoch 1251 head B head_i_epoch 1 batch 200: avg loss -2.022223 avg loss no lamb -2.022223 time 2019-03-04 18:32:04.389493
last batch sz 160
Pre: time 2019-03-04 18:33:45.100723: 
 	std: 0.050466616
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51145, 0.6142833, 0.61413336, 0.61415, 0.5109]
	train_accs: [0.51145, 0.6142833, 0.61413336, 0.61415, 0.5109]
	best_train_sub_head: 1
	worst: 0.5109
	avg: 0.5729834
	best: 0.6142833

Starting e_i: 1252
Model ind 640 epoch 1252 head A head_i_epoch 0 batch 0: avg loss -3.279976 avg loss no lamb -3.279976 time 2019-03-04 18:33:48.213685
Model ind 640 epoch 1252 head A head_i_epoch 0 batch 100: avg loss -3.163330 avg loss no lamb -3.163330 time 2019-03-04 18:35:38.689060
Model ind 640 epoch 1252 head A head_i_epoch 0 batch 200: avg loss -3.287621 avg loss no lamb -3.287621 time 2019-03-04 18:37:30.212112
last batch sz 160
Model ind 640 epoch 1252 head B head_i_epoch 0 batch 0: avg loss -1.979513 avg loss no lamb -1.979513 time 2019-03-04 18:38:51.796899
Model ind 640 epoch 1252 head B head_i_epoch 0 batch 100: avg loss -1.916206 avg loss no lamb -1.916206 time 2019-03-04 18:40:40.871887
Model ind 640 epoch 1252 head B head_i_epoch 0 batch 200: avg loss -1.977872 avg loss no lamb -1.977872 time 2019-03-04 18:42:29.951463
last batch sz 160
Model ind 640 epoch 1252 head B head_i_epoch 1 batch 0: avg loss -1.956575 avg loss no lamb -1.956575 time 2019-03-04 18:43:51.381703
Model ind 640 epoch 1252 head B head_i_epoch 1 batch 100: avg loss -1.914909 avg loss no lamb -1.914909 time 2019-03-04 18:45:40.358834
Model ind 640 epoch 1252 head B head_i_epoch 1 batch 200: avg loss -1.997399 avg loss no lamb -1.997399 time 2019-03-04 18:47:29.172783
last batch sz 160
Pre: time 2019-03-04 18:49:10.307389: 
 	std: 0.05050033
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51185, 0.6149333, 0.6149667, 0.615, 0.51191664]
	train_accs: [0.51185, 0.6149333, 0.6149667, 0.615, 0.51191664]
	best_train_sub_head: 3
	worst: 0.51185
	avg: 0.5737333
	best: 0.615

Starting e_i: 1253
Model ind 640 epoch 1253 head A head_i_epoch 0 batch 0: avg loss -3.297929 avg loss no lamb -3.297929 time 2019-03-04 18:49:12.864313
Model ind 640 epoch 1253 head A head_i_epoch 0 batch 100: avg loss -3.216065 avg loss no lamb -3.216065 time 2019-03-04 18:51:08.237531
Model ind 640 epoch 1253 head A head_i_epoch 0 batch 200: avg loss -3.203111 avg loss no lamb -3.203111 time 2019-03-04 18:52:58.404500
last batch sz 160
Model ind 640 epoch 1253 head B head_i_epoch 0 batch 0: avg loss -2.005071 avg loss no lamb -2.005071 time 2019-03-04 18:54:18.207936
Model ind 640 epoch 1253 head B head_i_epoch 0 batch 100: avg loss -1.949723 avg loss no lamb -1.949723 time 2019-03-04 18:56:09.932018
Model ind 640 epoch 1253 head B head_i_epoch 0 batch 200: avg loss -1.983773 avg loss no lamb -1.983773 time 2019-03-04 18:58:00.999830
last batch sz 160
Model ind 640 epoch 1253 head B head_i_epoch 1 batch 0: avg loss -1.971497 avg loss no lamb -1.971497 time 2019-03-04 18:59:19.529167
Model ind 640 epoch 1253 head B head_i_epoch 1 batch 100: avg loss -1.987830 avg loss no lamb -1.987830 time 2019-03-04 19:01:11.006440
Model ind 640 epoch 1253 head B head_i_epoch 1 batch 200: avg loss -1.932110 avg loss no lamb -1.932110 time 2019-03-04 19:03:01.820133
last batch sz 160
Pre: time 2019-03-04 19:04:41.619048: 
 	std: 0.051108602
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5098, 0.6141, 0.61411667, 0.61413336, 0.5097833]
	train_accs: [0.5098, 0.6141, 0.61411667, 0.61413336, 0.5097833]
	best_train_sub_head: 3
	worst: 0.5097833
	avg: 0.5723866
	best: 0.61413336

Starting e_i: 1254
Model ind 640 epoch 1254 head A head_i_epoch 0 batch 0: avg loss -3.281838 avg loss no lamb -3.281838 time 2019-03-04 19:04:44.569243
Model ind 640 epoch 1254 head A head_i_epoch 0 batch 100: avg loss -3.288321 avg loss no lamb -3.288321 time 2019-03-04 19:06:33.675236
Model ind 640 epoch 1254 head A head_i_epoch 0 batch 200: avg loss -3.274062 avg loss no lamb -3.274062 time 2019-03-04 19:08:25.562756
last batch sz 160
Model ind 640 epoch 1254 head B head_i_epoch 0 batch 0: avg loss -1.989840 avg loss no lamb -1.989840 time 2019-03-04 19:09:46.586858
Model ind 640 epoch 1254 head B head_i_epoch 0 batch 100: avg loss -1.964777 avg loss no lamb -1.964777 time 2019-03-04 19:11:36.333010
Model ind 640 epoch 1254 head B head_i_epoch 0 batch 200: avg loss -2.000365 avg loss no lamb -2.000365 time 2019-03-04 19:13:27.453665
last batch sz 160
Model ind 640 epoch 1254 head B head_i_epoch 1 batch 0: avg loss -2.007484 avg loss no lamb -2.007484 time 2019-03-04 19:14:49.315719
Model ind 640 epoch 1254 head B head_i_epoch 1 batch 100: avg loss -2.004782 avg loss no lamb -2.004782 time 2019-03-04 19:16:38.580988
Model ind 640 epoch 1254 head B head_i_epoch 1 batch 200: avg loss -1.962923 avg loss no lamb -1.962923 time 2019-03-04 19:18:28.268736
last batch sz 160
Pre: time 2019-03-04 19:20:09.367952: 
 	std: 0.050801083
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51105, 0.6148833, 0.61481667, 0.61476666, 0.5112]
	train_accs: [0.51105, 0.6148833, 0.61481667, 0.61476666, 0.5112]
	best_train_sub_head: 1
	worst: 0.51105
	avg: 0.57334334
	best: 0.6148833

Starting e_i: 1255
Model ind 640 epoch 1255 head A head_i_epoch 0 batch 0: avg loss -3.242378 avg loss no lamb -3.242378 time 2019-03-04 19:20:11.996827
Model ind 640 epoch 1255 head A head_i_epoch 0 batch 100: avg loss -3.214854 avg loss no lamb -3.214854 time 2019-03-04 19:22:03.441727
Model ind 640 epoch 1255 head A head_i_epoch 0 batch 200: avg loss -3.307179 avg loss no lamb -3.307179 time 2019-03-04 19:23:53.246309
last batch sz 160
Model ind 640 epoch 1255 head B head_i_epoch 0 batch 0: avg loss -1.940611 avg loss no lamb -1.940611 time 2019-03-04 19:25:12.433928
Model ind 640 epoch 1255 head B head_i_epoch 0 batch 100: avg loss -1.947310 avg loss no lamb -1.947310 time 2019-03-04 19:27:03.489451
Model ind 640 epoch 1255 head B head_i_epoch 0 batch 200: avg loss -2.005627 avg loss no lamb -2.005627 time 2019-03-04 19:28:54.050479
last batch sz 160
Model ind 640 epoch 1255 head B head_i_epoch 1 batch 0: avg loss -1.935710 avg loss no lamb -1.935710 time 2019-03-04 19:30:12.367471
Model ind 640 epoch 1255 head B head_i_epoch 1 batch 100: avg loss -1.836546 avg loss no lamb -1.836546 time 2019-03-04 19:32:02.672301
Model ind 640 epoch 1255 head B head_i_epoch 1 batch 200: avg loss -2.025326 avg loss no lamb -2.025326 time 2019-03-04 19:33:54.525805
last batch sz 160
Pre: time 2019-03-04 19:35:35.702228: 
 	std: 0.05037786
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51246667, 0.61523336, 0.61525, 0.6152667, 0.51236665]
	train_accs: [0.51246667, 0.61523336, 0.61525, 0.6152667, 0.51236665]
	best_train_sub_head: 3
	worst: 0.51236665
	avg: 0.5741167
	best: 0.6152667

Starting e_i: 1256
Model ind 640 epoch 1256 head A head_i_epoch 0 batch 0: avg loss -3.251621 avg loss no lamb -3.251621 time 2019-03-04 19:35:38.807158
Model ind 640 epoch 1256 head A head_i_epoch 0 batch 100: avg loss -3.242218 avg loss no lamb -3.242218 time 2019-03-04 19:37:26.850283
Model ind 640 epoch 1256 head A head_i_epoch 0 batch 200: avg loss -3.293541 avg loss no lamb -3.293541 time 2019-03-04 19:39:19.392052
last batch sz 160
Model ind 640 epoch 1256 head B head_i_epoch 0 batch 0: avg loss -1.953851 avg loss no lamb -1.953851 time 2019-03-04 19:40:41.269652
Model ind 640 epoch 1256 head B head_i_epoch 0 batch 100: avg loss -1.882886 avg loss no lamb -1.882886 time 2019-03-04 19:42:30.426152
Model ind 640 epoch 1256 head B head_i_epoch 0 batch 200: avg loss -1.995340 avg loss no lamb -1.995340 time 2019-03-04 19:44:20.431619
last batch sz 160
Model ind 640 epoch 1256 head B head_i_epoch 1 batch 0: avg loss -1.932312 avg loss no lamb -1.932312 time 2019-03-04 19:45:42.192157
Model ind 640 epoch 1256 head B head_i_epoch 1 batch 100: avg loss -1.881482 avg loss no lamb -1.881482 time 2019-03-04 19:47:32.053183
Model ind 640 epoch 1256 head B head_i_epoch 1 batch 200: avg loss -2.033241 avg loss no lamb -2.033241 time 2019-03-04 19:49:21.422963
last batch sz 160
Pre: time 2019-03-04 19:51:02.269224: 
 	std: 0.050628312
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111667, 0.6143, 0.6145, 0.61448336, 0.511]
	train_accs: [0.5111667, 0.6143, 0.6145, 0.61448336, 0.511]
	best_train_sub_head: 2
	worst: 0.511
	avg: 0.57308996
	best: 0.6145

Starting e_i: 1257
Model ind 640 epoch 1257 head A head_i_epoch 0 batch 0: avg loss -3.382533 avg loss no lamb -3.382533 time 2019-03-04 19:51:05.340387
Model ind 640 epoch 1257 head A head_i_epoch 0 batch 100: avg loss -3.207382 avg loss no lamb -3.207382 time 2019-03-04 19:52:56.673304
Model ind 640 epoch 1257 head A head_i_epoch 0 batch 200: avg loss -3.316581 avg loss no lamb -3.316581 time 2019-03-04 19:54:46.012845
last batch sz 160
Model ind 640 epoch 1257 head B head_i_epoch 0 batch 0: avg loss -1.949726 avg loss no lamb -1.949726 time 2019-03-04 19:56:07.336329
Model ind 640 epoch 1257 head B head_i_epoch 0 batch 100: avg loss -1.961681 avg loss no lamb -1.961681 time 2019-03-04 19:57:58.640009
Model ind 640 epoch 1257 head B head_i_epoch 0 batch 200: avg loss -1.934147 avg loss no lamb -1.934147 time 2019-03-04 19:59:49.152738
last batch sz 160
Model ind 640 epoch 1257 head B head_i_epoch 1 batch 0: avg loss -1.945280 avg loss no lamb -1.945280 time 2019-03-04 20:01:07.675387
Model ind 640 epoch 1257 head B head_i_epoch 1 batch 100: avg loss -1.923546 avg loss no lamb -1.923546 time 2019-03-04 20:02:58.470987
Model ind 640 epoch 1257 head B head_i_epoch 1 batch 200: avg loss -1.971543 avg loss no lamb -1.971543 time 2019-03-04 20:04:50.323148
last batch sz 160
Pre: time 2019-03-04 20:06:31.217871: 
 	std: 0.0507412
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115833, 0.61506665, 0.6151, 0.61513335, 0.5114667]
	train_accs: [0.5115833, 0.61506665, 0.6151, 0.61513335, 0.5114667]
	best_train_sub_head: 3
	worst: 0.5114667
	avg: 0.57367
	best: 0.61513335

Starting e_i: 1258
Model ind 640 epoch 1258 head A head_i_epoch 0 batch 0: avg loss -3.289907 avg loss no lamb -3.289907 time 2019-03-04 20:06:35.417023
Model ind 640 epoch 1258 head A head_i_epoch 0 batch 100: avg loss -3.250398 avg loss no lamb -3.250398 time 2019-03-04 20:08:24.718166
Model ind 640 epoch 1258 head A head_i_epoch 0 batch 200: avg loss -3.386981 avg loss no lamb -3.386981 time 2019-03-04 20:10:15.264453
last batch sz 160
Model ind 640 epoch 1258 head B head_i_epoch 0 batch 0: avg loss -2.018338 avg loss no lamb -2.018338 time 2019-03-04 20:11:38.435783
Model ind 640 epoch 1258 head B head_i_epoch 0 batch 100: avg loss -1.933702 avg loss no lamb -1.933702 time 2019-03-04 20:13:28.481901
Model ind 640 epoch 1258 head B head_i_epoch 0 batch 200: avg loss -1.945187 avg loss no lamb -1.945187 time 2019-03-04 20:15:18.181038
last batch sz 160
Model ind 640 epoch 1258 head B head_i_epoch 1 batch 0: avg loss -1.922634 avg loss no lamb -1.922634 time 2019-03-04 20:16:40.391448
Model ind 640 epoch 1258 head B head_i_epoch 1 batch 100: avg loss -1.866751 avg loss no lamb -1.866751 time 2019-03-04 20:18:29.890758
Model ind 640 epoch 1258 head B head_i_epoch 1 batch 200: avg loss -1.974450 avg loss no lamb -1.974450 time 2019-03-04 20:20:19.294158
last batch sz 160
Pre: time 2019-03-04 20:22:00.088367: 
 	std: 0.0508392
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114167, 0.61511666, 0.61506665, 0.6152667, 0.51133335]
	train_accs: [0.5114167, 0.61511666, 0.61506665, 0.6152667, 0.51133335]
	best_train_sub_head: 3
	worst: 0.51133335
	avg: 0.57364
	best: 0.6152667

Starting e_i: 1259
Model ind 640 epoch 1259 head A head_i_epoch 0 batch 0: avg loss -3.306465 avg loss no lamb -3.306465 time 2019-03-04 20:22:03.576080
Model ind 640 epoch 1259 head A head_i_epoch 0 batch 100: avg loss -3.215916 avg loss no lamb -3.215916 time 2019-03-04 20:23:55.121654
Model ind 640 epoch 1259 head A head_i_epoch 0 batch 200: avg loss -3.290427 avg loss no lamb -3.290427 time 2019-03-04 20:25:43.871066
last batch sz 160
Model ind 640 epoch 1259 head B head_i_epoch 0 batch 0: avg loss -1.973651 avg loss no lamb -1.973651 time 2019-03-04 20:27:04.153717
Model ind 640 epoch 1259 head B head_i_epoch 0 batch 100: avg loss -1.910825 avg loss no lamb -1.910825 time 2019-03-04 20:28:55.795173
Model ind 640 epoch 1259 head B head_i_epoch 0 batch 200: avg loss -2.024944 avg loss no lamb -2.024944 time 2019-03-04 20:30:47.573576
last batch sz 160
Model ind 640 epoch 1259 head B head_i_epoch 1 batch 0: avg loss -1.943491 avg loss no lamb -1.943491 time 2019-03-04 20:32:06.780363
Model ind 640 epoch 1259 head B head_i_epoch 1 batch 100: avg loss -1.969613 avg loss no lamb -1.969613 time 2019-03-04 20:33:56.876955
Model ind 640 epoch 1259 head B head_i_epoch 1 batch 200: avg loss -1.995744 avg loss no lamb -1.995744 time 2019-03-04 20:35:47.854581
last batch sz 160
Pre: time 2019-03-04 20:37:28.955278: 
 	std: 0.050672036
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51163334, 0.61478335, 0.6148667, 0.6148, 0.5111333]
	train_accs: [0.51163334, 0.61478335, 0.6148667, 0.6148, 0.5111333]
	best_train_sub_head: 2
	worst: 0.5111333
	avg: 0.5734433
	best: 0.6148667

Starting e_i: 1260
Model ind 640 epoch 1260 head A head_i_epoch 0 batch 0: avg loss -3.299904 avg loss no lamb -3.299904 time 2019-03-04 20:37:31.996749
Model ind 640 epoch 1260 head A head_i_epoch 0 batch 100: avg loss -3.396980 avg loss no lamb -3.396980 time 2019-03-04 20:39:21.488629
Model ind 640 epoch 1260 head A head_i_epoch 0 batch 200: avg loss -3.334851 avg loss no lamb -3.334851 time 2019-03-04 20:41:10.357562
last batch sz 160
Model ind 640 epoch 1260 head B head_i_epoch 0 batch 0: avg loss -1.988130 avg loss no lamb -1.988130 time 2019-03-04 20:42:32.612677
Model ind 640 epoch 1260 head B head_i_epoch 0 batch 100: avg loss -1.891913 avg loss no lamb -1.891913 time 2019-03-04 20:44:22.520050
Model ind 640 epoch 1260 head B head_i_epoch 0 batch 200: avg loss -1.940096 avg loss no lamb -1.940096 time 2019-03-04 20:46:11.356207
last batch sz 160
Model ind 640 epoch 1260 head B head_i_epoch 1 batch 0: avg loss -1.965126 avg loss no lamb -1.965126 time 2019-03-04 20:47:32.587920
Model ind 640 epoch 1260 head B head_i_epoch 1 batch 100: avg loss -1.881576 avg loss no lamb -1.881576 time 2019-03-04 20:49:22.546737
Model ind 640 epoch 1260 head B head_i_epoch 1 batch 200: avg loss -2.114377 avg loss no lamb -2.114377 time 2019-03-04 20:51:11.836997
last batch sz 160
Pre: time 2019-03-04 20:52:53.392843: 
 	std: 0.050669275
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51198334, 0.61505, 0.6153167, 0.61536664, 0.51165]
	train_accs: [0.51198334, 0.61505, 0.6153167, 0.61536664, 0.51165]
	best_train_sub_head: 3
	worst: 0.51165
	avg: 0.5738734
	best: 0.61536664

Starting e_i: 1261
Model ind 640 epoch 1261 head A head_i_epoch 0 batch 0: avg loss -3.354516 avg loss no lamb -3.354516 time 2019-03-04 20:53:00.799432
Model ind 640 epoch 1261 head A head_i_epoch 0 batch 100: avg loss -3.244235 avg loss no lamb -3.244235 time 2019-03-04 20:54:52.525661
Model ind 640 epoch 1261 head A head_i_epoch 0 batch 200: avg loss -3.250353 avg loss no lamb -3.250353 time 2019-03-04 20:56:43.700590
last batch sz 160
Model ind 640 epoch 1261 head B head_i_epoch 0 batch 0: avg loss -1.865867 avg loss no lamb -1.865867 time 2019-03-04 20:58:02.053903
Model ind 640 epoch 1261 head B head_i_epoch 0 batch 100: avg loss -1.846780 avg loss no lamb -1.846780 time 2019-03-04 20:59:55.289621
Model ind 640 epoch 1261 head B head_i_epoch 0 batch 200: avg loss -2.033538 avg loss no lamb -2.033538 time 2019-03-04 21:01:47.025896
last batch sz 160
Model ind 640 epoch 1261 head B head_i_epoch 1 batch 0: avg loss -1.954107 avg loss no lamb -1.954107 time 2019-03-04 21:03:07.205922
Model ind 640 epoch 1261 head B head_i_epoch 1 batch 100: avg loss -1.904832 avg loss no lamb -1.904832 time 2019-03-04 21:04:57.298440
Model ind 640 epoch 1261 head B head_i_epoch 1 batch 200: avg loss -1.952713 avg loss no lamb -1.952713 time 2019-03-04 21:06:48.308537
last batch sz 160
Pre: time 2019-03-04 21:08:31.808247: 
 	std: 0.050492276
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5122667, 0.6151, 0.61518335, 0.6152167, 0.5119333]
	train_accs: [0.5122667, 0.6151, 0.61518335, 0.6152167, 0.5119333]
	best_train_sub_head: 3
	worst: 0.5119333
	avg: 0.57394004
	best: 0.6152167

Starting e_i: 1262
Model ind 640 epoch 1262 head A head_i_epoch 0 batch 0: avg loss -3.328608 avg loss no lamb -3.328608 time 2019-03-04 21:08:34.989837
Model ind 640 epoch 1262 head A head_i_epoch 0 batch 100: avg loss -3.206795 avg loss no lamb -3.206795 time 2019-03-04 21:10:24.976048
Model ind 640 epoch 1262 head A head_i_epoch 0 batch 200: avg loss -3.340764 avg loss no lamb -3.340764 time 2019-03-04 21:12:14.118440
last batch sz 160
Model ind 640 epoch 1262 head B head_i_epoch 0 batch 0: avg loss -1.959254 avg loss no lamb -1.959254 time 2019-03-04 21:13:35.424198
Model ind 640 epoch 1262 head B head_i_epoch 0 batch 100: avg loss -1.916268 avg loss no lamb -1.916268 time 2019-03-04 21:15:27.746790
Model ind 640 epoch 1262 head B head_i_epoch 0 batch 200: avg loss -1.963183 avg loss no lamb -1.963183 time 2019-03-04 21:17:17.510135
last batch sz 160
Model ind 640 epoch 1262 head B head_i_epoch 1 batch 0: avg loss -1.912032 avg loss no lamb -1.912032 time 2019-03-04 21:18:37.367215
Model ind 640 epoch 1262 head B head_i_epoch 1 batch 100: avg loss -1.934323 avg loss no lamb -1.934323 time 2019-03-04 21:20:28.830926
Model ind 640 epoch 1262 head B head_i_epoch 1 batch 200: avg loss -1.973678 avg loss no lamb -1.973678 time 2019-03-04 21:22:18.358844
last batch sz 160
Pre: time 2019-03-04 21:23:58.143762: 
 	std: 0.050769888
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51085, 0.6142833, 0.61433333, 0.6142833, 0.5104833]
	train_accs: [0.51085, 0.6142833, 0.61433333, 0.6142833, 0.5104833]
	best_train_sub_head: 2
	worst: 0.5104833
	avg: 0.57284665
	best: 0.61433333

Starting e_i: 1263
Model ind 640 epoch 1263 head A head_i_epoch 0 batch 0: avg loss -3.314873 avg loss no lamb -3.314873 time 2019-03-04 21:24:01.449417
Model ind 640 epoch 1263 head A head_i_epoch 0 batch 100: avg loss -3.219174 avg loss no lamb -3.219174 time 2019-03-04 21:25:52.205142
Model ind 640 epoch 1263 head A head_i_epoch 0 batch 200: avg loss -3.279619 avg loss no lamb -3.279619 time 2019-03-04 21:27:42.971741
last batch sz 160
Model ind 640 epoch 1263 head B head_i_epoch 0 batch 0: avg loss -1.945391 avg loss no lamb -1.945391 time 2019-03-04 21:29:02.435207
Model ind 640 epoch 1263 head B head_i_epoch 0 batch 100: avg loss -1.914973 avg loss no lamb -1.914973 time 2019-03-04 21:30:52.185030
Model ind 640 epoch 1263 head B head_i_epoch 0 batch 200: avg loss -2.001588 avg loss no lamb -2.001588 time 2019-03-04 21:32:44.712775
last batch sz 160
Model ind 640 epoch 1263 head B head_i_epoch 1 batch 0: avg loss -1.987276 avg loss no lamb -1.987276 time 2019-03-04 21:34:05.450580
Model ind 640 epoch 1263 head B head_i_epoch 1 batch 100: avg loss -1.916684 avg loss no lamb -1.916684 time 2019-03-04 21:35:55.586735
Model ind 640 epoch 1263 head B head_i_epoch 1 batch 200: avg loss -1.959068 avg loss no lamb -1.959068 time 2019-03-04 21:37:45.557535
last batch sz 160
Pre: time 2019-03-04 21:39:28.714507: 
 	std: 0.05073032
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115, 0.6149667, 0.61508334, 0.61513335, 0.5115167]
	train_accs: [0.5115, 0.6149667, 0.61508334, 0.61513335, 0.5115167]
	best_train_sub_head: 3
	worst: 0.5115
	avg: 0.57364005
	best: 0.61513335

Starting e_i: 1264
Model ind 640 epoch 1264 head A head_i_epoch 0 batch 0: avg loss -3.312538 avg loss no lamb -3.312538 time 2019-03-04 21:39:31.672829
Model ind 640 epoch 1264 head A head_i_epoch 0 batch 100: avg loss -3.241976 avg loss no lamb -3.241976 time 2019-03-04 21:41:22.360266
Model ind 640 epoch 1264 head A head_i_epoch 0 batch 200: avg loss -3.315488 avg loss no lamb -3.315488 time 2019-03-04 21:43:12.758993
last batch sz 160
Model ind 640 epoch 1264 head B head_i_epoch 0 batch 0: avg loss -2.025857 avg loss no lamb -2.025857 time 2019-03-04 21:44:33.971432
Model ind 640 epoch 1264 head B head_i_epoch 0 batch 100: avg loss -2.055643 avg loss no lamb -2.055643 time 2019-03-04 21:46:24.184651
Model ind 640 epoch 1264 head B head_i_epoch 0 batch 200: avg loss -2.040592 avg loss no lamb -2.040592 time 2019-03-04 21:48:15.441089
last batch sz 160
Model ind 640 epoch 1264 head B head_i_epoch 1 batch 0: avg loss -1.993460 avg loss no lamb -1.993460 time 2019-03-04 21:49:34.751870
Model ind 640 epoch 1264 head B head_i_epoch 1 batch 100: avg loss -1.981143 avg loss no lamb -1.981143 time 2019-03-04 21:51:26.219271
Model ind 640 epoch 1264 head B head_i_epoch 1 batch 200: avg loss -2.010751 avg loss no lamb -2.010751 time 2019-03-04 21:53:16.210597
last batch sz 160
Pre: time 2019-03-04 21:54:55.886539: 
 	std: 0.0499016
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5136667, 0.6154, 0.6154, 0.61548334, 0.51346666]
	train_accs: [0.5136667, 0.6154, 0.6154, 0.61548334, 0.51346666]
	best_train_sub_head: 3
	worst: 0.51346666
	avg: 0.5746833
	best: 0.61548334

Starting e_i: 1265
Model ind 640 epoch 1265 head A head_i_epoch 0 batch 0: avg loss -3.333898 avg loss no lamb -3.333898 time 2019-03-04 21:54:59.019033
Model ind 640 epoch 1265 head A head_i_epoch 0 batch 100: avg loss -3.210864 avg loss no lamb -3.210864 time 2019-03-04 21:56:50.391516
Model ind 640 epoch 1265 head A head_i_epoch 0 batch 200: avg loss -3.324657 avg loss no lamb -3.324657 time 2019-03-04 21:58:41.665707
last batch sz 160
Model ind 640 epoch 1265 head B head_i_epoch 0 batch 0: avg loss -1.951225 avg loss no lamb -1.951225 time 2019-03-04 22:00:02.498814
Model ind 640 epoch 1265 head B head_i_epoch 0 batch 100: avg loss -1.908295 avg loss no lamb -1.908295 time 2019-03-04 22:01:50.251158
Model ind 640 epoch 1265 head B head_i_epoch 0 batch 200: avg loss -1.969129 avg loss no lamb -1.969129 time 2019-03-04 22:03:41.919739
last batch sz 160
Model ind 640 epoch 1265 head B head_i_epoch 1 batch 0: avg loss -1.968305 avg loss no lamb -1.968305 time 2019-03-04 22:05:03.039065
Model ind 640 epoch 1265 head B head_i_epoch 1 batch 100: avg loss -1.946408 avg loss no lamb -1.946408 time 2019-03-04 22:06:52.945714
Model ind 640 epoch 1265 head B head_i_epoch 1 batch 200: avg loss -1.992821 avg loss no lamb -1.992821 time 2019-03-04 22:08:43.083983
last batch sz 160
Pre: time 2019-03-04 22:10:25.522608: 
 	std: 0.050508507
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51195, 0.61515, 0.6151, 0.61505, 0.51205]
	train_accs: [0.51195, 0.61515, 0.6151, 0.61505, 0.51205]
	best_train_sub_head: 1
	worst: 0.51195
	avg: 0.57386
	best: 0.61515

Starting e_i: 1266
Model ind 640 epoch 1266 head A head_i_epoch 0 batch 0: avg loss -3.292102 avg loss no lamb -3.292102 time 2019-03-04 22:10:28.224906
Model ind 640 epoch 1266 head A head_i_epoch 0 batch 100: avg loss -3.211528 avg loss no lamb -3.211528 time 2019-03-04 22:12:18.671418
Model ind 640 epoch 1266 head A head_i_epoch 0 batch 200: avg loss -3.289659 avg loss no lamb -3.289659 time 2019-03-04 22:14:07.835783
last batch sz 160
Model ind 640 epoch 1266 head B head_i_epoch 0 batch 0: avg loss -1.911036 avg loss no lamb -1.911036 time 2019-03-04 22:15:27.522966
Model ind 640 epoch 1266 head B head_i_epoch 0 batch 100: avg loss -1.933508 avg loss no lamb -1.933508 time 2019-03-04 22:17:18.276146
Model ind 640 epoch 1266 head B head_i_epoch 0 batch 200: avg loss -1.943557 avg loss no lamb -1.943557 time 2019-03-04 22:19:07.447330
last batch sz 160
Model ind 640 epoch 1266 head B head_i_epoch 1 batch 0: avg loss -1.979314 avg loss no lamb -1.979314 time 2019-03-04 22:20:26.588367
Model ind 640 epoch 1266 head B head_i_epoch 1 batch 100: avg loss -1.942945 avg loss no lamb -1.942945 time 2019-03-04 22:22:18.434160
Model ind 640 epoch 1266 head B head_i_epoch 1 batch 200: avg loss -1.997911 avg loss no lamb -1.997911 time 2019-03-04 22:24:09.522248
last batch sz 160
Pre: time 2019-03-04 22:25:50.245436: 
 	std: 0.050692394
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51088333, 0.61455, 0.6146167, 0.6145833, 0.51133335]
	train_accs: [0.51088333, 0.61455, 0.6146167, 0.6145833, 0.51133335]
	best_train_sub_head: 2
	worst: 0.51088333
	avg: 0.5731934
	best: 0.6146167

Starting e_i: 1267
Model ind 640 epoch 1267 head A head_i_epoch 0 batch 0: avg loss -3.228736 avg loss no lamb -3.228736 time 2019-03-04 22:25:53.724816
Model ind 640 epoch 1267 head A head_i_epoch 0 batch 100: avg loss -3.362581 avg loss no lamb -3.362581 time 2019-03-04 22:27:43.435164
Model ind 640 epoch 1267 head A head_i_epoch 0 batch 200: avg loss -3.334863 avg loss no lamb -3.334863 time 2019-03-04 22:29:34.137855
last batch sz 160
Model ind 640 epoch 1267 head B head_i_epoch 0 batch 0: avg loss -1.912685 avg loss no lamb -1.912685 time 2019-03-04 22:30:54.409205
Model ind 640 epoch 1267 head B head_i_epoch 0 batch 100: avg loss -1.916589 avg loss no lamb -1.916589 time 2019-03-04 22:32:44.230106
Model ind 640 epoch 1267 head B head_i_epoch 0 batch 200: avg loss -2.028303 avg loss no lamb -2.028303 time 2019-03-04 22:34:33.419363
last batch sz 160
Model ind 640 epoch 1267 head B head_i_epoch 1 batch 0: avg loss -1.902169 avg loss no lamb -1.902169 time 2019-03-04 22:35:57.136670
Model ind 640 epoch 1267 head B head_i_epoch 1 batch 100: avg loss -1.903061 avg loss no lamb -1.903061 time 2019-03-04 22:37:47.163277
Model ind 640 epoch 1267 head B head_i_epoch 1 batch 200: avg loss -2.005728 avg loss no lamb -2.005728 time 2019-03-04 22:39:36.619027
last batch sz 160
Pre: time 2019-03-04 22:41:19.323224: 
 	std: 0.050179258
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51165, 0.6141667, 0.6141667, 0.6143, 0.51191664]
	train_accs: [0.51165, 0.6141667, 0.6141667, 0.6143, 0.51191664]
	best_train_sub_head: 3
	worst: 0.51165
	avg: 0.57324
	best: 0.6143

Starting e_i: 1268
Model ind 640 epoch 1268 head A head_i_epoch 0 batch 0: avg loss -3.288310 avg loss no lamb -3.288310 time 2019-03-04 22:41:22.111736
Model ind 640 epoch 1268 head A head_i_epoch 0 batch 100: avg loss -3.238231 avg loss no lamb -3.238231 time 2019-03-04 22:43:14.147168
Model ind 640 epoch 1268 head A head_i_epoch 0 batch 200: avg loss -3.319813 avg loss no lamb -3.319813 time 2019-03-04 22:45:04.221299
last batch sz 160
Model ind 640 epoch 1268 head B head_i_epoch 0 batch 0: avg loss -1.994011 avg loss no lamb -1.994011 time 2019-03-04 22:46:23.822013
Model ind 640 epoch 1268 head B head_i_epoch 0 batch 100: avg loss -1.892860 avg loss no lamb -1.892860 time 2019-03-04 22:48:15.236353
Model ind 640 epoch 1268 head B head_i_epoch 0 batch 200: avg loss -2.022887 avg loss no lamb -2.022887 time 2019-03-04 22:50:05.828892
last batch sz 160
Model ind 640 epoch 1268 head B head_i_epoch 1 batch 0: avg loss -2.004322 avg loss no lamb -2.004322 time 2019-03-04 22:51:24.304903
Model ind 640 epoch 1268 head B head_i_epoch 1 batch 100: avg loss -1.831526 avg loss no lamb -1.831526 time 2019-03-04 22:53:17.531991
Model ind 640 epoch 1268 head B head_i_epoch 1 batch 200: avg loss -2.054539 avg loss no lamb -2.054539 time 2019-03-04 22:55:09.017887
last batch sz 160
Pre: time 2019-03-04 22:56:48.818496: 
 	std: 0.05057926
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51143336, 0.6148, 0.61468333, 0.6147, 0.5115333]
	train_accs: [0.51143336, 0.6148, 0.61468333, 0.6147, 0.5115333]
	best_train_sub_head: 1
	worst: 0.51143336
	avg: 0.57343
	best: 0.6148

Starting e_i: 1269
Model ind 640 epoch 1269 head A head_i_epoch 0 batch 0: avg loss -3.350883 avg loss no lamb -3.350883 time 2019-03-04 22:56:52.893664
Model ind 640 epoch 1269 head A head_i_epoch 0 batch 100: avg loss -3.275560 avg loss no lamb -3.275560 time 2019-03-04 22:58:43.569065
Model ind 640 epoch 1269 head A head_i_epoch 0 batch 200: avg loss -3.314840 avg loss no lamb -3.314840 time 2019-03-04 23:00:34.054818
last batch sz 160
Model ind 640 epoch 1269 head B head_i_epoch 0 batch 0: avg loss -1.990143 avg loss no lamb -1.990143 time 2019-03-04 23:01:55.291676
Model ind 640 epoch 1269 head B head_i_epoch 0 batch 100: avg loss -1.867928 avg loss no lamb -1.867928 time 2019-03-04 23:03:45.437339
Model ind 640 epoch 1269 head B head_i_epoch 0 batch 200: avg loss -1.989272 avg loss no lamb -1.989272 time 2019-03-04 23:05:34.591619
last batch sz 160
Model ind 640 epoch 1269 head B head_i_epoch 1 batch 0: avg loss -1.998153 avg loss no lamb -1.998153 time 2019-03-04 23:06:56.538095
Model ind 640 epoch 1269 head B head_i_epoch 1 batch 100: avg loss -1.906289 avg loss no lamb -1.906289 time 2019-03-04 23:08:48.781058
Model ind 640 epoch 1269 head B head_i_epoch 1 batch 200: avg loss -1.932995 avg loss no lamb -1.932995 time 2019-03-04 23:10:37.818637
last batch sz 160
Pre: time 2019-03-04 23:12:19.069146: 
 	std: 0.051198423
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51058334, 0.61511666, 0.61516666, 0.61511666, 0.51066667]
	train_accs: [0.51058334, 0.61511666, 0.61516666, 0.61511666, 0.51066667]
	best_train_sub_head: 2
	worst: 0.51058334
	avg: 0.57333
	best: 0.61516666

Starting e_i: 1270
Model ind 640 epoch 1270 head A head_i_epoch 0 batch 0: avg loss -3.317935 avg loss no lamb -3.317935 time 2019-03-04 23:12:21.886471
Model ind 640 epoch 1270 head A head_i_epoch 0 batch 100: avg loss -3.325359 avg loss no lamb -3.325359 time 2019-03-04 23:14:13.314735
Model ind 640 epoch 1270 head A head_i_epoch 0 batch 200: avg loss -3.374269 avg loss no lamb -3.374269 time 2019-03-04 23:16:03.371961
last batch sz 160
Model ind 640 epoch 1270 head B head_i_epoch 0 batch 0: avg loss -1.925708 avg loss no lamb -1.925708 time 2019-03-04 23:17:21.847590
Model ind 640 epoch 1270 head B head_i_epoch 0 batch 100: avg loss -1.955544 avg loss no lamb -1.955544 time 2019-03-04 23:19:13.200705
Model ind 640 epoch 1270 head B head_i_epoch 0 batch 200: avg loss -1.947916 avg loss no lamb -1.947916 time 2019-03-04 23:21:10.711317
last batch sz 160
Model ind 640 epoch 1270 head B head_i_epoch 1 batch 0: avg loss -1.974138 avg loss no lamb -1.974138 time 2019-03-04 23:22:23.674142
Model ind 640 epoch 1270 head B head_i_epoch 1 batch 100: avg loss -1.961918 avg loss no lamb -1.961918 time 2019-03-04 23:24:05.596935
Model ind 640 epoch 1270 head B head_i_epoch 1 batch 200: avg loss -1.974726 avg loss no lamb -1.974726 time 2019-03-04 23:25:55.160287
last batch sz 160
Pre: time 2019-03-04 23:27:35.005873: 
 	std: 0.049912456
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51283336, 0.6147, 0.6146167, 0.61473334, 0.51276666]
	train_accs: [0.51283336, 0.6147, 0.6146167, 0.61473334, 0.51276666]
	best_train_sub_head: 3
	worst: 0.51276666
	avg: 0.57393
	best: 0.61473334

Starting e_i: 1271
Model ind 640 epoch 1271 head A head_i_epoch 0 batch 0: avg loss -3.419587 avg loss no lamb -3.419587 time 2019-03-04 23:27:42.986052
Model ind 640 epoch 1271 head A head_i_epoch 0 batch 100: avg loss -3.321731 avg loss no lamb -3.321731 time 2019-03-04 23:29:33.925597
Model ind 640 epoch 1271 head A head_i_epoch 0 batch 200: avg loss -3.347109 avg loss no lamb -3.347109 time 2019-03-04 23:31:24.797878
last batch sz 160
Model ind 640 epoch 1271 head B head_i_epoch 0 batch 0: avg loss -1.958694 avg loss no lamb -1.958694 time 2019-03-04 23:32:44.031343
Model ind 640 epoch 1271 head B head_i_epoch 0 batch 100: avg loss -1.911972 avg loss no lamb -1.911972 time 2019-03-04 23:34:32.950256
Model ind 640 epoch 1271 head B head_i_epoch 0 batch 200: avg loss -2.013143 avg loss no lamb -2.013143 time 2019-03-04 23:36:23.164267
last batch sz 160
Model ind 640 epoch 1271 head B head_i_epoch 1 batch 0: avg loss -1.919469 avg loss no lamb -1.919469 time 2019-03-04 23:37:43.166923
Model ind 640 epoch 1271 head B head_i_epoch 1 batch 100: avg loss -1.907788 avg loss no lamb -1.907788 time 2019-03-04 23:39:32.210874
Model ind 640 epoch 1271 head B head_i_epoch 1 batch 200: avg loss -2.028440 avg loss no lamb -2.028440 time 2019-03-04 23:41:23.338386
last batch sz 160
Pre: time 2019-03-04 23:43:06.297724: 
 	std: 0.050843377
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5115167, 0.61513335, 0.61508334, 0.61513335, 0.51115]
	train_accs: [0.5115167, 0.61513335, 0.61508334, 0.61513335, 0.51115]
	best_train_sub_head: 1
	worst: 0.51115
	avg: 0.57360333
	best: 0.61513335

Starting e_i: 1272
Model ind 640 epoch 1272 head A head_i_epoch 0 batch 0: avg loss -3.293741 avg loss no lamb -3.293741 time 2019-03-04 23:43:09.854693
Model ind 640 epoch 1272 head A head_i_epoch 0 batch 100: avg loss -3.303912 avg loss no lamb -3.303912 time 2019-03-04 23:45:00.392738
Model ind 640 epoch 1272 head A head_i_epoch 0 batch 200: avg loss -3.269703 avg loss no lamb -3.269703 time 2019-03-04 23:46:49.761324
last batch sz 160
Model ind 640 epoch 1272 head B head_i_epoch 0 batch 0: avg loss -1.947883 avg loss no lamb -1.947883 time 2019-03-04 23:48:10.616713
Model ind 640 epoch 1272 head B head_i_epoch 0 batch 100: avg loss -1.880726 avg loss no lamb -1.880726 time 2019-03-04 23:50:02.578727
Model ind 640 epoch 1272 head B head_i_epoch 0 batch 200: avg loss -1.980084 avg loss no lamb -1.980084 time 2019-03-04 23:51:52.065935
last batch sz 160
Model ind 640 epoch 1272 head B head_i_epoch 1 batch 0: avg loss -1.863674 avg loss no lamb -1.863674 time 2019-03-04 23:53:11.477721
Model ind 640 epoch 1272 head B head_i_epoch 1 batch 100: avg loss -1.946915 avg loss no lamb -1.946915 time 2019-03-04 23:55:02.386445
Model ind 640 epoch 1272 head B head_i_epoch 1 batch 200: avg loss -1.896923 avg loss no lamb -1.896923 time 2019-03-04 23:56:54.359003
last batch sz 160
Pre: time 2019-03-04 23:58:34.681270: 
 	std: 0.05093038
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51126665, 0.61516666, 0.6153333, 0.6153333, 0.51136667]
	train_accs: [0.51126665, 0.61516666, 0.6153333, 0.6153333, 0.51136667]
	best_train_sub_head: 2
	worst: 0.51126665
	avg: 0.57369334
	best: 0.6153333

Starting e_i: 1273
Model ind 640 epoch 1273 head A head_i_epoch 0 batch 0: avg loss -3.337650 avg loss no lamb -3.337650 time 2019-03-04 23:58:37.843673
Model ind 640 epoch 1273 head A head_i_epoch 0 batch 100: avg loss -3.223301 avg loss no lamb -3.223301 time 2019-03-05 00:00:28.116143
Model ind 640 epoch 1273 head A head_i_epoch 0 batch 200: avg loss -3.311746 avg loss no lamb -3.311746 time 2019-03-05 00:02:19.804180
last batch sz 160
Model ind 640 epoch 1273 head B head_i_epoch 0 batch 0: avg loss -2.015046 avg loss no lamb -2.015046 time 2019-03-05 00:03:40.010580
Model ind 640 epoch 1273 head B head_i_epoch 0 batch 100: avg loss -1.981683 avg loss no lamb -1.981683 time 2019-03-05 00:05:29.151989
Model ind 640 epoch 1273 head B head_i_epoch 0 batch 200: avg loss -2.062417 avg loss no lamb -2.062417 time 2019-03-05 00:07:19.721939
last batch sz 160
Model ind 640 epoch 1273 head B head_i_epoch 1 batch 0: avg loss -1.959647 avg loss no lamb -1.959647 time 2019-03-05 00:08:40.490523
Model ind 640 epoch 1273 head B head_i_epoch 1 batch 100: avg loss -1.886332 avg loss no lamb -1.886332 time 2019-03-05 00:10:29.144729
Model ind 640 epoch 1273 head B head_i_epoch 1 batch 200: avg loss -2.027529 avg loss no lamb -2.027529 time 2019-03-05 00:12:19.308999
last batch sz 160
Pre: time 2019-03-05 00:14:03.472982: 
 	std: 0.050582
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118167, 0.61518335, 0.61516666, 0.61505, 0.51195]
	train_accs: [0.5118167, 0.61518335, 0.61516666, 0.61505, 0.51195]
	best_train_sub_head: 1
	worst: 0.5118167
	avg: 0.57383335
	best: 0.61518335

Starting e_i: 1274
Model ind 640 epoch 1274 head A head_i_epoch 0 batch 0: avg loss -3.297432 avg loss no lamb -3.297432 time 2019-03-05 00:14:06.108008
Model ind 640 epoch 1274 head A head_i_epoch 0 batch 100: avg loss -3.292924 avg loss no lamb -3.292924 time 2019-03-05 00:15:56.865021
Model ind 640 epoch 1274 head A head_i_epoch 0 batch 200: avg loss -3.311914 avg loss no lamb -3.311914 time 2019-03-05 00:17:46.617467
last batch sz 160
Model ind 640 epoch 1274 head B head_i_epoch 0 batch 0: avg loss -1.931955 avg loss no lamb -1.931955 time 2019-03-05 00:19:06.496902
Model ind 640 epoch 1274 head B head_i_epoch 0 batch 100: avg loss -1.942696 avg loss no lamb -1.942696 time 2019-03-05 00:20:58.400903
Model ind 640 epoch 1274 head B head_i_epoch 0 batch 200: avg loss -2.017387 avg loss no lamb -2.017387 time 2019-03-05 00:22:47.835662
last batch sz 160
Model ind 640 epoch 1274 head B head_i_epoch 1 batch 0: avg loss -1.881909 avg loss no lamb -1.881909 time 2019-03-05 00:24:06.975723
Model ind 640 epoch 1274 head B head_i_epoch 1 batch 100: avg loss -1.867607 avg loss no lamb -1.867607 time 2019-03-05 00:25:56.912925
Model ind 640 epoch 1274 head B head_i_epoch 1 batch 200: avg loss -1.945708 avg loss no lamb -1.945708 time 2019-03-05 00:27:47.042879
last batch sz 160
Pre: time 2019-03-05 00:29:30.030571: 
 	std: 0.05072361
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5103667, 0.61411667, 0.61408335, 0.61401665, 0.5107]
	train_accs: [0.5103667, 0.61411667, 0.61408335, 0.61401665, 0.5107]
	best_train_sub_head: 1
	worst: 0.5103667
	avg: 0.5726567
	best: 0.61411667

Starting e_i: 1275
Model ind 640 epoch 1275 head A head_i_epoch 0 batch 0: avg loss -3.348401 avg loss no lamb -3.348401 time 2019-03-05 00:29:33.247287
Model ind 640 epoch 1275 head A head_i_epoch 0 batch 100: avg loss -3.294278 avg loss no lamb -3.294278 time 2019-03-05 00:31:22.969412
Model ind 640 epoch 1275 head A head_i_epoch 0 batch 200: avg loss -3.327055 avg loss no lamb -3.327055 time 2019-03-05 00:33:14.419328
last batch sz 160
Model ind 640 epoch 1275 head B head_i_epoch 0 batch 0: avg loss -1.949841 avg loss no lamb -1.949841 time 2019-03-05 00:34:35.289350
Model ind 640 epoch 1275 head B head_i_epoch 0 batch 100: avg loss -1.987889 avg loss no lamb -1.987889 time 2019-03-05 00:36:25.929843
Model ind 640 epoch 1275 head B head_i_epoch 0 batch 200: avg loss -2.020159 avg loss no lamb -2.020159 time 2019-03-05 00:38:16.360091
last batch sz 160
Model ind 640 epoch 1275 head B head_i_epoch 1 batch 0: avg loss -1.964373 avg loss no lamb -1.964373 time 2019-03-05 00:39:37.800856
Model ind 640 epoch 1275 head B head_i_epoch 1 batch 100: avg loss -1.911698 avg loss no lamb -1.911698 time 2019-03-05 00:41:26.964482
Model ind 640 epoch 1275 head B head_i_epoch 1 batch 200: avg loss -2.056081 avg loss no lamb -2.056081 time 2019-03-05 00:43:15.437628
last batch sz 160
Pre: time 2019-03-05 00:44:59.243385: 
 	std: 0.05086778
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115833, 0.6153333, 0.61541665, 0.61555, 0.51161665]
	train_accs: [0.5115833, 0.6153333, 0.61541665, 0.61555, 0.51161665]
	best_train_sub_head: 3
	worst: 0.5115833
	avg: 0.5739
	best: 0.61555

Starting e_i: 1276
Model ind 640 epoch 1276 head A head_i_epoch 0 batch 0: avg loss -3.314997 avg loss no lamb -3.314997 time 2019-03-05 00:45:02.493137
Model ind 640 epoch 1276 head A head_i_epoch 0 batch 100: avg loss -3.332413 avg loss no lamb -3.332413 time 2019-03-05 00:46:53.722307
Model ind 640 epoch 1276 head A head_i_epoch 0 batch 200: avg loss -3.322209 avg loss no lamb -3.322209 time 2019-03-05 00:48:42.978938
last batch sz 160
Model ind 640 epoch 1276 head B head_i_epoch 0 batch 0: avg loss -1.930163 avg loss no lamb -1.930163 time 2019-03-05 00:50:02
Model ind 640 epoch 1276 head B head_i_epoch 0 batch 100: avg loss -1.893819 avg loss no lamb -1.893819 time 2019-03-05 00:51:52.670307
Model ind 640 epoch 1276 head B head_i_epoch 0 batch 200: avg loss -2.058713 avg loss no lamb -2.058713 time 2019-03-05 00:53:43.113892
last batch sz 160
Model ind 640 epoch 1276 head B head_i_epoch 1 batch 0: avg loss -2.010739 avg loss no lamb -2.010739 time 2019-03-05 00:55:02.580785
Model ind 640 epoch 1276 head B head_i_epoch 1 batch 100: avg loss -1.992773 avg loss no lamb -1.992773 time 2019-03-05 00:56:53.739905
Model ind 640 epoch 1276 head B head_i_epoch 1 batch 200: avg loss -1.953876 avg loss no lamb -1.953876 time 2019-03-05 00:58:44.231729
last batch sz 160
Pre: time 2019-03-05 01:00:24.433070: 
 	std: 0.05086777
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51095, 0.61468333, 0.61468333, 0.61483335, 0.51085]
	train_accs: [0.51095, 0.61468333, 0.61468333, 0.61483335, 0.51085]
	best_train_sub_head: 3
	worst: 0.51085
	avg: 0.5732
	best: 0.61483335

Starting e_i: 1277
Model ind 640 epoch 1277 head A head_i_epoch 0 batch 0: avg loss -3.368755 avg loss no lamb -3.368755 time 2019-03-05 01:00:28.656545
Model ind 640 epoch 1277 head A head_i_epoch 0 batch 100: avg loss -3.238719 avg loss no lamb -3.238719 time 2019-03-05 01:02:20.272604
Model ind 640 epoch 1277 head A head_i_epoch 0 batch 200: avg loss -3.368493 avg loss no lamb -3.368493 time 2019-03-05 01:04:12.696617
last batch sz 160
Model ind 640 epoch 1277 head B head_i_epoch 0 batch 0: avg loss -1.993362 avg loss no lamb -1.993362 time 2019-03-05 01:05:34.453419
Model ind 640 epoch 1277 head B head_i_epoch 0 batch 100: avg loss -1.942462 avg loss no lamb -1.942462 time 2019-03-05 01:07:23.739653
Model ind 640 epoch 1277 head B head_i_epoch 0 batch 200: avg loss -1.972946 avg loss no lamb -1.972946 time 2019-03-05 01:09:13.926246
last batch sz 160
Model ind 640 epoch 1277 head B head_i_epoch 1 batch 0: avg loss -1.971537 avg loss no lamb -1.971537 time 2019-03-05 01:10:35.663388
Model ind 640 epoch 1277 head B head_i_epoch 1 batch 100: avg loss -1.872205 avg loss no lamb -1.872205 time 2019-03-05 01:12:25.101934
Model ind 640 epoch 1277 head B head_i_epoch 1 batch 200: avg loss -1.972013 avg loss no lamb -1.972013 time 2019-03-05 01:14:14.039080
last batch sz 160
Pre: time 2019-03-05 01:15:54.803784: 
 	std: 0.05063777
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51198334, 0.61536664, 0.61543334, 0.6153167, 0.51203334]
	train_accs: [0.51198334, 0.61536664, 0.61543334, 0.6153167, 0.51203334]
	best_train_sub_head: 2
	worst: 0.51198334
	avg: 0.5740267
	best: 0.61543334

Starting e_i: 1278
Model ind 640 epoch 1278 head A head_i_epoch 0 batch 0: avg loss -3.313309 avg loss no lamb -3.313309 time 2019-03-05 01:15:57.639370
Model ind 640 epoch 1278 head A head_i_epoch 0 batch 100: avg loss -3.288109 avg loss no lamb -3.288109 time 2019-03-05 01:17:51.889179
Model ind 640 epoch 1278 head A head_i_epoch 0 batch 200: avg loss -3.217487 avg loss no lamb -3.217487 time 2019-03-05 01:19:42.457704
last batch sz 160
Model ind 640 epoch 1278 head B head_i_epoch 0 batch 0: avg loss -2.002743 avg loss no lamb -2.002743 time 2019-03-05 01:21:02.178241
Model ind 640 epoch 1278 head B head_i_epoch 0 batch 100: avg loss -1.846263 avg loss no lamb -1.846263 time 2019-03-05 01:22:54.456312
Model ind 640 epoch 1278 head B head_i_epoch 0 batch 200: avg loss -1.956037 avg loss no lamb -1.956037 time 2019-03-05 01:24:45.141833
last batch sz 160
Model ind 640 epoch 1278 head B head_i_epoch 1 batch 0: avg loss -1.982785 avg loss no lamb -1.982785 time 2019-03-05 01:26:03.955942
Model ind 640 epoch 1278 head B head_i_epoch 1 batch 100: avg loss -1.913954 avg loss no lamb -1.913954 time 2019-03-05 01:27:53.674958
Model ind 640 epoch 1278 head B head_i_epoch 1 batch 200: avg loss -2.017020 avg loss no lamb -2.017020 time 2019-03-05 01:29:45.443616
last batch sz 160
Pre: time 2019-03-05 01:31:26.523095: 
 	std: 0.050806537
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5111167, 0.61476666, 0.6149833, 0.6148, 0.5111667]
	train_accs: [0.5111167, 0.61476666, 0.6149833, 0.6148, 0.5111667]
	best_train_sub_head: 2
	worst: 0.5111167
	avg: 0.57336664
	best: 0.6149833

Starting e_i: 1279
Model ind 640 epoch 1279 head A head_i_epoch 0 batch 0: avg loss -3.247142 avg loss no lamb -3.247142 time 2019-03-05 01:31:29.620948
Model ind 640 epoch 1279 head A head_i_epoch 0 batch 100: avg loss -3.206762 avg loss no lamb -3.206762 time 2019-03-05 01:33:20.940058
Model ind 640 epoch 1279 head A head_i_epoch 0 batch 200: avg loss -3.319981 avg loss no lamb -3.319981 time 2019-03-05 01:35:10.200126
last batch sz 160
Model ind 640 epoch 1279 head B head_i_epoch 0 batch 0: avg loss -1.924758 avg loss no lamb -1.924758 time 2019-03-05 01:36:31.449355
Model ind 640 epoch 1279 head B head_i_epoch 0 batch 100: avg loss -1.918909 avg loss no lamb -1.918909 time 2019-03-05 01:38:20.427899
Model ind 640 epoch 1279 head B head_i_epoch 0 batch 200: avg loss -1.966012 avg loss no lamb -1.966012 time 2019-03-05 01:40:09.186406
last batch sz 160
Model ind 640 epoch 1279 head B head_i_epoch 1 batch 0: avg loss -1.962840 avg loss no lamb -1.962840 time 2019-03-05 01:41:30.408330
Model ind 640 epoch 1279 head B head_i_epoch 1 batch 100: avg loss -1.897349 avg loss no lamb -1.897349 time 2019-03-05 01:43:20.625430
Model ind 640 epoch 1279 head B head_i_epoch 1 batch 200: avg loss -2.000932 avg loss no lamb -2.000932 time 2019-03-05 01:45:09.943178
last batch sz 160
Pre: time 2019-03-05 01:46:49.962573: 
 	std: 0.050790187
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51166666, 0.61525, 0.6153167, 0.6152833, 0.51155]
	train_accs: [0.51166666, 0.61525, 0.6153167, 0.6152833, 0.51155]
	best_train_sub_head: 2
	worst: 0.51155
	avg: 0.5738133
	best: 0.6153167

Starting e_i: 1280
Model ind 640 epoch 1280 head A head_i_epoch 0 batch 0: avg loss -3.330416 avg loss no lamb -3.330416 time 2019-03-05 01:46:53.850629
Model ind 640 epoch 1280 head A head_i_epoch 0 batch 100: avg loss -3.240299 avg loss no lamb -3.240299 time 2019-03-05 01:48:46.323101
Model ind 640 epoch 1280 head A head_i_epoch 0 batch 200: avg loss -3.255862 avg loss no lamb -3.255862 time 2019-03-05 01:50:37.577966
last batch sz 160
Model ind 640 epoch 1280 head B head_i_epoch 0 batch 0: avg loss -1.956668 avg loss no lamb -1.956668 time 2019-03-05 01:51:56.412496
Model ind 640 epoch 1280 head B head_i_epoch 0 batch 100: avg loss -1.899732 avg loss no lamb -1.899732 time 2019-03-05 01:53:47.787019
Model ind 640 epoch 1280 head B head_i_epoch 0 batch 200: avg loss -1.998049 avg loss no lamb -1.998049 time 2019-03-05 01:55:39.423407
last batch sz 160
Model ind 640 epoch 1280 head B head_i_epoch 1 batch 0: avg loss -1.980982 avg loss no lamb -1.980982 time 2019-03-05 01:56:58.366202
Model ind 640 epoch 1280 head B head_i_epoch 1 batch 100: avg loss -1.893032 avg loss no lamb -1.893032 time 2019-03-05 01:58:48.878177
Model ind 640 epoch 1280 head B head_i_epoch 1 batch 200: avg loss -1.945131 avg loss no lamb -1.945131 time 2019-03-05 02:00:40.042714
last batch sz 160
Pre: time 2019-03-05 02:02:21.552773: 
 	std: 0.051165905
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51091665, 0.61511666, 0.6152833, 0.61515, 0.51056665]
	train_accs: [0.51091665, 0.61511666, 0.6152833, 0.61515, 0.51056665]
	best_train_sub_head: 2
	worst: 0.51056665
	avg: 0.57340664
	best: 0.6152833

Starting e_i: 1281
Model ind 640 epoch 1281 head A head_i_epoch 0 batch 0: avg loss -3.359641 avg loss no lamb -3.359641 time 2019-03-05 02:02:29.689236
Model ind 640 epoch 1281 head A head_i_epoch 0 batch 100: avg loss -3.234334 avg loss no lamb -3.234334 time 2019-03-05 02:04:18.180819
Model ind 640 epoch 1281 head A head_i_epoch 0 batch 200: avg loss -3.245498 avg loss no lamb -3.245498 time 2019-03-05 02:06:10.032237
last batch sz 160
Model ind 640 epoch 1281 head B head_i_epoch 0 batch 0: avg loss -1.969077 avg loss no lamb -1.969077 time 2019-03-05 02:07:32.195240
Model ind 640 epoch 1281 head B head_i_epoch 0 batch 100: avg loss -1.989870 avg loss no lamb -1.989870 time 2019-03-05 02:09:21.723046
Model ind 640 epoch 1281 head B head_i_epoch 0 batch 200: avg loss -1.959566 avg loss no lamb -1.959566 time 2019-03-05 02:11:11.346300
last batch sz 160
Model ind 640 epoch 1281 head B head_i_epoch 1 batch 0: avg loss -1.978758 avg loss no lamb -1.978758 time 2019-03-05 02:12:32.463214
Model ind 640 epoch 1281 head B head_i_epoch 1 batch 100: avg loss -1.952247 avg loss no lamb -1.952247 time 2019-03-05 02:14:22.860707
Model ind 640 epoch 1281 head B head_i_epoch 1 batch 200: avg loss -1.972010 avg loss no lamb -1.972010 time 2019-03-05 02:16:12.531142
last batch sz 160
Pre: time 2019-03-05 02:17:52.599717: 
 	std: 0.05085166
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5115, 0.61508334, 0.61505, 0.61501664, 0.511]
	train_accs: [0.5115, 0.61508334, 0.61505, 0.61501664, 0.511]
	best_train_sub_head: 1
	worst: 0.511
	avg: 0.57353
	best: 0.61508334

Starting e_i: 1282
Model ind 640 epoch 1282 head A head_i_epoch 0 batch 0: avg loss -3.266822 avg loss no lamb -3.266822 time 2019-03-05 02:17:55.794511
Model ind 640 epoch 1282 head A head_i_epoch 0 batch 100: avg loss -3.335303 avg loss no lamb -3.335303 time 2019-03-05 02:19:45.959919
Model ind 640 epoch 1282 head A head_i_epoch 0 batch 200: avg loss -3.352102 avg loss no lamb -3.352102 time 2019-03-05 02:21:40.369008
last batch sz 160
Model ind 640 epoch 1282 head B head_i_epoch 0 batch 0: avg loss -1.964389 avg loss no lamb -1.964389 time 2019-03-05 02:22:58.735513
Model ind 640 epoch 1282 head B head_i_epoch 0 batch 100: avg loss -1.935231 avg loss no lamb -1.935231 time 2019-03-05 02:24:48.147083
Model ind 640 epoch 1282 head B head_i_epoch 0 batch 200: avg loss -1.972136 avg loss no lamb -1.972136 time 2019-03-05 02:26:39.743016
last batch sz 160
Model ind 640 epoch 1282 head B head_i_epoch 1 batch 0: avg loss -2.007730 avg loss no lamb -2.007730 time 2019-03-05 02:27:59.838148
Model ind 640 epoch 1282 head B head_i_epoch 1 batch 100: avg loss -1.917791 avg loss no lamb -1.917791 time 2019-03-05 02:29:49.292024
Model ind 640 epoch 1282 head B head_i_epoch 1 batch 200: avg loss -2.002094 avg loss no lamb -2.002094 time 2019-03-05 02:31:39.427504
last batch sz 160
Pre: time 2019-03-05 02:33:21.677982: 
 	std: 0.051110674
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51233333, 0.6163, 0.61615, 0.61628336, 0.5115]
	train_accs: [0.51233333, 0.6163, 0.61615, 0.61628336, 0.5115]
	best_train_sub_head: 1
	worst: 0.5115
	avg: 0.5745133
	best: 0.6163

Starting e_i: 1283
Model ind 640 epoch 1283 head A head_i_epoch 0 batch 0: avg loss -3.303962 avg loss no lamb -3.303962 time 2019-03-05 02:33:25.766003
Model ind 640 epoch 1283 head A head_i_epoch 0 batch 100: avg loss -3.283047 avg loss no lamb -3.283047 time 2019-03-05 02:35:15.287807
Model ind 640 epoch 1283 head A head_i_epoch 0 batch 200: avg loss -3.352960 avg loss no lamb -3.352960 time 2019-03-05 02:37:04.500174
last batch sz 160
Model ind 640 epoch 1283 head B head_i_epoch 0 batch 0: avg loss -2.028888 avg loss no lamb -2.028888 time 2019-03-05 02:38:27.964982
Model ind 640 epoch 1283 head B head_i_epoch 0 batch 100: avg loss -1.947511 avg loss no lamb -1.947511 time 2019-03-05 02:40:18.175772
Model ind 640 epoch 1283 head B head_i_epoch 0 batch 200: avg loss -1.995295 avg loss no lamb -1.995295 time 2019-03-05 02:42:07.121086
last batch sz 160
Model ind 640 epoch 1283 head B head_i_epoch 1 batch 0: avg loss -1.974495 avg loss no lamb -1.974495 time 2019-03-05 02:43:28.303688
Model ind 640 epoch 1283 head B head_i_epoch 1 batch 100: avg loss -1.902487 avg loss no lamb -1.902487 time 2019-03-05 02:45:19.134170
Model ind 640 epoch 1283 head B head_i_epoch 1 batch 200: avg loss -2.051856 avg loss no lamb -2.051856 time 2019-03-05 02:47:09.668397
last batch sz 160
Pre: time 2019-03-05 02:48:50.419613: 
 	std: 0.050861016
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50986665, 0.61356664, 0.61371666, 0.6135, 0.5096833]
	train_accs: [0.50986665, 0.61356664, 0.61371666, 0.6135, 0.5096833]
	best_train_sub_head: 2
	worst: 0.5096833
	avg: 0.57206666
	best: 0.61371666

Starting e_i: 1284
Model ind 640 epoch 1284 head A head_i_epoch 0 batch 0: avg loss -3.285238 avg loss no lamb -3.285238 time 2019-03-05 02:48:53.442701
Model ind 640 epoch 1284 head A head_i_epoch 0 batch 100: avg loss -3.214971 avg loss no lamb -3.214971 time 2019-03-05 02:50:43.268806
Model ind 640 epoch 1284 head A head_i_epoch 0 batch 200: avg loss -3.325400 avg loss no lamb -3.325400 time 2019-03-05 02:52:34.131390
last batch sz 160
Model ind 640 epoch 1284 head B head_i_epoch 0 batch 0: avg loss -1.985021 avg loss no lamb -1.985021 time 2019-03-05 02:53:56.321330
Model ind 640 epoch 1284 head B head_i_epoch 0 batch 100: avg loss -1.983785 avg loss no lamb -1.983785 time 2019-03-05 02:55:45.577062
Model ind 640 epoch 1284 head B head_i_epoch 0 batch 200: avg loss -1.949892 avg loss no lamb -1.949892 time 2019-03-05 02:57:36.112465
last batch sz 160
Model ind 640 epoch 1284 head B head_i_epoch 1 batch 0: avg loss -1.974833 avg loss no lamb -1.974833 time 2019-03-05 02:58:56.433261
Model ind 640 epoch 1284 head B head_i_epoch 1 batch 100: avg loss -1.849505 avg loss no lamb -1.849505 time 2019-03-05 03:00:45.931381
Model ind 640 epoch 1284 head B head_i_epoch 1 batch 200: avg loss -2.055123 avg loss no lamb -2.055123 time 2019-03-05 03:02:37.139398
last batch sz 160
Pre: time 2019-03-05 03:04:19.419303: 
 	std: 0.050485395
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118833, 0.6149667, 0.6148833, 0.61478335, 0.5117667]
	train_accs: [0.5118833, 0.6149667, 0.6148833, 0.61478335, 0.5117667]
	best_train_sub_head: 1
	worst: 0.5117667
	avg: 0.5736567
	best: 0.6149667

Starting e_i: 1285
Model ind 640 epoch 1285 head A head_i_epoch 0 batch 0: avg loss -3.322053 avg loss no lamb -3.322053 time 2019-03-05 03:04:22.260460
Model ind 640 epoch 1285 head A head_i_epoch 0 batch 100: avg loss -3.255985 avg loss no lamb -3.255985 time 2019-03-05 03:06:12.858248
Model ind 640 epoch 1285 head A head_i_epoch 0 batch 200: avg loss -3.314484 avg loss no lamb -3.314484 time 2019-03-05 03:08:00.969662
last batch sz 160
Model ind 640 epoch 1285 head B head_i_epoch 0 batch 0: avg loss -1.985822 avg loss no lamb -1.985822 time 2019-03-05 03:09:22.489743
Model ind 640 epoch 1285 head B head_i_epoch 0 batch 100: avg loss -1.869882 avg loss no lamb -1.869882 time 2019-03-05 03:11:15.203494
Model ind 640 epoch 1285 head B head_i_epoch 0 batch 200: avg loss -2.012886 avg loss no lamb -2.012886 time 2019-03-05 03:13:04.394851
last batch sz 160
Model ind 640 epoch 1285 head B head_i_epoch 1 batch 0: avg loss -2.002483 avg loss no lamb -2.002483 time 2019-03-05 03:14:23.641964
Model ind 640 epoch 1285 head B head_i_epoch 1 batch 100: avg loss -1.960471 avg loss no lamb -1.960471 time 2019-03-05 03:16:15.286073
Model ind 640 epoch 1285 head B head_i_epoch 1 batch 200: avg loss -1.952372 avg loss no lamb -1.952372 time 2019-03-05 03:18:04.733700
last batch sz 160
Pre: time 2019-03-05 03:19:46.470236: 
 	std: 0.050964404
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5115167, 0.6156667, 0.61546665, 0.61558336, 0.51156664]
	train_accs: [0.5115167, 0.6156667, 0.61546665, 0.61558336, 0.51156664]
	best_train_sub_head: 1
	worst: 0.5115167
	avg: 0.57396
	best: 0.6156667

Starting e_i: 1286
Model ind 640 epoch 1286 head A head_i_epoch 0 batch 0: avg loss -3.368700 avg loss no lamb -3.368700 time 2019-03-05 03:19:49.530627
Model ind 640 epoch 1286 head A head_i_epoch 0 batch 100: avg loss -3.298836 avg loss no lamb -3.298836 time 2019-03-05 03:21:39.860440
Model ind 640 epoch 1286 head A head_i_epoch 0 batch 200: avg loss -3.314951 avg loss no lamb -3.314951 time 2019-03-05 03:23:30.613503
last batch sz 160
Model ind 640 epoch 1286 head B head_i_epoch 0 batch 0: avg loss -2.012786 avg loss no lamb -2.012786 time 2019-03-05 03:24:50.800790
Model ind 640 epoch 1286 head B head_i_epoch 0 batch 100: avg loss -1.848001 avg loss no lamb -1.848001 time 2019-03-05 03:26:41.080593
Model ind 640 epoch 1286 head B head_i_epoch 0 batch 200: avg loss -1.960607 avg loss no lamb -1.960607 time 2019-03-05 03:28:31.991852
last batch sz 160
Model ind 640 epoch 1286 head B head_i_epoch 1 batch 0: avg loss -2.052503 avg loss no lamb -2.052503 time 2019-03-05 03:29:52.882713
Model ind 640 epoch 1286 head B head_i_epoch 1 batch 100: avg loss -1.903477 avg loss no lamb -1.903477 time 2019-03-05 03:31:42.372990
Model ind 640 epoch 1286 head B head_i_epoch 1 batch 200: avg loss -1.971285 avg loss no lamb -1.971285 time 2019-03-05 03:33:32.122396
last batch sz 160
Pre: time 2019-03-05 03:35:14.260182: 
 	std: 0.05031661
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51185, 0.6145333, 0.6145, 0.6145667, 0.5118]
	train_accs: [0.51185, 0.6145333, 0.6145, 0.6145667, 0.5118]
	best_train_sub_head: 3
	worst: 0.5118
	avg: 0.57345
	best: 0.6145667

Starting e_i: 1287
Model ind 640 epoch 1287 head A head_i_epoch 0 batch 0: avg loss -3.367842 avg loss no lamb -3.367842 time 2019-03-05 03:35:16.951869
Model ind 640 epoch 1287 head A head_i_epoch 0 batch 100: avg loss -3.248959 avg loss no lamb -3.248959 time 2019-03-05 03:37:08.136025
Model ind 640 epoch 1287 head A head_i_epoch 0 batch 200: avg loss -3.341787 avg loss no lamb -3.341787 time 2019-03-05 03:38:56.992859
last batch sz 160
Model ind 640 epoch 1287 head B head_i_epoch 0 batch 0: avg loss -1.957342 avg loss no lamb -1.957342 time 2019-03-05 03:40:15.697078
Model ind 640 epoch 1287 head B head_i_epoch 0 batch 100: avg loss -1.951985 avg loss no lamb -1.951985 time 2019-03-05 03:42:10.039742
Model ind 640 epoch 1287 head B head_i_epoch 0 batch 200: avg loss -2.010273 avg loss no lamb -2.010273 time 2019-03-05 03:44:00.341335
last batch sz 160
Model ind 640 epoch 1287 head B head_i_epoch 1 batch 0: avg loss -2.012064 avg loss no lamb -2.012064 time 2019-03-05 03:45:19.097675
Model ind 640 epoch 1287 head B head_i_epoch 1 batch 100: avg loss -1.898155 avg loss no lamb -1.898155 time 2019-03-05 03:47:10.415218
Model ind 640 epoch 1287 head B head_i_epoch 1 batch 200: avg loss -1.936780 avg loss no lamb -1.936780 time 2019-03-05 03:49:01.069870
last batch sz 160
Pre: time 2019-03-05 03:50:40.843294: 
 	std: 0.05066501
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.512, 0.6155, 0.61555, 0.61553335, 0.5122167]
	train_accs: [0.512, 0.6155, 0.61555, 0.61553335, 0.5122167]
	best_train_sub_head: 2
	worst: 0.512
	avg: 0.57416
	best: 0.61555

Starting e_i: 1288
Model ind 640 epoch 1288 head A head_i_epoch 0 batch 0: avg loss -3.314243 avg loss no lamb -3.314243 time 2019-03-05 03:50:43.986462
Model ind 640 epoch 1288 head A head_i_epoch 0 batch 100: avg loss -3.183615 avg loss no lamb -3.183615 time 2019-03-05 03:52:33.056829
Model ind 640 epoch 1288 head A head_i_epoch 0 batch 200: avg loss -3.332515 avg loss no lamb -3.332515 time 2019-03-05 03:54:23.573474
last batch sz 160
Model ind 640 epoch 1288 head B head_i_epoch 0 batch 0: avg loss -1.984633 avg loss no lamb -1.984633 time 2019-03-05 03:55:44.695153
Model ind 640 epoch 1288 head B head_i_epoch 0 batch 100: avg loss -1.907672 avg loss no lamb -1.907672 time 2019-03-05 03:57:34.226676
Model ind 640 epoch 1288 head B head_i_epoch 0 batch 200: avg loss -1.992162 avg loss no lamb -1.992162 time 2019-03-05 03:59:25.747196
last batch sz 160
Model ind 640 epoch 1288 head B head_i_epoch 1 batch 0: avg loss -1.987393 avg loss no lamb -1.987393 time 2019-03-05 04:00:47.479410
Model ind 640 epoch 1288 head B head_i_epoch 1 batch 100: avg loss -1.908502 avg loss no lamb -1.908502 time 2019-03-05 04:02:36.597177
Model ind 640 epoch 1288 head B head_i_epoch 1 batch 200: avg loss -1.976735 avg loss no lamb -1.976735 time 2019-03-05 04:04:25.360327
last batch sz 160
Pre: time 2019-03-05 04:06:01.659854: 
 	std: 0.05082017
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118833, 0.61546665, 0.61553335, 0.6156833, 0.5117667]
	train_accs: [0.5118833, 0.61546665, 0.61553335, 0.6156833, 0.5117667]
	best_train_sub_head: 3
	worst: 0.5117667
	avg: 0.5740667
	best: 0.6156833

Starting e_i: 1289
Model ind 640 epoch 1289 head A head_i_epoch 0 batch 0: avg loss -3.351562 avg loss no lamb -3.351562 time 2019-03-05 04:06:04.199246
Model ind 640 epoch 1289 head A head_i_epoch 0 batch 100: avg loss -3.252600 avg loss no lamb -3.252600 time 2019-03-05 04:07:48.976646
Model ind 640 epoch 1289 head A head_i_epoch 0 batch 200: avg loss -3.359458 avg loss no lamb -3.359458 time 2019-03-05 04:09:34.858505
last batch sz 160
Model ind 640 epoch 1289 head B head_i_epoch 0 batch 0: avg loss -1.986087 avg loss no lamb -1.986087 time 2019-03-05 04:10:51.066773
Model ind 640 epoch 1289 head B head_i_epoch 0 batch 100: avg loss -1.884735 avg loss no lamb -1.884735 time 2019-03-05 04:12:32.068971
Model ind 640 epoch 1289 head B head_i_epoch 0 batch 200: avg loss -2.004749 avg loss no lamb -2.004749 time 2019-03-05 04:14:15.952281
last batch sz 160
Model ind 640 epoch 1289 head B head_i_epoch 1 batch 0: avg loss -1.977345 avg loss no lamb -1.977345 time 2019-03-05 04:15:30.601736
Model ind 640 epoch 1289 head B head_i_epoch 1 batch 100: avg loss -2.045118 avg loss no lamb -2.045118 time 2019-03-05 04:17:13.296734
Model ind 640 epoch 1289 head B head_i_epoch 1 batch 200: avg loss -1.950856 avg loss no lamb -1.950856 time 2019-03-05 04:18:55.785544
last batch sz 160
Pre: time 2019-03-05 04:20:29.978999: 
 	std: 0.05082012
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51091665, 0.61466664, 0.6147, 0.6146167, 0.51093334]
	train_accs: [0.51091665, 0.61466664, 0.6147, 0.6146167, 0.51093334]
	best_train_sub_head: 2
	worst: 0.51091665
	avg: 0.57316667
	best: 0.6147

Starting e_i: 1290
Model ind 640 epoch 1290 head A head_i_epoch 0 batch 0: avg loss -3.350446 avg loss no lamb -3.350446 time 2019-03-05 04:20:32.513338
Model ind 640 epoch 1290 head A head_i_epoch 0 batch 100: avg loss -3.318385 avg loss no lamb -3.318385 time 2019-03-05 04:22:16.636367
Model ind 640 epoch 1290 head A head_i_epoch 0 batch 200: avg loss -3.300971 avg loss no lamb -3.300971 time 2019-03-05 04:24:00.627515
last batch sz 160
Model ind 640 epoch 1290 head B head_i_epoch 0 batch 0: avg loss -1.985135 avg loss no lamb -1.985135 time 2019-03-05 04:25:16.503257
Model ind 640 epoch 1290 head B head_i_epoch 0 batch 100: avg loss -1.930756 avg loss no lamb -1.930756 time 2019-03-05 04:27:00.512190
Model ind 640 epoch 1290 head B head_i_epoch 0 batch 200: avg loss -1.939559 avg loss no lamb -1.939559 time 2019-03-05 04:28:41.345968
last batch sz 160
Model ind 640 epoch 1290 head B head_i_epoch 1 batch 0: avg loss -1.971793 avg loss no lamb -1.971793 time 2019-03-05 04:29:57.217746
Model ind 640 epoch 1290 head B head_i_epoch 1 batch 100: avg loss -1.873025 avg loss no lamb -1.873025 time 2019-03-05 04:31:39.632794
Model ind 640 epoch 1290 head B head_i_epoch 1 batch 200: avg loss -1.991332 avg loss no lamb -1.991332 time 2019-03-05 04:33:23.244012
last batch sz 160
Pre: time 2019-03-05 04:34:57.519049: 
 	std: 0.050693665
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114667, 0.61478335, 0.61478335, 0.61476666, 0.5111333]
	train_accs: [0.5114667, 0.61478335, 0.61478335, 0.61476666, 0.5111333]
	best_train_sub_head: 1
	worst: 0.5111333
	avg: 0.57338667
	best: 0.61478335

Starting e_i: 1291
Model ind 640 epoch 1291 head A head_i_epoch 0 batch 0: avg loss -3.289805 avg loss no lamb -3.289805 time 2019-03-05 04:35:03.954478
Model ind 640 epoch 1291 head A head_i_epoch 0 batch 100: avg loss -3.252364 avg loss no lamb -3.252364 time 2019-03-05 04:36:46.949004
Model ind 640 epoch 1291 head A head_i_epoch 0 batch 200: avg loss -3.324743 avg loss no lamb -3.324743 time 2019-03-05 04:38:29.972488
last batch sz 160
Model ind 640 epoch 1291 head B head_i_epoch 0 batch 0: avg loss -1.956629 avg loss no lamb -1.956629 time 2019-03-05 04:39:45.245999
Model ind 640 epoch 1291 head B head_i_epoch 0 batch 100: avg loss -1.871837 avg loss no lamb -1.871837 time 2019-03-05 04:41:28.389531
Model ind 640 epoch 1291 head B head_i_epoch 0 batch 200: avg loss -1.964686 avg loss no lamb -1.964686 time 2019-03-05 04:43:10.021772
last batch sz 160
Model ind 640 epoch 1291 head B head_i_epoch 1 batch 0: avg loss -2.010941 avg loss no lamb -2.010941 time 2019-03-05 04:44:23.057091
Model ind 640 epoch 1291 head B head_i_epoch 1 batch 100: avg loss -1.885423 avg loss no lamb -1.885423 time 2019-03-05 04:46:08.196545
Model ind 640 epoch 1291 head B head_i_epoch 1 batch 200: avg loss -2.054560 avg loss no lamb -2.054560 time 2019-03-05 04:47:51.942182
last batch sz 160
Pre: time 2019-03-05 04:49:27.040854: 
 	std: 0.050433647
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5115833, 0.6146333, 0.6145333, 0.6145, 0.51163334]
	train_accs: [0.5115833, 0.6146333, 0.6145333, 0.6145, 0.51163334]
	best_train_sub_head: 1
	worst: 0.5115833
	avg: 0.57337666
	best: 0.6146333

Starting e_i: 1292
Model ind 640 epoch 1292 head A head_i_epoch 0 batch 0: avg loss -3.271594 avg loss no lamb -3.271594 time 2019-03-05 04:49:29.555840
Model ind 640 epoch 1292 head A head_i_epoch 0 batch 100: avg loss -3.268918 avg loss no lamb -3.268918 time 2019-03-05 04:51:12.373358
Model ind 640 epoch 1292 head A head_i_epoch 0 batch 200: avg loss -3.387541 avg loss no lamb -3.387541 time 2019-03-05 04:52:55.331726
last batch sz 160
Model ind 640 epoch 1292 head B head_i_epoch 0 batch 0: avg loss -1.933984 avg loss no lamb -1.933984 time 2019-03-05 04:54:10.096913
Model ind 640 epoch 1292 head B head_i_epoch 0 batch 100: avg loss -1.923611 avg loss no lamb -1.923611 time 2019-03-05 04:55:53.172603
Model ind 640 epoch 1292 head B head_i_epoch 0 batch 200: avg loss -2.027114 avg loss no lamb -2.027114 time 2019-03-05 04:57:35.612999
last batch sz 160
Model ind 640 epoch 1292 head B head_i_epoch 1 batch 0: avg loss -1.956666 avg loss no lamb -1.956666 time 2019-03-05 04:58:49.768748
Model ind 640 epoch 1292 head B head_i_epoch 1 batch 100: avg loss -1.957510 avg loss no lamb -1.957510 time 2019-03-05 05:00:31.655799
Model ind 640 epoch 1292 head B head_i_epoch 1 batch 200: avg loss -2.043805 avg loss no lamb -2.043805 time 2019-03-05 05:02:15.423357
last batch sz 160
Pre: time 2019-03-05 05:03:50.171809: 
 	std: 0.051114064
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5106, 0.6149333, 0.6149833, 0.6149667, 0.51065]
	train_accs: [0.5106, 0.6149333, 0.6149833, 0.6149667, 0.51065]
	best_train_sub_head: 2
	worst: 0.5106
	avg: 0.57322663
	best: 0.6149833

Starting e_i: 1293
Model ind 640 epoch 1293 head A head_i_epoch 0 batch 0: avg loss -3.338469 avg loss no lamb -3.338469 time 2019-03-05 05:03:52.690816
Model ind 640 epoch 1293 head A head_i_epoch 0 batch 100: avg loss -3.288907 avg loss no lamb -3.288907 time 2019-03-05 05:05:35.960225
Model ind 640 epoch 1293 head A head_i_epoch 0 batch 200: avg loss -3.349536 avg loss no lamb -3.349536 time 2019-03-05 05:07:18.755326
last batch sz 160
Model ind 640 epoch 1293 head B head_i_epoch 0 batch 0: avg loss -2.040179 avg loss no lamb -2.040179 time 2019-03-05 05:08:33.589193
Model ind 640 epoch 1293 head B head_i_epoch 0 batch 100: avg loss -1.865353 avg loss no lamb -1.865353 time 2019-03-05 05:10:16.928105
Model ind 640 epoch 1293 head B head_i_epoch 0 batch 200: avg loss -1.913124 avg loss no lamb -1.913124 time 2019-03-05 05:11:59.580479
last batch sz 160
Model ind 640 epoch 1293 head B head_i_epoch 1 batch 0: avg loss -1.955366 avg loss no lamb -1.955366 time 2019-03-05 05:13:15.243305
Model ind 640 epoch 1293 head B head_i_epoch 1 batch 100: avg loss -1.952169 avg loss no lamb -1.952169 time 2019-03-05 05:14:57.147563
Model ind 640 epoch 1293 head B head_i_epoch 1 batch 200: avg loss -1.968457 avg loss no lamb -1.968457 time 2019-03-05 05:16:39.083859
last batch sz 160
Pre: time 2019-03-05 05:18:14.809315: 
 	std: 0.050769974
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51215, 0.6155667, 0.61555, 0.61553335, 0.51168334]
	train_accs: [0.51215, 0.6155667, 0.61555, 0.61553335, 0.51168334]
	best_train_sub_head: 1
	worst: 0.51168334
	avg: 0.5740967
	best: 0.6155667

Starting e_i: 1294
Model ind 640 epoch 1294 head A head_i_epoch 0 batch 0: avg loss -3.333812 avg loss no lamb -3.333812 time 2019-03-05 05:18:18.218488
Model ind 640 epoch 1294 head A head_i_epoch 0 batch 100: avg loss -3.287946 avg loss no lamb -3.287946 time 2019-03-05 05:20:01.689687
Model ind 640 epoch 1294 head A head_i_epoch 0 batch 200: avg loss -3.306504 avg loss no lamb -3.306504 time 2019-03-05 05:21:44.920281
last batch sz 160
Model ind 640 epoch 1294 head B head_i_epoch 0 batch 0: avg loss -1.982105 avg loss no lamb -1.982105 time 2019-03-05 05:23:00.283724
Model ind 640 epoch 1294 head B head_i_epoch 0 batch 100: avg loss -1.916754 avg loss no lamb -1.916754 time 2019-03-05 05:24:45.185781
Model ind 640 epoch 1294 head B head_i_epoch 0 batch 200: avg loss -1.996984 avg loss no lamb -1.996984 time 2019-03-05 05:26:28.884651
last batch sz 160
Model ind 640 epoch 1294 head B head_i_epoch 1 batch 0: avg loss -1.931044 avg loss no lamb -1.931044 time 2019-03-05 05:27:44.174851
Model ind 640 epoch 1294 head B head_i_epoch 1 batch 100: avg loss -1.957859 avg loss no lamb -1.957859 time 2019-03-05 05:29:27.502077
Model ind 640 epoch 1294 head B head_i_epoch 1 batch 200: avg loss -1.992883 avg loss no lamb -1.992883 time 2019-03-05 05:31:08.615304
last batch sz 160
Pre: time 2019-03-05 05:32:41.642861: 
 	std: 0.05036969
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125, 0.61525, 0.61535, 0.61535, 0.5125]
	train_accs: [0.5125, 0.61525, 0.61535, 0.61535, 0.5125]
	best_train_sub_head: 2
	worst: 0.5125
	avg: 0.57419
	best: 0.61535

Starting e_i: 1295
Model ind 640 epoch 1295 head A head_i_epoch 0 batch 0: avg loss -3.254678 avg loss no lamb -3.254678 time 2019-03-05 05:32:44.234569
Model ind 640 epoch 1295 head A head_i_epoch 0 batch 100: avg loss -3.197456 avg loss no lamb -3.197456 time 2019-03-05 05:34:28.252621
Model ind 640 epoch 1295 head A head_i_epoch 0 batch 200: avg loss -3.302862 avg loss no lamb -3.302862 time 2019-03-05 05:36:11.291912
last batch sz 160
Model ind 640 epoch 1295 head B head_i_epoch 0 batch 0: avg loss -2.048777 avg loss no lamb -2.048777 time 2019-03-05 05:37:26.240703
Model ind 640 epoch 1295 head B head_i_epoch 0 batch 100: avg loss -1.905083 avg loss no lamb -1.905083 time 2019-03-05 05:39:09.085690
Model ind 640 epoch 1295 head B head_i_epoch 0 batch 200: avg loss -1.921576 avg loss no lamb -1.921576 time 2019-03-05 05:40:52.008251
last batch sz 160
Model ind 640 epoch 1295 head B head_i_epoch 1 batch 0: avg loss -1.966326 avg loss no lamb -1.966326 time 2019-03-05 05:42:06.898444
Model ind 640 epoch 1295 head B head_i_epoch 1 batch 100: avg loss -1.884231 avg loss no lamb -1.884231 time 2019-03-05 05:43:49.808086
Model ind 640 epoch 1295 head B head_i_epoch 1 batch 200: avg loss -1.952190 avg loss no lamb -1.952190 time 2019-03-05 05:45:32.388095
last batch sz 160
Pre: time 2019-03-05 05:47:04.361755: 
 	std: 0.05090043
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51196665, 0.6157333, 0.61586666, 0.6159, 0.5119]
	train_accs: [0.51196665, 0.6157333, 0.61586666, 0.6159, 0.5119]
	best_train_sub_head: 3
	worst: 0.5119
	avg: 0.5742733
	best: 0.6159

Starting e_i: 1296
Model ind 640 epoch 1296 head A head_i_epoch 0 batch 0: avg loss -3.232856 avg loss no lamb -3.232856 time 2019-03-05 05:47:06.821145
Model ind 640 epoch 1296 head A head_i_epoch 0 batch 100: avg loss -3.312606 avg loss no lamb -3.312606 time 2019-03-05 05:48:49.969309
Model ind 640 epoch 1296 head A head_i_epoch 0 batch 200: avg loss -3.323945 avg loss no lamb -3.323945 time 2019-03-05 05:50:33.701934
last batch sz 160
Model ind 640 epoch 1296 head B head_i_epoch 0 batch 0: avg loss -1.967599 avg loss no lamb -1.967599 time 2019-03-05 05:51:48.598109
Model ind 640 epoch 1296 head B head_i_epoch 0 batch 100: avg loss -1.961662 avg loss no lamb -1.961662 time 2019-03-05 05:53:31.230196
Model ind 640 epoch 1296 head B head_i_epoch 0 batch 200: avg loss -2.016236 avg loss no lamb -2.016236 time 2019-03-05 05:55:14.174682
last batch sz 160
Model ind 640 epoch 1296 head B head_i_epoch 1 batch 0: avg loss -1.958192 avg loss no lamb -1.958192 time 2019-03-05 05:56:28.992185
Model ind 640 epoch 1296 head B head_i_epoch 1 batch 100: avg loss -1.902308 avg loss no lamb -1.902308 time 2019-03-05 05:58:11.758766
Model ind 640 epoch 1296 head B head_i_epoch 1 batch 200: avg loss -2.037978 avg loss no lamb -2.037978 time 2019-03-05 05:59:54.602712
last batch sz 160
Pre: time 2019-03-05 06:01:29.010752: 
 	std: 0.0508583
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.511, 0.61465, 0.61476666, 0.61465, 0.51075]
	train_accs: [0.511, 0.61465, 0.61476666, 0.61465, 0.51075]
	best_train_sub_head: 2
	worst: 0.51075
	avg: 0.57316333
	best: 0.61476666

Starting e_i: 1297
Model ind 640 epoch 1297 head A head_i_epoch 0 batch 0: avg loss -3.369849 avg loss no lamb -3.369849 time 2019-03-05 06:01:32.262541
Model ind 640 epoch 1297 head A head_i_epoch 0 batch 100: avg loss -3.202887 avg loss no lamb -3.202887 time 2019-03-05 06:03:13.521363
Model ind 640 epoch 1297 head A head_i_epoch 0 batch 200: avg loss -3.287992 avg loss no lamb -3.287992 time 2019-03-05 06:04:57.362845
last batch sz 160
Model ind 640 epoch 1297 head B head_i_epoch 0 batch 0: avg loss -1.960042 avg loss no lamb -1.960042 time 2019-03-05 06:06:12.530347
Model ind 640 epoch 1297 head B head_i_epoch 0 batch 100: avg loss -1.860561 avg loss no lamb -1.860561 time 2019-03-05 06:07:55.299209
Model ind 640 epoch 1297 head B head_i_epoch 0 batch 200: avg loss -2.014915 avg loss no lamb -2.014915 time 2019-03-05 06:09:37.956273
last batch sz 160
Model ind 640 epoch 1297 head B head_i_epoch 1 batch 0: avg loss -1.983270 avg loss no lamb -1.983270 time 2019-03-05 06:10:52.526310
Model ind 640 epoch 1297 head B head_i_epoch 1 batch 100: avg loss -1.892111 avg loss no lamb -1.892111 time 2019-03-05 06:12:35.609486
Model ind 640 epoch 1297 head B head_i_epoch 1 batch 200: avg loss -1.937455 avg loss no lamb -1.937455 time 2019-03-05 06:14:18.974546
last batch sz 160
Pre: time 2019-03-05 06:15:55.185155: 
 	std: 0.05028125
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51233333, 0.6149833, 0.6149667, 0.61508334, 0.51241666]
	train_accs: [0.51233333, 0.6149833, 0.6149667, 0.61508334, 0.51241666]
	best_train_sub_head: 3
	worst: 0.51233333
	avg: 0.5739566
	best: 0.61508334

Starting e_i: 1298
Model ind 640 epoch 1298 head A head_i_epoch 0 batch 0: avg loss -3.315112 avg loss no lamb -3.315112 time 2019-03-05 06:15:57.737576
Model ind 640 epoch 1298 head A head_i_epoch 0 batch 100: avg loss -3.237567 avg loss no lamb -3.237567 time 2019-03-05 06:17:40.167456
Model ind 640 epoch 1298 head A head_i_epoch 0 batch 200: avg loss -3.294080 avg loss no lamb -3.294080 time 2019-03-05 06:19:21.103514
last batch sz 160
Model ind 640 epoch 1298 head B head_i_epoch 0 batch 0: avg loss -1.940086 avg loss no lamb -1.940086 time 2019-03-05 06:20:37.151661
Model ind 640 epoch 1298 head B head_i_epoch 0 batch 100: avg loss -1.945627 avg loss no lamb -1.945627 time 2019-03-05 06:22:19.696075
Model ind 640 epoch 1298 head B head_i_epoch 0 batch 200: avg loss -1.961012 avg loss no lamb -1.961012 time 2019-03-05 06:24:02.432693
last batch sz 160
Model ind 640 epoch 1298 head B head_i_epoch 1 batch 0: avg loss -1.987274 avg loss no lamb -1.987274 time 2019-03-05 06:25:17.010395
Model ind 640 epoch 1298 head B head_i_epoch 1 batch 100: avg loss -1.926956 avg loss no lamb -1.926956 time 2019-03-05 06:26:59.643676
Model ind 640 epoch 1298 head B head_i_epoch 1 batch 200: avg loss -2.011534 avg loss no lamb -2.011534 time 2019-03-05 06:28:42.857313
last batch sz 160
Pre: time 2019-03-05 06:30:17.591582: 
 	std: 0.050591636
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51121664, 0.61471665, 0.61466664, 0.61465, 0.5116]
	train_accs: [0.51121664, 0.61471665, 0.61466664, 0.61465, 0.5116]
	best_train_sub_head: 1
	worst: 0.51121664
	avg: 0.57337
	best: 0.61471665

Starting e_i: 1299
Model ind 640 epoch 1299 head A head_i_epoch 0 batch 0: avg loss -3.333658 avg loss no lamb -3.333658 time 2019-03-05 06:30:20.133570
Model ind 640 epoch 1299 head A head_i_epoch 0 batch 100: avg loss -3.346098 avg loss no lamb -3.346098 time 2019-03-05 06:32:03.110414
Model ind 640 epoch 1299 head A head_i_epoch 0 batch 200: avg loss -3.383344 avg loss no lamb -3.383344 time 2019-03-05 06:33:45.779806
last batch sz 160
Model ind 640 epoch 1299 head B head_i_epoch 0 batch 0: avg loss -2.029460 avg loss no lamb -2.029460 time 2019-03-05 06:34:59.164133
Model ind 640 epoch 1299 head B head_i_epoch 0 batch 100: avg loss -1.874574 avg loss no lamb -1.874574 time 2019-03-05 06:36:43.495603
Model ind 640 epoch 1299 head B head_i_epoch 0 batch 200: avg loss -2.010638 avg loss no lamb -2.010638 time 2019-03-05 06:38:26.184388
last batch sz 160
Model ind 640 epoch 1299 head B head_i_epoch 1 batch 0: avg loss -1.997630 avg loss no lamb -1.997630 time 2019-03-05 06:39:40.898934
Model ind 640 epoch 1299 head B head_i_epoch 1 batch 100: avg loss -1.942465 avg loss no lamb -1.942465 time 2019-03-05 06:41:23.670336
Model ind 640 epoch 1299 head B head_i_epoch 1 batch 200: avg loss -1.968889 avg loss no lamb -1.968889 time 2019-03-05 06:43:07.136141
last batch sz 160
Pre: time 2019-03-05 06:44:42.955762: 
 	std: 0.050478626
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5119, 0.61506665, 0.61515, 0.615, 0.5121667]
	train_accs: [0.5119, 0.61506665, 0.61515, 0.615, 0.5121667]
	best_train_sub_head: 2
	worst: 0.5119
	avg: 0.5738567
	best: 0.61515

Starting e_i: 1300
Model ind 640 epoch 1300 head A head_i_epoch 0 batch 0: avg loss -3.291613 avg loss no lamb -3.291613 time 2019-03-05 06:44:46.389034
Model ind 640 epoch 1300 head A head_i_epoch 0 batch 100: avg loss -3.290507 avg loss no lamb -3.290507 time 2019-03-05 06:46:29.891240
Model ind 640 epoch 1300 head A head_i_epoch 0 batch 200: avg loss -3.341209 avg loss no lamb -3.341209 time 2019-03-05 06:48:13.825889
last batch sz 160
Model ind 640 epoch 1300 head B head_i_epoch 0 batch 0: avg loss -1.990067 avg loss no lamb -1.990067 time 2019-03-05 06:49:28.250398
Model ind 640 epoch 1300 head B head_i_epoch 0 batch 100: avg loss -1.984297 avg loss no lamb -1.984297 time 2019-03-05 06:51:09.588115
Model ind 640 epoch 1300 head B head_i_epoch 0 batch 200: avg loss -2.028891 avg loss no lamb -2.028891 time 2019-03-05 06:52:54.470244
last batch sz 160
Model ind 640 epoch 1300 head B head_i_epoch 1 batch 0: avg loss -1.972274 avg loss no lamb -1.972274 time 2019-03-05 06:54:09.129086
Model ind 640 epoch 1300 head B head_i_epoch 1 batch 100: avg loss -1.863707 avg loss no lamb -1.863707 time 2019-03-05 06:55:51.952585
Model ind 640 epoch 1300 head B head_i_epoch 1 batch 200: avg loss -1.940926 avg loss no lamb -1.940926 time 2019-03-05 06:57:34.794693
last batch sz 160
Pre: time 2019-03-05 06:59:09.980373: 
 	std: 0.050513986
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51205, 0.6153, 0.6153, 0.61518335, 0.51225]
	train_accs: [0.51205, 0.6153, 0.6153, 0.61518335, 0.51225]
	best_train_sub_head: 1
	worst: 0.51205
	avg: 0.5740167
	best: 0.6153

Starting e_i: 1301
Model ind 640 epoch 1301 head A head_i_epoch 0 batch 0: avg loss -3.303515 avg loss no lamb -3.303515 time 2019-03-05 06:59:15.643964
Model ind 640 epoch 1301 head A head_i_epoch 0 batch 100: avg loss -3.238083 avg loss no lamb -3.238083 time 2019-03-05 07:01:00.598446
Model ind 640 epoch 1301 head A head_i_epoch 0 batch 200: avg loss -3.276395 avg loss no lamb -3.276395 time 2019-03-05 07:02:46.294255
last batch sz 160
Model ind 640 epoch 1301 head B head_i_epoch 0 batch 0: avg loss -1.921533 avg loss no lamb -1.921533 time 2019-03-05 07:04:03.333361
Model ind 640 epoch 1301 head B head_i_epoch 0 batch 100: avg loss -1.983518 avg loss no lamb -1.983518 time 2019-03-05 07:05:45.964745
Model ind 640 epoch 1301 head B head_i_epoch 0 batch 200: avg loss -2.032180 avg loss no lamb -2.032180 time 2019-03-05 07:07:28.222006
last batch sz 160
Model ind 640 epoch 1301 head B head_i_epoch 1 batch 0: avg loss -1.993096 avg loss no lamb -1.993096 time 2019-03-05 07:08:44.055863
Model ind 640 epoch 1301 head B head_i_epoch 1 batch 100: avg loss -1.909888 avg loss no lamb -1.909888 time 2019-03-05 07:10:26.535344
Model ind 640 epoch 1301 head B head_i_epoch 1 batch 200: avg loss -1.960531 avg loss no lamb -1.960531 time 2019-03-05 07:12:09.163669
last batch sz 160
Pre: time 2019-03-05 07:13:43.020672: 
 	std: 0.05093992
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114, 0.6152833, 0.6153, 0.61523336, 0.5111833]
	train_accs: [0.5114, 0.6152833, 0.6153, 0.61523336, 0.5111833]
	best_train_sub_head: 2
	worst: 0.5111833
	avg: 0.57368
	best: 0.6153

Starting e_i: 1302
Model ind 640 epoch 1302 head A head_i_epoch 0 batch 0: avg loss -3.290883 avg loss no lamb -3.290883 time 2019-03-05 07:13:45.536403
Model ind 640 epoch 1302 head A head_i_epoch 0 batch 100: avg loss -3.191860 avg loss no lamb -3.191860 time 2019-03-05 07:15:28.212571
Model ind 640 epoch 1302 head A head_i_epoch 0 batch 200: avg loss -3.382307 avg loss no lamb -3.382307 time 2019-03-05 07:17:11.177440
last batch sz 160
Model ind 640 epoch 1302 head B head_i_epoch 0 batch 0: avg loss -1.937977 avg loss no lamb -1.937977 time 2019-03-05 07:18:25.960048
Model ind 640 epoch 1302 head B head_i_epoch 0 batch 100: avg loss -1.990781 avg loss no lamb -1.990781 time 2019-03-05 07:20:08.855274
Model ind 640 epoch 1302 head B head_i_epoch 0 batch 200: avg loss -1.977328 avg loss no lamb -1.977328 time 2019-03-05 07:21:50.045116
last batch sz 160
Model ind 640 epoch 1302 head B head_i_epoch 1 batch 0: avg loss -1.989130 avg loss no lamb -1.989130 time 2019-03-05 07:23:04.408724
Model ind 640 epoch 1302 head B head_i_epoch 1 batch 100: avg loss -1.923496 avg loss no lamb -1.923496 time 2019-03-05 07:24:48.574993
Model ind 640 epoch 1302 head B head_i_epoch 1 batch 200: avg loss -1.981597 avg loss no lamb -1.981597 time 2019-03-05 07:26:31.270350
last batch sz 160
Pre: time 2019-03-05 07:28:06.550476: 
 	std: 0.050841924
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51088333, 0.6147, 0.61455, 0.6145667, 0.5107667]
	train_accs: [0.51088333, 0.6147, 0.61455, 0.6145667, 0.5107667]
	best_train_sub_head: 1
	worst: 0.5107667
	avg: 0.57309335
	best: 0.6147

Starting e_i: 1303
Model ind 640 epoch 1303 head A head_i_epoch 0 batch 0: avg loss -3.285804 avg loss no lamb -3.285804 time 2019-03-05 07:28:09.041359
Model ind 640 epoch 1303 head A head_i_epoch 0 batch 100: avg loss -3.277988 avg loss no lamb -3.277988 time 2019-03-05 07:29:52.301188
Model ind 640 epoch 1303 head A head_i_epoch 0 batch 200: avg loss -3.341480 avg loss no lamb -3.341480 time 2019-03-05 07:31:35.388036
last batch sz 160
Model ind 640 epoch 1303 head B head_i_epoch 0 batch 0: avg loss -1.950390 avg loss no lamb -1.950390 time 2019-03-05 07:32:50.715614
Model ind 640 epoch 1303 head B head_i_epoch 0 batch 100: avg loss -1.901790 avg loss no lamb -1.901790 time 2019-03-05 07:34:33.334198
Model ind 640 epoch 1303 head B head_i_epoch 0 batch 200: avg loss -1.929100 avg loss no lamb -1.929100 time 2019-03-05 07:36:16.491603
last batch sz 160
Model ind 640 epoch 1303 head B head_i_epoch 1 batch 0: avg loss -2.021752 avg loss no lamb -2.021752 time 2019-03-05 07:37:29.753893
Model ind 640 epoch 1303 head B head_i_epoch 1 batch 100: avg loss -1.927195 avg loss no lamb -1.927195 time 2019-03-05 07:39:11.614865
Model ind 640 epoch 1303 head B head_i_epoch 1 batch 200: avg loss -2.023370 avg loss no lamb -2.023370 time 2019-03-05 07:40:55.622975
last batch sz 160
Pre: time 2019-03-05 07:42:29.732177: 
 	std: 0.05030448
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5126333, 0.61515, 0.61513335, 0.61511666, 0.5122667]
	train_accs: [0.5126333, 0.61515, 0.61513335, 0.61511666, 0.5122667]
	best_train_sub_head: 1
	worst: 0.5122667
	avg: 0.57405996
	best: 0.61515

Starting e_i: 1304
Model ind 640 epoch 1304 head A head_i_epoch 0 batch 0: avg loss -3.354195 avg loss no lamb -3.354195 time 2019-03-05 07:42:32.260861
Model ind 640 epoch 1304 head A head_i_epoch 0 batch 100: avg loss -3.303974 avg loss no lamb -3.303974 time 2019-03-05 07:44:15.014101
Model ind 640 epoch 1304 head A head_i_epoch 0 batch 200: avg loss -3.306431 avg loss no lamb -3.306431 time 2019-03-05 07:45:57.872085
last batch sz 160
Model ind 640 epoch 1304 head B head_i_epoch 0 batch 0: avg loss -2.007106 avg loss no lamb -2.007106 time 2019-03-05 07:47:12.499980
Model ind 640 epoch 1304 head B head_i_epoch 0 batch 100: avg loss -1.988611 avg loss no lamb -1.988611 time 2019-03-05 07:48:55.171615
Model ind 640 epoch 1304 head B head_i_epoch 0 batch 200: avg loss -2.021448 avg loss no lamb -2.021448 time 2019-03-05 07:50:38.119244
last batch sz 160
Model ind 640 epoch 1304 head B head_i_epoch 1 batch 0: avg loss -1.989747 avg loss no lamb -1.989747 time 2019-03-05 07:51:53.008152
Model ind 640 epoch 1304 head B head_i_epoch 1 batch 100: avg loss -1.909991 avg loss no lamb -1.909991 time 2019-03-05 07:53:34.082743
Model ind 640 epoch 1304 head B head_i_epoch 1 batch 200: avg loss -1.938006 avg loss no lamb -1.938006 time 2019-03-05 07:55:16.249611
last batch sz 160
Pre: time 2019-03-05 07:56:51.407796: 
 	std: 0.050783582
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51175, 0.6152, 0.61515, 0.61518335, 0.51128334]
	train_accs: [0.51175, 0.6152, 0.61515, 0.61518335, 0.51128334]
	best_train_sub_head: 1
	worst: 0.51128334
	avg: 0.57371336
	best: 0.6152

Starting e_i: 1305
Model ind 640 epoch 1305 head A head_i_epoch 0 batch 0: avg loss -3.281011 avg loss no lamb -3.281011 time 2019-03-05 07:56:53.927770
Model ind 640 epoch 1305 head A head_i_epoch 0 batch 100: avg loss -3.222106 avg loss no lamb -3.222106 time 2019-03-05 07:58:36.550275
Model ind 640 epoch 1305 head A head_i_epoch 0 batch 200: avg loss -3.328408 avg loss no lamb -3.328408 time 2019-03-05 08:00:19.941915
last batch sz 160
Model ind 640 epoch 1305 head B head_i_epoch 0 batch 0: avg loss -1.984923 avg loss no lamb -1.984923 time 2019-03-05 08:01:35.087646
Model ind 640 epoch 1305 head B head_i_epoch 0 batch 100: avg loss -1.911936 avg loss no lamb -1.911936 time 2019-03-05 08:03:17.509965
Model ind 640 epoch 1305 head B head_i_epoch 0 batch 200: avg loss -2.023117 avg loss no lamb -2.023117 time 2019-03-05 08:05:00.308248
last batch sz 160
Model ind 640 epoch 1305 head B head_i_epoch 1 batch 0: avg loss -1.940207 avg loss no lamb -1.940207 time 2019-03-05 08:06:15.351491
Model ind 640 epoch 1305 head B head_i_epoch 1 batch 100: avg loss -1.920091 avg loss no lamb -1.920091 time 2019-03-05 08:07:59.522706
Model ind 640 epoch 1305 head B head_i_epoch 1 batch 200: avg loss -2.052982 avg loss no lamb -2.052982 time 2019-03-05 08:09:40.081787
last batch sz 160
Pre: time 2019-03-05 08:11:15.185910: 
 	std: 0.050997045
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51091665, 0.6149667, 0.61505, 0.61495, 0.51086664]
	train_accs: [0.51091665, 0.6149667, 0.61505, 0.61495, 0.51086664]
	best_train_sub_head: 2
	worst: 0.51086664
	avg: 0.57335
	best: 0.61505

Starting e_i: 1306
Model ind 640 epoch 1306 head A head_i_epoch 0 batch 0: avg loss -3.277145 avg loss no lamb -3.277145 time 2019-03-05 08:11:17.824529
Model ind 640 epoch 1306 head A head_i_epoch 0 batch 100: avg loss -3.202891 avg loss no lamb -3.202891 time 2019-03-05 08:13:00.615176
Model ind 640 epoch 1306 head A head_i_epoch 0 batch 200: avg loss -3.260911 avg loss no lamb -3.260911 time 2019-03-05 08:14:43.421796
last batch sz 160
Model ind 640 epoch 1306 head B head_i_epoch 0 batch 0: avg loss -1.971970 avg loss no lamb -1.971970 time 2019-03-05 08:15:58.093379
Model ind 640 epoch 1306 head B head_i_epoch 0 batch 100: avg loss -1.924291 avg loss no lamb -1.924291 time 2019-03-05 08:17:40.663533
Model ind 640 epoch 1306 head B head_i_epoch 0 batch 200: avg loss -2.001009 avg loss no lamb -2.001009 time 2019-03-05 08:19:23.381152
last batch sz 160
Model ind 640 epoch 1306 head B head_i_epoch 1 batch 0: avg loss -2.003734 avg loss no lamb -2.003734 time 2019-03-05 08:20:38.171186
Model ind 640 epoch 1306 head B head_i_epoch 1 batch 100: avg loss -1.977968 avg loss no lamb -1.977968 time 2019-03-05 08:22:20.701842
Model ind 640 epoch 1306 head B head_i_epoch 1 batch 200: avg loss -2.014848 avg loss no lamb -2.014848 time 2019-03-05 08:24:03.540938
last batch sz 160
Pre: time 2019-03-05 08:25:35.707996: 
 	std: 0.05098477
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51138335, 0.61546665, 0.61541665, 0.6153833, 0.51131666]
	train_accs: [0.51138335, 0.61546665, 0.61541665, 0.6153833, 0.51131666]
	best_train_sub_head: 1
	worst: 0.51131666
	avg: 0.5737933
	best: 0.61546665

Starting e_i: 1307
Model ind 640 epoch 1307 head A head_i_epoch 0 batch 0: avg loss -3.326642 avg loss no lamb -3.326642 time 2019-03-05 08:25:38.178327
Model ind 640 epoch 1307 head A head_i_epoch 0 batch 100: avg loss -3.221068 avg loss no lamb -3.221068 time 2019-03-05 08:27:22.400191
Model ind 640 epoch 1307 head A head_i_epoch 0 batch 200: avg loss -3.326946 avg loss no lamb -3.326946 time 2019-03-05 08:29:05.735570
last batch sz 160
Model ind 640 epoch 1307 head B head_i_epoch 0 batch 0: avg loss -2.046341 avg loss no lamb -2.046341 time 2019-03-05 08:30:20.417247
Model ind 640 epoch 1307 head B head_i_epoch 0 batch 100: avg loss -1.912265 avg loss no lamb -1.912265 time 2019-03-05 08:32:03.061379
Model ind 640 epoch 1307 head B head_i_epoch 0 batch 200: avg loss -1.977040 avg loss no lamb -1.977040 time 2019-03-05 08:33:45.706747
last batch sz 160
Model ind 640 epoch 1307 head B head_i_epoch 1 batch 0: avg loss -1.986550 avg loss no lamb -1.986550 time 2019-03-05 08:35:00.359210
Model ind 640 epoch 1307 head B head_i_epoch 1 batch 100: avg loss -1.863105 avg loss no lamb -1.863105 time 2019-03-05 08:36:43.961065
Model ind 640 epoch 1307 head B head_i_epoch 1 batch 200: avg loss -1.961186 avg loss no lamb -1.961186 time 2019-03-05 08:38:27.194680
last batch sz 160
Pre: time 2019-03-05 08:40:00.956067: 
 	std: 0.050134256
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5134, 0.6157, 0.61576664, 0.6157167, 0.5133833]
	train_accs: [0.5134, 0.6157, 0.61576664, 0.6157167, 0.5133833]
	best_train_sub_head: 2
	worst: 0.5133833
	avg: 0.57479334
	best: 0.61576664

Starting e_i: 1308
Model ind 640 epoch 1308 head A head_i_epoch 0 batch 0: avg loss -3.347118 avg loss no lamb -3.347118 time 2019-03-05 08:40:04.293918
Model ind 640 epoch 1308 head A head_i_epoch 0 batch 100: avg loss -3.293788 avg loss no lamb -3.293788 time 2019-03-05 08:41:45.287072
Model ind 640 epoch 1308 head A head_i_epoch 0 batch 200: avg loss -3.289108 avg loss no lamb -3.289108 time 2019-03-05 08:43:29.532530
last batch sz 160
Model ind 640 epoch 1308 head B head_i_epoch 0 batch 0: avg loss -1.940957 avg loss no lamb -1.940957 time 2019-03-05 08:44:44.664541
Model ind 640 epoch 1308 head B head_i_epoch 0 batch 100: avg loss -1.872211 avg loss no lamb -1.872211 time 2019-03-05 08:46:27.825495
Model ind 640 epoch 1308 head B head_i_epoch 0 batch 200: avg loss -2.022922 avg loss no lamb -2.022922 time 2019-03-05 08:48:11.224692
last batch sz 160
Model ind 640 epoch 1308 head B head_i_epoch 1 batch 0: avg loss -1.983245 avg loss no lamb -1.983245 time 2019-03-05 08:49:25.744892
Model ind 640 epoch 1308 head B head_i_epoch 1 batch 100: avg loss -1.928785 avg loss no lamb -1.928785 time 2019-03-05 08:51:08.709388
Model ind 640 epoch 1308 head B head_i_epoch 1 batch 200: avg loss -2.049714 avg loss no lamb -2.049714 time 2019-03-05 08:52:52.399536
last batch sz 160
Pre: time 2019-03-05 08:54:27.495061: 
 	std: 0.050098907
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51271665, 0.61483335, 0.6149333, 0.61495, 0.5125667]
	train_accs: [0.51271665, 0.61483335, 0.6149333, 0.61495, 0.5125667]
	best_train_sub_head: 3
	worst: 0.5125667
	avg: 0.574
	best: 0.61495

Starting e_i: 1309
Model ind 640 epoch 1309 head A head_i_epoch 0 batch 0: avg loss -3.349133 avg loss no lamb -3.349133 time 2019-03-05 08:54:30.163406
Model ind 640 epoch 1309 head A head_i_epoch 0 batch 100: avg loss -3.239522 avg loss no lamb -3.239522 time 2019-03-05 08:56:12.502029
Model ind 640 epoch 1309 head A head_i_epoch 0 batch 200: avg loss -3.326877 avg loss no lamb -3.326877 time 2019-03-05 08:57:54.075910
last batch sz 160
Model ind 640 epoch 1309 head B head_i_epoch 0 batch 0: avg loss -1.954091 avg loss no lamb -1.954091 time 2019-03-05 08:59:10.157836
Model ind 640 epoch 1309 head B head_i_epoch 0 batch 100: avg loss -1.958965 avg loss no lamb -1.958965 time 2019-03-05 09:00:52.875316
Model ind 640 epoch 1309 head B head_i_epoch 0 batch 200: avg loss -1.956500 avg loss no lamb -1.956500 time 2019-03-05 09:02:35.648686
last batch sz 160
Model ind 640 epoch 1309 head B head_i_epoch 1 batch 0: avg loss -1.911817 avg loss no lamb -1.911817 time 2019-03-05 09:03:50.416783
Model ind 640 epoch 1309 head B head_i_epoch 1 batch 100: avg loss -1.987890 avg loss no lamb -1.987890 time 2019-03-05 09:05:33.589596
Model ind 640 epoch 1309 head B head_i_epoch 1 batch 200: avg loss -1.993450 avg loss no lamb -1.993450 time 2019-03-05 09:07:17.012534
last batch sz 160
Pre: time 2019-03-05 09:08:51.149588: 
 	std: 0.050928984
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51068336, 0.6147, 0.6146167, 0.6146333, 0.5107]
	train_accs: [0.51068336, 0.6147, 0.6146167, 0.6146333, 0.5107]
	best_train_sub_head: 1
	worst: 0.51068336
	avg: 0.57306665
	best: 0.6147

Starting e_i: 1310
Model ind 640 epoch 1310 head A head_i_epoch 0 batch 0: avg loss -3.260058 avg loss no lamb -3.260058 time 2019-03-05 09:08:53.643313
Model ind 640 epoch 1310 head A head_i_epoch 0 batch 100: avg loss -3.316807 avg loss no lamb -3.316807 time 2019-03-05 09:10:37.062526
Model ind 640 epoch 1310 head A head_i_epoch 0 batch 200: avg loss -3.363877 avg loss no lamb -3.363877 time 2019-03-05 09:12:19.220572
last batch sz 160
Model ind 640 epoch 1310 head B head_i_epoch 0 batch 0: avg loss -1.964977 avg loss no lamb -1.964977 time 2019-03-05 09:13:32.734982
Model ind 640 epoch 1310 head B head_i_epoch 0 batch 100: avg loss -1.836619 avg loss no lamb -1.836619 time 2019-03-05 09:15:16.710323
Model ind 640 epoch 1310 head B head_i_epoch 0 batch 200: avg loss -1.963102 avg loss no lamb -1.963102 time 2019-03-05 09:16:59.110783
last batch sz 160
Model ind 640 epoch 1310 head B head_i_epoch 1 batch 0: avg loss -1.913429 avg loss no lamb -1.913429 time 2019-03-05 09:18:13.572421
Model ind 640 epoch 1310 head B head_i_epoch 1 batch 100: avg loss -1.877387 avg loss no lamb -1.877387 time 2019-03-05 09:19:56.053799
Model ind 640 epoch 1310 head B head_i_epoch 1 batch 200: avg loss -1.994921 avg loss no lamb -1.994921 time 2019-03-05 09:21:38.753467
last batch sz 160
Pre: time 2019-03-05 09:23:12.702599: 
 	std: 0.05077113
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51165, 0.61523336, 0.61525, 0.61525, 0.51156664]
	train_accs: [0.51165, 0.61523336, 0.61525, 0.61525, 0.51156664]
	best_train_sub_head: 2
	worst: 0.51156664
	avg: 0.57378995
	best: 0.61525

Starting e_i: 1311
Model ind 640 epoch 1311 head A head_i_epoch 0 batch 0: avg loss -3.298702 avg loss no lamb -3.298702 time 2019-03-05 09:23:21.369880
Model ind 640 epoch 1311 head A head_i_epoch 0 batch 100: avg loss -3.325037 avg loss no lamb -3.325037 time 2019-03-05 09:25:04.041497
Model ind 640 epoch 1311 head A head_i_epoch 0 batch 200: avg loss -3.413182 avg loss no lamb -3.413182 time 2019-03-05 09:26:46.720124
last batch sz 160
Model ind 640 epoch 1311 head B head_i_epoch 0 batch 0: avg loss -1.984906 avg loss no lamb -1.984906 time 2019-03-05 09:28:00.812991
Model ind 640 epoch 1311 head B head_i_epoch 0 batch 100: avg loss -1.935995 avg loss no lamb -1.935995 time 2019-03-05 09:29:42.691739
Model ind 640 epoch 1311 head B head_i_epoch 0 batch 200: avg loss -2.019212 avg loss no lamb -2.019212 time 2019-03-05 09:31:26.260235
last batch sz 160
Model ind 640 epoch 1311 head B head_i_epoch 1 batch 0: avg loss -1.960725 avg loss no lamb -1.960725 time 2019-03-05 09:32:40.865867
Model ind 640 epoch 1311 head B head_i_epoch 1 batch 100: avg loss -1.907740 avg loss no lamb -1.907740 time 2019-03-05 09:34:23.609214
Model ind 640 epoch 1311 head B head_i_epoch 1 batch 200: avg loss -2.014256 avg loss no lamb -2.014256 time 2019-03-05 09:36:06.354393
last batch sz 160
Pre: time 2019-03-05 09:37:40.203778: 
 	std: 0.05080382
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51135, 0.61501664, 0.61495, 0.6149167, 0.5111667]
	train_accs: [0.51135, 0.61501664, 0.61495, 0.6149167, 0.5111667]
	best_train_sub_head: 1
	worst: 0.5111667
	avg: 0.57347995
	best: 0.61501664

Starting e_i: 1312
Model ind 640 epoch 1312 head A head_i_epoch 0 batch 0: avg loss -3.244375 avg loss no lamb -3.244375 time 2019-03-05 09:37:42.725467
Model ind 640 epoch 1312 head A head_i_epoch 0 batch 100: avg loss -3.326307 avg loss no lamb -3.326307 time 2019-03-05 09:39:26.348630
Model ind 640 epoch 1312 head A head_i_epoch 0 batch 200: avg loss -3.397749 avg loss no lamb -3.397749 time 2019-03-05 09:41:09.515818
last batch sz 160
Model ind 640 epoch 1312 head B head_i_epoch 0 batch 0: avg loss -1.943760 avg loss no lamb -1.943760 time 2019-03-05 09:42:24.238441
Model ind 640 epoch 1312 head B head_i_epoch 0 batch 100: avg loss -1.936405 avg loss no lamb -1.936405 time 2019-03-05 09:44:05.469539
Model ind 640 epoch 1312 head B head_i_epoch 0 batch 200: avg loss -2.036305 avg loss no lamb -2.036305 time 2019-03-05 09:45:47.280365
last batch sz 160
Model ind 640 epoch 1312 head B head_i_epoch 1 batch 0: avg loss -1.969579 avg loss no lamb -1.969579 time 2019-03-05 09:47:02.809761
Model ind 640 epoch 1312 head B head_i_epoch 1 batch 100: avg loss -1.910109 avg loss no lamb -1.910109 time 2019-03-05 09:48:45.539315
Model ind 640 epoch 1312 head B head_i_epoch 1 batch 200: avg loss -2.044592 avg loss no lamb -2.044592 time 2019-03-05 09:50:28.202803
last batch sz 160
Pre: time 2019-03-05 09:52:03.095340: 
 	std: 0.050368313
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5126167, 0.61545, 0.61543334, 0.61543334, 0.5126333]
	train_accs: [0.5126167, 0.61545, 0.61543334, 0.61543334, 0.5126333]
	best_train_sub_head: 1
	worst: 0.5126167
	avg: 0.57431334
	best: 0.61545

Starting e_i: 1313
Model ind 640 epoch 1313 head A head_i_epoch 0 batch 0: avg loss -3.345660 avg loss no lamb -3.345660 time 2019-03-05 09:52:05.670456
Model ind 640 epoch 1313 head A head_i_epoch 0 batch 100: avg loss -3.329074 avg loss no lamb -3.329074 time 2019-03-05 09:53:49.730734
Model ind 640 epoch 1313 head A head_i_epoch 0 batch 200: avg loss -3.336701 avg loss no lamb -3.336701 time 2019-03-05 09:55:34.234090
last batch sz 160
Model ind 640 epoch 1313 head B head_i_epoch 0 batch 0: avg loss -1.927757 avg loss no lamb -1.927757 time 2019-03-05 09:56:49.019222
Model ind 640 epoch 1313 head B head_i_epoch 0 batch 100: avg loss -1.899630 avg loss no lamb -1.899630 time 2019-03-05 09:58:33.763285
Model ind 640 epoch 1313 head B head_i_epoch 0 batch 200: avg loss -2.023773 avg loss no lamb -2.023773 time 2019-03-05 10:00:16.292005
last batch sz 160
Model ind 640 epoch 1313 head B head_i_epoch 1 batch 0: avg loss -1.973905 avg loss no lamb -1.973905 time 2019-03-05 10:01:31.159378
Model ind 640 epoch 1313 head B head_i_epoch 1 batch 100: avg loss -1.987349 avg loss no lamb -1.987349 time 2019-03-05 10:03:15.344566
Model ind 640 epoch 1313 head B head_i_epoch 1 batch 200: avg loss -1.965429 avg loss no lamb -1.965429 time 2019-03-05 10:04:57.993927
last batch sz 160
Pre: time 2019-03-05 10:06:31.916160: 
 	std: 0.051026992
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5108167, 0.61505, 0.61508334, 0.61511666, 0.51103336]
	train_accs: [0.5108167, 0.61505, 0.61508334, 0.61511666, 0.51103336]
	best_train_sub_head: 3
	worst: 0.5108167
	avg: 0.57342
	best: 0.61511666

Starting e_i: 1314
Model ind 640 epoch 1314 head A head_i_epoch 0 batch 0: avg loss -3.364674 avg loss no lamb -3.364674 time 2019-03-05 10:06:35.369504
Model ind 640 epoch 1314 head A head_i_epoch 0 batch 100: avg loss -3.275314 avg loss no lamb -3.275314 time 2019-03-05 10:08:17.993517
Model ind 640 epoch 1314 head A head_i_epoch 0 batch 200: avg loss -3.333915 avg loss no lamb -3.333915 time 2019-03-05 10:10:01.150282
last batch sz 160
Model ind 640 epoch 1314 head B head_i_epoch 0 batch 0: avg loss -1.990604 avg loss no lamb -1.990604 time 2019-03-05 10:11:15.984530
Model ind 640 epoch 1314 head B head_i_epoch 0 batch 100: avg loss -1.901566 avg loss no lamb -1.901566 time 2019-03-05 10:12:59.008148
Model ind 640 epoch 1314 head B head_i_epoch 0 batch 200: avg loss -2.028502 avg loss no lamb -2.028502 time 2019-03-05 10:14:41.597297
last batch sz 160
Model ind 640 epoch 1314 head B head_i_epoch 1 batch 0: avg loss -1.948557 avg loss no lamb -1.948557 time 2019-03-05 10:15:54.701388
Model ind 640 epoch 1314 head B head_i_epoch 1 batch 100: avg loss -1.973381 avg loss no lamb -1.973381 time 2019-03-05 10:17:36.860463
Model ind 640 epoch 1314 head B head_i_epoch 1 batch 200: avg loss -2.050052 avg loss no lamb -2.050052 time 2019-03-05 10:19:21.545115
last batch sz 160
Pre: time 2019-03-05 10:20:55.798265: 
 	std: 0.05055882
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5125667, 0.6157333, 0.6157333, 0.61576664, 0.5125167]
	train_accs: [0.5125667, 0.6157333, 0.6157333, 0.61576664, 0.5125167]
	best_train_sub_head: 3
	worst: 0.5125167
	avg: 0.5744633
	best: 0.61576664

Starting e_i: 1315
Model ind 640 epoch 1315 head A head_i_epoch 0 batch 0: avg loss -3.278926 avg loss no lamb -3.278926 time 2019-03-05 10:20:58.435710
Model ind 640 epoch 1315 head A head_i_epoch 0 batch 100: avg loss -3.254056 avg loss no lamb -3.254056 time 2019-03-05 10:22:41.685524
Model ind 640 epoch 1315 head A head_i_epoch 0 batch 200: avg loss -3.309850 avg loss no lamb -3.309850 time 2019-03-05 10:24:24.384706
last batch sz 160
Model ind 640 epoch 1315 head B head_i_epoch 0 batch 0: avg loss -1.992003 avg loss no lamb -1.992003 time 2019-03-05 10:25:44.668342
Model ind 640 epoch 1315 head B head_i_epoch 0 batch 100: avg loss -1.914165 avg loss no lamb -1.914165 time 2019-03-05 10:27:43.042322
Model ind 640 epoch 1315 head B head_i_epoch 0 batch 200: avg loss -2.010492 avg loss no lamb -2.010492 time 2019-03-05 10:29:26.351882
last batch sz 160
Model ind 640 epoch 1315 head B head_i_epoch 1 batch 0: avg loss -1.957964 avg loss no lamb -1.957964 time 2019-03-05 10:30:41.753760
Model ind 640 epoch 1315 head B head_i_epoch 1 batch 100: avg loss -1.906289 avg loss no lamb -1.906289 time 2019-03-05 10:32:24.297313
Model ind 640 epoch 1315 head B head_i_epoch 1 batch 200: avg loss -2.032719 avg loss no lamb -2.032719 time 2019-03-05 10:34:08.573011
last batch sz 160
Pre: time 2019-03-05 10:35:42.886137: 
 	std: 0.050828267
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51215, 0.6159, 0.6159, 0.61588335, 0.51213336]
	train_accs: [0.51215, 0.6159, 0.6159, 0.61588335, 0.51213336]
	best_train_sub_head: 1
	worst: 0.51213336
	avg: 0.57439333
	best: 0.6159

Starting e_i: 1316
Model ind 640 epoch 1316 head A head_i_epoch 0 batch 0: avg loss -3.269382 avg loss no lamb -3.269382 time 2019-03-05 10:35:45.507020
Model ind 640 epoch 1316 head A head_i_epoch 0 batch 100: avg loss -3.236565 avg loss no lamb -3.236565 time 2019-03-05 10:37:28.409071
Model ind 640 epoch 1316 head A head_i_epoch 0 batch 200: avg loss -3.435499 avg loss no lamb -3.435499 time 2019-03-05 10:39:10.986544
last batch sz 160
Model ind 640 epoch 1316 head B head_i_epoch 0 batch 0: avg loss -2.010495 avg loss no lamb -2.010495 time 2019-03-05 10:40:26.071891
Model ind 640 epoch 1316 head B head_i_epoch 0 batch 100: avg loss -1.958828 avg loss no lamb -1.958828 time 2019-03-05 10:42:08.826045
Model ind 640 epoch 1316 head B head_i_epoch 0 batch 200: avg loss -1.995907 avg loss no lamb -1.995907 time 2019-03-05 10:43:51.239929
last batch sz 160
Model ind 640 epoch 1316 head B head_i_epoch 1 batch 0: avg loss -2.002698 avg loss no lamb -2.002698 time 2019-03-05 10:45:05.730074
Model ind 640 epoch 1316 head B head_i_epoch 1 batch 100: avg loss -1.880190 avg loss no lamb -1.880190 time 2019-03-05 10:46:48.597040
Model ind 640 epoch 1316 head B head_i_epoch 1 batch 200: avg loss -2.009487 avg loss no lamb -2.009487 time 2019-03-05 10:48:29.328477
last batch sz 160
Pre: time 2019-03-05 10:50:05.955263: 
 	std: 0.050599664
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114667, 0.6147, 0.61468333, 0.6148, 0.5114167]
	train_accs: [0.5114667, 0.6147, 0.61468333, 0.6148, 0.5114167]
	best_train_sub_head: 3
	worst: 0.5114167
	avg: 0.5734133
	best: 0.6148

Starting e_i: 1317
Model ind 640 epoch 1317 head A head_i_epoch 0 batch 0: avg loss -3.324569 avg loss no lamb -3.324569 time 2019-03-05 10:50:09.401443
Model ind 640 epoch 1317 head A head_i_epoch 0 batch 100: avg loss -3.230609 avg loss no lamb -3.230609 time 2019-03-05 10:51:52.300943
Model ind 640 epoch 1317 head A head_i_epoch 0 batch 200: avg loss -3.348405 avg loss no lamb -3.348405 time 2019-03-05 10:53:34.886369
last batch sz 160
Model ind 640 epoch 1317 head B head_i_epoch 0 batch 0: avg loss -1.931121 avg loss no lamb -1.931121 time 2019-03-05 10:54:49.286821
Model ind 640 epoch 1317 head B head_i_epoch 0 batch 100: avg loss -1.924686 avg loss no lamb -1.924686 time 2019-03-05 10:56:31.561310
Model ind 640 epoch 1317 head B head_i_epoch 0 batch 200: avg loss -2.013896 avg loss no lamb -2.013896 time 2019-03-05 10:58:14.531682
last batch sz 160
Model ind 640 epoch 1317 head B head_i_epoch 1 batch 0: avg loss -1.991706 avg loss no lamb -1.991706 time 2019-03-05 10:59:28.868735
Model ind 640 epoch 1317 head B head_i_epoch 1 batch 100: avg loss -1.934175 avg loss no lamb -1.934175 time 2019-03-05 11:01:10.952836
Model ind 640 epoch 1317 head B head_i_epoch 1 batch 200: avg loss -1.972856 avg loss no lamb -1.972856 time 2019-03-05 11:02:52.625130
last batch sz 160
Pre: time 2019-03-05 11:04:24.997572: 
 	std: 0.050807897
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114833, 0.61513335, 0.6151, 0.61515, 0.51135]
	train_accs: [0.5114833, 0.61513335, 0.6151, 0.61515, 0.51135]
	best_train_sub_head: 3
	worst: 0.51135
	avg: 0.5736433
	best: 0.61515

Starting e_i: 1318
Model ind 640 epoch 1318 head A head_i_epoch 0 batch 0: avg loss -3.343359 avg loss no lamb -3.343359 time 2019-03-05 11:04:28.309297
Model ind 640 epoch 1318 head A head_i_epoch 0 batch 100: avg loss -3.293504 avg loss no lamb -3.293504 time 2019-03-05 11:06:11.801846
Model ind 640 epoch 1318 head A head_i_epoch 0 batch 200: avg loss -3.338522 avg loss no lamb -3.338522 time 2019-03-05 11:07:54.448843
last batch sz 160
Model ind 640 epoch 1318 head B head_i_epoch 0 batch 0: avg loss -1.987719 avg loss no lamb -1.987719 time 2019-03-05 11:09:09.690820
Model ind 640 epoch 1318 head B head_i_epoch 0 batch 100: avg loss -1.849023 avg loss no lamb -1.849023 time 2019-03-05 11:10:52.354483
Model ind 640 epoch 1318 head B head_i_epoch 0 batch 200: avg loss -2.001860 avg loss no lamb -2.001860 time 2019-03-05 11:12:35.146859
last batch sz 160
Model ind 640 epoch 1318 head B head_i_epoch 1 batch 0: avg loss -1.961665 avg loss no lamb -1.961665 time 2019-03-05 11:13:50.343067
Model ind 640 epoch 1318 head B head_i_epoch 1 batch 100: avg loss -1.930475 avg loss no lamb -1.930475 time 2019-03-05 11:15:34.390274
Model ind 640 epoch 1318 head B head_i_epoch 1 batch 200: avg loss -2.045311 avg loss no lamb -2.045311 time 2019-03-05 11:17:18.344766
last batch sz 160
Pre: time 2019-03-05 11:18:51.441021: 
 	std: 0.050926354
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5116, 0.6153333, 0.61545, 0.61545, 0.51131666]
	train_accs: [0.5116, 0.6153333, 0.61545, 0.61545, 0.51131666]
	best_train_sub_head: 2
	worst: 0.51131666
	avg: 0.57383
	best: 0.61545

Starting e_i: 1319
Model ind 640 epoch 1319 head A head_i_epoch 0 batch 0: avg loss -3.287138 avg loss no lamb -3.287138 time 2019-03-05 11:18:53.993203
Model ind 640 epoch 1319 head A head_i_epoch 0 batch 100: avg loss -3.213013 avg loss no lamb -3.213013 time 2019-03-05 11:20:37.123538
Model ind 640 epoch 1319 head A head_i_epoch 0 batch 200: avg loss -3.350074 avg loss no lamb -3.350074 time 2019-03-05 11:22:22.185256
last batch sz 160
Model ind 640 epoch 1319 head B head_i_epoch 0 batch 0: avg loss -1.888675 avg loss no lamb -1.888675 time 2019-03-05 11:23:37.878643
Model ind 640 epoch 1319 head B head_i_epoch 0 batch 100: avg loss -1.984868 avg loss no lamb -1.984868 time 2019-03-05 11:25:21.400042
Model ind 640 epoch 1319 head B head_i_epoch 0 batch 200: avg loss -1.963941 avg loss no lamb -1.963941 time 2019-03-05 11:27:05.295616
last batch sz 160
Model ind 640 epoch 1319 head B head_i_epoch 1 batch 0: avg loss -1.992052 avg loss no lamb -1.992052 time 2019-03-05 11:28:20.104609
Model ind 640 epoch 1319 head B head_i_epoch 1 batch 100: avg loss -1.909423 avg loss no lamb -1.909423 time 2019-03-05 11:30:02.638516
Model ind 640 epoch 1319 head B head_i_epoch 1 batch 200: avg loss -1.940621 avg loss no lamb -1.940621 time 2019-03-05 11:31:45.452309
last batch sz 160
Pre: time 2019-03-05 11:33:19.485320: 
 	std: 0.05023637
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51278335, 0.6152833, 0.61515, 0.6152, 0.51255]
	train_accs: [0.51278335, 0.6152833, 0.61515, 0.6152, 0.51255]
	best_train_sub_head: 1
	worst: 0.51255
	avg: 0.5741933
	best: 0.6152833

Starting e_i: 1320
Model ind 640 epoch 1320 head A head_i_epoch 0 batch 0: avg loss -3.373546 avg loss no lamb -3.373546 time 2019-03-05 11:33:22.908455
Model ind 640 epoch 1320 head A head_i_epoch 0 batch 100: avg loss -3.309030 avg loss no lamb -3.309030 time 2019-03-05 11:35:04.148469
Model ind 640 epoch 1320 head A head_i_epoch 0 batch 200: avg loss -3.329357 avg loss no lamb -3.329357 time 2019-03-05 11:36:46.229389
last batch sz 160
Model ind 640 epoch 1320 head B head_i_epoch 0 batch 0: avg loss -2.032960 avg loss no lamb -2.032960 time 2019-03-05 11:38:02.296211
Model ind 640 epoch 1320 head B head_i_epoch 0 batch 100: avg loss -1.859935 avg loss no lamb -1.859935 time 2019-03-05 11:39:45.065432
Model ind 640 epoch 1320 head B head_i_epoch 0 batch 200: avg loss -1.990205 avg loss no lamb -1.990205 time 2019-03-05 11:41:27.678039
last batch sz 160
Model ind 640 epoch 1320 head B head_i_epoch 1 batch 0: avg loss -1.928557 avg loss no lamb -1.928557 time 2019-03-05 11:42:42.206833
Model ind 640 epoch 1320 head B head_i_epoch 1 batch 100: avg loss -1.943624 avg loss no lamb -1.943624 time 2019-03-05 11:44:25.467438
Model ind 640 epoch 1320 head B head_i_epoch 1 batch 200: avg loss -1.991424 avg loss no lamb -1.991424 time 2019-03-05 11:46:09.576017
last batch sz 160
Pre: time 2019-03-05 11:47:44.798886: 
 	std: 0.05081606
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51121664, 0.6149833, 0.61483335, 0.6149167, 0.51115]
	train_accs: [0.51121664, 0.6149833, 0.61483335, 0.6149167, 0.51115]
	best_train_sub_head: 1
	worst: 0.51115
	avg: 0.5734199
	best: 0.6149833

Starting e_i: 1321
Model ind 640 epoch 1321 head A head_i_epoch 0 batch 0: avg loss -3.354645 avg loss no lamb -3.354645 time 2019-03-05 11:47:51.032526
Model ind 640 epoch 1321 head A head_i_epoch 0 batch 100: avg loss -3.301558 avg loss no lamb -3.301558 time 2019-03-05 11:49:35.423552
Model ind 640 epoch 1321 head A head_i_epoch 0 batch 200: avg loss -3.353068 avg loss no lamb -3.353068 time 2019-03-05 11:51:16.865955
last batch sz 160
Model ind 640 epoch 1321 head B head_i_epoch 0 batch 0: avg loss -1.986333 avg loss no lamb -1.986333 time 2019-03-05 11:52:31.495647
Model ind 640 epoch 1321 head B head_i_epoch 0 batch 100: avg loss -1.889851 avg loss no lamb -1.889851 time 2019-03-05 11:54:15.616010
Model ind 640 epoch 1321 head B head_i_epoch 0 batch 200: avg loss -2.077615 avg loss no lamb -2.077615 time 2019-03-05 11:55:58.616076
last batch sz 160
Model ind 640 epoch 1321 head B head_i_epoch 1 batch 0: avg loss -1.940468 avg loss no lamb -1.940468 time 2019-03-05 11:57:13.806917
Model ind 640 epoch 1321 head B head_i_epoch 1 batch 100: avg loss -1.889714 avg loss no lamb -1.889714 time 2019-03-05 11:58:57.080282
Model ind 640 epoch 1321 head B head_i_epoch 1 batch 200: avg loss -1.961321 avg loss no lamb -1.961321 time 2019-03-05 12:00:41.513029
last batch sz 160
Pre: time 2019-03-05 12:02:16.500885: 
 	std: 0.050937187
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51018333, 0.61405, 0.61403334, 0.6142167, 0.5100667]
	train_accs: [0.51018333, 0.61405, 0.61403334, 0.6142167, 0.5100667]
	best_train_sub_head: 3
	worst: 0.5100667
	avg: 0.57251
	best: 0.6142167

Starting e_i: 1322
Model ind 640 epoch 1322 head A head_i_epoch 0 batch 0: avg loss -3.359965 avg loss no lamb -3.359965 time 2019-03-05 12:02:19.120445
Model ind 640 epoch 1322 head A head_i_epoch 0 batch 100: avg loss -3.300611 avg loss no lamb -3.300611 time 2019-03-05 12:04:02.321883
Model ind 640 epoch 1322 head A head_i_epoch 0 batch 200: avg loss -3.306492 avg loss no lamb -3.306492 time 2019-03-05 12:05:44.834975
last batch sz 160
Model ind 640 epoch 1322 head B head_i_epoch 0 batch 0: avg loss -1.947650 avg loss no lamb -1.947650 time 2019-03-05 12:06:58.205221
Model ind 640 epoch 1322 head B head_i_epoch 0 batch 100: avg loss -2.013128 avg loss no lamb -2.013128 time 2019-03-05 12:08:40.239892
Model ind 640 epoch 1322 head B head_i_epoch 0 batch 200: avg loss -1.921876 avg loss no lamb -1.921876 time 2019-03-05 12:10:24.014526
last batch sz 160
Model ind 640 epoch 1322 head B head_i_epoch 1 batch 0: avg loss -1.949443 avg loss no lamb -1.949443 time 2019-03-05 12:11:38.707938
Model ind 640 epoch 1322 head B head_i_epoch 1 batch 100: avg loss -1.871009 avg loss no lamb -1.871009 time 2019-03-05 12:13:21.749227
Model ind 640 epoch 1322 head B head_i_epoch 1 batch 200: avg loss -1.977496 avg loss no lamb -1.977496 time 2019-03-05 12:15:04.339752
last batch sz 160
Pre: time 2019-03-05 12:16:38.782181: 
 	std: 0.050101604
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51318336, 0.6155, 0.6153833, 0.61535, 0.5131]
	train_accs: [0.51318336, 0.6155, 0.6153833, 0.61535, 0.5131]
	best_train_sub_head: 1
	worst: 0.5131
	avg: 0.5745033
	best: 0.6155

Starting e_i: 1323
Model ind 640 epoch 1323 head A head_i_epoch 0 batch 0: avg loss -3.253441 avg loss no lamb -3.253441 time 2019-03-05 12:16:41.277687
Model ind 640 epoch 1323 head A head_i_epoch 0 batch 100: avg loss -3.253222 avg loss no lamb -3.253222 time 2019-03-05 12:18:23.933118
Model ind 640 epoch 1323 head A head_i_epoch 0 batch 200: avg loss -3.303446 avg loss no lamb -3.303446 time 2019-03-05 12:20:06.795478
last batch sz 160
Model ind 640 epoch 1323 head B head_i_epoch 0 batch 0: avg loss -1.948619 avg loss no lamb -1.948619 time 2019-03-05 12:21:22.004895
Model ind 640 epoch 1323 head B head_i_epoch 0 batch 100: avg loss -1.982451 avg loss no lamb -1.982451 time 2019-03-05 12:23:02.799636
Model ind 640 epoch 1323 head B head_i_epoch 0 batch 200: avg loss -2.058086 avg loss no lamb -2.058086 time 2019-03-05 12:24:46.161657
last batch sz 160
Model ind 640 epoch 1323 head B head_i_epoch 1 batch 0: avg loss -1.974079 avg loss no lamb -1.974079 time 2019-03-05 12:26:00.779248
Model ind 640 epoch 1323 head B head_i_epoch 1 batch 100: avg loss -1.863913 avg loss no lamb -1.863913 time 2019-03-05 12:27:43.559637
Model ind 640 epoch 1323 head B head_i_epoch 1 batch 200: avg loss -1.998779 avg loss no lamb -1.998779 time 2019-03-05 12:29:26.653618
last batch sz 160
Pre: time 2019-03-05 12:31:01.133816: 
 	std: 0.05065136
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5116, 0.6150333, 0.6150333, 0.6150333, 0.51168334]
	train_accs: [0.5116, 0.6150333, 0.6150333, 0.6150333, 0.51168334]
	best_train_sub_head: 1
	worst: 0.5116
	avg: 0.5736767
	best: 0.6150333

Starting e_i: 1324
Model ind 640 epoch 1324 head A head_i_epoch 0 batch 0: avg loss -3.262029 avg loss no lamb -3.262029 time 2019-03-05 12:31:03.775273
Model ind 640 epoch 1324 head A head_i_epoch 0 batch 100: avg loss -3.240862 avg loss no lamb -3.240862 time 2019-03-05 12:32:47.750949
Model ind 640 epoch 1324 head A head_i_epoch 0 batch 200: avg loss -3.272765 avg loss no lamb -3.272765 time 2019-03-05 12:34:31.553141
last batch sz 160
Model ind 640 epoch 1324 head B head_i_epoch 0 batch 0: avg loss -1.976666 avg loss no lamb -1.976666 time 2019-03-05 12:35:46.869040
Model ind 640 epoch 1324 head B head_i_epoch 0 batch 100: avg loss -1.922761 avg loss no lamb -1.922761 time 2019-03-05 12:37:30.834638
Model ind 640 epoch 1324 head B head_i_epoch 0 batch 200: avg loss -2.036229 avg loss no lamb -2.036229 time 2019-03-05 12:39:11.747050
last batch sz 160
Model ind 640 epoch 1324 head B head_i_epoch 1 batch 0: avg loss -1.903911 avg loss no lamb -1.903911 time 2019-03-05 12:40:26.867248
Model ind 640 epoch 1324 head B head_i_epoch 1 batch 100: avg loss -1.905210 avg loss no lamb -1.905210 time 2019-03-05 12:42:10.820720
Model ind 640 epoch 1324 head B head_i_epoch 1 batch 200: avg loss -2.011810 avg loss no lamb -2.011810 time 2019-03-05 12:43:53.355480
last batch sz 160
Pre: time 2019-03-05 12:45:27.042386: 
 	std: 0.050754905
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125167, 0.61593336, 0.61595, 0.61595, 0.5121667]
	train_accs: [0.5125167, 0.61593336, 0.61595, 0.61595, 0.5121667]
	best_train_sub_head: 2
	worst: 0.5121667
	avg: 0.57450336
	best: 0.61595

Starting e_i: 1325
Model ind 640 epoch 1325 head A head_i_epoch 0 batch 0: avg loss -3.323129 avg loss no lamb -3.323129 time 2019-03-05 12:45:30.095979
Model ind 640 epoch 1325 head A head_i_epoch 0 batch 100: avg loss -3.325060 avg loss no lamb -3.325060 time 2019-03-05 12:47:12.339556
Model ind 640 epoch 1325 head A head_i_epoch 0 batch 200: avg loss -3.352419 avg loss no lamb -3.352419 time 2019-03-05 12:48:54.639192
last batch sz 160
Model ind 640 epoch 1325 head B head_i_epoch 0 batch 0: avg loss -1.921856 avg loss no lamb -1.921856 time 2019-03-05 12:50:09.087925
Model ind 640 epoch 1325 head B head_i_epoch 0 batch 100: avg loss -1.903550 avg loss no lamb -1.903550 time 2019-03-05 12:51:51.671834
Model ind 640 epoch 1325 head B head_i_epoch 0 batch 200: avg loss -1.972002 avg loss no lamb -1.972002 time 2019-03-05 12:53:33.429285
last batch sz 160
Model ind 640 epoch 1325 head B head_i_epoch 1 batch 0: avg loss -1.987246 avg loss no lamb -1.987246 time 2019-03-05 12:54:46.291999
Model ind 640 epoch 1325 head B head_i_epoch 1 batch 100: avg loss -1.966377 avg loss no lamb -1.966377 time 2019-03-05 12:56:30.329073
Model ind 640 epoch 1325 head B head_i_epoch 1 batch 200: avg loss -2.051204 avg loss no lamb -2.051204 time 2019-03-05 12:58:12.911100
last batch sz 160
Pre: time 2019-03-05 12:59:47.085317: 
 	std: 0.050405122
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51315, 0.6160333, 0.61591667, 0.61616665, 0.51315]
	train_accs: [0.51315, 0.6160333, 0.61591667, 0.61616665, 0.51315]
	best_train_sub_head: 3
	worst: 0.51315
	avg: 0.57488334
	best: 0.61616665

Starting e_i: 1326
Model ind 640 epoch 1326 head A head_i_epoch 0 batch 0: avg loss -3.240797 avg loss no lamb -3.240797 time 2019-03-05 12:59:49.667362
Model ind 640 epoch 1326 head A head_i_epoch 0 batch 100: avg loss -3.239756 avg loss no lamb -3.239756 time 2019-03-05 13:01:33.790567
Model ind 640 epoch 1326 head A head_i_epoch 0 batch 200: avg loss -3.424452 avg loss no lamb -3.424452 time 2019-03-05 13:03:17.370443
last batch sz 160
Model ind 640 epoch 1326 head B head_i_epoch 0 batch 0: avg loss -1.965908 avg loss no lamb -1.965908 time 2019-03-05 13:04:32.646834
Model ind 640 epoch 1326 head B head_i_epoch 0 batch 100: avg loss -1.912622 avg loss no lamb -1.912622 time 2019-03-05 13:06:16.042605
Model ind 640 epoch 1326 head B head_i_epoch 0 batch 200: avg loss -2.027791 avg loss no lamb -2.027791 time 2019-03-05 13:07:58.909495
last batch sz 160
Model ind 640 epoch 1326 head B head_i_epoch 1 batch 0: avg loss -1.954836 avg loss no lamb -1.954836 time 2019-03-05 13:09:13.180859
Model ind 640 epoch 1326 head B head_i_epoch 1 batch 100: avg loss -1.891403 avg loss no lamb -1.891403 time 2019-03-05 13:10:53.842495
Model ind 640 epoch 1326 head B head_i_epoch 1 batch 200: avg loss -1.962744 avg loss no lamb -1.962744 time 2019-03-05 13:12:38.057828
last batch sz 160
Pre: time 2019-03-05 13:14:12.336358: 
 	std: 0.05070174
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5117, 0.61525, 0.6152667, 0.6152167, 0.5118]
	train_accs: [0.5117, 0.61525, 0.6152667, 0.6152167, 0.5118]
	best_train_sub_head: 2
	worst: 0.5117
	avg: 0.5738467
	best: 0.6152667

Starting e_i: 1327
Model ind 640 epoch 1327 head A head_i_epoch 0 batch 0: avg loss -3.323578 avg loss no lamb -3.323578 time 2019-03-05 13:14:14.870886
Model ind 640 epoch 1327 head A head_i_epoch 0 batch 100: avg loss -3.240455 avg loss no lamb -3.240455 time 2019-03-05 13:15:57.354235
Model ind 640 epoch 1327 head A head_i_epoch 0 batch 200: avg loss -3.299954 avg loss no lamb -3.299954 time 2019-03-05 13:17:40.055327
last batch sz 160
Model ind 640 epoch 1327 head B head_i_epoch 0 batch 0: avg loss -1.972643 avg loss no lamb -1.972643 time 2019-03-05 13:18:55.852563
Model ind 640 epoch 1327 head B head_i_epoch 0 batch 100: avg loss -1.820889 avg loss no lamb -1.820889 time 2019-03-05 13:20:39.513557
Model ind 640 epoch 1327 head B head_i_epoch 0 batch 200: avg loss -2.012791 avg loss no lamb -2.012791 time 2019-03-05 13:22:22.492751
last batch sz 160
Model ind 640 epoch 1327 head B head_i_epoch 1 batch 0: avg loss -1.920416 avg loss no lamb -1.920416 time 2019-03-05 13:23:37.142902
Model ind 640 epoch 1327 head B head_i_epoch 1 batch 100: avg loss -1.891026 avg loss no lamb -1.891026 time 2019-03-05 13:25:19.684737
Model ind 640 epoch 1327 head B head_i_epoch 1 batch 200: avg loss -2.017467 avg loss no lamb -2.017467 time 2019-03-05 13:27:00.485089
last batch sz 160
Pre: time 2019-03-05 13:28:37.356959: 
 	std: 0.050769843
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51128334, 0.6148, 0.61473334, 0.61476666, 0.51098335]
	train_accs: [0.51128334, 0.6148, 0.61473334, 0.61476666, 0.51098335]
	best_train_sub_head: 1
	worst: 0.51098335
	avg: 0.57331336
	best: 0.6148

Starting e_i: 1328
Model ind 640 epoch 1328 head A head_i_epoch 0 batch 0: avg loss -3.312380 avg loss no lamb -3.312380 time 2019-03-05 13:28:40.939998
Model ind 640 epoch 1328 head A head_i_epoch 0 batch 100: avg loss -3.259886 avg loss no lamb -3.259886 time 2019-03-05 13:30:24.371274
Model ind 640 epoch 1328 head A head_i_epoch 0 batch 200: avg loss -3.291206 avg loss no lamb -3.291206 time 2019-03-05 13:32:07.161041
last batch sz 160
Model ind 640 epoch 1328 head B head_i_epoch 0 batch 0: avg loss -2.070460 avg loss no lamb -2.070460 time 2019-03-05 13:33:22.379194
Model ind 640 epoch 1328 head B head_i_epoch 0 batch 100: avg loss -1.929188 avg loss no lamb -1.929188 time 2019-03-05 13:35:05.916854
Model ind 640 epoch 1328 head B head_i_epoch 0 batch 200: avg loss -2.063695 avg loss no lamb -2.063695 time 2019-03-05 13:36:51.067461
last batch sz 160
Model ind 640 epoch 1328 head B head_i_epoch 1 batch 0: avg loss -1.990071 avg loss no lamb -1.990071 time 2019-03-05 13:38:07.329678
Model ind 640 epoch 1328 head B head_i_epoch 1 batch 100: avg loss -1.905937 avg loss no lamb -1.905937 time 2019-03-05 13:39:51.841620
Model ind 640 epoch 1328 head B head_i_epoch 1 batch 200: avg loss -1.901402 avg loss no lamb -1.901402 time 2019-03-05 13:41:34.580940
last batch sz 160
Pre: time 2019-03-05 13:43:08.483941: 
 	std: 0.050595593
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5108333, 0.61411667, 0.614, 0.61411667, 0.5107667]
	train_accs: [0.5108333, 0.61411667, 0.614, 0.61411667, 0.5107667]
	best_train_sub_head: 1
	worst: 0.5107667
	avg: 0.57276666
	best: 0.61411667

Starting e_i: 1329
Model ind 640 epoch 1329 head A head_i_epoch 0 batch 0: avg loss -3.374786 avg loss no lamb -3.374786 time 2019-03-05 13:43:11.005385
Model ind 640 epoch 1329 head A head_i_epoch 0 batch 100: avg loss -3.289867 avg loss no lamb -3.289867 time 2019-03-05 13:44:55.929593
Model ind 640 epoch 1329 head A head_i_epoch 0 batch 200: avg loss -3.330484 avg loss no lamb -3.330484 time 2019-03-05 13:46:39.023894
last batch sz 160
Model ind 640 epoch 1329 head B head_i_epoch 0 batch 0: avg loss -1.988776 avg loss no lamb -1.988776 time 2019-03-05 13:47:53.828978
Model ind 640 epoch 1329 head B head_i_epoch 0 batch 100: avg loss -1.991443 avg loss no lamb -1.991443 time 2019-03-05 13:49:38.279207
Model ind 640 epoch 1329 head B head_i_epoch 0 batch 200: avg loss -2.049828 avg loss no lamb -2.049828 time 2019-03-05 13:51:22.226551
last batch sz 160
Model ind 640 epoch 1329 head B head_i_epoch 1 batch 0: avg loss -1.963415 avg loss no lamb -1.963415 time 2019-03-05 13:52:37.751694
Model ind 640 epoch 1329 head B head_i_epoch 1 batch 100: avg loss -1.968052 avg loss no lamb -1.968052 time 2019-03-05 13:54:23.263351
Model ind 640 epoch 1329 head B head_i_epoch 1 batch 200: avg loss -2.005484 avg loss no lamb -2.005484 time 2019-03-05 13:56:06.691957
last batch sz 160
Pre: time 2019-03-05 13:57:39.352701: 
 	std: 0.05069494
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51115, 0.61468333, 0.61465, 0.61473334, 0.51126665]
	train_accs: [0.51115, 0.61468333, 0.61465, 0.61473334, 0.51126665]
	best_train_sub_head: 3
	worst: 0.51115
	avg: 0.57329667
	best: 0.61473334

Starting e_i: 1330
Model ind 640 epoch 1330 head A head_i_epoch 0 batch 0: avg loss -3.254080 avg loss no lamb -3.254080 time 2019-03-05 13:57:41.941525
Model ind 640 epoch 1330 head A head_i_epoch 0 batch 100: avg loss -3.236776 avg loss no lamb -3.236776 time 2019-03-05 13:59:24.523310
Model ind 640 epoch 1330 head A head_i_epoch 0 batch 200: avg loss -3.319387 avg loss no lamb -3.319387 time 2019-03-05 14:01:08.343113
last batch sz 160
Model ind 640 epoch 1330 head B head_i_epoch 0 batch 0: avg loss -1.992638 avg loss no lamb -1.992638 time 2019-03-05 14:02:23.110478
Model ind 640 epoch 1330 head B head_i_epoch 0 batch 100: avg loss -1.920641 avg loss no lamb -1.920641 time 2019-03-05 14:04:05.682110
Model ind 640 epoch 1330 head B head_i_epoch 0 batch 200: avg loss -1.964331 avg loss no lamb -1.964331 time 2019-03-05 14:05:49.163411
last batch sz 160
Model ind 640 epoch 1330 head B head_i_epoch 1 batch 0: avg loss -1.970713 avg loss no lamb -1.970713 time 2019-03-05 14:07:04.177223
Model ind 640 epoch 1330 head B head_i_epoch 1 batch 100: avg loss -1.965435 avg loss no lamb -1.965435 time 2019-03-05 14:08:46.908891
Model ind 640 epoch 1330 head B head_i_epoch 1 batch 200: avg loss -2.030549 avg loss no lamb -2.030549 time 2019-03-05 14:10:29.400584
last batch sz 160
Pre: time 2019-03-05 14:12:03.879863: 
 	std: 0.05058746
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114167, 0.61468333, 0.61483335, 0.61476666, 0.5115833]
	train_accs: [0.5114167, 0.61468333, 0.61483335, 0.61476666, 0.5115833]
	best_train_sub_head: 2
	worst: 0.5114167
	avg: 0.57345665
	best: 0.61483335

Starting e_i: 1331
Model ind 640 epoch 1331 head A head_i_epoch 0 batch 0: avg loss -3.302008 avg loss no lamb -3.302008 time 2019-03-05 14:12:11.688442
Model ind 640 epoch 1331 head A head_i_epoch 0 batch 100: avg loss -3.259766 avg loss no lamb -3.259766 time 2019-03-05 14:13:52.522829
Model ind 640 epoch 1331 head A head_i_epoch 0 batch 200: avg loss -3.296188 avg loss no lamb -3.296188 time 2019-03-05 14:15:37.070268
last batch sz 160
Model ind 640 epoch 1331 head B head_i_epoch 0 batch 0: avg loss -1.894621 avg loss no lamb -1.894621 time 2019-03-05 14:16:52.483085
Model ind 640 epoch 1331 head B head_i_epoch 0 batch 100: avg loss -1.897065 avg loss no lamb -1.897065 time 2019-03-05 14:18:35.165770
Model ind 640 epoch 1331 head B head_i_epoch 0 batch 200: avg loss -1.989645 avg loss no lamb -1.989645 time 2019-03-05 14:20:17.818745
last batch sz 160
Model ind 640 epoch 1331 head B head_i_epoch 1 batch 0: avg loss -1.958234 avg loss no lamb -1.958234 time 2019-03-05 14:21:32.864153
Model ind 640 epoch 1331 head B head_i_epoch 1 batch 100: avg loss -1.957652 avg loss no lamb -1.957652 time 2019-03-05 14:23:15.740174
Model ind 640 epoch 1331 head B head_i_epoch 1 batch 200: avg loss -2.047186 avg loss no lamb -2.047186 time 2019-03-05 14:24:58.381281
last batch sz 160
Pre: time 2019-03-05 14:26:32.535463: 
 	std: 0.050700393
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51115, 0.61478335, 0.61466664, 0.61465, 0.51126665]
	train_accs: [0.51115, 0.61478335, 0.61466664, 0.61465, 0.51126665]
	best_train_sub_head: 1
	worst: 0.51115
	avg: 0.57330334
	best: 0.61478335

Starting e_i: 1332
Model ind 640 epoch 1332 head A head_i_epoch 0 batch 0: avg loss -3.308537 avg loss no lamb -3.308537 time 2019-03-05 14:26:35.207759
Model ind 640 epoch 1332 head A head_i_epoch 0 batch 100: avg loss -3.283474 avg loss no lamb -3.283474 time 2019-03-05 14:28:17.698962
Model ind 640 epoch 1332 head A head_i_epoch 0 batch 200: avg loss -3.316282 avg loss no lamb -3.316282 time 2019-03-05 14:29:58.563340
last batch sz 160
Model ind 640 epoch 1332 head B head_i_epoch 0 batch 0: avg loss -1.997338 avg loss no lamb -1.997338 time 2019-03-05 14:31:13.343114
Model ind 640 epoch 1332 head B head_i_epoch 0 batch 100: avg loss -1.990330 avg loss no lamb -1.990330 time 2019-03-05 14:32:57.604639
Model ind 640 epoch 1332 head B head_i_epoch 0 batch 200: avg loss -2.027816 avg loss no lamb -2.027816 time 2019-03-05 14:34:40.276439
last batch sz 160
Model ind 640 epoch 1332 head B head_i_epoch 1 batch 0: avg loss -1.992765 avg loss no lamb -1.992765 time 2019-03-05 14:35:54.872281
Model ind 640 epoch 1332 head B head_i_epoch 1 batch 100: avg loss -1.982179 avg loss no lamb -1.982179 time 2019-03-05 14:37:37.688763
Model ind 640 epoch 1332 head B head_i_epoch 1 batch 200: avg loss -2.051366 avg loss no lamb -2.051366 time 2019-03-05 14:39:21.196357
last batch sz 160
Pre: time 2019-03-05 14:40:55.289338: 
 	std: 0.050748
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111, 0.6146333, 0.6146, 0.6146333, 0.51096666]
	train_accs: [0.5111, 0.6146333, 0.6146, 0.6146333, 0.51096666]
	best_train_sub_head: 1
	worst: 0.51096666
	avg: 0.5731867
	best: 0.6146333

Starting e_i: 1333
Model ind 640 epoch 1333 head A head_i_epoch 0 batch 0: avg loss -3.342930 avg loss no lamb -3.342930 time 2019-03-05 14:40:57.900204
Model ind 640 epoch 1333 head A head_i_epoch 0 batch 100: avg loss -3.303909 avg loss no lamb -3.303909 time 2019-03-05 14:42:40.531376
Model ind 640 epoch 1333 head A head_i_epoch 0 batch 200: avg loss -3.378078 avg loss no lamb -3.378078 time 2019-03-05 14:44:23.105171
last batch sz 160
Model ind 640 epoch 1333 head B head_i_epoch 0 batch 0: avg loss -1.960460 avg loss no lamb -1.960460 time 2019-03-05 14:45:36.256542
Model ind 640 epoch 1333 head B head_i_epoch 0 batch 100: avg loss -1.917234 avg loss no lamb -1.917234 time 2019-03-05 14:47:20.794129
Model ind 640 epoch 1333 head B head_i_epoch 0 batch 200: avg loss -1.973304 avg loss no lamb -1.973304 time 2019-03-05 14:49:04.234986
last batch sz 160
Model ind 640 epoch 1333 head B head_i_epoch 1 batch 0: avg loss -1.960305 avg loss no lamb -1.960305 time 2019-03-05 14:50:19.095226
Model ind 640 epoch 1333 head B head_i_epoch 1 batch 100: avg loss -1.965707 avg loss no lamb -1.965707 time 2019-03-05 14:52:01.722741
Model ind 640 epoch 1333 head B head_i_epoch 1 batch 200: avg loss -2.087546 avg loss no lamb -2.087546 time 2019-03-05 14:53:44.911640
last batch sz 160
Pre: time 2019-03-05 14:55:18.912713: 
 	std: 0.05112771
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51121664, 0.61543334, 0.6156, 0.61548334, 0.5110667]
	train_accs: [0.51121664, 0.61543334, 0.6156, 0.61548334, 0.5110667]
	best_train_sub_head: 2
	worst: 0.5110667
	avg: 0.57376
	best: 0.6156

Starting e_i: 1334
Model ind 640 epoch 1334 head A head_i_epoch 0 batch 0: avg loss -3.330751 avg loss no lamb -3.330751 time 2019-03-05 14:55:22.875018
Model ind 640 epoch 1334 head A head_i_epoch 0 batch 100: avg loss -3.219004 avg loss no lamb -3.219004 time 2019-03-05 14:57:05.971204
Model ind 640 epoch 1334 head A head_i_epoch 0 batch 200: avg loss -3.312592 avg loss no lamb -3.312592 time 2019-03-05 14:58:48.982740
last batch sz 160
Model ind 640 epoch 1334 head B head_i_epoch 0 batch 0: avg loss -2.017277 avg loss no lamb -2.017277 time 2019-03-05 15:00:03.965084
Model ind 640 epoch 1334 head B head_i_epoch 0 batch 100: avg loss -1.957970 avg loss no lamb -1.957970 time 2019-03-05 15:01:44.737407
Model ind 640 epoch 1334 head B head_i_epoch 0 batch 200: avg loss -1.953517 avg loss no lamb -1.953517 time 2019-03-05 15:03:28.874710
last batch sz 160
Model ind 640 epoch 1334 head B head_i_epoch 1 batch 0: avg loss -1.900092 avg loss no lamb -1.900092 time 2019-03-05 15:04:43.968775
Model ind 640 epoch 1334 head B head_i_epoch 1 batch 100: avg loss -1.927572 avg loss no lamb -1.927572 time 2019-03-05 15:06:26.671399
Model ind 640 epoch 1334 head B head_i_epoch 1 batch 200: avg loss -2.018571 avg loss no lamb -2.018571 time 2019-03-05 15:08:09.681497
last batch sz 160
Pre: time 2019-03-05 15:09:45.134255: 
 	std: 0.050664995
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51196665, 0.6153167, 0.61536664, 0.6153, 0.51185]
	train_accs: [0.51196665, 0.6153167, 0.61536664, 0.6153, 0.51185]
	best_train_sub_head: 2
	worst: 0.51185
	avg: 0.57396
	best: 0.61536664

Starting e_i: 1335
Model ind 640 epoch 1335 head A head_i_epoch 0 batch 0: avg loss -3.315173 avg loss no lamb -3.315173 time 2019-03-05 15:09:47.849034
Model ind 640 epoch 1335 head A head_i_epoch 0 batch 100: avg loss -3.241724 avg loss no lamb -3.241724 time 2019-03-05 15:11:30.496858
Model ind 640 epoch 1335 head A head_i_epoch 0 batch 200: avg loss -3.335403 avg loss no lamb -3.335403 time 2019-03-05 15:13:13.525388
last batch sz 160
Model ind 640 epoch 1335 head B head_i_epoch 0 batch 0: avg loss -1.994403 avg loss no lamb -1.994403 time 2019-03-05 15:14:28.464775
Model ind 640 epoch 1335 head B head_i_epoch 0 batch 100: avg loss -1.911504 avg loss no lamb -1.911504 time 2019-03-05 15:16:11.088516
Model ind 640 epoch 1335 head B head_i_epoch 0 batch 200: avg loss -2.054214 avg loss no lamb -2.054214 time 2019-03-05 15:17:54.518575
last batch sz 160
Model ind 640 epoch 1335 head B head_i_epoch 1 batch 0: avg loss -1.951899 avg loss no lamb -1.951899 time 2019-03-05 15:19:10.079342
Model ind 640 epoch 1335 head B head_i_epoch 1 batch 100: avg loss -1.970625 avg loss no lamb -1.970625 time 2019-03-05 15:20:53.086108
Model ind 640 epoch 1335 head B head_i_epoch 1 batch 200: avg loss -1.965968 avg loss no lamb -1.965968 time 2019-03-05 15:22:36.459947
last batch sz 160
Pre: time 2019-03-05 15:24:10.674741: 
 	std: 0.050757736
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51236665, 0.6157, 0.61575, 0.6158, 0.51191664]
	train_accs: [0.51236665, 0.6157, 0.61575, 0.6158, 0.51191664]
	best_train_sub_head: 3
	worst: 0.51191664
	avg: 0.57430667
	best: 0.6158

Starting e_i: 1336
Model ind 640 epoch 1336 head A head_i_epoch 0 batch 0: avg loss -3.319851 avg loss no lamb -3.319851 time 2019-03-05 15:24:13.442342
Model ind 640 epoch 1336 head A head_i_epoch 0 batch 100: avg loss -3.218613 avg loss no lamb -3.218613 time 2019-03-05 15:25:57.678950
Model ind 640 epoch 1336 head A head_i_epoch 0 batch 200: avg loss -3.365430 avg loss no lamb -3.365430 time 2019-03-05 15:27:40.895695
last batch sz 160
Model ind 640 epoch 1336 head B head_i_epoch 0 batch 0: avg loss -1.971174 avg loss no lamb -1.971174 time 2019-03-05 15:28:56.579115
Model ind 640 epoch 1336 head B head_i_epoch 0 batch 100: avg loss -1.909864 avg loss no lamb -1.909864 time 2019-03-05 15:30:40.133836
Model ind 640 epoch 1336 head B head_i_epoch 0 batch 200: avg loss -1.991320 avg loss no lamb -1.991320 time 2019-03-05 15:32:22.082635
last batch sz 160
Model ind 640 epoch 1336 head B head_i_epoch 1 batch 0: avg loss -1.959958 avg loss no lamb -1.959958 time 2019-03-05 15:33:35.219421
Model ind 640 epoch 1336 head B head_i_epoch 1 batch 100: avg loss -1.950310 avg loss no lamb -1.950310 time 2019-03-05 15:35:19.257750
Model ind 640 epoch 1336 head B head_i_epoch 1 batch 200: avg loss -1.981880 avg loss no lamb -1.981880 time 2019-03-05 15:37:02.785164
last batch sz 160
Pre: time 2019-03-05 15:38:36.832499: 
 	std: 0.050719418
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51225, 0.6157167, 0.61576664, 0.6156833, 0.51213336]
	train_accs: [0.51225, 0.6157167, 0.61576664, 0.6156833, 0.51213336]
	best_train_sub_head: 2
	worst: 0.51213336
	avg: 0.57430995
	best: 0.61576664

Starting e_i: 1337
Model ind 640 epoch 1337 head A head_i_epoch 0 batch 0: avg loss -3.321690 avg loss no lamb -3.321690 time 2019-03-05 15:38:40.411097
Model ind 640 epoch 1337 head A head_i_epoch 0 batch 100: avg loss -3.246377 avg loss no lamb -3.246377 time 2019-03-05 15:40:23.483397
Model ind 640 epoch 1337 head A head_i_epoch 0 batch 200: avg loss -3.338710 avg loss no lamb -3.338710 time 2019-03-05 15:42:07.229911
last batch sz 160
Model ind 640 epoch 1337 head B head_i_epoch 0 batch 0: avg loss -1.924636 avg loss no lamb -1.924636 time 2019-03-05 15:43:22.595249
Model ind 640 epoch 1337 head B head_i_epoch 0 batch 100: avg loss -1.904331 avg loss no lamb -1.904331 time 2019-03-05 15:45:06.238564
Model ind 640 epoch 1337 head B head_i_epoch 0 batch 200: avg loss -1.946265 avg loss no lamb -1.946265 time 2019-03-05 15:46:49.251604
last batch sz 160
Model ind 640 epoch 1337 head B head_i_epoch 1 batch 0: avg loss -1.926514 avg loss no lamb -1.926514 time 2019-03-05 15:48:02.843491
Model ind 640 epoch 1337 head B head_i_epoch 1 batch 100: avg loss -1.875231 avg loss no lamb -1.875231 time 2019-03-05 15:49:44.529042
Model ind 640 epoch 1337 head B head_i_epoch 1 batch 200: avg loss -1.942312 avg loss no lamb -1.942312 time 2019-03-05 15:51:27.970936
last batch sz 160
Pre: time 2019-03-05 15:53:02.132584: 
 	std: 0.05073059
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51171666, 0.6149667, 0.615, 0.61501664, 0.5111667]
	train_accs: [0.51171666, 0.6149667, 0.615, 0.61501664, 0.5111667]
	best_train_sub_head: 3
	worst: 0.5111667
	avg: 0.5735733
	best: 0.61501664

Starting e_i: 1338
Model ind 640 epoch 1338 head A head_i_epoch 0 batch 0: avg loss -3.331503 avg loss no lamb -3.331503 time 2019-03-05 15:53:04.788568
Model ind 640 epoch 1338 head A head_i_epoch 0 batch 100: avg loss -3.232557 avg loss no lamb -3.232557 time 2019-03-05 15:54:47.405555
Model ind 640 epoch 1338 head A head_i_epoch 0 batch 200: avg loss -3.357375 avg loss no lamb -3.357375 time 2019-03-05 15:56:30.684022
last batch sz 160
Model ind 640 epoch 1338 head B head_i_epoch 0 batch 0: avg loss -2.008402 avg loss no lamb -2.008402 time 2019-03-05 15:57:45.930994
Model ind 640 epoch 1338 head B head_i_epoch 0 batch 100: avg loss -1.875095 avg loss no lamb -1.875095 time 2019-03-05 15:59:29.572640
Model ind 640 epoch 1338 head B head_i_epoch 0 batch 200: avg loss -1.946868 avg loss no lamb -1.946868 time 2019-03-05 16:01:12.786673
last batch sz 160
Model ind 640 epoch 1338 head B head_i_epoch 1 batch 0: avg loss -1.911467 avg loss no lamb -1.911467 time 2019-03-05 16:02:27.969603
Model ind 640 epoch 1338 head B head_i_epoch 1 batch 100: avg loss -1.886734 avg loss no lamb -1.886734 time 2019-03-05 16:04:09.660093
Model ind 640 epoch 1338 head B head_i_epoch 1 batch 200: avg loss -2.024618 avg loss no lamb -2.024618 time 2019-03-05 16:05:51.745047
last batch sz 160
Pre: time 2019-03-05 16:07:28.080135: 
 	std: 0.05053712
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51255, 0.6156833, 0.61555, 0.6155667, 0.51233333]
	train_accs: [0.51255, 0.6156833, 0.61555, 0.6155667, 0.51233333]
	best_train_sub_head: 1
	worst: 0.51233333
	avg: 0.57433665
	best: 0.6156833

Starting e_i: 1339
Model ind 640 epoch 1339 head A head_i_epoch 0 batch 0: avg loss -3.314835 avg loss no lamb -3.314835 time 2019-03-05 16:07:30.869349
Model ind 640 epoch 1339 head A head_i_epoch 0 batch 100: avg loss -3.317577 avg loss no lamb -3.317577 time 2019-03-05 16:09:15.696090
Model ind 640 epoch 1339 head A head_i_epoch 0 batch 200: avg loss -3.327495 avg loss no lamb -3.327495 time 2019-03-05 16:10:59.422346
last batch sz 160
Model ind 640 epoch 1339 head B head_i_epoch 0 batch 0: avg loss -1.945931 avg loss no lamb -1.945931 time 2019-03-05 16:12:14.188139
Model ind 640 epoch 1339 head B head_i_epoch 0 batch 100: avg loss -1.851645 avg loss no lamb -1.851645 time 2019-03-05 16:13:56.975599
Model ind 640 epoch 1339 head B head_i_epoch 0 batch 200: avg loss -2.049681 avg loss no lamb -2.049681 time 2019-03-05 16:15:39.728493
last batch sz 160
Model ind 640 epoch 1339 head B head_i_epoch 1 batch 0: avg loss -1.921313 avg loss no lamb -1.921313 time 2019-03-05 16:16:54.234753
Model ind 640 epoch 1339 head B head_i_epoch 1 batch 100: avg loss -1.916337 avg loss no lamb -1.916337 time 2019-03-05 16:18:37.709326
Model ind 640 epoch 1339 head B head_i_epoch 1 batch 200: avg loss -1.985697 avg loss no lamb -1.985697 time 2019-03-05 16:20:19.365708
last batch sz 160
Pre: time 2019-03-05 16:21:53.419038: 
 	std: 0.05073848
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5110667, 0.61468333, 0.6146167, 0.6145333, 0.51101667]
	train_accs: [0.5110667, 0.61468333, 0.6146167, 0.6145333, 0.51101667]
	best_train_sub_head: 1
	worst: 0.51101667
	avg: 0.5731833
	best: 0.61468333

Starting e_i: 1340
Model ind 640 epoch 1340 head A head_i_epoch 0 batch 0: avg loss -3.257585 avg loss no lamb -3.257585 time 2019-03-05 16:21:56.903409
Model ind 640 epoch 1340 head A head_i_epoch 0 batch 100: avg loss -3.272242 avg loss no lamb -3.272242 time 2019-03-05 16:23:41.589172
Model ind 640 epoch 1340 head A head_i_epoch 0 batch 200: avg loss -3.284888 avg loss no lamb -3.284888 time 2019-03-05 16:25:25.288314
last batch sz 160
Model ind 640 epoch 1340 head B head_i_epoch 0 batch 0: avg loss -2.022005 avg loss no lamb -2.022005 time 2019-03-05 16:26:39.974664
Model ind 640 epoch 1340 head B head_i_epoch 0 batch 100: avg loss -1.884172 avg loss no lamb -1.884172 time 2019-03-05 16:28:23.080921
Model ind 640 epoch 1340 head B head_i_epoch 0 batch 200: avg loss -2.004927 avg loss no lamb -2.004927 time 2019-03-05 16:30:07.364637
last batch sz 160
Model ind 640 epoch 1340 head B head_i_epoch 1 batch 0: avg loss -1.970730 avg loss no lamb -1.970730 time 2019-03-05 16:31:22.462586
Model ind 640 epoch 1340 head B head_i_epoch 1 batch 100: avg loss -1.932811 avg loss no lamb -1.932811 time 2019-03-05 16:33:05.417557
Model ind 640 epoch 1340 head B head_i_epoch 1 batch 200: avg loss -1.992680 avg loss no lamb -1.992680 time 2019-03-05 16:34:49.077535
last batch sz 160
Pre: time 2019-03-05 16:36:21.314587: 
 	std: 0.050614633
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5117667, 0.61505, 0.61501664, 0.61513335, 0.51173335]
	train_accs: [0.5117667, 0.61505, 0.61501664, 0.61513335, 0.51173335]
	best_train_sub_head: 3
	worst: 0.51173335
	avg: 0.57374
	best: 0.61513335

Starting e_i: 1341
Model ind 640 epoch 1341 head A head_i_epoch 0 batch 0: avg loss -3.208462 avg loss no lamb -3.208462 time 2019-03-05 16:36:27.222409
Model ind 640 epoch 1341 head A head_i_epoch 0 batch 100: avg loss -3.319561 avg loss no lamb -3.319561 time 2019-03-05 16:38:11.097146
Model ind 640 epoch 1341 head A head_i_epoch 0 batch 200: avg loss -3.341250 avg loss no lamb -3.341250 time 2019-03-05 16:39:54.311038
last batch sz 160
Model ind 640 epoch 1341 head B head_i_epoch 0 batch 0: avg loss -1.901127 avg loss no lamb -1.901127 time 2019-03-05 16:41:09.018589
Model ind 640 epoch 1341 head B head_i_epoch 0 batch 100: avg loss -1.960608 avg loss no lamb -1.960608 time 2019-03-05 16:42:51.711565
Model ind 640 epoch 1341 head B head_i_epoch 0 batch 200: avg loss -1.988688 avg loss no lamb -1.988688 time 2019-03-05 16:44:34.428616
last batch sz 160
Model ind 640 epoch 1341 head B head_i_epoch 1 batch 0: avg loss -2.041870 avg loss no lamb -2.041870 time 2019-03-05 16:45:49.541646
Model ind 640 epoch 1341 head B head_i_epoch 1 batch 100: avg loss -1.971557 avg loss no lamb -1.971557 time 2019-03-05 16:47:34.259232
Model ind 640 epoch 1341 head B head_i_epoch 1 batch 200: avg loss -2.058363 avg loss no lamb -2.058363 time 2019-03-05 16:49:18.008085
last batch sz 160
Pre: time 2019-03-05 16:50:52.788181: 
 	std: 0.050599713
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5119, 0.6152167, 0.61505, 0.61506665, 0.51175]
	train_accs: [0.5119, 0.6152167, 0.61505, 0.61506665, 0.51175]
	best_train_sub_head: 1
	worst: 0.51175
	avg: 0.5737967
	best: 0.6152167

Starting e_i: 1342
Model ind 640 epoch 1342 head A head_i_epoch 0 batch 0: avg loss -3.269211 avg loss no lamb -3.269211 time 2019-03-05 16:50:55.390907
Model ind 640 epoch 1342 head A head_i_epoch 0 batch 100: avg loss -3.268321 avg loss no lamb -3.268321 time 2019-03-05 16:52:36.703086
Model ind 640 epoch 1342 head A head_i_epoch 0 batch 200: avg loss -3.274358 avg loss no lamb -3.274358 time 2019-03-05 16:54:21.152913
last batch sz 160
Model ind 640 epoch 1342 head B head_i_epoch 0 batch 0: avg loss -1.967849 avg loss no lamb -1.967849 time 2019-03-05 16:55:36.144815
Model ind 640 epoch 1342 head B head_i_epoch 0 batch 100: avg loss -1.950532 avg loss no lamb -1.950532 time 2019-03-05 16:57:18.911518
Model ind 640 epoch 1342 head B head_i_epoch 0 batch 200: avg loss -2.059550 avg loss no lamb -2.059550 time 2019-03-05 16:59:02.776235
last batch sz 160
Model ind 640 epoch 1342 head B head_i_epoch 1 batch 0: avg loss -1.953201 avg loss no lamb -1.953201 time 2019-03-05 17:00:18.388039
Model ind 640 epoch 1342 head B head_i_epoch 1 batch 100: avg loss -1.907910 avg loss no lamb -1.907910 time 2019-03-05 17:02:02.417758
Model ind 640 epoch 1342 head B head_i_epoch 1 batch 200: avg loss -1.978127 avg loss no lamb -1.978127 time 2019-03-05 17:03:45.765482
last batch sz 160
Pre: time 2019-03-05 17:05:22.697669: 
 	std: 0.05117822
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51165, 0.616, 0.61586666, 0.61583334, 0.51121664]
	train_accs: [0.51165, 0.616, 0.61586666, 0.61583334, 0.51121664]
	best_train_sub_head: 1
	worst: 0.51121664
	avg: 0.5741133
	best: 0.616

Starting e_i: 1343
Model ind 640 epoch 1343 head A head_i_epoch 0 batch 0: avg loss -3.336131 avg loss no lamb -3.336131 time 2019-03-05 17:05:25.454320
Model ind 640 epoch 1343 head A head_i_epoch 0 batch 100: avg loss -3.319507 avg loss no lamb -3.319507 time 2019-03-05 17:07:08.699553
Model ind 640 epoch 1343 head A head_i_epoch 0 batch 200: avg loss -3.320322 avg loss no lamb -3.320322 time 2019-03-05 17:08:49.773453
last batch sz 160
Model ind 640 epoch 1343 head B head_i_epoch 0 batch 0: avg loss -1.985775 avg loss no lamb -1.985775 time 2019-03-05 17:10:04.705257
Model ind 640 epoch 1343 head B head_i_epoch 0 batch 100: avg loss -1.923133 avg loss no lamb -1.923133 time 2019-03-05 17:11:44.525126
Model ind 640 epoch 1343 head B head_i_epoch 0 batch 200: avg loss -2.045569 avg loss no lamb -2.045569 time 2019-03-05 17:13:24.379391
last batch sz 160
Model ind 640 epoch 1343 head B head_i_epoch 1 batch 0: avg loss -1.993993 avg loss no lamb -1.993993 time 2019-03-05 17:14:37.003448
Model ind 640 epoch 1343 head B head_i_epoch 1 batch 100: avg loss -1.899096 avg loss no lamb -1.899096 time 2019-03-05 17:16:16.808319
Model ind 640 epoch 1343 head B head_i_epoch 1 batch 200: avg loss -1.976051 avg loss no lamb -1.976051 time 2019-03-05 17:17:56.675594
last batch sz 160
Pre: time 2019-03-05 17:19:27.887329: 
 	std: 0.05086367
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51165, 0.6153833, 0.61541665, 0.61545, 0.5115333]
	train_accs: [0.51165, 0.6153833, 0.61541665, 0.61545, 0.5115333]
	best_train_sub_head: 3
	worst: 0.5115333
	avg: 0.5738867
	best: 0.61545

Starting e_i: 1344
Model ind 640 epoch 1344 head A head_i_epoch 0 batch 0: avg loss -3.315222 avg loss no lamb -3.315222 time 2019-03-05 17:19:30.557272
Model ind 640 epoch 1344 head A head_i_epoch 0 batch 100: avg loss -3.262533 avg loss no lamb -3.262533 time 2019-03-05 17:21:11.284620
Model ind 640 epoch 1344 head A head_i_epoch 0 batch 200: avg loss -3.402074 avg loss no lamb -3.402074 time 2019-03-05 17:22:51.341144
last batch sz 160
Model ind 640 epoch 1344 head B head_i_epoch 0 batch 0: avg loss -1.968327 avg loss no lamb -1.968327 time 2019-03-05 17:24:04.171053
Model ind 640 epoch 1344 head B head_i_epoch 0 batch 100: avg loss -1.953405 avg loss no lamb -1.953405 time 2019-03-05 17:25:44.247899
Model ind 640 epoch 1344 head B head_i_epoch 0 batch 200: avg loss -1.922384 avg loss no lamb -1.922384 time 2019-03-05 17:27:24.225999
last batch sz 160
Model ind 640 epoch 1344 head B head_i_epoch 1 batch 0: avg loss -1.974218 avg loss no lamb -1.974218 time 2019-03-05 17:28:36.840659
Model ind 640 epoch 1344 head B head_i_epoch 1 batch 100: avg loss -1.966141 avg loss no lamb -1.966141 time 2019-03-05 17:30:16.890184
Model ind 640 epoch 1344 head B head_i_epoch 1 batch 200: avg loss -2.003602 avg loss no lamb -2.003602 time 2019-03-05 17:31:56.855896
last batch sz 160
Pre: time 2019-03-05 17:33:28.143278: 
 	std: 0.050594248
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118333, 0.6149833, 0.61516666, 0.61515, 0.5118167]
	train_accs: [0.5118333, 0.6149833, 0.61516666, 0.61515, 0.5118167]
	best_train_sub_head: 2
	worst: 0.5118167
	avg: 0.57379
	best: 0.61516666

Starting e_i: 1345
Model ind 640 epoch 1345 head A head_i_epoch 0 batch 0: avg loss -3.289779 avg loss no lamb -3.289779 time 2019-03-05 17:33:30.840448
Model ind 640 epoch 1345 head A head_i_epoch 0 batch 100: avg loss -3.314843 avg loss no lamb -3.314843 time 2019-03-05 17:35:11.236564
Model ind 640 epoch 1345 head A head_i_epoch 0 batch 200: avg loss -3.338583 avg loss no lamb -3.338583 time 2019-03-05 17:36:51.089547
last batch sz 160
Model ind 640 epoch 1345 head B head_i_epoch 0 batch 0: avg loss -1.947395 avg loss no lamb -1.947395 time 2019-03-05 17:38:03.667980
Model ind 640 epoch 1345 head B head_i_epoch 0 batch 100: avg loss -1.906335 avg loss no lamb -1.906335 time 2019-03-05 17:39:43.400672
Model ind 640 epoch 1345 head B head_i_epoch 0 batch 200: avg loss -2.066094 avg loss no lamb -2.066094 time 2019-03-05 17:41:23.195349
last batch sz 160
Model ind 640 epoch 1345 head B head_i_epoch 1 batch 0: avg loss -2.019067 avg loss no lamb -2.019067 time 2019-03-05 17:42:35.621756
Model ind 640 epoch 1345 head B head_i_epoch 1 batch 100: avg loss -1.898398 avg loss no lamb -1.898398 time 2019-03-05 17:44:15.332367
Model ind 640 epoch 1345 head B head_i_epoch 1 batch 200: avg loss -1.987406 avg loss no lamb -1.987406 time 2019-03-05 17:45:55.052599
last batch sz 160
Pre: time 2019-03-05 17:47:27.093646: 
 	std: 0.050692223
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51208335, 0.6156667, 0.6156167, 0.6156167, 0.5122333]
	train_accs: [0.51208335, 0.6156667, 0.6156167, 0.6156167, 0.5122333]
	best_train_sub_head: 1
	worst: 0.51208335
	avg: 0.57424337
	best: 0.6156667

Starting e_i: 1346
Model ind 640 epoch 1346 head A head_i_epoch 0 batch 0: avg loss -3.353622 avg loss no lamb -3.353622 time 2019-03-05 17:47:29.716838
Model ind 640 epoch 1346 head A head_i_epoch 0 batch 100: avg loss -3.179361 avg loss no lamb -3.179361 time 2019-03-05 17:49:09.895855
Model ind 640 epoch 1346 head A head_i_epoch 0 batch 200: avg loss -3.379956 avg loss no lamb -3.379956 time 2019-03-05 17:50:50.130311
last batch sz 160
Model ind 640 epoch 1346 head B head_i_epoch 0 batch 0: avg loss -1.978173 avg loss no lamb -1.978173 time 2019-03-05 17:52:02.928906
Model ind 640 epoch 1346 head B head_i_epoch 0 batch 100: avg loss -1.930192 avg loss no lamb -1.930192 time 2019-03-05 17:53:42.925730
Model ind 640 epoch 1346 head B head_i_epoch 0 batch 200: avg loss -1.980609 avg loss no lamb -1.980609 time 2019-03-05 17:55:22.917886
last batch sz 160
Model ind 640 epoch 1346 head B head_i_epoch 1 batch 0: avg loss -1.905015 avg loss no lamb -1.905015 time 2019-03-05 17:56:35.639593
Model ind 640 epoch 1346 head B head_i_epoch 1 batch 100: avg loss -1.823461 avg loss no lamb -1.823461 time 2019-03-05 17:58:15.679504
Model ind 640 epoch 1346 head B head_i_epoch 1 batch 200: avg loss -2.015250 avg loss no lamb -2.015250 time 2019-03-05 17:59:55.717670
last batch sz 160
Pre: time 2019-03-05 18:01:27.053050: 
 	std: 0.050871838
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51161665, 0.61543334, 0.61551666, 0.6154, 0.5116]
	train_accs: [0.51161665, 0.61543334, 0.61551666, 0.6154, 0.5116]
	best_train_sub_head: 2
	worst: 0.5116
	avg: 0.57391334
	best: 0.61551666

Starting e_i: 1347
Model ind 640 epoch 1347 head A head_i_epoch 0 batch 0: avg loss -3.363225 avg loss no lamb -3.363225 time 2019-03-05 18:01:29.573550
Model ind 640 epoch 1347 head A head_i_epoch 0 batch 100: avg loss -3.328922 avg loss no lamb -3.328922 time 2019-03-05 18:03:09.791937
Model ind 640 epoch 1347 head A head_i_epoch 0 batch 200: avg loss -3.356230 avg loss no lamb -3.356230 time 2019-03-05 18:04:49.885019
last batch sz 160
Model ind 640 epoch 1347 head B head_i_epoch 0 batch 0: avg loss -1.970093 avg loss no lamb -1.970093 time 2019-03-05 18:06:02.665244
Model ind 640 epoch 1347 head B head_i_epoch 0 batch 100: avg loss -1.945054 avg loss no lamb -1.945054 time 2019-03-05 18:07:42.596026
Model ind 640 epoch 1347 head B head_i_epoch 0 batch 200: avg loss -2.003711 avg loss no lamb -2.003711 time 2019-03-05 18:09:22.496317
last batch sz 160
Model ind 640 epoch 1347 head B head_i_epoch 1 batch 0: avg loss -1.996333 avg loss no lamb -1.996333 time 2019-03-05 18:10:35.099388
Model ind 640 epoch 1347 head B head_i_epoch 1 batch 100: avg loss -1.815640 avg loss no lamb -1.815640 time 2019-03-05 18:12:15.008523
Model ind 640 epoch 1347 head B head_i_epoch 1 batch 200: avg loss -2.023355 avg loss no lamb -2.023355 time 2019-03-05 18:13:54.937413
last batch sz 160
Pre: time 2019-03-05 18:15:26.185153: 
 	std: 0.051065095
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5107167, 0.61505, 0.61506665, 0.6149667, 0.51086664]
	train_accs: [0.5107167, 0.61505, 0.61506665, 0.6149667, 0.51086664]
	best_train_sub_head: 2
	worst: 0.5107167
	avg: 0.5733334
	best: 0.61506665

Starting e_i: 1348
Model ind 640 epoch 1348 head A head_i_epoch 0 batch 0: avg loss -3.340139 avg loss no lamb -3.340139 time 2019-03-05 18:15:28.672196
Model ind 640 epoch 1348 head A head_i_epoch 0 batch 100: avg loss -3.233450 avg loss no lamb -3.233450 time 2019-03-05 18:17:08.367929
Model ind 640 epoch 1348 head A head_i_epoch 0 batch 200: avg loss -3.321494 avg loss no lamb -3.321494 time 2019-03-05 18:18:49.473243
last batch sz 160
Model ind 640 epoch 1348 head B head_i_epoch 0 batch 0: avg loss -2.039943 avg loss no lamb -2.039943 time 2019-03-05 18:20:02.320638
Model ind 640 epoch 1348 head B head_i_epoch 0 batch 100: avg loss -1.900639 avg loss no lamb -1.900639 time 2019-03-05 18:21:42.388021
Model ind 640 epoch 1348 head B head_i_epoch 0 batch 200: avg loss -2.061554 avg loss no lamb -2.061554 time 2019-03-05 18:23:22.493353
last batch sz 160
Model ind 640 epoch 1348 head B head_i_epoch 1 batch 0: avg loss -2.027300 avg loss no lamb -2.027300 time 2019-03-05 18:24:35.249052
Model ind 640 epoch 1348 head B head_i_epoch 1 batch 100: avg loss -1.937337 avg loss no lamb -1.937337 time 2019-03-05 18:26:15.411176
Model ind 640 epoch 1348 head B head_i_epoch 1 batch 200: avg loss -2.040192 avg loss no lamb -2.040192 time 2019-03-05 18:27:55.460902
last batch sz 160
Pre: time 2019-03-05 18:29:26.720049: 
 	std: 0.050187383
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.512, 0.6142667, 0.61436665, 0.6144, 0.5118]
	train_accs: [0.512, 0.6142667, 0.61436665, 0.6144, 0.5118]
	best_train_sub_head: 3
	worst: 0.5118
	avg: 0.5733667
	best: 0.6144

Starting e_i: 1349
Model ind 640 epoch 1349 head A head_i_epoch 0 batch 0: avg loss -3.307628 avg loss no lamb -3.307628 time 2019-03-05 18:29:29.284401
Model ind 640 epoch 1349 head A head_i_epoch 0 batch 100: avg loss -3.305248 avg loss no lamb -3.305248 time 2019-03-05 18:31:10.028830
Model ind 640 epoch 1349 head A head_i_epoch 0 batch 200: avg loss -3.355108 avg loss no lamb -3.355108 time 2019-03-05 18:32:50.791628
last batch sz 160
Model ind 640 epoch 1349 head B head_i_epoch 0 batch 0: avg loss -2.011522 avg loss no lamb -2.011522 time 2019-03-05 18:34:03.634967
Model ind 640 epoch 1349 head B head_i_epoch 0 batch 100: avg loss -1.934774 avg loss no lamb -1.934774 time 2019-03-05 18:35:47.111514
Model ind 640 epoch 1349 head B head_i_epoch 0 batch 200: avg loss -2.026798 avg loss no lamb -2.026798 time 2019-03-05 18:37:33.754830
last batch sz 160
Model ind 640 epoch 1349 head B head_i_epoch 1 batch 0: avg loss -1.934596 avg loss no lamb -1.934596 time 2019-03-05 18:38:49.852473
Model ind 640 epoch 1349 head B head_i_epoch 1 batch 100: avg loss -1.927455 avg loss no lamb -1.927455 time 2019-03-05 18:40:29.363579
Model ind 640 epoch 1349 head B head_i_epoch 1 batch 200: avg loss -2.096685 avg loss no lamb -2.096685 time 2019-03-05 18:42:08.877778
last batch sz 160
Pre: time 2019-03-05 18:43:39.977817: 
 	std: 0.050735768
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5122833, 0.61583334, 0.61595, 0.61588335, 0.51236665]
	train_accs: [0.5122833, 0.61583334, 0.61595, 0.61588335, 0.51236665]
	best_train_sub_head: 2
	worst: 0.5122833
	avg: 0.57446337
	best: 0.61595

Starting e_i: 1350
Model ind 640 epoch 1350 head A head_i_epoch 0 batch 0: avg loss -3.204652 avg loss no lamb -3.204652 time 2019-03-05 18:43:42.464800
Model ind 640 epoch 1350 head A head_i_epoch 0 batch 100: avg loss -3.372251 avg loss no lamb -3.372251 time 2019-03-05 18:45:22.178114
Model ind 640 epoch 1350 head A head_i_epoch 0 batch 200: avg loss -3.333328 avg loss no lamb -3.333328 time 2019-03-05 18:47:02.024280
last batch sz 160
Model ind 640 epoch 1350 head B head_i_epoch 0 batch 0: avg loss -2.003955 avg loss no lamb -2.003955 time 2019-03-05 18:48:14.465865
Model ind 640 epoch 1350 head B head_i_epoch 0 batch 100: avg loss -1.907369 avg loss no lamb -1.907369 time 2019-03-05 18:49:54.532742
Model ind 640 epoch 1350 head B head_i_epoch 0 batch 200: avg loss -2.031723 avg loss no lamb -2.031723 time 2019-03-05 18:51:34.244502
last batch sz 160
Model ind 640 epoch 1350 head B head_i_epoch 1 batch 0: avg loss -1.902024 avg loss no lamb -1.902024 time 2019-03-05 18:52:46.699179
Model ind 640 epoch 1350 head B head_i_epoch 1 batch 100: avg loss -1.937398 avg loss no lamb -1.937398 time 2019-03-05 18:54:26.399150
Model ind 640 epoch 1350 head B head_i_epoch 1 batch 200: avg loss -2.091858 avg loss no lamb -2.091858 time 2019-03-05 18:56:06.090849
last batch sz 160
Pre: time 2019-03-05 18:57:37.266683: 
 	std: 0.050882734
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114167, 0.6152833, 0.6152667, 0.61541665, 0.5115]
	train_accs: [0.5114167, 0.6152833, 0.6152667, 0.61541665, 0.5115]
	best_train_sub_head: 3
	worst: 0.5114167
	avg: 0.5737766
	best: 0.61541665

Starting e_i: 1351
Model ind 640 epoch 1351 head A head_i_epoch 0 batch 0: avg loss -3.365070 avg loss no lamb -3.365070 time 2019-03-05 18:57:46.592905
Model ind 640 epoch 1351 head A head_i_epoch 0 batch 100: avg loss -3.211048 avg loss no lamb -3.211048 time 2019-03-05 18:59:26.480774
Model ind 640 epoch 1351 head A head_i_epoch 0 batch 200: avg loss -3.259824 avg loss no lamb -3.259824 time 2019-03-05 19:01:06.404117
last batch sz 160
Model ind 640 epoch 1351 head B head_i_epoch 0 batch 0: avg loss -2.073184 avg loss no lamb -2.073184 time 2019-03-05 19:02:18.991167
Model ind 640 epoch 1351 head B head_i_epoch 0 batch 100: avg loss -1.862592 avg loss no lamb -1.862592 time 2019-03-05 19:03:59.063798
Model ind 640 epoch 1351 head B head_i_epoch 0 batch 200: avg loss -2.070886 avg loss no lamb -2.070886 time 2019-03-05 19:05:38.819467
last batch sz 160
Model ind 640 epoch 1351 head B head_i_epoch 1 batch 0: avg loss -1.936012 avg loss no lamb -1.936012 time 2019-03-05 19:06:51.289118
Model ind 640 epoch 1351 head B head_i_epoch 1 batch 100: avg loss -1.954093 avg loss no lamb -1.954093 time 2019-03-05 19:08:31.047767
Model ind 640 epoch 1351 head B head_i_epoch 1 batch 200: avg loss -1.986581 avg loss no lamb -1.986581 time 2019-03-05 19:10:10.772604
last batch sz 160
Pre: time 2019-03-05 19:11:42.025645: 
 	std: 0.05028133
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51171666, 0.61446667, 0.6145667, 0.6145, 0.51203334]
	train_accs: [0.51171666, 0.61446667, 0.6145667, 0.6145, 0.51203334]
	best_train_sub_head: 2
	worst: 0.51171666
	avg: 0.57345665
	best: 0.6145667

Starting e_i: 1352
Model ind 640 epoch 1352 head A head_i_epoch 0 batch 0: avg loss -3.335496 avg loss no lamb -3.335496 time 2019-03-05 19:11:44.603226
Model ind 640 epoch 1352 head A head_i_epoch 0 batch 100: avg loss -3.305863 avg loss no lamb -3.305863 time 2019-03-05 19:13:24.508507
Model ind 640 epoch 1352 head A head_i_epoch 0 batch 200: avg loss -3.363962 avg loss no lamb -3.363962 time 2019-03-05 19:15:04.402883
last batch sz 160
Model ind 640 epoch 1352 head B head_i_epoch 0 batch 0: avg loss -1.982882 avg loss no lamb -1.982882 time 2019-03-05 19:16:16.989612
Model ind 640 epoch 1352 head B head_i_epoch 0 batch 100: avg loss -1.936712 avg loss no lamb -1.936712 time 2019-03-05 19:17:56.709357
Model ind 640 epoch 1352 head B head_i_epoch 0 batch 200: avg loss -1.953333 avg loss no lamb -1.953333 time 2019-03-05 19:19:36.358574
last batch sz 160
Model ind 640 epoch 1352 head B head_i_epoch 1 batch 0: avg loss -1.980748 avg loss no lamb -1.980748 time 2019-03-05 19:20:48.826031
Model ind 640 epoch 1352 head B head_i_epoch 1 batch 100: avg loss -1.845587 avg loss no lamb -1.845587 time 2019-03-05 19:22:28.511968
Model ind 640 epoch 1352 head B head_i_epoch 1 batch 200: avg loss -2.030737 avg loss no lamb -2.030737 time 2019-03-05 19:24:08.101337
last batch sz 160
Pre: time 2019-03-05 19:25:39.220706: 
 	std: 0.05072628
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51173335, 0.61523336, 0.61545, 0.6153, 0.5118333]
	train_accs: [0.51173335, 0.61523336, 0.61545, 0.6153, 0.5118333]
	best_train_sub_head: 2
	worst: 0.51173335
	avg: 0.57391006
	best: 0.61545

Starting e_i: 1353
Model ind 640 epoch 1353 head A head_i_epoch 0 batch 0: avg loss -3.339379 avg loss no lamb -3.339379 time 2019-03-05 19:25:41.680928
Model ind 640 epoch 1353 head A head_i_epoch 0 batch 100: avg loss -3.271111 avg loss no lamb -3.271111 time 2019-03-05 19:27:21.356003
Model ind 640 epoch 1353 head A head_i_epoch 0 batch 200: avg loss -3.337620 avg loss no lamb -3.337620 time 2019-03-05 19:29:01.050168
last batch sz 160
Model ind 640 epoch 1353 head B head_i_epoch 0 batch 0: avg loss -1.992768 avg loss no lamb -1.992768 time 2019-03-05 19:30:13.504368
Model ind 640 epoch 1353 head B head_i_epoch 0 batch 100: avg loss -1.992928 avg loss no lamb -1.992928 time 2019-03-05 19:31:52.967185
Model ind 640 epoch 1353 head B head_i_epoch 0 batch 200: avg loss -2.020675 avg loss no lamb -2.020675 time 2019-03-05 19:33:32.581663
last batch sz 160
Model ind 640 epoch 1353 head B head_i_epoch 1 batch 0: avg loss -2.003223 avg loss no lamb -2.003223 time 2019-03-05 19:34:44.925289
Model ind 640 epoch 1353 head B head_i_epoch 1 batch 100: avg loss -1.916644 avg loss no lamb -1.916644 time 2019-03-05 19:36:24.535358
Model ind 640 epoch 1353 head B head_i_epoch 1 batch 200: avg loss -2.026762 avg loss no lamb -2.026762 time 2019-03-05 19:38:04.105736
last batch sz 160
Pre: time 2019-03-05 19:39:35.172677: 
 	std: 0.0505194
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51178336, 0.6148667, 0.61481667, 0.61478335, 0.51161665]
	train_accs: [0.51178336, 0.6148667, 0.61481667, 0.61478335, 0.51161665]
	best_train_sub_head: 1
	worst: 0.51161665
	avg: 0.57357335
	best: 0.6148667

Starting e_i: 1354
Model ind 640 epoch 1354 head A head_i_epoch 0 batch 0: avg loss -3.285103 avg loss no lamb -3.285103 time 2019-03-05 19:39:38.533034
Model ind 640 epoch 1354 head A head_i_epoch 0 batch 100: avg loss -3.283182 avg loss no lamb -3.283182 time 2019-03-05 19:41:18.405776
Model ind 640 epoch 1354 head A head_i_epoch 0 batch 200: avg loss -3.335345 avg loss no lamb -3.335345 time 2019-03-05 19:42:58.278925
last batch sz 160
Model ind 640 epoch 1354 head B head_i_epoch 0 batch 0: avg loss -1.945524 avg loss no lamb -1.945524 time 2019-03-05 19:44:10.861845
Model ind 640 epoch 1354 head B head_i_epoch 0 batch 100: avg loss -1.931885 avg loss no lamb -1.931885 time 2019-03-05 19:45:50.659389
Model ind 640 epoch 1354 head B head_i_epoch 0 batch 200: avg loss -2.020180 avg loss no lamb -2.020180 time 2019-03-05 19:47:35.313810
last batch sz 160
Model ind 640 epoch 1354 head B head_i_epoch 1 batch 0: avg loss -1.954173 avg loss no lamb -1.954173 time 2019-03-05 19:48:51.753481
Model ind 640 epoch 1354 head B head_i_epoch 1 batch 100: avg loss -1.872728 avg loss no lamb -1.872728 time 2019-03-05 19:50:31.189835
Model ind 640 epoch 1354 head B head_i_epoch 1 batch 200: avg loss -2.058461 avg loss no lamb -2.058461 time 2019-03-05 19:52:10.606755
last batch sz 160
Pre: time 2019-03-05 19:53:41.613210: 
 	std: 0.05065139
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51133335, 0.6147, 0.61478335, 0.61481667, 0.5114167]
	train_accs: [0.51133335, 0.6147, 0.61478335, 0.61481667, 0.5114167]
	best_train_sub_head: 3
	worst: 0.51133335
	avg: 0.57341
	best: 0.61481667

Starting e_i: 1355
Model ind 640 epoch 1355 head A head_i_epoch 0 batch 0: avg loss -3.418495 avg loss no lamb -3.418495 time 2019-03-05 19:53:44.113517
Model ind 640 epoch 1355 head A head_i_epoch 0 batch 100: avg loss -3.318464 avg loss no lamb -3.318464 time 2019-03-05 19:55:25.517694
Model ind 640 epoch 1355 head A head_i_epoch 0 batch 200: avg loss -3.348426 avg loss no lamb -3.348426 time 2019-03-05 19:57:08.042130
last batch sz 160
Model ind 640 epoch 1355 head B head_i_epoch 0 batch 0: avg loss -1.963964 avg loss no lamb -1.963964 time 2019-03-05 19:58:24.406536
Model ind 640 epoch 1355 head B head_i_epoch 0 batch 100: avg loss -1.925805 avg loss no lamb -1.925805 time 2019-03-05 20:00:04.089547
Model ind 640 epoch 1355 head B head_i_epoch 0 batch 200: avg loss -2.007832 avg loss no lamb -2.007832 time 2019-03-05 20:01:43.521356
last batch sz 160
Model ind 640 epoch 1355 head B head_i_epoch 1 batch 0: avg loss -1.992503 avg loss no lamb -1.992503 time 2019-03-05 20:02:55.834052
Model ind 640 epoch 1355 head B head_i_epoch 1 batch 100: avg loss -1.938442 avg loss no lamb -1.938442 time 2019-03-05 20:04:35.396118
Model ind 640 epoch 1355 head B head_i_epoch 1 batch 200: avg loss -2.092093 avg loss no lamb -2.092093 time 2019-03-05 20:06:15.026718
last batch sz 160
Pre: time 2019-03-05 20:07:46.061487: 
 	std: 0.05006488
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5122, 0.61448336, 0.6145, 0.61445, 0.51236665]
	train_accs: [0.5122, 0.61448336, 0.6145, 0.61445, 0.51236665]
	best_train_sub_head: 2
	worst: 0.5122
	avg: 0.5736
	best: 0.6145

Starting e_i: 1356
Model ind 640 epoch 1356 head A head_i_epoch 0 batch 0: avg loss -3.310390 avg loss no lamb -3.310390 time 2019-03-05 20:07:48.833973
Model ind 640 epoch 1356 head A head_i_epoch 0 batch 100: avg loss -3.303357 avg loss no lamb -3.303357 time 2019-03-05 20:09:28.681700
Model ind 640 epoch 1356 head A head_i_epoch 0 batch 200: avg loss -3.383076 avg loss no lamb -3.383076 time 2019-03-05 20:11:08.529789
last batch sz 160
Model ind 640 epoch 1356 head B head_i_epoch 0 batch 0: avg loss -1.951422 avg loss no lamb -1.951422 time 2019-03-05 20:12:21.114998
Model ind 640 epoch 1356 head B head_i_epoch 0 batch 100: avg loss -1.879609 avg loss no lamb -1.879609 time 2019-03-05 20:14:00.793527
Model ind 640 epoch 1356 head B head_i_epoch 0 batch 200: avg loss -2.015622 avg loss no lamb -2.015622 time 2019-03-05 20:15:40.565282
last batch sz 160
Model ind 640 epoch 1356 head B head_i_epoch 1 batch 0: avg loss -1.983513 avg loss no lamb -1.983513 time 2019-03-05 20:16:52.989009
Model ind 640 epoch 1356 head B head_i_epoch 1 batch 100: avg loss -1.934333 avg loss no lamb -1.934333 time 2019-03-05 20:18:32.688373
Model ind 640 epoch 1356 head B head_i_epoch 1 batch 200: avg loss -1.983171 avg loss no lamb -1.983171 time 2019-03-05 20:20:12.562412
last batch sz 160
Pre: time 2019-03-05 20:21:43.844808: 
 	std: 0.050325062
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51311666, 0.61613333, 0.61611664, 0.6161, 0.5136667]
	train_accs: [0.51311666, 0.61613333, 0.61611664, 0.6161, 0.5136667]
	best_train_sub_head: 1
	worst: 0.51311666
	avg: 0.57502663
	best: 0.61613333

Starting e_i: 1357
Model ind 640 epoch 1357 head A head_i_epoch 0 batch 0: avg loss -3.351974 avg loss no lamb -3.351974 time 2019-03-05 20:21:47.254383
Model ind 640 epoch 1357 head A head_i_epoch 0 batch 100: avg loss -3.336381 avg loss no lamb -3.336381 time 2019-03-05 20:23:27.288989
Model ind 640 epoch 1357 head A head_i_epoch 0 batch 200: avg loss -3.258767 avg loss no lamb -3.258767 time 2019-03-05 20:25:07.433160
last batch sz 160
Model ind 640 epoch 1357 head B head_i_epoch 0 batch 0: avg loss -1.958361 avg loss no lamb -1.958361 time 2019-03-05 20:26:20.298657
Model ind 640 epoch 1357 head B head_i_epoch 0 batch 100: avg loss -1.860004 avg loss no lamb -1.860004 time 2019-03-05 20:28:00.363431
Model ind 640 epoch 1357 head B head_i_epoch 0 batch 200: avg loss -2.005792 avg loss no lamb -2.005792 time 2019-03-05 20:29:40.385621
last batch sz 160
Model ind 640 epoch 1357 head B head_i_epoch 1 batch 0: avg loss -2.036674 avg loss no lamb -2.036674 time 2019-03-05 20:30:53.109096
Model ind 640 epoch 1357 head B head_i_epoch 1 batch 100: avg loss -1.917632 avg loss no lamb -1.917632 time 2019-03-05 20:32:33.106928
Model ind 640 epoch 1357 head B head_i_epoch 1 batch 200: avg loss -1.970973 avg loss no lamb -1.970973 time 2019-03-05 20:34:13.127798
last batch sz 160
Pre: time 2019-03-05 20:35:48.566304: 
 	std: 0.050748
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51208335, 0.61576664, 0.6156333, 0.6157167, 0.51215]
	train_accs: [0.51208335, 0.61576664, 0.6156333, 0.6157167, 0.51215]
	best_train_sub_head: 1
	worst: 0.51208335
	avg: 0.57427
	best: 0.61576664

Starting e_i: 1358
Model ind 640 epoch 1358 head A head_i_epoch 0 batch 0: avg loss -3.340729 avg loss no lamb -3.340729 time 2019-03-05 20:35:51.093738
Model ind 640 epoch 1358 head A head_i_epoch 0 batch 100: avg loss -3.289531 avg loss no lamb -3.289531 time 2019-03-05 20:37:30.967387
Model ind 640 epoch 1358 head A head_i_epoch 0 batch 200: avg loss -3.387800 avg loss no lamb -3.387800 time 2019-03-05 20:39:10.886832
last batch sz 160
Model ind 640 epoch 1358 head B head_i_epoch 0 batch 0: avg loss -2.064817 avg loss no lamb -2.064817 time 2019-03-05 20:40:23.238526
Model ind 640 epoch 1358 head B head_i_epoch 0 batch 100: avg loss -1.933086 avg loss no lamb -1.933086 time 2019-03-05 20:42:02.704137
Model ind 640 epoch 1358 head B head_i_epoch 0 batch 200: avg loss -2.004650 avg loss no lamb -2.004650 time 2019-03-05 20:43:46.289107
last batch sz 160
Model ind 640 epoch 1358 head B head_i_epoch 1 batch 0: avg loss -2.053991 avg loss no lamb -2.053991 time 2019-03-05 20:44:59.084623
Model ind 640 epoch 1358 head B head_i_epoch 1 batch 100: avg loss -1.936281 avg loss no lamb -1.936281 time 2019-03-05 20:46:42.208954
Model ind 640 epoch 1358 head B head_i_epoch 1 batch 200: avg loss -1.994820 avg loss no lamb -1.994820 time 2019-03-05 20:48:21.886911
last batch sz 160
Pre: time 2019-03-05 20:49:56.571951: 
 	std: 0.050833724
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5104167, 0.6142167, 0.61411667, 0.6141833, 0.5104]
	train_accs: [0.5104167, 0.6142167, 0.61411667, 0.6141833, 0.5104]
	best_train_sub_head: 1
	worst: 0.5104
	avg: 0.5726667
	best: 0.6142167

Starting e_i: 1359
Model ind 640 epoch 1359 head A head_i_epoch 0 batch 0: avg loss -3.386912 avg loss no lamb -3.386912 time 2019-03-05 20:49:59.208771
Model ind 640 epoch 1359 head A head_i_epoch 0 batch 100: avg loss -3.212737 avg loss no lamb -3.212737 time 2019-03-05 20:51:42.176717
Model ind 640 epoch 1359 head A head_i_epoch 0 batch 200: avg loss -3.324910 avg loss no lamb -3.324910 time 2019-03-05 20:53:25.478509
last batch sz 160
Model ind 640 epoch 1359 head B head_i_epoch 0 batch 0: avg loss -1.940666 avg loss no lamb -1.940666 time 2019-03-05 20:54:38.067802
Model ind 640 epoch 1359 head B head_i_epoch 0 batch 100: avg loss -1.966874 avg loss no lamb -1.966874 time 2019-03-05 20:56:18.796525
Model ind 640 epoch 1359 head B head_i_epoch 0 batch 200: avg loss -1.986020 avg loss no lamb -1.986020 time 2019-03-05 20:58:04.292833
last batch sz 160
Model ind 640 epoch 1359 head B head_i_epoch 1 batch 0: avg loss -1.961654 avg loss no lamb -1.961654 time 2019-03-05 20:59:16.807061
Model ind 640 epoch 1359 head B head_i_epoch 1 batch 100: avg loss -1.912541 avg loss no lamb -1.912541 time 2019-03-05 21:00:59.208168
Model ind 640 epoch 1359 head B head_i_epoch 1 batch 200: avg loss -1.990682 avg loss no lamb -1.990682 time 2019-03-05 21:02:39.006785
last batch sz 160
Pre: time 2019-03-05 21:04:09.956555: 
 	std: 0.050684072
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51198334, 0.61548334, 0.61535, 0.6153167, 0.5118667]
	train_accs: [0.51198334, 0.61548334, 0.61535, 0.6153167, 0.5118667]
	best_train_sub_head: 1
	worst: 0.5118667
	avg: 0.574
	best: 0.61548334

Starting e_i: 1360
Model ind 640 epoch 1360 head A head_i_epoch 0 batch 0: avg loss -3.397536 avg loss no lamb -3.397536 time 2019-03-05 21:04:13.338485
Model ind 640 epoch 1360 head A head_i_epoch 0 batch 100: avg loss -3.310603 avg loss no lamb -3.310603 time 2019-03-05 21:05:53.510753
Model ind 640 epoch 1360 head A head_i_epoch 0 batch 200: avg loss -3.363915 avg loss no lamb -3.363915 time 2019-03-05 21:07:33.284483
last batch sz 160
Model ind 640 epoch 1360 head B head_i_epoch 0 batch 0: avg loss -2.072591 avg loss no lamb -2.072591 time 2019-03-05 21:08:45.777416
Model ind 640 epoch 1360 head B head_i_epoch 0 batch 100: avg loss -1.925196 avg loss no lamb -1.925196 time 2019-03-05 21:10:25.518557
Model ind 640 epoch 1360 head B head_i_epoch 0 batch 200: avg loss -1.989864 avg loss no lamb -1.989864 time 2019-03-05 21:12:05.321466
last batch sz 160
Model ind 640 epoch 1360 head B head_i_epoch 1 batch 0: avg loss -2.005068 avg loss no lamb -2.005068 time 2019-03-05 21:13:17.908093
Model ind 640 epoch 1360 head B head_i_epoch 1 batch 100: avg loss -1.829746 avg loss no lamb -1.829746 time 2019-03-05 21:14:57.779693
Model ind 640 epoch 1360 head B head_i_epoch 1 batch 200: avg loss -2.032784 avg loss no lamb -2.032784 time 2019-03-05 21:16:37.633113
last batch sz 160
Pre: time 2019-03-05 21:18:08.969943: 
 	std: 0.0504609
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51245, 0.61543334, 0.6153, 0.61545, 0.51233333]
	train_accs: [0.51245, 0.61543334, 0.6153, 0.61545, 0.51233333]
	best_train_sub_head: 3
	worst: 0.51233333
	avg: 0.57419336
	best: 0.61545

Starting e_i: 1361
Model ind 640 epoch 1361 head A head_i_epoch 0 batch 0: avg loss -3.367255 avg loss no lamb -3.367255 time 2019-03-05 21:18:16.775049
Model ind 640 epoch 1361 head A head_i_epoch 0 batch 100: avg loss -3.273281 avg loss no lamb -3.273281 time 2019-03-05 21:19:56.730631
Model ind 640 epoch 1361 head A head_i_epoch 0 batch 200: avg loss -3.231117 avg loss no lamb -3.231117 time 2019-03-05 21:21:36.641060
last batch sz 160
Model ind 640 epoch 1361 head B head_i_epoch 0 batch 0: avg loss -1.954110 avg loss no lamb -1.954110 time 2019-03-05 21:22:49.212048
Model ind 640 epoch 1361 head B head_i_epoch 0 batch 100: avg loss -1.950592 avg loss no lamb -1.950592 time 2019-03-05 21:24:28.890469
Model ind 640 epoch 1361 head B head_i_epoch 0 batch 200: avg loss -1.952915 avg loss no lamb -1.952915 time 2019-03-05 21:26:08.524860
last batch sz 160
Model ind 640 epoch 1361 head B head_i_epoch 1 batch 0: avg loss -1.972854 avg loss no lamb -1.972854 time 2019-03-05 21:27:21.092699
Model ind 640 epoch 1361 head B head_i_epoch 1 batch 100: avg loss -1.930926 avg loss no lamb -1.930926 time 2019-03-05 21:29:00.863099
Model ind 640 epoch 1361 head B head_i_epoch 1 batch 200: avg loss -1.998293 avg loss no lamb -1.998293 time 2019-03-05 21:30:40.619860
last batch sz 160
Pre: time 2019-03-05 21:32:11.896130: 
 	std: 0.050943967
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.50976664, 0.61371666, 0.6138333, 0.6138167, 0.50983334]
	train_accs: [0.50976664, 0.61371666, 0.6138333, 0.6138167, 0.50983334]
	best_train_sub_head: 2
	worst: 0.50976664
	avg: 0.5721933
	best: 0.6138333

Starting e_i: 1362
Model ind 640 epoch 1362 head A head_i_epoch 0 batch 0: avg loss -3.397303 avg loss no lamb -3.397303 time 2019-03-05 21:32:14.527420
Model ind 640 epoch 1362 head A head_i_epoch 0 batch 100: avg loss -3.250275 avg loss no lamb -3.250275 time 2019-03-05 21:33:54.952920
Model ind 640 epoch 1362 head A head_i_epoch 0 batch 200: avg loss -3.326073 avg loss no lamb -3.326073 time 2019-03-05 21:35:34.869411
last batch sz 160
Model ind 640 epoch 1362 head B head_i_epoch 0 batch 0: avg loss -2.015187 avg loss no lamb -2.015187 time 2019-03-05 21:36:47.524085
Model ind 640 epoch 1362 head B head_i_epoch 0 batch 100: avg loss -1.912015 avg loss no lamb -1.912015 time 2019-03-05 21:38:27.349597
Model ind 640 epoch 1362 head B head_i_epoch 0 batch 200: avg loss -2.058208 avg loss no lamb -2.058208 time 2019-03-05 21:40:06.684117
last batch sz 160
Model ind 640 epoch 1362 head B head_i_epoch 1 batch 0: avg loss -1.954708 avg loss no lamb -1.954708 time 2019-03-05 21:41:18.871949
Model ind 640 epoch 1362 head B head_i_epoch 1 batch 100: avg loss -1.986638 avg loss no lamb -1.986638 time 2019-03-05 21:43:02.427043
Model ind 640 epoch 1362 head B head_i_epoch 1 batch 200: avg loss -1.963282 avg loss no lamb -1.963282 time 2019-03-05 21:44:41.822215
last batch sz 160
Pre: time 2019-03-05 21:46:14.073980: 
 	std: 0.050760247
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5121667, 0.61581665, 0.61581665, 0.6156833, 0.51215]
	train_accs: [0.5121667, 0.61581665, 0.61581665, 0.6156833, 0.51215]
	best_train_sub_head: 1
	worst: 0.51215
	avg: 0.57432663
	best: 0.61581665

Starting e_i: 1363
Model ind 640 epoch 1363 head A head_i_epoch 0 batch 0: avg loss -3.333656 avg loss no lamb -3.333656 time 2019-03-05 21:46:18.267632
Model ind 640 epoch 1363 head A head_i_epoch 0 batch 100: avg loss -3.240472 avg loss no lamb -3.240472 time 2019-03-05 21:48:00.656702
Model ind 640 epoch 1363 head A head_i_epoch 0 batch 200: avg loss -3.299116 avg loss no lamb -3.299116 time 2019-03-05 21:49:41.302316
last batch sz 160
Model ind 640 epoch 1363 head B head_i_epoch 0 batch 0: avg loss -1.979526 avg loss no lamb -1.979526 time 2019-03-05 21:50:57.308028
Model ind 640 epoch 1363 head B head_i_epoch 0 batch 100: avg loss -1.944846 avg loss no lamb -1.944846 time 2019-03-05 21:52:42.888716
Model ind 640 epoch 1363 head B head_i_epoch 0 batch 200: avg loss -1.996126 avg loss no lamb -1.996126 time 2019-03-05 21:54:30.456794
last batch sz 160
Model ind 640 epoch 1363 head B head_i_epoch 1 batch 0: avg loss -2.038019 avg loss no lamb -2.038019 time 2019-03-05 21:55:44.490603
Model ind 640 epoch 1363 head B head_i_epoch 1 batch 100: avg loss -1.874607 avg loss no lamb -1.874607 time 2019-03-05 21:57:32.324826
Model ind 640 epoch 1363 head B head_i_epoch 1 batch 200: avg loss -2.073185 avg loss no lamb -2.073185 time 2019-03-05 21:59:22.484527
last batch sz 160
Pre: time 2019-03-05 22:01:01.802486: 
 	std: 0.05088278
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51088333, 0.6147, 0.61465, 0.6145667, 0.51066667]
	train_accs: [0.51088333, 0.6147, 0.61465, 0.6145667, 0.51066667]
	best_train_sub_head: 1
	worst: 0.51066667
	avg: 0.57309335
	best: 0.6147

Starting e_i: 1364
Model ind 640 epoch 1364 head A head_i_epoch 0 batch 0: avg loss -3.346843 avg loss no lamb -3.346843 time 2019-03-05 22:01:04.757211
Model ind 640 epoch 1364 head A head_i_epoch 0 batch 100: avg loss -3.238796 avg loss no lamb -3.238796 time 2019-03-05 22:02:54.070145
Model ind 640 epoch 1364 head A head_i_epoch 0 batch 200: avg loss -3.332163 avg loss no lamb -3.332163 time 2019-03-05 22:04:43.971268
last batch sz 160
Model ind 640 epoch 1364 head B head_i_epoch 0 batch 0: avg loss -1.968612 avg loss no lamb -1.968612 time 2019-03-05 22:06:05.466431
Model ind 640 epoch 1364 head B head_i_epoch 0 batch 100: avg loss -1.903199 avg loss no lamb -1.903199 time 2019-03-05 22:07:51.504247
Model ind 640 epoch 1364 head B head_i_epoch 0 batch 200: avg loss -1.982787 avg loss no lamb -1.982787 time 2019-03-05 22:09:40.173322
last batch sz 160
Model ind 640 epoch 1364 head B head_i_epoch 1 batch 0: avg loss -1.923152 avg loss no lamb -1.923152 time 2019-03-05 22:11:00.634535
Model ind 640 epoch 1364 head B head_i_epoch 1 batch 100: avg loss -1.925349 avg loss no lamb -1.925349 time 2019-03-05 22:12:47.904722
Model ind 640 epoch 1364 head B head_i_epoch 1 batch 200: avg loss -2.002065 avg loss no lamb -2.002065 time 2019-03-05 22:14:50.484469
last batch sz 160
Pre: time 2019-03-05 22:16:48.050962: 
 	std: 0.05059558
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115167, 0.61481667, 0.61473334, 0.6148833, 0.51155]
	train_accs: [0.5115167, 0.61481667, 0.61473334, 0.6148833, 0.51155]
	best_train_sub_head: 3
	worst: 0.5115167
	avg: 0.5735
	best: 0.6148833

Starting e_i: 1365
Model ind 640 epoch 1365 head A head_i_epoch 0 batch 0: avg loss -3.307962 avg loss no lamb -3.307962 time 2019-03-05 22:16:51.238277
Model ind 640 epoch 1365 head A head_i_epoch 0 batch 100: avg loss -3.286877 avg loss no lamb -3.286877 time 2019-03-05 22:18:40.477883
Model ind 640 epoch 1365 head A head_i_epoch 0 batch 200: avg loss -3.236346 avg loss no lamb -3.236346 time 2019-03-05 22:20:41.519464
last batch sz 160
Model ind 640 epoch 1365 head B head_i_epoch 0 batch 0: avg loss -2.051988 avg loss no lamb -2.051988 time 2019-03-05 22:22:07.918794
Model ind 640 epoch 1365 head B head_i_epoch 0 batch 100: avg loss -1.970804 avg loss no lamb -1.970804 time 2019-03-05 22:24:01.921152
Model ind 640 epoch 1365 head B head_i_epoch 0 batch 200: avg loss -2.030217 avg loss no lamb -2.030217 time 2019-03-05 22:26:00.179148
last batch sz 160
Model ind 640 epoch 1365 head B head_i_epoch 1 batch 0: avg loss -1.924271 avg loss no lamb -1.924271 time 2019-03-05 22:27:25.365317
Model ind 640 epoch 1365 head B head_i_epoch 1 batch 100: avg loss -2.015290 avg loss no lamb -2.015290 time 2019-03-05 22:29:18.451531
Model ind 640 epoch 1365 head B head_i_epoch 1 batch 200: avg loss -2.005690 avg loss no lamb -2.005690 time 2019-03-05 22:31:14.899076
last batch sz 160
Pre: time 2019-03-05 22:33:02.555402: 
 	std: 0.05039283
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51196665, 0.61481667, 0.61481667, 0.61478335, 0.51191664]
	train_accs: [0.51196665, 0.61481667, 0.61481667, 0.61478335, 0.51191664]
	best_train_sub_head: 1
	worst: 0.51191664
	avg: 0.57366
	best: 0.61481667

Starting e_i: 1366
Model ind 640 epoch 1366 head A head_i_epoch 0 batch 0: avg loss -3.395277 avg loss no lamb -3.395277 time 2019-03-05 22:33:06.858049
Model ind 640 epoch 1366 head A head_i_epoch 0 batch 100: avg loss -3.235596 avg loss no lamb -3.235596 time 2019-03-05 22:34:58.346688
Model ind 640 epoch 1366 head A head_i_epoch 0 batch 200: avg loss -3.357212 avg loss no lamb -3.357212 time 2019-03-05 22:36:58.355512
last batch sz 160
Model ind 640 epoch 1366 head B head_i_epoch 0 batch 0: avg loss -2.010774 avg loss no lamb -2.010774 time 2019-03-05 22:38:24.780827
Model ind 640 epoch 1366 head B head_i_epoch 0 batch 100: avg loss -1.879925 avg loss no lamb -1.879925 time 2019-03-05 22:40:17.360002
Model ind 640 epoch 1366 head B head_i_epoch 0 batch 200: avg loss -2.041348 avg loss no lamb -2.041348 time 2019-03-05 22:42:14.163259
last batch sz 160
Model ind 640 epoch 1366 head B head_i_epoch 1 batch 0: avg loss -2.044519 avg loss no lamb -2.044519 time 2019-03-05 22:43:39.366764
Model ind 640 epoch 1366 head B head_i_epoch 1 batch 100: avg loss -1.995652 avg loss no lamb -1.995652 time 2019-03-05 22:45:34.281488
Model ind 640 epoch 1366 head B head_i_epoch 1 batch 200: avg loss -2.014948 avg loss no lamb -2.014948 time 2019-03-05 22:47:29.409066
last batch sz 160
Pre: time 2019-03-05 22:49:18.893435: 
 	std: 0.050308447
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5128, 0.61541665, 0.61546665, 0.61546665, 0.51271665]
	train_accs: [0.5128, 0.61541665, 0.61546665, 0.61546665, 0.51271665]
	best_train_sub_head: 2
	worst: 0.51271665
	avg: 0.57437336
	best: 0.61546665

Starting e_i: 1367
Model ind 640 epoch 1367 head A head_i_epoch 0 batch 0: avg loss -3.384049 avg loss no lamb -3.384049 time 2019-03-05 22:49:21.984660
Model ind 640 epoch 1367 head A head_i_epoch 0 batch 100: avg loss -3.318163 avg loss no lamb -3.318163 time 2019-03-05 22:51:21.189073
Model ind 640 epoch 1367 head A head_i_epoch 0 batch 200: avg loss -3.344456 avg loss no lamb -3.344456 time 2019-03-05 22:53:12.996467
last batch sz 160
Model ind 640 epoch 1367 head B head_i_epoch 0 batch 0: avg loss -1.937906 avg loss no lamb -1.937906 time 2019-03-05 22:54:42.046102
Model ind 640 epoch 1367 head B head_i_epoch 0 batch 100: avg loss -1.960484 avg loss no lamb -1.960484 time 2019-03-05 22:56:36.768132
Model ind 640 epoch 1367 head B head_i_epoch 0 batch 200: avg loss -2.084877 avg loss no lamb -2.084877 time 2019-03-05 22:58:31.993821
last batch sz 160
Model ind 640 epoch 1367 head B head_i_epoch 1 batch 0: avg loss -2.010413 avg loss no lamb -2.010413 time 2019-03-05 22:59:58.377166
Model ind 640 epoch 1367 head B head_i_epoch 1 batch 100: avg loss -1.942704 avg loss no lamb -1.942704 time 2019-03-05 23:01:54.358566
Model ind 640 epoch 1367 head B head_i_epoch 1 batch 200: avg loss -1.986315 avg loss no lamb -1.986315 time 2019-03-05 23:03:48.628820
last batch sz 160
Pre: time 2019-03-05 23:05:33.308786: 
 	std: 0.0509984
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51175, 0.6158, 0.6158, 0.6159, 0.51171666]
	train_accs: [0.51175, 0.6158, 0.6158, 0.6159, 0.51171666]
	best_train_sub_head: 3
	worst: 0.51171666
	avg: 0.57419336
	best: 0.6159

Starting e_i: 1368
Model ind 640 epoch 1368 head A head_i_epoch 0 batch 0: avg loss -3.372100 avg loss no lamb -3.372100 time 2019-03-05 23:05:36.717868
Model ind 640 epoch 1368 head A head_i_epoch 0 batch 100: avg loss -3.255139 avg loss no lamb -3.255139 time 2019-03-05 23:07:34.279745
Model ind 640 epoch 1368 head A head_i_epoch 0 batch 200: avg loss -3.298750 avg loss no lamb -3.298750 time 2019-03-05 23:09:26.273885
last batch sz 160
Model ind 640 epoch 1368 head B head_i_epoch 0 batch 0: avg loss -1.983192 avg loss no lamb -1.983192 time 2019-03-05 23:10:54.296521
Model ind 640 epoch 1368 head B head_i_epoch 0 batch 100: avg loss -1.958043 avg loss no lamb -1.958043 time 2019-03-05 23:12:55.626927
Model ind 640 epoch 1368 head B head_i_epoch 0 batch 200: avg loss -1.951743 avg loss no lamb -1.951743 time 2019-03-05 23:14:46.626897
last batch sz 160
Model ind 640 epoch 1368 head B head_i_epoch 1 batch 0: avg loss -1.967052 avg loss no lamb -1.967052 time 2019-03-05 23:16:14.600271
Model ind 640 epoch 1368 head B head_i_epoch 1 batch 100: avg loss -1.927612 avg loss no lamb -1.927612 time 2019-03-05 23:18:09.690318
Model ind 640 epoch 1368 head B head_i_epoch 1 batch 200: avg loss -1.961255 avg loss no lamb -1.961255 time 2019-03-05 23:20:03.851463
last batch sz 160
Pre: time 2019-03-05 23:21:51.417828: 
 	std: 0.050454106
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51136667, 0.6142833, 0.6142333, 0.6142, 0.5111333]
	train_accs: [0.51136667, 0.6142833, 0.6142333, 0.6142, 0.5111333]
	best_train_sub_head: 1
	worst: 0.5111333
	avg: 0.5730432
	best: 0.6142833

Starting e_i: 1369
Model ind 640 epoch 1369 head A head_i_epoch 0 batch 0: avg loss -3.331577 avg loss no lamb -3.331577 time 2019-03-05 23:21:55.489034
Model ind 640 epoch 1369 head A head_i_epoch 0 batch 100: avg loss -3.301332 avg loss no lamb -3.301332 time 2019-03-05 23:23:53.660649
Model ind 640 epoch 1369 head A head_i_epoch 0 batch 200: avg loss -3.284613 avg loss no lamb -3.284613 time 2019-03-05 23:25:49.293854
last batch sz 160
Model ind 640 epoch 1369 head B head_i_epoch 0 batch 0: avg loss -1.962073 avg loss no lamb -1.962073 time 2019-03-05 23:27:17.510510
Model ind 640 epoch 1369 head B head_i_epoch 0 batch 100: avg loss -1.967346 avg loss no lamb -1.967346 time 2019-03-05 23:29:14.364644
Model ind 640 epoch 1369 head B head_i_epoch 0 batch 200: avg loss -2.037436 avg loss no lamb -2.037436 time 2019-03-05 23:31:07.732481
last batch sz 160
Model ind 640 epoch 1369 head B head_i_epoch 1 batch 0: avg loss -1.959207 avg loss no lamb -1.959207 time 2019-03-05 23:32:33.937741
Model ind 640 epoch 1369 head B head_i_epoch 1 batch 100: avg loss -1.935185 avg loss no lamb -1.935185 time 2019-03-05 23:34:29.536344
Model ind 640 epoch 1369 head B head_i_epoch 1 batch 200: avg loss -2.029298 avg loss no lamb -2.029298 time 2019-03-05 23:36:27.355312
last batch sz 160
Pre: time 2019-03-05 23:38:10.358573: 
 	std: 0.050748013
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118833, 0.61545, 0.61548334, 0.61553335, 0.51191664]
	train_accs: [0.5118833, 0.61545, 0.61548334, 0.61553335, 0.51191664]
	best_train_sub_head: 3
	worst: 0.5118833
	avg: 0.57405335
	best: 0.61553335

Starting e_i: 1370
Model ind 640 epoch 1370 head A head_i_epoch 0 batch 0: avg loss -3.390387 avg loss no lamb -3.390387 time 2019-03-05 23:38:14.211373
Model ind 640 epoch 1370 head A head_i_epoch 0 batch 100: avg loss -3.290052 avg loss no lamb -3.290052 time 2019-03-05 23:40:11.083344
Model ind 640 epoch 1370 head A head_i_epoch 0 batch 200: avg loss -3.369998 avg loss no lamb -3.369998 time 2019-03-05 23:42:11.779594
last batch sz 160
Model ind 640 epoch 1370 head B head_i_epoch 0 batch 0: avg loss -2.023705 avg loss no lamb -2.023705 time 2019-03-05 23:43:31.505998
Model ind 640 epoch 1370 head B head_i_epoch 0 batch 100: avg loss -1.944401 avg loss no lamb -1.944401 time 2019-03-05 23:45:31.401913
Model ind 640 epoch 1370 head B head_i_epoch 0 batch 200: avg loss -1.914046 avg loss no lamb -1.914046 time 2019-03-05 23:47:33.387715
last batch sz 160
Model ind 640 epoch 1370 head B head_i_epoch 1 batch 0: avg loss -2.009418 avg loss no lamb -2.009418 time 2019-03-05 23:48:51.531339
Model ind 640 epoch 1370 head B head_i_epoch 1 batch 100: avg loss -1.892852 avg loss no lamb -1.892852 time 2019-03-05 23:50:55.491812
Model ind 640 epoch 1370 head B head_i_epoch 1 batch 200: avg loss -2.045105 avg loss no lamb -2.045105 time 2019-03-05 23:52:54.836465
last batch sz 160
Pre: time 2019-03-05 23:54:37.201130: 
 	std: 0.050971214
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5114, 0.61551666, 0.61558336, 0.61548334, 0.51156664]
	train_accs: [0.5114, 0.61551666, 0.61558336, 0.61548334, 0.51156664]
	best_train_sub_head: 2
	worst: 0.5114
	avg: 0.57391
	best: 0.61558336

Starting e_i: 1371
Model ind 640 epoch 1371 head A head_i_epoch 0 batch 0: avg loss -3.427609 avg loss no lamb -3.427609 time 2019-03-05 23:54:44.296136
Model ind 640 epoch 1371 head A head_i_epoch 0 batch 100: avg loss -3.215233 avg loss no lamb -3.215233 time 2019-03-05 23:56:44.681436
Model ind 640 epoch 1371 head A head_i_epoch 0 batch 200: avg loss -3.321047 avg loss no lamb -3.321047 time 2019-03-05 23:58:43.571646
last batch sz 160
Model ind 640 epoch 1371 head B head_i_epoch 0 batch 0: avg loss -1.999261 avg loss no lamb -1.999261 time 2019-03-06 00:00:04.021565
Model ind 640 epoch 1371 head B head_i_epoch 0 batch 100: avg loss -1.952137 avg loss no lamb -1.952137 time 2019-03-06 00:02:04.877867
Model ind 640 epoch 1371 head B head_i_epoch 0 batch 200: avg loss -2.035402 avg loss no lamb -2.035402 time 2019-03-06 00:04:01.990821
last batch sz 160
Model ind 640 epoch 1371 head B head_i_epoch 1 batch 0: avg loss -2.028142 avg loss no lamb -2.028142 time 2019-03-06 00:05:25.285701
Model ind 640 epoch 1371 head B head_i_epoch 1 batch 100: avg loss -1.961738 avg loss no lamb -1.961738 time 2019-03-06 00:07:26.462678
Model ind 640 epoch 1371 head B head_i_epoch 1 batch 200: avg loss -2.018163 avg loss no lamb -2.018163 time 2019-03-06 00:09:20.215917
last batch sz 160
Pre: time 2019-03-06 00:11:05.609297: 
 	std: 0.050353367
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51155, 0.61425, 0.6142667, 0.61433333, 0.51145]
	train_accs: [0.51155, 0.61425, 0.6142667, 0.61433333, 0.51145]
	best_train_sub_head: 3
	worst: 0.51145
	avg: 0.57317007
	best: 0.61433333

Starting e_i: 1372
Model ind 640 epoch 1372 head A head_i_epoch 0 batch 0: avg loss -3.375380 avg loss no lamb -3.375380 time 2019-03-06 00:11:09.744026
Model ind 640 epoch 1372 head A head_i_epoch 0 batch 100: avg loss -3.321604 avg loss no lamb -3.321604 time 2019-03-06 00:13:07.901489
Model ind 640 epoch 1372 head A head_i_epoch 0 batch 200: avg loss -3.275830 avg loss no lamb -3.275830 time 2019-03-06 00:15:01.549415
last batch sz 160
Model ind 640 epoch 1372 head B head_i_epoch 0 batch 0: avg loss -2.001486 avg loss no lamb -2.001486 time 2019-03-06 00:16:25.243592
Model ind 640 epoch 1372 head B head_i_epoch 0 batch 100: avg loss -1.927531 avg loss no lamb -1.927531 time 2019-03-06 00:18:19.706539
Model ind 640 epoch 1372 head B head_i_epoch 0 batch 200: avg loss -2.039946 avg loss no lamb -2.039946 time 2019-03-06 00:20:15.175135
last batch sz 160
Model ind 640 epoch 1372 head B head_i_epoch 1 batch 0: avg loss -1.970403 avg loss no lamb -1.970403 time 2019-03-06 00:21:35.895207
Model ind 640 epoch 1372 head B head_i_epoch 1 batch 100: avg loss -1.935374 avg loss no lamb -1.935374 time 2019-03-06 00:23:34.786660
Model ind 640 epoch 1372 head B head_i_epoch 1 batch 200: avg loss -2.046296 avg loss no lamb -2.046296 time 2019-03-06 00:25:25.875195
last batch sz 160
Pre: time 2019-03-06 00:27:15.850605: 
 	std: 0.050870463
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115333, 0.61536664, 0.61541665, 0.6153833, 0.51156664]
	train_accs: [0.5115333, 0.61536664, 0.61541665, 0.6153833, 0.51156664]
	best_train_sub_head: 2
	worst: 0.5115333
	avg: 0.5738533
	best: 0.61541665

Starting e_i: 1373
Model ind 640 epoch 1373 head A head_i_epoch 0 batch 0: avg loss -3.308699 avg loss no lamb -3.308699 time 2019-03-06 00:27:19.201786
Model ind 640 epoch 1373 head A head_i_epoch 0 batch 100: avg loss -3.333311 avg loss no lamb -3.333311 time 2019-03-06 00:29:15.299641
Model ind 640 epoch 1373 head A head_i_epoch 0 batch 200: avg loss -3.296426 avg loss no lamb -3.296426 time 2019-03-06 00:31:08.024800
last batch sz 160
Model ind 640 epoch 1373 head B head_i_epoch 0 batch 0: avg loss -1.982793 avg loss no lamb -1.982793 time 2019-03-06 00:32:39.298489
Model ind 640 epoch 1373 head B head_i_epoch 0 batch 100: avg loss -1.922631 avg loss no lamb -1.922631 time 2019-03-06 00:34:30.698031
Model ind 640 epoch 1373 head B head_i_epoch 0 batch 200: avg loss -2.045877 avg loss no lamb -2.045877 time 2019-03-06 00:36:32.445747
last batch sz 160
Model ind 640 epoch 1373 head B head_i_epoch 1 batch 0: avg loss -1.999773 avg loss no lamb -1.999773 time 2019-03-06 00:37:59.689051
Model ind 640 epoch 1373 head B head_i_epoch 1 batch 100: avg loss -1.905971 avg loss no lamb -1.905971 time 2019-03-06 00:39:55.898212
Model ind 640 epoch 1373 head B head_i_epoch 1 batch 200: avg loss -2.033494 avg loss no lamb -2.033494 time 2019-03-06 00:41:52.599511
last batch sz 160
Pre: time 2019-03-06 00:43:40.076993: 
 	std: 0.050810836
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5121167, 0.6155667, 0.6156167, 0.6155667, 0.51161665]
	train_accs: [0.5121167, 0.6155667, 0.6156167, 0.6155667, 0.51161665]
	best_train_sub_head: 2
	worst: 0.51161665
	avg: 0.5740967
	best: 0.6156167

Starting e_i: 1374
Model ind 640 epoch 1374 head A head_i_epoch 0 batch 0: avg loss -3.313608 avg loss no lamb -3.313608 time 2019-03-06 00:43:43.085057
Model ind 640 epoch 1374 head A head_i_epoch 0 batch 100: avg loss -3.263313 avg loss no lamb -3.263313 time 2019-03-06 00:45:41.735489
Model ind 640 epoch 1374 head A head_i_epoch 0 batch 200: avg loss -3.330770 avg loss no lamb -3.330770 time 2019-03-06 00:47:32.710550
last batch sz 160
Model ind 640 epoch 1374 head B head_i_epoch 0 batch 0: avg loss -2.008755 avg loss no lamb -2.008755 time 2019-03-06 00:49:01.699945
Model ind 640 epoch 1374 head B head_i_epoch 0 batch 100: avg loss -1.924256 avg loss no lamb -1.924256 time 2019-03-06 00:50:56.118526
Model ind 640 epoch 1374 head B head_i_epoch 0 batch 200: avg loss -2.034671 avg loss no lamb -2.034671 time 2019-03-06 00:52:50.692510
last batch sz 160
Model ind 640 epoch 1374 head B head_i_epoch 1 batch 0: avg loss -1.983500 avg loss no lamb -1.983500 time 2019-03-06 00:54:17.136063
Model ind 640 epoch 1374 head B head_i_epoch 1 batch 100: avg loss -1.930915 avg loss no lamb -1.930915 time 2019-03-06 00:56:13.495808
Model ind 640 epoch 1374 head B head_i_epoch 1 batch 200: avg loss -2.030416 avg loss no lamb -2.030416 time 2019-03-06 00:58:05.203074
last batch sz 160
Pre: time 2019-03-06 00:59:55.523410: 
 	std: 0.050304458
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51255, 0.6151, 0.61513335, 0.61501664, 0.51225]
	train_accs: [0.51255, 0.6151, 0.61513335, 0.61501664, 0.51225]
	best_train_sub_head: 2
	worst: 0.51225
	avg: 0.57401
	best: 0.61513335

Starting e_i: 1375
Model ind 640 epoch 1375 head A head_i_epoch 0 batch 0: avg loss -3.293098 avg loss no lamb -3.293098 time 2019-03-06 00:59:59.531673
Model ind 640 epoch 1375 head A head_i_epoch 0 batch 100: avg loss -3.373168 avg loss no lamb -3.373168 time 2019-03-06 01:01:56.090773
Model ind 640 epoch 1375 head A head_i_epoch 0 batch 200: avg loss -3.328064 avg loss no lamb -3.328064 time 2019-03-06 01:03:50.706620
last batch sz 160
Model ind 640 epoch 1375 head B head_i_epoch 0 batch 0: avg loss -2.010031 avg loss no lamb -2.010031 time 2019-03-06 01:05:14.970370
Model ind 640 epoch 1375 head B head_i_epoch 0 batch 100: avg loss -1.896835 avg loss no lamb -1.896835 time 2019-03-06 01:07:10.366460
Model ind 640 epoch 1375 head B head_i_epoch 0 batch 200: avg loss -2.040981 avg loss no lamb -2.040981 time 2019-03-06 01:09:09.576259
last batch sz 160
Model ind 640 epoch 1375 head B head_i_epoch 1 batch 0: avg loss -1.897399 avg loss no lamb -1.897399 time 2019-03-06 01:10:30.998544
Model ind 640 epoch 1375 head B head_i_epoch 1 batch 100: avg loss -1.902695 avg loss no lamb -1.902695 time 2019-03-06 01:12:30.426481
Model ind 640 epoch 1375 head B head_i_epoch 1 batch 200: avg loss -2.014158 avg loss no lamb -2.014158 time 2019-03-06 01:14:31.485832
last batch sz 160
Pre: time 2019-03-06 01:16:13.460819: 
 	std: 0.05101881
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51075, 0.61483335, 0.6149333, 0.61483335, 0.5107]
	train_accs: [0.51075, 0.61483335, 0.6149333, 0.61483335, 0.5107]
	best_train_sub_head: 2
	worst: 0.5107
	avg: 0.57321
	best: 0.6149333

Starting e_i: 1376
Model ind 640 epoch 1376 head A head_i_epoch 0 batch 0: avg loss -3.303008 avg loss no lamb -3.303008 time 2019-03-06 01:16:16.543844
Model ind 640 epoch 1376 head A head_i_epoch 0 batch 100: avg loss -3.218239 avg loss no lamb -3.218239 time 2019-03-06 01:18:20.205711
Model ind 640 epoch 1376 head A head_i_epoch 0 batch 200: avg loss -3.332752 avg loss no lamb -3.332752 time 2019-03-06 01:20:23.936959
last batch sz 160
Model ind 640 epoch 1376 head B head_i_epoch 0 batch 0: avg loss -1.945828 avg loss no lamb -1.945828 time 2019-03-06 01:21:44.511438
Model ind 640 epoch 1376 head B head_i_epoch 0 batch 100: avg loss -1.934722 avg loss no lamb -1.934722 time 2019-03-06 01:23:44.654948
Model ind 640 epoch 1376 head B head_i_epoch 0 batch 200: avg loss -2.088904 avg loss no lamb -2.088904 time 2019-03-06 01:25:44.583690
last batch sz 160
Model ind 640 epoch 1376 head B head_i_epoch 1 batch 0: avg loss -1.957460 avg loss no lamb -1.957460 time 2019-03-06 01:27:02.981328
Model ind 640 epoch 1376 head B head_i_epoch 1 batch 100: avg loss -1.964341 avg loss no lamb -1.964341 time 2019-03-06 01:29:05.665922
Model ind 640 epoch 1376 head B head_i_epoch 1 batch 200: avg loss -2.040154 avg loss no lamb -2.040154 time 2019-03-06 01:31:02.496380
last batch sz 160
Pre: time 2019-03-06 01:32:48.063836: 
 	std: 0.05068812
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51208335, 0.61546665, 0.61558336, 0.6155, 0.51201665]
	train_accs: [0.51208335, 0.61546665, 0.61558336, 0.6155, 0.51201665]
	best_train_sub_head: 2
	worst: 0.51201665
	avg: 0.57412994
	best: 0.61558336

Starting e_i: 1377
Model ind 640 epoch 1377 head A head_i_epoch 0 batch 0: avg loss -3.336964 avg loss no lamb -3.336964 time 2019-03-06 01:32:51.289971
Model ind 640 epoch 1377 head A head_i_epoch 0 batch 100: avg loss -3.203398 avg loss no lamb -3.203398 time 2019-03-06 01:34:47.227449
Model ind 640 epoch 1377 head A head_i_epoch 0 batch 200: avg loss -3.260323 avg loss no lamb -3.260323 time 2019-03-06 01:36:39.733077
last batch sz 160
Model ind 640 epoch 1377 head B head_i_epoch 0 batch 0: avg loss -1.997150 avg loss no lamb -1.997150 time 2019-03-06 01:38:07.791191
Model ind 640 epoch 1377 head B head_i_epoch 0 batch 100: avg loss -1.891764 avg loss no lamb -1.891764 time 2019-03-06 01:40:06.306755
Model ind 640 epoch 1377 head B head_i_epoch 0 batch 200: avg loss -2.022812 avg loss no lamb -2.022812 time 2019-03-06 01:41:57.353961
last batch sz 160
Model ind 640 epoch 1377 head B head_i_epoch 1 batch 0: avg loss -2.038116 avg loss no lamb -2.038116 time 2019-03-06 01:43:28.140534
Model ind 640 epoch 1377 head B head_i_epoch 1 batch 100: avg loss -1.846401 avg loss no lamb -1.846401 time 2019-03-06 01:45:20.116444
Model ind 640 epoch 1377 head B head_i_epoch 1 batch 200: avg loss -2.012063 avg loss no lamb -2.012063 time 2019-03-06 01:47:15.616996
last batch sz 160
Pre: time 2019-03-06 01:49:06.110069: 
 	std: 0.050847337
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51063335, 0.61436665, 0.61436665, 0.61431664, 0.5104833]
	train_accs: [0.51063335, 0.61436665, 0.61436665, 0.61431664, 0.5104833]
	best_train_sub_head: 1
	worst: 0.5104833
	avg: 0.5728333
	best: 0.61436665

Starting e_i: 1378
Model ind 640 epoch 1378 head A head_i_epoch 0 batch 0: avg loss -3.361811 avg loss no lamb -3.361811 time 2019-03-06 01:49:10.142609
Model ind 640 epoch 1378 head A head_i_epoch 0 batch 100: avg loss -3.260058 avg loss no lamb -3.260058 time 2019-03-06 01:51:08.506325
Model ind 640 epoch 1378 head A head_i_epoch 0 batch 200: avg loss -3.379133 avg loss no lamb -3.379133 time 2019-03-06 01:53:00.982868
last batch sz 160
Model ind 640 epoch 1378 head B head_i_epoch 0 batch 0: avg loss -1.958367 avg loss no lamb -1.958367 time 2019-03-06 01:54:31.133853
Model ind 640 epoch 1378 head B head_i_epoch 0 batch 100: avg loss -1.972961 avg loss no lamb -1.972961 time 2019-03-06 01:56:26.740358
Model ind 640 epoch 1378 head B head_i_epoch 0 batch 200: avg loss -1.929952 avg loss no lamb -1.929952 time 2019-03-06 01:58:22.808015
last batch sz 160
Model ind 640 epoch 1378 head B head_i_epoch 1 batch 0: avg loss -1.952699 avg loss no lamb -1.952699 time 2019-03-06 01:59:47.032282
Model ind 640 epoch 1378 head B head_i_epoch 1 batch 100: avg loss -1.886939 avg loss no lamb -1.886939 time 2019-03-06 02:01:43.259095
Model ind 640 epoch 1378 head B head_i_epoch 1 batch 200: avg loss -1.996836 avg loss no lamb -1.996836 time 2019-03-06 02:03:42.217856
last batch sz 160
Pre: time 2019-03-06 02:05:23.999493: 
 	std: 0.05105965
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.511, 0.61513335, 0.61513335, 0.61518335, 0.51085]
	train_accs: [0.511, 0.61513335, 0.61513335, 0.61518335, 0.51085]
	best_train_sub_head: 3
	worst: 0.51085
	avg: 0.57346
	best: 0.61518335

Starting e_i: 1379
Model ind 640 epoch 1379 head A head_i_epoch 0 batch 0: avg loss -3.328127 avg loss no lamb -3.328127 time 2019-03-06 02:05:29.369368
Model ind 640 epoch 1379 head A head_i_epoch 0 batch 100: avg loss -3.325423 avg loss no lamb -3.325423 time 2019-03-06 02:07:26.996123
Model ind 640 epoch 1379 head A head_i_epoch 0 batch 200: avg loss -3.375371 avg loss no lamb -3.375371 time 2019-03-06 02:09:34.497635
last batch sz 160
Model ind 640 epoch 1379 head B head_i_epoch 0 batch 0: avg loss -2.008372 avg loss no lamb -2.008372 time 2019-03-06 02:10:54.069146
Model ind 640 epoch 1379 head B head_i_epoch 0 batch 100: avg loss -2.035171 avg loss no lamb -2.035171 time 2019-03-06 02:12:54.514587
Model ind 640 epoch 1379 head B head_i_epoch 0 batch 200: avg loss -2.048189 avg loss no lamb -2.048189 time 2019-03-06 02:14:57.262792
last batch sz 160
Model ind 640 epoch 1379 head B head_i_epoch 1 batch 0: avg loss -1.985058 avg loss no lamb -1.985058 time 2019-03-06 02:16:16.519379
Model ind 640 epoch 1379 head B head_i_epoch 1 batch 100: avg loss -1.966282 avg loss no lamb -1.966282 time 2019-03-06 02:18:17.540411
Model ind 640 epoch 1379 head B head_i_epoch 1 batch 200: avg loss -2.022391 avg loss no lamb -2.022391 time 2019-03-06 02:20:22.565321
last batch sz 160
Pre: time 2019-03-06 02:22:05.824439: 
 	std: 0.051071897
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107833, 0.6149167, 0.6149833, 0.615, 0.51065]
	train_accs: [0.5107833, 0.6149167, 0.6149833, 0.615, 0.51065]
	best_train_sub_head: 3
	worst: 0.51065
	avg: 0.5732666
	best: 0.615

Starting e_i: 1380
Model ind 640 epoch 1380 head A head_i_epoch 0 batch 0: avg loss -3.323020 avg loss no lamb -3.323020 time 2019-03-06 02:22:10.801526
Model ind 640 epoch 1380 head A head_i_epoch 0 batch 100: avg loss -3.278309 avg loss no lamb -3.278309 time 2019-03-06 02:24:06.167859
Model ind 640 epoch 1380 head A head_i_epoch 0 batch 200: avg loss -3.358257 avg loss no lamb -3.358257 time 2019-03-06 02:26:04.294264
last batch sz 160
Model ind 640 epoch 1380 head B head_i_epoch 0 batch 0: avg loss -2.061916 avg loss no lamb -2.061916 time 2019-03-06 02:27:25.649165
Model ind 640 epoch 1380 head B head_i_epoch 0 batch 100: avg loss -1.892780 avg loss no lamb -1.892780 time 2019-03-06 02:29:21.602267
Model ind 640 epoch 1380 head B head_i_epoch 0 batch 200: avg loss -1.974820 avg loss no lamb -1.974820 time 2019-03-06 02:31:13.702469
last batch sz 160
Model ind 640 epoch 1380 head B head_i_epoch 1 batch 0: avg loss -2.000473 avg loss no lamb -2.000473 time 2019-03-06 02:32:42.712094
Model ind 640 epoch 1380 head B head_i_epoch 1 batch 100: avg loss -1.902436 avg loss no lamb -1.902436 time 2019-03-06 02:34:40.408363
Model ind 640 epoch 1380 head B head_i_epoch 1 batch 200: avg loss -1.993656 avg loss no lamb -1.993656 time 2019-03-06 02:36:32.741827
last batch sz 160
Pre: time 2019-03-06 02:38:28.754801: 
 	std: 0.050531723
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114667, 0.61443335, 0.61441666, 0.61466664, 0.51125]
	train_accs: [0.5114667, 0.61443335, 0.61441666, 0.61466664, 0.51125]
	best_train_sub_head: 3
	worst: 0.51125
	avg: 0.57324666
	best: 0.61466664

Starting e_i: 1381
Model ind 640 epoch 1381 head A head_i_epoch 0 batch 0: avg loss -3.378336 avg loss no lamb -3.378336 time 2019-03-06 02:38:36.357617
Model ind 640 epoch 1381 head A head_i_epoch 0 batch 100: avg loss -3.243008 avg loss no lamb -3.243008 time 2019-03-06 02:40:39.664798
Model ind 640 epoch 1381 head A head_i_epoch 0 batch 200: avg loss -3.356030 avg loss no lamb -3.356030 time 2019-03-06 02:42:29.199753
last batch sz 160
Model ind 640 epoch 1381 head B head_i_epoch 0 batch 0: avg loss -1.951291 avg loss no lamb -1.951291 time 2019-03-06 02:44:01.258215
Model ind 640 epoch 1381 head B head_i_epoch 0 batch 100: avg loss -1.932523 avg loss no lamb -1.932523 time 2019-03-06 02:45:59.669294
Model ind 640 epoch 1381 head B head_i_epoch 0 batch 200: avg loss -1.990651 avg loss no lamb -1.990651 time 2019-03-06 02:47:54.806554
last batch sz 160
Model ind 640 epoch 1381 head B head_i_epoch 1 batch 0: avg loss -2.015605 avg loss no lamb -2.015605 time 2019-03-06 02:49:25.147278
Model ind 640 epoch 1381 head B head_i_epoch 1 batch 100: avg loss -1.962276 avg loss no lamb -1.962276 time 2019-03-06 02:51:20.753903
Model ind 640 epoch 1381 head B head_i_epoch 1 batch 200: avg loss -1.986894 avg loss no lamb -1.986894 time 2019-03-06 02:53:15.788806
last batch sz 160
Pre: time 2019-03-06 02:55:02.600152: 
 	std: 0.050621446
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5122833, 0.6156167, 0.6155667, 0.61558336, 0.5122333]
	train_accs: [0.5122833, 0.6156167, 0.6155667, 0.61558336, 0.5122333]
	best_train_sub_head: 1
	worst: 0.5122333
	avg: 0.57425666
	best: 0.6156167

Starting e_i: 1382
Model ind 640 epoch 1382 head A head_i_epoch 0 batch 0: avg loss -3.329969 avg loss no lamb -3.329969 time 2019-03-06 02:55:05.828012
Model ind 640 epoch 1382 head A head_i_epoch 0 batch 100: avg loss -3.248851 avg loss no lamb -3.248851 time 2019-03-06 02:56:59.939181
Model ind 640 epoch 1382 head A head_i_epoch 0 batch 200: avg loss -3.314019 avg loss no lamb -3.314019 time 2019-03-06 02:58:58.424211
last batch sz 160
Model ind 640 epoch 1382 head B head_i_epoch 0 batch 0: avg loss -2.026105 avg loss no lamb -2.026105 time 2019-03-06 03:00:21.452010
Model ind 640 epoch 1382 head B head_i_epoch 0 batch 100: avg loss -2.027648 avg loss no lamb -2.027648 time 2019-03-06 03:02:17.527787
Model ind 640 epoch 1382 head B head_i_epoch 0 batch 200: avg loss -1.979756 avg loss no lamb -1.979756 time 2019-03-06 03:04:23.685858
last batch sz 160
Model ind 640 epoch 1382 head B head_i_epoch 1 batch 0: avg loss -1.994931 avg loss no lamb -1.994931 time 2019-03-06 03:05:43.362050
Model ind 640 epoch 1382 head B head_i_epoch 1 batch 100: avg loss -1.972867 avg loss no lamb -1.972867 time 2019-03-06 03:07:43.653343
Model ind 640 epoch 1382 head B head_i_epoch 1 batch 200: avg loss -2.053317 avg loss no lamb -2.053317 time 2019-03-06 03:09:43.670981
last batch sz 160
Pre: time 2019-03-06 03:11:30.997966: 
 	std: 0.05060249
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51178336, 0.6148667, 0.6150333, 0.6152, 0.5117]
	train_accs: [0.51178336, 0.6148667, 0.6150333, 0.6152, 0.5117]
	best_train_sub_head: 3
	worst: 0.5117
	avg: 0.5737167
	best: 0.6152

Starting e_i: 1383
Model ind 640 epoch 1383 head A head_i_epoch 0 batch 0: avg loss -3.344366 avg loss no lamb -3.344366 time 2019-03-06 03:11:34.328580
Model ind 640 epoch 1383 head A head_i_epoch 0 batch 100: avg loss -3.312869 avg loss no lamb -3.312869 time 2019-03-06 03:13:27.277562
Model ind 640 epoch 1383 head A head_i_epoch 0 batch 200: avg loss -3.381601 avg loss no lamb -3.381601 time 2019-03-06 03:15:27.078285
last batch sz 160
Model ind 640 epoch 1383 head B head_i_epoch 0 batch 0: avg loss -2.040045 avg loss no lamb -2.040045 time 2019-03-06 03:16:49.686045
Model ind 640 epoch 1383 head B head_i_epoch 0 batch 100: avg loss -1.954811 avg loss no lamb -1.954811 time 2019-03-06 03:18:46.121100
Model ind 640 epoch 1383 head B head_i_epoch 0 batch 200: avg loss -2.041511 avg loss no lamb -2.041511 time 2019-03-06 03:20:41.337758
last batch sz 160
Model ind 640 epoch 1383 head B head_i_epoch 1 batch 0: avg loss -2.046816 avg loss no lamb -2.046816 time 2019-03-06 03:22:06.109326
Model ind 640 epoch 1383 head B head_i_epoch 1 batch 100: avg loss -1.845862 avg loss no lamb -1.845862 time 2019-03-06 03:24:01.323349
Model ind 640 epoch 1383 head B head_i_epoch 1 batch 200: avg loss -2.007766 avg loss no lamb -2.007766 time 2019-03-06 03:25:55.009220
last batch sz 160
Pre: time 2019-03-06 03:27:46.871418: 
 	std: 0.05048276
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5122833, 0.61525, 0.61508334, 0.61518335, 0.51196665]
	train_accs: [0.5122833, 0.61525, 0.61508334, 0.61518335, 0.51196665]
	best_train_sub_head: 1
	worst: 0.51196665
	avg: 0.57395333
	best: 0.61525

Starting e_i: 1384
Model ind 640 epoch 1384 head A head_i_epoch 0 batch 0: avg loss -3.359385 avg loss no lamb -3.359385 time 2019-03-06 03:27:50.346864
Model ind 640 epoch 1384 head A head_i_epoch 0 batch 100: avg loss -3.276351 avg loss no lamb -3.276351 time 2019-03-06 03:29:54.430592
Model ind 640 epoch 1384 head A head_i_epoch 0 batch 200: avg loss -3.399916 avg loss no lamb -3.399916 time 2019-03-06 03:31:44.752032
last batch sz 160
Model ind 640 epoch 1384 head B head_i_epoch 0 batch 0: avg loss -1.950311 avg loss no lamb -1.950311 time 2019-03-06 03:33:17.048083
Model ind 640 epoch 1384 head B head_i_epoch 0 batch 100: avg loss -1.966254 avg loss no lamb -1.966254 time 2019-03-06 03:35:13.161507
Model ind 640 epoch 1384 head B head_i_epoch 0 batch 200: avg loss -1.955718 avg loss no lamb -1.955718 time 2019-03-06 03:37:06.539558
last batch sz 160
Model ind 640 epoch 1384 head B head_i_epoch 1 batch 0: avg loss -2.003760 avg loss no lamb -2.003760 time 2019-03-06 03:38:36.760204
Model ind 640 epoch 1384 head B head_i_epoch 1 batch 100: avg loss -1.991302 avg loss no lamb -1.991302 time 2019-03-06 03:40:32.279992
Model ind 640 epoch 1384 head B head_i_epoch 1 batch 200: avg loss -2.076733 avg loss no lamb -2.076733 time 2019-03-06 03:42:26.474923
last batch sz 160
Pre: time 2019-03-06 03:44:14.656183: 
 	std: 0.05017514
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5136167, 0.61583334, 0.6159667, 0.6159833, 0.5134]
	train_accs: [0.5136167, 0.61583334, 0.6159667, 0.6159833, 0.5134]
	best_train_sub_head: 3
	worst: 0.5134
	avg: 0.57496005
	best: 0.6159833

Starting e_i: 1385
Model ind 640 epoch 1385 head A head_i_epoch 0 batch 0: avg loss -3.322011 avg loss no lamb -3.322011 time 2019-03-06 03:44:18.106642
Model ind 640 epoch 1385 head A head_i_epoch 0 batch 100: avg loss -3.209684 avg loss no lamb -3.209684 time 2019-03-06 03:46:12.660357
Model ind 640 epoch 1385 head A head_i_epoch 0 batch 200: avg loss -3.435471 avg loss no lamb -3.435471 time 2019-03-06 03:48:09.978255
last batch sz 160
Model ind 640 epoch 1385 head B head_i_epoch 0 batch 0: avg loss -2.058283 avg loss no lamb -2.058283 time 2019-03-06 03:49:30.994543
Model ind 640 epoch 1385 head B head_i_epoch 0 batch 100: avg loss -1.952700 avg loss no lamb -1.952700 time 2019-03-06 03:51:28.405712
Model ind 640 epoch 1385 head B head_i_epoch 0 batch 200: avg loss -2.011611 avg loss no lamb -2.011611 time 2019-03-06 03:53:29.536110
last batch sz 160
Model ind 640 epoch 1385 head B head_i_epoch 1 batch 0: avg loss -1.935965 avg loss no lamb -1.935965 time 2019-03-06 03:54:49.062534
Model ind 640 epoch 1385 head B head_i_epoch 1 batch 100: avg loss -1.914746 avg loss no lamb -1.914746 time 2019-03-06 03:56:50.095827
Model ind 640 epoch 1385 head B head_i_epoch 1 batch 200: avg loss -2.032751 avg loss no lamb -2.032751 time 2019-03-06 03:58:50.444684
last batch sz 160
Pre: time 2019-03-06 04:00:32.083630: 
 	std: 0.05095899
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51156664, 0.6157333, 0.61546665, 0.61558336, 0.5115833]
	train_accs: [0.51156664, 0.6157333, 0.61546665, 0.61558336, 0.5115833]
	best_train_sub_head: 1
	worst: 0.51156664
	avg: 0.57398665
	best: 0.6157333

Starting e_i: 1386
Model ind 640 epoch 1386 head A head_i_epoch 0 batch 0: avg loss -3.348576 avg loss no lamb -3.348576 time 2019-03-06 04:00:35.599166
Model ind 640 epoch 1386 head A head_i_epoch 0 batch 100: avg loss -3.323757 avg loss no lamb -3.323757 time 2019-03-06 04:02:34.272733
Model ind 640 epoch 1386 head A head_i_epoch 0 batch 200: avg loss -3.292169 avg loss no lamb -3.292169 time 2019-03-06 04:04:36.702225
last batch sz 160
Model ind 640 epoch 1386 head B head_i_epoch 0 batch 0: avg loss -1.943667 avg loss no lamb -1.943667 time 2019-03-06 04:05:57.730100
Model ind 640 epoch 1386 head B head_i_epoch 0 batch 100: avg loss -1.951284 avg loss no lamb -1.951284 time 2019-03-06 04:07:56.450415
Model ind 640 epoch 1386 head B head_i_epoch 0 batch 200: avg loss -2.001443 avg loss no lamb -2.001443 time 2019-03-06 04:09:52.320833
last batch sz 160
Model ind 640 epoch 1386 head B head_i_epoch 1 batch 0: avg loss -1.992621 avg loss no lamb -1.992621 time 2019-03-06 04:11:16.107012
Model ind 640 epoch 1386 head B head_i_epoch 1 batch 100: avg loss -1.967883 avg loss no lamb -1.967883 time 2019-03-06 04:13:10.643543
Model ind 640 epoch 1386 head B head_i_epoch 1 batch 200: avg loss -2.061952 avg loss no lamb -2.061952 time 2019-03-06 04:15:03.836404
last batch sz 160
Pre: time 2019-03-06 04:16:52.449675: 
 	std: 0.050331634
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51236665, 0.61523336, 0.6152167, 0.6152167, 0.5126]
	train_accs: [0.51236665, 0.61523336, 0.6152167, 0.6152167, 0.5126]
	best_train_sub_head: 1
	worst: 0.51236665
	avg: 0.57412666
	best: 0.61523336

Starting e_i: 1387
Model ind 640 epoch 1387 head A head_i_epoch 0 batch 0: avg loss -3.393760 avg loss no lamb -3.393760 time 2019-03-06 04:16:56.533410
Model ind 640 epoch 1387 head A head_i_epoch 0 batch 100: avg loss -3.305325 avg loss no lamb -3.305325 time 2019-03-06 04:18:53.970367
Model ind 640 epoch 1387 head A head_i_epoch 0 batch 200: avg loss -3.330735 avg loss no lamb -3.330735 time 2019-03-06 04:20:45.907043
last batch sz 160
Model ind 640 epoch 1387 head B head_i_epoch 0 batch 0: avg loss -1.958842 avg loss no lamb -1.958842 time 2019-03-06 04:22:20.063059
Model ind 640 epoch 1387 head B head_i_epoch 0 batch 100: avg loss -1.943986 avg loss no lamb -1.943986 time 2019-03-06 04:24:12.904993
Model ind 640 epoch 1387 head B head_i_epoch 0 batch 200: avg loss -1.976341 avg loss no lamb -1.976341 time 2019-03-06 04:26:10.892688
last batch sz 160
Model ind 640 epoch 1387 head B head_i_epoch 1 batch 0: avg loss -2.017827 avg loss no lamb -2.017827 time 2019-03-06 04:27:44.292510
Model ind 640 epoch 1387 head B head_i_epoch 1 batch 100: avg loss -1.971630 avg loss no lamb -1.971630 time 2019-03-06 04:29:40.249173
Model ind 640 epoch 1387 head B head_i_epoch 1 batch 200: avg loss -2.069311 avg loss no lamb -2.069311 time 2019-03-06 04:31:37.211214
last batch sz 160
Pre: time 2019-03-06 04:33:31.841635: 
 	std: 0.05026354
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51178336, 0.61445, 0.61445, 0.61445, 0.51191664]
	train_accs: [0.51178336, 0.61445, 0.61445, 0.61445, 0.51191664]
	best_train_sub_head: 1
	worst: 0.51178336
	avg: 0.57341
	best: 0.61445

Starting e_i: 1388
Model ind 640 epoch 1388 head A head_i_epoch 0 batch 0: avg loss -3.366700 avg loss no lamb -3.366700 time 2019-03-06 04:33:34.935524
Model ind 640 epoch 1388 head A head_i_epoch 0 batch 100: avg loss -3.295891 avg loss no lamb -3.295891 time 2019-03-06 04:35:31.143279
Model ind 640 epoch 1388 head A head_i_epoch 0 batch 200: avg loss -3.401021 avg loss no lamb -3.401021 time 2019-03-06 04:37:25.619421
last batch sz 160
Model ind 640 epoch 1388 head B head_i_epoch 0 batch 0: avg loss -2.000755 avg loss no lamb -2.000755 time 2019-03-06 04:38:49.250447
Model ind 640 epoch 1388 head B head_i_epoch 0 batch 100: avg loss -1.911188 avg loss no lamb -1.911188 time 2019-03-06 04:40:42.968510
Model ind 640 epoch 1388 head B head_i_epoch 0 batch 200: avg loss -2.030022 avg loss no lamb -2.030022 time 2019-03-06 04:42:43.668376
last batch sz 160
Model ind 640 epoch 1388 head B head_i_epoch 1 batch 0: avg loss -1.934470 avg loss no lamb -1.934470 time 2019-03-06 04:44:04.382972
Model ind 640 epoch 1388 head B head_i_epoch 1 batch 100: avg loss -1.944197 avg loss no lamb -1.944197 time 2019-03-06 04:46:05.168400
Model ind 640 epoch 1388 head B head_i_epoch 1 batch 200: avg loss -1.984368 avg loss no lamb -1.984368 time 2019-03-06 04:48:11.809016
last batch sz 160
Pre: time 2019-03-06 04:49:52.731359: 
 	std: 0.050598346
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118833, 0.6152167, 0.6153167, 0.6152667, 0.51208335]
	train_accs: [0.5118833, 0.6152167, 0.6153167, 0.6152667, 0.51208335]
	best_train_sub_head: 2
	worst: 0.5118833
	avg: 0.5739533
	best: 0.6153167

Starting e_i: 1389
Model ind 640 epoch 1389 head A head_i_epoch 0 batch 0: avg loss -3.316875 avg loss no lamb -3.316875 time 2019-03-06 04:49:57.148766
Model ind 640 epoch 1389 head A head_i_epoch 0 batch 100: avg loss -3.274534 avg loss no lamb -3.274534 time 2019-03-06 04:51:58.854498
Model ind 640 epoch 1389 head A head_i_epoch 0 batch 200: avg loss -3.389681 avg loss no lamb -3.389681 time 2019-03-06 04:54:04.774276
last batch sz 160
Model ind 640 epoch 1389 head B head_i_epoch 0 batch 0: avg loss -1.983904 avg loss no lamb -1.983904 time 2019-03-06 04:55:21.783272
Model ind 640 epoch 1389 head B head_i_epoch 0 batch 100: avg loss -1.848621 avg loss no lamb -1.848621 time 2019-03-06 04:57:25.666533
Model ind 640 epoch 1389 head B head_i_epoch 0 batch 200: avg loss -2.050195 avg loss no lamb -2.050195 time 2019-03-06 04:59:25.162686
last batch sz 160
Model ind 640 epoch 1389 head B head_i_epoch 1 batch 0: avg loss -1.991514 avg loss no lamb -1.991514 time 2019-03-06 05:00:45.055093
Model ind 640 epoch 1389 head B head_i_epoch 1 batch 100: avg loss -1.919825 avg loss no lamb -1.919825 time 2019-03-06 05:02:43.771128
Model ind 640 epoch 1389 head B head_i_epoch 1 batch 200: avg loss -1.996549 avg loss no lamb -1.996549 time 2019-03-06 05:04:37.586654
last batch sz 160
Pre: time 2019-03-06 05:06:24.397634: 
 	std: 0.051007967
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51175, 0.6156833, 0.61586666, 0.61588335, 0.51163334]
	train_accs: [0.51175, 0.6156833, 0.61586666, 0.61588335, 0.51163334]
	best_train_sub_head: 3
	worst: 0.51163334
	avg: 0.5741633
	best: 0.61588335

Starting e_i: 1390
Model ind 640 epoch 1390 head A head_i_epoch 0 batch 0: avg loss -3.391329 avg loss no lamb -3.391329 time 2019-03-06 05:06:28.519210
Model ind 640 epoch 1390 head A head_i_epoch 0 batch 100: avg loss -3.290387 avg loss no lamb -3.290387 time 2019-03-06 05:08:25.216814
Model ind 640 epoch 1390 head A head_i_epoch 0 batch 200: avg loss -3.412613 avg loss no lamb -3.412613 time 2019-03-06 05:10:18.732047
last batch sz 160
Model ind 640 epoch 1390 head B head_i_epoch 0 batch 0: avg loss -1.883907 avg loss no lamb -1.883907 time 2019-03-06 05:11:48.538227
Model ind 640 epoch 1390 head B head_i_epoch 0 batch 100: avg loss -1.934800 avg loss no lamb -1.934800 time 2019-03-06 05:13:40.680856
Model ind 640 epoch 1390 head B head_i_epoch 0 batch 200: avg loss -2.008578 avg loss no lamb -2.008578 time 2019-03-06 05:15:39.833177
last batch sz 160
Model ind 640 epoch 1390 head B head_i_epoch 1 batch 0: avg loss -1.982727 avg loss no lamb -1.982727 time 2019-03-06 05:17:15.146676
Model ind 640 epoch 1390 head B head_i_epoch 1 batch 100: avg loss -1.936057 avg loss no lamb -1.936057 time 2019-03-06 05:19:05.531126
Model ind 640 epoch 1390 head B head_i_epoch 1 batch 200: avg loss -2.013434 avg loss no lamb -2.013434 time 2019-03-06 05:21:06.303319
last batch sz 160
Pre: time 2019-03-06 05:22:59.033564: 
 	std: 0.05083103
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51105, 0.6149, 0.6148667, 0.6149333, 0.5112333]
	train_accs: [0.51105, 0.6149, 0.6148667, 0.6149333, 0.5112333]
	best_train_sub_head: 3
	worst: 0.51105
	avg: 0.5733966
	best: 0.6149333

Starting e_i: 1391
Model ind 640 epoch 1391 head A head_i_epoch 0 batch 0: avg loss -3.359869 avg loss no lamb -3.359869 time 2019-03-06 05:23:05.667306
Model ind 640 epoch 1391 head A head_i_epoch 0 batch 100: avg loss -3.289186 avg loss no lamb -3.289186 time 2019-03-06 05:25:02.334248
Model ind 640 epoch 1391 head A head_i_epoch 0 batch 200: avg loss -3.311269 avg loss no lamb -3.311269 time 2019-03-06 05:26:55.150100
last batch sz 160
Model ind 640 epoch 1391 head B head_i_epoch 0 batch 0: avg loss -2.030641 avg loss no lamb -2.030641 time 2019-03-06 05:28:22.777451
Model ind 640 epoch 1391 head B head_i_epoch 0 batch 100: avg loss -1.851516 avg loss no lamb -1.851516 time 2019-03-06 05:30:16.553799
Model ind 640 epoch 1391 head B head_i_epoch 0 batch 200: avg loss -2.005940 avg loss no lamb -2.005940 time 2019-03-06 05:32:14.410872
last batch sz 160
Model ind 640 epoch 1391 head B head_i_epoch 1 batch 0: avg loss -1.979078 avg loss no lamb -1.979078 time 2019-03-06 05:33:37.595533
Model ind 640 epoch 1391 head B head_i_epoch 1 batch 100: avg loss -1.930301 avg loss no lamb -1.930301 time 2019-03-06 05:35:34.872516
Model ind 640 epoch 1391 head B head_i_epoch 1 batch 200: avg loss -1.964978 avg loss no lamb -1.964978 time 2019-03-06 05:37:35.596250
last batch sz 160
Pre: time 2019-03-06 05:39:16.492775: 
 	std: 0.050209135
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5119333, 0.6145, 0.61446667, 0.61455, 0.5121]
	train_accs: [0.5119333, 0.6145, 0.61446667, 0.61455, 0.5121]
	best_train_sub_head: 3
	worst: 0.5119333
	avg: 0.57351005
	best: 0.61455

Starting e_i: 1392
Model ind 640 epoch 1392 head A head_i_epoch 0 batch 0: avg loss -3.365679 avg loss no lamb -3.365679 time 2019-03-06 05:39:19.740821
Model ind 640 epoch 1392 head A head_i_epoch 0 batch 100: avg loss -3.277790 avg loss no lamb -3.277790 time 2019-03-06 05:41:20.789985
Model ind 640 epoch 1392 head A head_i_epoch 0 batch 200: avg loss -3.385759 avg loss no lamb -3.385759 time 2019-03-06 05:43:23.062682
last batch sz 160
Model ind 640 epoch 1392 head B head_i_epoch 0 batch 0: avg loss -2.002362 avg loss no lamb -2.002362 time 2019-03-06 05:44:41.199768
Model ind 640 epoch 1392 head B head_i_epoch 0 batch 100: avg loss -1.914775 avg loss no lamb -1.914775 time 2019-03-06 05:46:42.095197
Model ind 640 epoch 1392 head B head_i_epoch 0 batch 200: avg loss -2.017077 avg loss no lamb -2.017077 time 2019-03-06 05:48:37.445124
last batch sz 160
Model ind 640 epoch 1392 head B head_i_epoch 1 batch 0: avg loss -1.942647 avg loss no lamb -1.942647 time 2019-03-06 05:49:59.003202
Model ind 640 epoch 1392 head B head_i_epoch 1 batch 100: avg loss -1.957507 avg loss no lamb -1.957507 time 2019-03-06 05:52:02.876395
Model ind 640 epoch 1392 head B head_i_epoch 1 batch 200: avg loss -2.059872 avg loss no lamb -2.059872 time 2019-03-06 05:54:00.266524
last batch sz 160
Pre: time 2019-03-06 05:55:44.575318: 
 	std: 0.051084116
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.50985, 0.6141667, 0.61411667, 0.61406666, 0.50983334]
	train_accs: [0.50985, 0.6141667, 0.61411667, 0.61406666, 0.50983334]
	best_train_sub_head: 1
	worst: 0.50983334
	avg: 0.57240665
	best: 0.6141667

Starting e_i: 1393
Model ind 640 epoch 1393 head A head_i_epoch 0 batch 0: avg loss -3.287760 avg loss no lamb -3.287760 time 2019-03-06 05:55:49.329546
Model ind 640 epoch 1393 head A head_i_epoch 0 batch 100: avg loss -3.314032 avg loss no lamb -3.314032 time 2019-03-06 05:57:47.990175
Model ind 640 epoch 1393 head A head_i_epoch 0 batch 200: avg loss -3.316140 avg loss no lamb -3.316140 time 2019-03-06 05:59:42.204348
last batch sz 160
Model ind 640 epoch 1393 head B head_i_epoch 0 batch 0: avg loss -2.058473 avg loss no lamb -2.058473 time 2019-03-06 06:01:08.840800
Model ind 640 epoch 1393 head B head_i_epoch 0 batch 100: avg loss -1.866826 avg loss no lamb -1.866826 time 2019-03-06 06:03:03.773808
Model ind 640 epoch 1393 head B head_i_epoch 0 batch 200: avg loss -2.011875 avg loss no lamb -2.011875 time 2019-03-06 06:04:56.379176
last batch sz 160
Model ind 640 epoch 1393 head B head_i_epoch 1 batch 0: avg loss -2.000756 avg loss no lamb -2.000756 time 2019-03-06 06:06:28.534863
Model ind 640 epoch 1393 head B head_i_epoch 1 batch 100: avg loss -1.943985 avg loss no lamb -1.943985 time 2019-03-06 06:08:20.225778
Model ind 640 epoch 1393 head B head_i_epoch 1 batch 200: avg loss -1.998032 avg loss no lamb -1.998032 time 2019-03-06 06:10:18.278819
last batch sz 160
Pre: time 2019-03-06 06:12:16.095761: 
 	std: 0.050507132
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5126, 0.61575, 0.6157167, 0.61575, 0.51268333]
	train_accs: [0.5126, 0.61575, 0.6157167, 0.61575, 0.51268333]
	best_train_sub_head: 1
	worst: 0.5126
	avg: 0.5745
	best: 0.61575

Starting e_i: 1394
Model ind 640 epoch 1394 head A head_i_epoch 0 batch 0: avg loss -3.331644 avg loss no lamb -3.331644 time 2019-03-06 06:12:19.309845
Model ind 640 epoch 1394 head A head_i_epoch 0 batch 100: avg loss -3.274031 avg loss no lamb -3.274031 time 2019-03-06 06:14:12.507677
Model ind 640 epoch 1394 head A head_i_epoch 0 batch 200: avg loss -3.403301 avg loss no lamb -3.403301 time 2019-03-06 06:16:10.562475
last batch sz 160
Model ind 640 epoch 1394 head B head_i_epoch 0 batch 0: avg loss -2.070325 avg loss no lamb -2.070325 time 2019-03-06 06:17:44.132686
Model ind 640 epoch 1394 head B head_i_epoch 0 batch 100: avg loss -1.917563 avg loss no lamb -1.917563 time 2019-03-06 06:19:40.451725
Model ind 640 epoch 1394 head B head_i_epoch 0 batch 200: avg loss -1.969929 avg loss no lamb -1.969929 time 2019-03-06 06:21:36.168381
last batch sz 160
Model ind 640 epoch 1394 head B head_i_epoch 1 batch 0: avg loss -1.972906 avg loss no lamb -1.972906 time 2019-03-06 06:23:07.190058
Model ind 640 epoch 1394 head B head_i_epoch 1 batch 100: avg loss -1.905416 avg loss no lamb -1.905416 time 2019-03-06 06:24:59.986301
Model ind 640 epoch 1394 head B head_i_epoch 1 batch 200: avg loss -1.934670 avg loss no lamb -1.934670 time 2019-03-06 06:26:57.389068
last batch sz 160
Pre: time 2019-03-06 06:28:44.411539: 
 	std: 0.050587483
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51215, 0.6154, 0.61555, 0.61558336, 0.51235]
	train_accs: [0.51215, 0.6154, 0.61555, 0.61558336, 0.51235]
	best_train_sub_head: 3
	worst: 0.51215
	avg: 0.5742067
	best: 0.61558336

Starting e_i: 1395
Model ind 640 epoch 1395 head A head_i_epoch 0 batch 0: avg loss -3.284417 avg loss no lamb -3.284417 time 2019-03-06 06:28:48.545825
Model ind 640 epoch 1395 head A head_i_epoch 0 batch 100: avg loss -3.290662 avg loss no lamb -3.290662 time 2019-03-06 06:30:42.255057
Model ind 640 epoch 1395 head A head_i_epoch 0 batch 200: avg loss -3.409502 avg loss no lamb -3.409502 time 2019-03-06 06:32:40.952601
last batch sz 160
Model ind 640 epoch 1395 head B head_i_epoch 0 batch 0: avg loss -1.989649 avg loss no lamb -1.989649 time 2019-03-06 06:34:00.068486
Model ind 640 epoch 1395 head B head_i_epoch 0 batch 100: avg loss -1.968979 avg loss no lamb -1.968979 time 2019-03-06 06:36:00.821314
Model ind 640 epoch 1395 head B head_i_epoch 0 batch 200: avg loss -2.043169 avg loss no lamb -2.043169 time 2019-03-06 06:38:05.261750
last batch sz 160
Model ind 640 epoch 1395 head B head_i_epoch 1 batch 0: avg loss -1.978308 avg loss no lamb -1.978308 time 2019-03-06 06:39:22.855433
Model ind 640 epoch 1395 head B head_i_epoch 1 batch 100: avg loss -1.998254 avg loss no lamb -1.998254 time 2019-03-06 06:41:30.185204
Model ind 640 epoch 1395 head B head_i_epoch 1 batch 200: avg loss -2.025938 avg loss no lamb -2.025938 time 2019-03-06 06:43:27.756918
last batch sz 160
Pre: time 2019-03-06 06:45:11.545718: 
 	std: 0.050705843
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.511, 0.6145833, 0.6145667, 0.6146333, 0.5111833]
	train_accs: [0.511, 0.6145833, 0.6145667, 0.6146333, 0.5111833]
	best_train_sub_head: 3
	worst: 0.511
	avg: 0.5731933
	best: 0.6146333

Starting e_i: 1396
Model ind 640 epoch 1396 head A head_i_epoch 0 batch 0: avg loss -3.343765 avg loss no lamb -3.343765 time 2019-03-06 06:45:15.421253
Model ind 640 epoch 1396 head A head_i_epoch 0 batch 100: avg loss -3.244520 avg loss no lamb -3.244520 time 2019-03-06 06:47:21.145651
Model ind 640 epoch 1396 head A head_i_epoch 0 batch 200: avg loss -3.365032 avg loss no lamb -3.365032 time 2019-03-06 06:49:19.699559
last batch sz 160
Model ind 640 epoch 1396 head B head_i_epoch 0 batch 0: avg loss -1.971164 avg loss no lamb -1.971164 time 2019-03-06 06:50:40.956927
Model ind 640 epoch 1396 head B head_i_epoch 0 batch 100: avg loss -1.964566 avg loss no lamb -1.964566 time 2019-03-06 06:52:39.909558
Model ind 640 epoch 1396 head B head_i_epoch 0 batch 200: avg loss -1.999747 avg loss no lamb -1.999747 time 2019-03-06 06:54:33.448161
last batch sz 160
Model ind 640 epoch 1396 head B head_i_epoch 1 batch 0: avg loss -2.024236 avg loss no lamb -2.024236 time 2019-03-06 06:55:58.572288
Model ind 640 epoch 1396 head B head_i_epoch 1 batch 100: avg loss -1.854020 avg loss no lamb -1.854020 time 2019-03-06 06:57:51.498963
Model ind 640 epoch 1396 head B head_i_epoch 1 batch 200: avg loss -2.030717 avg loss no lamb -2.030717 time 2019-03-06 06:59:48.799136
last batch sz 160
Pre: time 2019-03-06 07:01:41.859183: 
 	std: 0.05049627
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5118333, 0.61478335, 0.6148667, 0.6149, 0.51171666]
	train_accs: [0.5118333, 0.61478335, 0.6148667, 0.6149, 0.51171666]
	best_train_sub_head: 3
	worst: 0.51171666
	avg: 0.57361996
	best: 0.6149

Starting e_i: 1397
Model ind 640 epoch 1397 head A head_i_epoch 0 batch 0: avg loss -3.375117 avg loss no lamb -3.375117 time 2019-03-06 07:01:45.270677
Model ind 640 epoch 1397 head A head_i_epoch 0 batch 100: avg loss -3.301877 avg loss no lamb -3.301877 time 2019-03-06 07:03:36.849586
Model ind 640 epoch 1397 head A head_i_epoch 0 batch 200: avg loss -3.381527 avg loss no lamb -3.381527 time 2019-03-06 07:05:35.508276
last batch sz 160
Model ind 640 epoch 1397 head B head_i_epoch 0 batch 0: avg loss -1.994208 avg loss no lamb -1.994208 time 2019-03-06 07:07:08.630773
Model ind 640 epoch 1397 head B head_i_epoch 0 batch 100: avg loss -1.953961 avg loss no lamb -1.953961 time 2019-03-06 07:09:00.975127
Model ind 640 epoch 1397 head B head_i_epoch 0 batch 200: avg loss -2.002577 avg loss no lamb -2.002577 time 2019-03-06 07:10:56.416770
last batch sz 160
Model ind 640 epoch 1397 head B head_i_epoch 1 batch 0: avg loss -1.967100 avg loss no lamb -1.967100 time 2019-03-06 07:12:25.414634
Model ind 640 epoch 1397 head B head_i_epoch 1 batch 100: avg loss -1.935543 avg loss no lamb -1.935543 time 2019-03-06 07:14:21.312227
Model ind 640 epoch 1397 head B head_i_epoch 1 batch 200: avg loss -2.001936 avg loss no lamb -2.001936 time 2019-03-06 07:16:19.042995
last batch sz 160
Pre: time 2019-03-06 07:18:05.611907: 
 	std: 0.05107741
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51103336, 0.61513335, 0.61511666, 0.61513335, 0.5107]
	train_accs: [0.51103336, 0.61513335, 0.61511666, 0.61513335, 0.5107]
	best_train_sub_head: 1
	worst: 0.5107
	avg: 0.5734233
	best: 0.61513335

Starting e_i: 1398
Model ind 640 epoch 1398 head A head_i_epoch 0 batch 0: avg loss -3.260780 avg loss no lamb -3.260780 time 2019-03-06 07:18:09.266659
Model ind 640 epoch 1398 head A head_i_epoch 0 batch 100: avg loss -3.267028 avg loss no lamb -3.267028 time 2019-03-06 07:20:04.775940
Model ind 640 epoch 1398 head A head_i_epoch 0 batch 200: avg loss -3.292987 avg loss no lamb -3.292987 time 2019-03-06 07:22:02.828341
last batch sz 160
Model ind 640 epoch 1398 head B head_i_epoch 0 batch 0: avg loss -2.034622 avg loss no lamb -2.034622 time 2019-03-06 07:23:23.487349
Model ind 640 epoch 1398 head B head_i_epoch 0 batch 100: avg loss -1.960607 avg loss no lamb -1.960607 time 2019-03-06 07:25:21.358956
Model ind 640 epoch 1398 head B head_i_epoch 0 batch 200: avg loss -2.000668 avg loss no lamb -2.000668 time 2019-03-06 07:27:20.992258
last batch sz 160
Model ind 640 epoch 1398 head B head_i_epoch 1 batch 0: avg loss -1.959476 avg loss no lamb -1.959476 time 2019-03-06 07:28:39.360303
Model ind 640 epoch 1398 head B head_i_epoch 1 batch 100: avg loss -1.925882 avg loss no lamb -1.925882 time 2019-03-06 07:30:44.213970
Model ind 640 epoch 1398 head B head_i_epoch 1 batch 200: avg loss -2.033967 avg loss no lamb -2.033967 time 2019-03-06 07:32:41.397868
last batch sz 160
Pre: time 2019-03-06 07:34:23.431210: 
 	std: 0.050269153
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5125333, 0.6148667, 0.61495, 0.61501664, 0.51213336]
	train_accs: [0.5125333, 0.6148667, 0.61495, 0.61501664, 0.51213336]
	best_train_sub_head: 3
	worst: 0.51213336
	avg: 0.5739
	best: 0.61501664

Starting e_i: 1399
Model ind 640 epoch 1399 head A head_i_epoch 0 batch 0: avg loss -3.335129 avg loss no lamb -3.335129 time 2019-03-06 07:34:28.513854
Model ind 640 epoch 1399 head A head_i_epoch 0 batch 100: avg loss -3.306201 avg loss no lamb -3.306201 time 2019-03-06 07:36:30.457003
Model ind 640 epoch 1399 head A head_i_epoch 0 batch 200: avg loss -3.368809 avg loss no lamb -3.368809 time 2019-03-06 07:38:31.480527
last batch sz 160
Model ind 640 epoch 1399 head B head_i_epoch 0 batch 0: avg loss -1.937562 avg loss no lamb -1.937562 time 2019-03-06 07:39:53.103843
Model ind 640 epoch 1399 head B head_i_epoch 0 batch 100: avg loss -2.001517 avg loss no lamb -2.001517 time 2019-03-06 07:41:53.945698
Model ind 640 epoch 1399 head B head_i_epoch 0 batch 200: avg loss -2.080299 avg loss no lamb -2.080299 time 2019-03-06 07:43:51.263574
last batch sz 160
Model ind 640 epoch 1399 head B head_i_epoch 1 batch 0: avg loss -2.003692 avg loss no lamb -2.003692 time 2019-03-06 07:45:15.402401
Model ind 640 epoch 1399 head B head_i_epoch 1 batch 100: avg loss -1.897159 avg loss no lamb -1.897159 time 2019-03-06 07:47:11.797513
Model ind 640 epoch 1399 head B head_i_epoch 1 batch 200: avg loss -2.024913 avg loss no lamb -2.024913 time 2019-03-06 07:49:06.899848
last batch sz 160
Pre: time 2019-03-06 07:50:56.662672: 
 	std: 0.05085697
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51196665, 0.61565, 0.61558336, 0.6156, 0.51163334]
	train_accs: [0.51196665, 0.61565, 0.61558336, 0.6156, 0.51163334]
	best_train_sub_head: 1
	worst: 0.51163334
	avg: 0.57408667
	best: 0.61565

Starting e_i: 1400
Model ind 640 epoch 1400 head A head_i_epoch 0 batch 0: avg loss -3.358855 avg loss no lamb -3.358855 time 2019-03-06 07:50:59.738641
Model ind 640 epoch 1400 head A head_i_epoch 0 batch 100: avg loss -3.340836 avg loss no lamb -3.340836 time 2019-03-06 07:52:57.409124
Model ind 640 epoch 1400 head A head_i_epoch 0 batch 200: avg loss -3.401361 avg loss no lamb -3.401361 time 2019-03-06 07:54:49.529740
last batch sz 160
Model ind 640 epoch 1400 head B head_i_epoch 0 batch 0: avg loss -2.048837 avg loss no lamb -2.048837 time 2019-03-06 07:56:24.273550
Model ind 640 epoch 1400 head B head_i_epoch 0 batch 100: avg loss -1.873728 avg loss no lamb -1.873728 time 2019-03-06 07:58:15.594926
Model ind 640 epoch 1400 head B head_i_epoch 0 batch 200: avg loss -2.061483 avg loss no lamb -2.061483 time 2019-03-06 08:00:13.553519
last batch sz 160
Model ind 640 epoch 1400 head B head_i_epoch 1 batch 0: avg loss -2.010079 avg loss no lamb -2.010079 time 2019-03-06 08:01:46.022437
Model ind 640 epoch 1400 head B head_i_epoch 1 batch 100: avg loss -1.973506 avg loss no lamb -1.973506 time 2019-03-06 08:03:41.115575
Model ind 640 epoch 1400 head B head_i_epoch 1 batch 200: avg loss -2.033530 avg loss no lamb -2.033530 time 2019-03-06 08:05:36.786280
last batch sz 160
Pre: time 2019-03-06 08:07:30.777785: 
 	std: 0.050798364
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5114, 0.61508334, 0.6150333, 0.61518335, 0.5114167]
	train_accs: [0.5114, 0.61508334, 0.6150333, 0.61518335, 0.5114167]
	best_train_sub_head: 3
	worst: 0.5114
	avg: 0.5736233
	best: 0.61518335

Starting e_i: 1401
Model ind 640 epoch 1401 head A head_i_epoch 0 batch 0: avg loss -3.333259 avg loss no lamb -3.333259 time 2019-03-06 08:07:38.928434
Model ind 640 epoch 1401 head A head_i_epoch 0 batch 100: avg loss -3.188457 avg loss no lamb -3.188457 time 2019-03-06 08:09:34.118099
Model ind 640 epoch 1401 head A head_i_epoch 0 batch 200: avg loss -3.320950 avg loss no lamb -3.320950 time 2019-03-06 08:11:28.794798
last batch sz 160
Model ind 640 epoch 1401 head B head_i_epoch 0 batch 0: avg loss -2.076989 avg loss no lamb -2.076989 time 2019-03-06 08:12:52.698069
Model ind 640 epoch 1401 head B head_i_epoch 0 batch 100: avg loss -1.998150 avg loss no lamb -1.998150 time 2019-03-06 08:14:47.757738
Model ind 640 epoch 1401 head B head_i_epoch 0 batch 200: avg loss -2.024617 avg loss no lamb -2.024617 time 2019-03-06 08:16:47.442144
last batch sz 160
Model ind 640 epoch 1401 head B head_i_epoch 1 batch 0: avg loss -2.018011 avg loss no lamb -2.018011 time 2019-03-06 08:18:07.080887
Model ind 640 epoch 1401 head B head_i_epoch 1 batch 100: avg loss -1.978886 avg loss no lamb -1.978886 time 2019-03-06 08:20:08.608724
Model ind 640 epoch 1401 head B head_i_epoch 1 batch 200: avg loss -2.017152 avg loss no lamb -2.017152 time 2019-03-06 08:22:12.011154
last batch sz 160
Pre: time 2019-03-06 08:23:51.459817: 
 	std: 0.05099576
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5104167, 0.61471665, 0.61466664, 0.61465, 0.51075]
	train_accs: [0.5104167, 0.61471665, 0.61466664, 0.61465, 0.51075]
	best_train_sub_head: 1
	worst: 0.5104167
	avg: 0.57304
	best: 0.61471665

Starting e_i: 1402
Model ind 640 epoch 1402 head A head_i_epoch 0 batch 0: avg loss -3.341071 avg loss no lamb -3.341071 time 2019-03-06 08:23:56.544552
Model ind 640 epoch 1402 head A head_i_epoch 0 batch 100: avg loss -3.300097 avg loss no lamb -3.300097 time 2019-03-06 08:25:59.092805
Model ind 640 epoch 1402 head A head_i_epoch 0 batch 200: avg loss -3.419719 avg loss no lamb -3.419719 time 2019-03-06 08:28:02.115297
last batch sz 160
Model ind 640 epoch 1402 head B head_i_epoch 0 batch 0: avg loss -1.954941 avg loss no lamb -1.954941 time 2019-03-06 08:29:20.358900
Model ind 640 epoch 1402 head B head_i_epoch 0 batch 100: avg loss -1.955591 avg loss no lamb -1.955591 time 2019-03-06 08:31:24.785856
Model ind 640 epoch 1402 head B head_i_epoch 0 batch 200: avg loss -2.047037 avg loss no lamb -2.047037 time 2019-03-06 08:33:24.010719
last batch sz 160
Model ind 640 epoch 1402 head B head_i_epoch 1 batch 0: avg loss -2.035205 avg loss no lamb -2.035205 time 2019-03-06 08:34:46.474159
Model ind 640 epoch 1402 head B head_i_epoch 1 batch 100: avg loss -1.887589 avg loss no lamb -1.887589 time 2019-03-06 08:36:44.206784
Model ind 640 epoch 1402 head B head_i_epoch 1 batch 200: avg loss -1.899441 avg loss no lamb -1.899441 time 2019-03-06 08:38:38.703595
last batch sz 160
Pre: time 2019-03-06 08:40:26.675410: 
 	std: 0.050609197
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125333, 0.61578333, 0.6158, 0.61578333, 0.51243335]
	train_accs: [0.5125333, 0.61578333, 0.6158, 0.61578333, 0.51243335]
	best_train_sub_head: 2
	worst: 0.51243335
	avg: 0.57446665
	best: 0.6158

Starting e_i: 1403
Model ind 640 epoch 1403 head A head_i_epoch 0 batch 0: avg loss -3.337563 avg loss no lamb -3.337563 time 2019-03-06 08:40:29.705295
Model ind 640 epoch 1403 head A head_i_epoch 0 batch 100: avg loss -3.266879 avg loss no lamb -3.266879 time 2019-03-06 08:42:28.783374
Model ind 640 epoch 1403 head A head_i_epoch 0 batch 200: avg loss -3.403741 avg loss no lamb -3.403741 time 2019-03-06 08:44:20.654999
last batch sz 160
Model ind 640 epoch 1403 head B head_i_epoch 0 batch 0: avg loss -2.002672 avg loss no lamb -2.002672 time 2019-03-06 08:45:53.039923
Model ind 640 epoch 1403 head B head_i_epoch 0 batch 100: avg loss -1.921484 avg loss no lamb -1.921484 time 2019-03-06 08:47:51.123686
Model ind 640 epoch 1403 head B head_i_epoch 0 batch 200: avg loss -1.975271 avg loss no lamb -1.975271 time 2019-03-06 08:49:43.966913
last batch sz 160
Model ind 640 epoch 1403 head B head_i_epoch 1 batch 0: avg loss -2.010830 avg loss no lamb -2.010830 time 2019-03-06 08:51:18.540032
Model ind 640 epoch 1403 head B head_i_epoch 1 batch 100: avg loss -1.994471 avg loss no lamb -1.994471 time 2019-03-06 08:53:14.098642
Model ind 640 epoch 1403 head B head_i_epoch 1 batch 200: avg loss -2.027550 avg loss no lamb -2.027550 time 2019-03-06 08:55:10.126775
last batch sz 160
Pre: time 2019-03-06 08:57:03.371641: 
 	std: 0.050770033
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51125, 0.6151, 0.6152, 0.61515, 0.51178336]
	train_accs: [0.51125, 0.6151, 0.6152, 0.61515, 0.51178336]
	best_train_sub_head: 2
	worst: 0.51125
	avg: 0.5736967
	best: 0.6152

Starting e_i: 1404
Model ind 640 epoch 1404 head A head_i_epoch 0 batch 0: avg loss -3.402835 avg loss no lamb -3.402835 time 2019-03-06 08:57:06.425888
Model ind 640 epoch 1404 head A head_i_epoch 0 batch 100: avg loss -3.310008 avg loss no lamb -3.310008 time 2019-03-06 08:59:00.594381
Model ind 640 epoch 1404 head A head_i_epoch 0 batch 200: avg loss -3.345196 avg loss no lamb -3.345196 time 2019-03-06 09:00:55.568666
last batch sz 160
Model ind 640 epoch 1404 head B head_i_epoch 0 batch 0: avg loss -1.962213 avg loss no lamb -1.962213 time 2019-03-06 09:02:18.651472
Model ind 640 epoch 1404 head B head_i_epoch 0 batch 100: avg loss -1.989705 avg loss no lamb -1.989705 time 2019-03-06 09:04:12.922534
Model ind 640 epoch 1404 head B head_i_epoch 0 batch 200: avg loss -1.998190 avg loss no lamb -1.998190 time 2019-03-06 09:06:13.450045
last batch sz 160
Model ind 640 epoch 1404 head B head_i_epoch 1 batch 0: avg loss -2.014326 avg loss no lamb -2.014326 time 2019-03-06 09:07:33.449230
Model ind 640 epoch 1404 head B head_i_epoch 1 batch 100: avg loss -1.974853 avg loss no lamb -1.974853 time 2019-03-06 09:09:33.176453
Model ind 640 epoch 1404 head B head_i_epoch 1 batch 200: avg loss -2.028273 avg loss no lamb -2.028273 time 2019-03-06 09:11:38.329524
last batch sz 160
Pre: time 2019-03-06 09:13:18.400558: 
 	std: 0.0500964
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5126333, 0.61508334, 0.61513335, 0.61518335, 0.51311666]
	train_accs: [0.5126333, 0.61508334, 0.61513335, 0.61518335, 0.51311666]
	best_train_sub_head: 3
	worst: 0.5126333
	avg: 0.57423
	best: 0.61518335

Starting e_i: 1405
Model ind 640 epoch 1405 head A head_i_epoch 0 batch 0: avg loss -3.367368 avg loss no lamb -3.367368 time 2019-03-06 09:13:22.810144
Model ind 640 epoch 1405 head A head_i_epoch 0 batch 100: avg loss -3.287591 avg loss no lamb -3.287591 time 2019-03-06 09:15:24.959445
Model ind 640 epoch 1405 head A head_i_epoch 0 batch 200: avg loss -3.364799 avg loss no lamb -3.364799 time 2019-03-06 09:17:30.650705
last batch sz 160
Model ind 640 epoch 1405 head B head_i_epoch 0 batch 0: avg loss -1.989263 avg loss no lamb -1.989263 time 2019-03-06 09:18:52.612066
Model ind 640 epoch 1405 head B head_i_epoch 0 batch 100: avg loss -1.929595 avg loss no lamb -1.929595 time 2019-03-06 09:20:54.194606
Model ind 640 epoch 1405 head B head_i_epoch 0 batch 200: avg loss -1.984222 avg loss no lamb -1.984222 time 2019-03-06 09:22:54.536593
last batch sz 160
Model ind 640 epoch 1405 head B head_i_epoch 1 batch 0: avg loss -2.018223 avg loss no lamb -2.018223 time 2019-03-06 09:24:14.763229
Model ind 640 epoch 1405 head B head_i_epoch 1 batch 100: avg loss -1.920525 avg loss no lamb -1.920525 time 2019-03-06 09:26:12.174640
Model ind 640 epoch 1405 head B head_i_epoch 1 batch 200: avg loss -2.045521 avg loss no lamb -2.045521 time 2019-03-06 09:28:06.374450
last batch sz 160
Pre: time 2019-03-06 09:29:53.765088: 
 	std: 0.050912816
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115167, 0.6156333, 0.6156167, 0.6157, 0.5119333]
	train_accs: [0.5115167, 0.6156333, 0.6156167, 0.6157, 0.5119333]
	best_train_sub_head: 3
	worst: 0.5115167
	avg: 0.57408
	best: 0.6157

Starting e_i: 1406
Model ind 640 epoch 1406 head A head_i_epoch 0 batch 0: avg loss -3.397654 avg loss no lamb -3.397654 time 2019-03-06 09:29:57.067258
Model ind 640 epoch 1406 head A head_i_epoch 0 batch 100: avg loss -3.245101 avg loss no lamb -3.245101 time 2019-03-06 09:31:55.631775
Model ind 640 epoch 1406 head A head_i_epoch 0 batch 200: avg loss -3.348656 avg loss no lamb -3.348656 time 2019-03-06 09:33:46.199025
last batch sz 160
Model ind 640 epoch 1406 head B head_i_epoch 0 batch 0: avg loss -1.952175 avg loss no lamb -1.952175 time 2019-03-06 09:35:18.659525
Model ind 640 epoch 1406 head B head_i_epoch 0 batch 100: avg loss -1.983024 avg loss no lamb -1.983024 time 2019-03-06 09:37:22.412480
Model ind 640 epoch 1406 head B head_i_epoch 0 batch 200: avg loss -1.961836 avg loss no lamb -1.961836 time 2019-03-06 09:39:12.465391
last batch sz 160
Model ind 640 epoch 1406 head B head_i_epoch 1 batch 0: avg loss -1.997935 avg loss no lamb -1.997935 time 2019-03-06 09:40:47.174396
Model ind 640 epoch 1406 head B head_i_epoch 1 batch 100: avg loss -1.924574 avg loss no lamb -1.924574 time 2019-03-06 09:42:42.246096
Model ind 640 epoch 1406 head B head_i_epoch 1 batch 200: avg loss -2.010318 avg loss no lamb -2.010318 time 2019-03-06 09:44:36.189418
last batch sz 160
Pre: time 2019-03-06 09:46:30.278902: 
 	std: 0.050712626
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5113, 0.61471665, 0.6149167, 0.61481667, 0.5113]
	train_accs: [0.5113, 0.61471665, 0.6149167, 0.61481667, 0.5113]
	best_train_sub_head: 2
	worst: 0.5113
	avg: 0.57341003
	best: 0.6149167

Starting e_i: 1407
Model ind 640 epoch 1407 head A head_i_epoch 0 batch 0: avg loss -3.429396 avg loss no lamb -3.429396 time 2019-03-06 09:46:33.335834
Model ind 640 epoch 1407 head A head_i_epoch 0 batch 100: avg loss -3.222598 avg loss no lamb -3.222598 time 2019-03-06 09:48:30.698565
Model ind 640 epoch 1407 head A head_i_epoch 0 batch 200: avg loss -3.415879 avg loss no lamb -3.415879 time 2019-03-06 09:50:24.892020
last batch sz 160
Model ind 640 epoch 1407 head B head_i_epoch 0 batch 0: avg loss -1.959718 avg loss no lamb -1.959718 time 2019-03-06 09:51:54.398068
Model ind 640 epoch 1407 head B head_i_epoch 0 batch 100: avg loss -1.956092 avg loss no lamb -1.956092 time 2019-03-06 09:53:50.155958
Model ind 640 epoch 1407 head B head_i_epoch 0 batch 200: avg loss -2.015118 avg loss no lamb -2.015118 time 2019-03-06 09:55:44.784242
last batch sz 160
Model ind 640 epoch 1407 head B head_i_epoch 1 batch 0: avg loss -2.015535 avg loss no lamb -2.015535 time 2019-03-06 09:57:07.772863
Model ind 640 epoch 1407 head B head_i_epoch 1 batch 100: avg loss -1.981634 avg loss no lamb -1.981634 time 2019-03-06 09:59:02.707363
Model ind 640 epoch 1407 head B head_i_epoch 1 batch 200: avg loss -1.995145 avg loss no lamb -1.995145 time 2019-03-06 10:01:00.268665
last batch sz 160
Pre: time 2019-03-06 10:02:44.211711: 
 	std: 0.050188694
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5122167, 0.6146167, 0.61471665, 0.61473334, 0.5122667]
	train_accs: [0.5122167, 0.6146167, 0.61471665, 0.61473334, 0.5122667]
	best_train_sub_head: 3
	worst: 0.5122167
	avg: 0.57371
	best: 0.61473334

Starting e_i: 1408
Model ind 640 epoch 1408 head A head_i_epoch 0 batch 0: avg loss -3.386641 avg loss no lamb -3.386641 time 2019-03-06 10:02:50.909657
Model ind 640 epoch 1408 head A head_i_epoch 0 batch 100: avg loss -3.316994 avg loss no lamb -3.316994 time 2019-03-06 10:04:47.884281
Model ind 640 epoch 1408 head A head_i_epoch 0 batch 200: avg loss -3.359782 avg loss no lamb -3.359782 time 2019-03-06 10:06:55.711697
last batch sz 160
Model ind 640 epoch 1408 head B head_i_epoch 0 batch 0: avg loss -2.025363 avg loss no lamb -2.025363 time 2019-03-06 10:08:15.193010
Model ind 640 epoch 1408 head B head_i_epoch 0 batch 100: avg loss -1.961426 avg loss no lamb -1.961426 time 2019-03-06 10:10:16.856301
Model ind 640 epoch 1408 head B head_i_epoch 0 batch 200: avg loss -2.014843 avg loss no lamb -2.014843 time 2019-03-06 10:12:22.510146
last batch sz 160
Model ind 640 epoch 1408 head B head_i_epoch 1 batch 0: avg loss -2.033348 avg loss no lamb -2.033348 time 2019-03-06 10:13:44.958733
Model ind 640 epoch 1408 head B head_i_epoch 1 batch 100: avg loss -1.923496 avg loss no lamb -1.923496 time 2019-03-06 10:15:46.251619
Model ind 640 epoch 1408 head B head_i_epoch 1 batch 200: avg loss -1.971441 avg loss no lamb -1.971441 time 2019-03-06 10:17:44.676300
last batch sz 160
Pre: time 2019-03-06 10:19:29.293653: 
 	std: 0.0504677
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125333, 0.61541665, 0.6155667, 0.61546665, 0.5124]
	train_accs: [0.5125333, 0.61541665, 0.6155667, 0.61546665, 0.5124]
	best_train_sub_head: 2
	worst: 0.5124
	avg: 0.5742766
	best: 0.6155667

Starting e_i: 1409
Model ind 640 epoch 1409 head A head_i_epoch 0 batch 0: avg loss -3.373060 avg loss no lamb -3.373060 time 2019-03-06 10:19:33.857717
Model ind 640 epoch 1409 head A head_i_epoch 0 batch 100: avg loss -3.338541 avg loss no lamb -3.338541 time 2019-03-06 10:21:32.418726
Model ind 640 epoch 1409 head A head_i_epoch 0 batch 200: avg loss -3.354014 avg loss no lamb -3.354014 time 2019-03-06 10:23:27.116257
last batch sz 160
Model ind 640 epoch 1409 head B head_i_epoch 0 batch 0: avg loss -1.995490 avg loss no lamb -1.995490 time 2019-03-06 10:24:52.147883
Model ind 640 epoch 1409 head B head_i_epoch 0 batch 100: avg loss -1.922073 avg loss no lamb -1.922073 time 2019-03-06 10:26:51.494877
Model ind 640 epoch 1409 head B head_i_epoch 0 batch 200: avg loss -1.987967 avg loss no lamb -1.987967 time 2019-03-06 10:28:41.675866
last batch sz 160
Model ind 640 epoch 1409 head B head_i_epoch 1 batch 0: avg loss -1.985228 avg loss no lamb -1.985228 time 2019-03-06 10:30:14.177740
Model ind 640 epoch 1409 head B head_i_epoch 1 batch 100: avg loss -1.978502 avg loss no lamb -1.978502 time 2019-03-06 10:32:13.179176
Model ind 640 epoch 1409 head B head_i_epoch 1 batch 200: avg loss -2.079076 avg loss no lamb -2.079076 time 2019-03-06 10:34:05.425835
last batch sz 160
Pre: time 2019-03-06 10:35:57.754478: 
 	std: 0.050594214
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51231664, 0.6156, 0.6155667, 0.6156333, 0.51233333]
	train_accs: [0.51231664, 0.6156, 0.6155667, 0.6156333, 0.51233333]
	best_train_sub_head: 3
	worst: 0.51231664
	avg: 0.57429
	best: 0.6156333

Starting e_i: 1410
Model ind 640 epoch 1410 head A head_i_epoch 0 batch 0: avg loss -3.324212 avg loss no lamb -3.324212 time 2019-03-06 10:36:01.125867
Model ind 640 epoch 1410 head A head_i_epoch 0 batch 100: avg loss -3.360192 avg loss no lamb -3.360192 time 2019-03-06 10:38:00.009520
Model ind 640 epoch 1410 head A head_i_epoch 0 batch 200: avg loss -3.401338 avg loss no lamb -3.401338 time 2019-03-06 10:39:54.645749
last batch sz 160
Model ind 640 epoch 1410 head B head_i_epoch 0 batch 0: avg loss -2.010078 avg loss no lamb -2.010078 time 2019-03-06 10:41:23.096305
Model ind 640 epoch 1410 head B head_i_epoch 0 batch 100: avg loss -1.943882 avg loss no lamb -1.943882 time 2019-03-06 10:43:18.905623
Model ind 640 epoch 1410 head B head_i_epoch 0 batch 200: avg loss -2.042680 avg loss no lamb -2.042680 time 2019-03-06 10:45:12.396381
last batch sz 160
Model ind 640 epoch 1410 head B head_i_epoch 1 batch 0: avg loss -1.960524 avg loss no lamb -1.960524 time 2019-03-06 10:46:36.398237
Model ind 640 epoch 1410 head B head_i_epoch 1 batch 100: avg loss -1.907078 avg loss no lamb -1.907078 time 2019-03-06 10:48:30.431437
Model ind 640 epoch 1410 head B head_i_epoch 1 batch 200: avg loss -1.983552 avg loss no lamb -1.983552 time 2019-03-06 10:50:29.266503
last batch sz 160
Pre: time 2019-03-06 10:52:14.201523: 
 	std: 0.050507125
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5125333, 0.6155667, 0.6156333, 0.6156167, 0.51248336]
	train_accs: [0.5125333, 0.6155667, 0.6156333, 0.6156167, 0.51248336]
	best_train_sub_head: 2
	worst: 0.51248336
	avg: 0.5743666
	best: 0.6156333

Starting e_i: 1411
Model ind 640 epoch 1411 head A head_i_epoch 0 batch 0: avg loss -3.382259 avg loss no lamb -3.382259 time 2019-03-06 10:52:25.229652
Model ind 640 epoch 1411 head A head_i_epoch 0 batch 100: avg loss -3.273343 avg loss no lamb -3.273343 time 2019-03-06 10:54:18.602778
Model ind 640 epoch 1411 head A head_i_epoch 0 batch 200: avg loss -3.351230 avg loss no lamb -3.351230 time 2019-03-06 10:56:24.037115
last batch sz 160
Model ind 640 epoch 1411 head B head_i_epoch 0 batch 0: avg loss -1.949653 avg loss no lamb -1.949653 time 2019-03-06 10:57:44.786799
Model ind 640 epoch 1411 head B head_i_epoch 0 batch 100: avg loss -1.944097 avg loss no lamb -1.944097 time 2019-03-06 10:59:46.426476
Model ind 640 epoch 1411 head B head_i_epoch 0 batch 200: avg loss -2.011258 avg loss no lamb -2.011258 time 2019-03-06 11:01:50.128349
last batch sz 160
Model ind 640 epoch 1411 head B head_i_epoch 1 batch 0: avg loss -1.949003 avg loss no lamb -1.949003 time 2019-03-06 11:03:12.127706
Model ind 640 epoch 1411 head B head_i_epoch 1 batch 100: avg loss -1.987427 avg loss no lamb -1.987427 time 2019-03-06 11:05:12.689705
Model ind 640 epoch 1411 head B head_i_epoch 1 batch 200: avg loss -2.013642 avg loss no lamb -2.013642 time 2019-03-06 11:07:15.430293
last batch sz 160
Pre: time 2019-03-06 11:08:59.034507: 
 	std: 0.050792918
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51196665, 0.6156333, 0.61575, 0.6157333, 0.51208335]
	train_accs: [0.51196665, 0.6156333, 0.61575, 0.6157333, 0.51208335]
	best_train_sub_head: 2
	worst: 0.51196665
	avg: 0.57423335
	best: 0.61575

Starting e_i: 1412
Model ind 640 epoch 1412 head A head_i_epoch 0 batch 0: avg loss -3.308939 avg loss no lamb -3.308939 time 2019-03-06 11:09:03.528948
Model ind 640 epoch 1412 head A head_i_epoch 0 batch 100: avg loss -3.280578 avg loss no lamb -3.280578 time 2019-03-06 11:10:59.912750
Model ind 640 epoch 1412 head A head_i_epoch 0 batch 200: avg loss -3.320826 avg loss no lamb -3.320826 time 2019-03-06 11:12:57.006698
last batch sz 160
Model ind 640 epoch 1412 head B head_i_epoch 0 batch 0: avg loss -1.919448 avg loss no lamb -1.919448 time 2019-03-06 11:14:20.235098
Model ind 640 epoch 1412 head B head_i_epoch 0 batch 100: avg loss -1.971853 avg loss no lamb -1.971853 time 2019-03-06 11:16:19.067829
Model ind 640 epoch 1412 head B head_i_epoch 0 batch 200: avg loss -2.014028 avg loss no lamb -2.014028 time 2019-03-06 11:18:10.769970
last batch sz 160
Model ind 640 epoch 1412 head B head_i_epoch 1 batch 0: avg loss -2.015515 avg loss no lamb -2.015515 time 2019-03-06 11:19:39.903268
Model ind 640 epoch 1412 head B head_i_epoch 1 batch 100: avg loss -1.957162 avg loss no lamb -1.957162 time 2019-03-06 11:21:33.453448
Model ind 640 epoch 1412 head B head_i_epoch 1 batch 200: avg loss -2.055015 avg loss no lamb -2.055015 time 2019-03-06 11:23:20.510528
last batch sz 160
Pre: time 2019-03-06 11:25:04.175407: 
 	std: 0.05077002
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51155, 0.615, 0.6148833, 0.6149167, 0.51105]
	train_accs: [0.51155, 0.615, 0.6148833, 0.6149167, 0.51105]
	best_train_sub_head: 1
	worst: 0.51105
	avg: 0.57348
	best: 0.615

Starting e_i: 1413
Model ind 640 epoch 1413 head A head_i_epoch 0 batch 0: avg loss -3.337435 avg loss no lamb -3.337435 time 2019-03-06 11:25:06.990905
Model ind 640 epoch 1413 head A head_i_epoch 0 batch 100: avg loss -3.341645 avg loss no lamb -3.341645 time 2019-03-06 11:26:57.744435
Model ind 640 epoch 1413 head A head_i_epoch 0 batch 200: avg loss -3.360829 avg loss no lamb -3.360829 time 2019-03-06 11:28:44.544452
last batch sz 160
Model ind 640 epoch 1413 head B head_i_epoch 0 batch 0: avg loss -2.071390 avg loss no lamb -2.071390 time 2019-03-06 11:30:08.105137
Model ind 640 epoch 1413 head B head_i_epoch 0 batch 100: avg loss -1.978668 avg loss no lamb -1.978668 time 2019-03-06 11:31:55.804850
Model ind 640 epoch 1413 head B head_i_epoch 0 batch 200: avg loss -2.001727 avg loss no lamb -2.001727 time 2019-03-06 11:33:46.651108
last batch sz 160
Model ind 640 epoch 1413 head B head_i_epoch 1 batch 0: avg loss -2.028641 avg loss no lamb -2.028641 time 2019-03-06 11:35:05.586199
Model ind 640 epoch 1413 head B head_i_epoch 1 batch 100: avg loss -1.897142 avg loss no lamb -1.897142 time 2019-03-06 11:36:54.472212
Model ind 640 epoch 1413 head B head_i_epoch 1 batch 200: avg loss -2.026207 avg loss no lamb -2.026207 time 2019-03-06 11:38:47.019745
last batch sz 160
Pre: time 2019-03-06 11:40:25.408673: 
 	std: 0.05086097
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5115167, 0.61523336, 0.61523336, 0.6152667, 0.51133335]
	train_accs: [0.5115167, 0.61523336, 0.61523336, 0.6152667, 0.51133335]
	best_train_sub_head: 3
	worst: 0.51133335
	avg: 0.57371676
	best: 0.6152667

Starting e_i: 1414
Model ind 640 epoch 1414 head A head_i_epoch 0 batch 0: avg loss -3.345989 avg loss no lamb -3.345989 time 2019-03-06 11:40:30.118641
Model ind 640 epoch 1414 head A head_i_epoch 0 batch 100: avg loss -3.334995 avg loss no lamb -3.334995 time 2019-03-06 11:42:20.481825
Model ind 640 epoch 1414 head A head_i_epoch 0 batch 200: avg loss -3.408876 avg loss no lamb -3.408876 time 2019-03-06 11:44:13.902966
last batch sz 160
Model ind 640 epoch 1414 head B head_i_epoch 0 batch 0: avg loss -2.081494 avg loss no lamb -2.081494 time 2019-03-06 11:45:30.345289
Model ind 640 epoch 1414 head B head_i_epoch 0 batch 100: avg loss -1.947153 avg loss no lamb -1.947153 time 2019-03-06 11:47:23.610844
Model ind 640 epoch 1414 head B head_i_epoch 0 batch 200: avg loss -2.028845 avg loss no lamb -2.028845 time 2019-03-06 11:49:11.667821
last batch sz 160
Model ind 640 epoch 1414 head B head_i_epoch 1 batch 0: avg loss -2.043038 avg loss no lamb -2.043038 time 2019-03-06 11:50:32.785899
Model ind 640 epoch 1414 head B head_i_epoch 1 batch 100: avg loss -1.933787 avg loss no lamb -1.933787 time 2019-03-06 11:52:24.860807
Model ind 640 epoch 1414 head B head_i_epoch 1 batch 200: avg loss -1.978525 avg loss no lamb -1.978525 time 2019-03-06 11:54:11.582502
last batch sz 160
Pre: time 2019-03-06 11:55:55.864303: 
 	std: 0.050982215
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51138335, 0.6152667, 0.6153, 0.61518335, 0.51098335]
	train_accs: [0.51138335, 0.6152667, 0.6153, 0.61518335, 0.51098335]
	best_train_sub_head: 2
	worst: 0.51098335
	avg: 0.57362336
	best: 0.6153

Starting e_i: 1415
Model ind 640 epoch 1415 head A head_i_epoch 0 batch 0: avg loss -3.337721 avg loss no lamb -3.337721 time 2019-03-06 11:55:58.719694
Model ind 640 epoch 1415 head A head_i_epoch 0 batch 100: avg loss -3.284365 avg loss no lamb -3.284365 time 2019-03-06 11:57:48.180013
Model ind 640 epoch 1415 head A head_i_epoch 0 batch 200: avg loss -3.305482 avg loss no lamb -3.305482 time 2019-03-06 11:59:35.824962
last batch sz 160
Model ind 640 epoch 1415 head B head_i_epoch 0 batch 0: avg loss -2.036389 avg loss no lamb -2.036389 time 2019-03-06 12:01:00.530861
Model ind 640 epoch 1415 head B head_i_epoch 0 batch 100: avg loss -1.969253 avg loss no lamb -1.969253 time 2019-03-06 12:02:46.213546
Model ind 640 epoch 1415 head B head_i_epoch 0 batch 200: avg loss -2.008431 avg loss no lamb -2.008431 time 2019-03-06 12:04:37.906435
last batch sz 160
Model ind 640 epoch 1415 head B head_i_epoch 1 batch 0: avg loss -1.965217 avg loss no lamb -1.965217 time 2019-03-06 12:05:57.709741
Model ind 640 epoch 1415 head B head_i_epoch 1 batch 100: avg loss -1.978549 avg loss no lamb -1.978549 time 2019-03-06 12:07:47.164843
Model ind 640 epoch 1415 head B head_i_epoch 1 batch 200: avg loss -2.001921 avg loss no lamb -2.001921 time 2019-03-06 12:09:41.037041
last batch sz 160
Pre: time 2019-03-06 12:11:18.753270: 
 	std: 0.050745424
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51205, 0.61545, 0.61543334, 0.61541665, 0.51165]
	train_accs: [0.51205, 0.61545, 0.61543334, 0.61541665, 0.51165]
	best_train_sub_head: 1
	worst: 0.51165
	avg: 0.574
	best: 0.61545

Starting e_i: 1416
Model ind 640 epoch 1416 head A head_i_epoch 0 batch 0: avg loss -3.341644 avg loss no lamb -3.341644 time 2019-03-06 12:11:22.921646
Model ind 640 epoch 1416 head A head_i_epoch 0 batch 100: avg loss -3.355032 avg loss no lamb -3.355032 time 2019-03-06 12:13:14.715627
Model ind 640 epoch 1416 head A head_i_epoch 0 batch 200: avg loss -3.331189 avg loss no lamb -3.331189 time 2019-03-06 12:15:07.304510
last batch sz 160
Model ind 640 epoch 1416 head B head_i_epoch 0 batch 0: avg loss -1.998462 avg loss no lamb -1.998462 time 2019-03-06 12:16:23.732277
Model ind 640 epoch 1416 head B head_i_epoch 0 batch 100: avg loss -1.957260 avg loss no lamb -1.957260 time 2019-03-06 12:18:17.313652
Model ind 640 epoch 1416 head B head_i_epoch 0 batch 200: avg loss -2.053665 avg loss no lamb -2.053665 time 2019-03-06 12:20:04.883481
last batch sz 160
Model ind 640 epoch 1416 head B head_i_epoch 1 batch 0: avg loss -2.011330 avg loss no lamb -2.011330 time 2019-03-06 12:21:26.551648
Model ind 640 epoch 1416 head B head_i_epoch 1 batch 100: avg loss -1.938129 avg loss no lamb -1.938129 time 2019-03-06 12:23:18.162824
Model ind 640 epoch 1416 head B head_i_epoch 1 batch 200: avg loss -2.034432 avg loss no lamb -2.034432 time 2019-03-06 12:25:04.389215
last batch sz 160
Pre: time 2019-03-06 12:26:48.950507: 
 	std: 0.050584808
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51266664, 0.6157333, 0.6158, 0.6157333, 0.51233333]
	train_accs: [0.51266664, 0.6157333, 0.6158, 0.6157333, 0.51233333]
	best_train_sub_head: 2
	worst: 0.51233333
	avg: 0.57445335
	best: 0.6158

Starting e_i: 1417
Model ind 640 epoch 1417 head A head_i_epoch 0 batch 0: avg loss -3.332523 avg loss no lamb -3.332523 time 2019-03-06 12:26:53.014948
Model ind 640 epoch 1417 head A head_i_epoch 0 batch 100: avg loss -3.304115 avg loss no lamb -3.304115 time 2019-03-06 12:28:41.943451
Model ind 640 epoch 1417 head A head_i_epoch 0 batch 200: avg loss -3.356639 avg loss no lamb -3.356639 time 2019-03-06 12:30:30.867072
last batch sz 160
Model ind 640 epoch 1417 head B head_i_epoch 0 batch 0: avg loss -1.971094 avg loss no lamb -1.971094 time 2019-03-06 12:31:54.313143
Model ind 640 epoch 1417 head B head_i_epoch 0 batch 100: avg loss -1.940781 avg loss no lamb -1.940781 time 2019-03-06 12:33:40.881402
Model ind 640 epoch 1417 head B head_i_epoch 0 batch 200: avg loss -2.054988 avg loss no lamb -2.054988 time 2019-03-06 12:35:28.681406
last batch sz 160
Model ind 640 epoch 1417 head B head_i_epoch 1 batch 0: avg loss -1.947265 avg loss no lamb -1.947265 time 2019-03-06 12:36:42.110011
Model ind 640 epoch 1417 head B head_i_epoch 1 batch 100: avg loss -1.903725 avg loss no lamb -1.903725 time 2019-03-06 12:38:22.624198
Model ind 640 epoch 1417 head B head_i_epoch 1 batch 200: avg loss -1.998583 avg loss no lamb -1.998583 time 2019-03-06 12:40:02.596997
last batch sz 160
Pre: time 2019-03-06 12:41:33.944304: 
 	std: 0.05086096
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51165, 0.61545, 0.61558336, 0.61555, 0.5117667]
	train_accs: [0.51165, 0.61545, 0.61558336, 0.61555, 0.5117667]
	best_train_sub_head: 2
	worst: 0.51165
	avg: 0.574
	best: 0.61558336

Starting e_i: 1418
Model ind 640 epoch 1418 head A head_i_epoch 0 batch 0: avg loss -3.347099 avg loss no lamb -3.347099 time 2019-03-06 12:41:36.517490
Model ind 640 epoch 1418 head A head_i_epoch 0 batch 100: avg loss -3.212822 avg loss no lamb -3.212822 time 2019-03-06 12:43:16.717774
Model ind 640 epoch 1418 head A head_i_epoch 0 batch 200: avg loss -3.322875 avg loss no lamb -3.322875 time 2019-03-06 12:44:57.000885
last batch sz 160
Model ind 640 epoch 1418 head B head_i_epoch 0 batch 0: avg loss -1.995215 avg loss no lamb -1.995215 time 2019-03-06 12:46:09.818088
Model ind 640 epoch 1418 head B head_i_epoch 0 batch 100: avg loss -1.931796 avg loss no lamb -1.931796 time 2019-03-06 12:47:49.858254
Model ind 640 epoch 1418 head B head_i_epoch 0 batch 200: avg loss -2.027046 avg loss no lamb -2.027046 time 2019-03-06 12:49:30.098231
last batch sz 160
Model ind 640 epoch 1418 head B head_i_epoch 1 batch 0: avg loss -1.887566 avg loss no lamb -1.887566 time 2019-03-06 12:50:42.794638
Model ind 640 epoch 1418 head B head_i_epoch 1 batch 100: avg loss -1.956959 avg loss no lamb -1.956959 time 2019-03-06 12:52:22.836883
Model ind 640 epoch 1418 head B head_i_epoch 1 batch 200: avg loss -2.036957 avg loss no lamb -2.036957 time 2019-03-06 12:54:02.704556
last batch sz 160
Pre: time 2019-03-06 12:55:33.988367: 
 	std: 0.050776634
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5114, 0.61518335, 0.6151, 0.61518335, 0.51161665]
	train_accs: [0.5114, 0.61518335, 0.6151, 0.61518335, 0.51161665]
	best_train_sub_head: 1
	worst: 0.5114
	avg: 0.57369673
	best: 0.61518335

Starting e_i: 1419
Model ind 640 epoch 1419 head A head_i_epoch 0 batch 0: avg loss -3.362326 avg loss no lamb -3.362326 time 2019-03-06 12:55:36.642653
Model ind 640 epoch 1419 head A head_i_epoch 0 batch 100: avg loss -3.247617 avg loss no lamb -3.247617 time 2019-03-06 12:57:16.567363
Model ind 640 epoch 1419 head A head_i_epoch 0 batch 200: avg loss -3.439353 avg loss no lamb -3.439353 time 2019-03-06 12:58:56.534602
last batch sz 160
Model ind 640 epoch 1419 head B head_i_epoch 0 batch 0: avg loss -2.054483 avg loss no lamb -2.054483 time 2019-03-06 13:00:09.301455
Model ind 640 epoch 1419 head B head_i_epoch 0 batch 100: avg loss -1.982540 avg loss no lamb -1.982540 time 2019-03-06 13:01:49.060658
Model ind 640 epoch 1419 head B head_i_epoch 0 batch 200: avg loss -2.011731 avg loss no lamb -2.011731 time 2019-03-06 13:03:28.846513
last batch sz 160
Model ind 640 epoch 1419 head B head_i_epoch 1 batch 0: avg loss -1.990274 avg loss no lamb -1.990274 time 2019-03-06 13:04:41.308450
Model ind 640 epoch 1419 head B head_i_epoch 1 batch 100: avg loss -1.990259 avg loss no lamb -1.990259 time 2019-03-06 13:06:21.068718
Model ind 640 epoch 1419 head B head_i_epoch 1 batch 200: avg loss -2.055508 avg loss no lamb -2.055508 time 2019-03-06 13:08:00.883835
last batch sz 160
Pre: time 2019-03-06 13:09:31.988613: 
 	std: 0.050992962
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51051664, 0.61465, 0.61471665, 0.6146, 0.51061666]
	train_accs: [0.51051664, 0.61465, 0.61471665, 0.6146, 0.51061666]
	best_train_sub_head: 2
	worst: 0.51051664
	avg: 0.57302
	best: 0.61471665

Starting e_i: 1420
Model ind 640 epoch 1420 head A head_i_epoch 0 batch 0: avg loss -3.375086 avg loss no lamb -3.375086 time 2019-03-06 13:09:35.434348
Model ind 640 epoch 1420 head A head_i_epoch 0 batch 100: avg loss -3.380814 avg loss no lamb -3.380814 time 2019-03-06 13:11:15.572333
Model ind 640 epoch 1420 head A head_i_epoch 0 batch 200: avg loss -3.350785 avg loss no lamb -3.350785 time 2019-03-06 13:12:55.656627
last batch sz 160
Model ind 640 epoch 1420 head B head_i_epoch 0 batch 0: avg loss -2.036659 avg loss no lamb -2.036659 time 2019-03-06 13:14:08.431021
Model ind 640 epoch 1420 head B head_i_epoch 0 batch 100: avg loss -1.905350 avg loss no lamb -1.905350 time 2019-03-06 13:15:48.952036
Model ind 640 epoch 1420 head B head_i_epoch 0 batch 200: avg loss -2.042986 avg loss no lamb -2.042986 time 2019-03-06 13:17:29.224140
last batch sz 160
Model ind 640 epoch 1420 head B head_i_epoch 1 batch 0: avg loss -1.972233 avg loss no lamb -1.972233 time 2019-03-06 13:18:41.915534
Model ind 640 epoch 1420 head B head_i_epoch 1 batch 100: avg loss -1.927054 avg loss no lamb -1.927054 time 2019-03-06 13:20:21.927613
Model ind 640 epoch 1420 head B head_i_epoch 1 batch 200: avg loss -2.038141 avg loss no lamb -2.038141 time 2019-03-06 13:22:01.964850
last batch sz 160
Pre: time 2019-03-06 13:23:33.359720: 
 	std: 0.050662383
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51145, 0.6146167, 0.6148833, 0.61471665, 0.5112]
	train_accs: [0.51145, 0.6146167, 0.6148833, 0.61471665, 0.5112]
	best_train_sub_head: 2
	worst: 0.5112
	avg: 0.5733733
	best: 0.6148833

Starting e_i: 1421
Model ind 640 epoch 1421 head A head_i_epoch 0 batch 0: avg loss -3.318469 avg loss no lamb -3.318469 time 2019-03-06 13:23:41.748735
Model ind 640 epoch 1421 head A head_i_epoch 0 batch 100: avg loss -3.359762 avg loss no lamb -3.359762 time 2019-03-06 13:25:21.828522
Model ind 640 epoch 1421 head A head_i_epoch 0 batch 200: avg loss -3.437052 avg loss no lamb -3.437052 time 2019-03-06 13:27:01.997143
last batch sz 160
Model ind 640 epoch 1421 head B head_i_epoch 0 batch 0: avg loss -2.018449 avg loss no lamb -2.018449 time 2019-03-06 13:28:14.822630
Model ind 640 epoch 1421 head B head_i_epoch 0 batch 100: avg loss -1.984789 avg loss no lamb -1.984789 time 2019-03-06 13:29:54.822069
Model ind 640 epoch 1421 head B head_i_epoch 0 batch 200: avg loss -2.044070 avg loss no lamb -2.044070 time 2019-03-06 13:31:34.709152
last batch sz 160
Model ind 640 epoch 1421 head B head_i_epoch 1 batch 0: avg loss -1.995981 avg loss no lamb -1.995981 time 2019-03-06 13:32:47.407094
Model ind 640 epoch 1421 head B head_i_epoch 1 batch 100: avg loss -1.970024 avg loss no lamb -1.970024 time 2019-03-06 13:34:27.428714
Model ind 640 epoch 1421 head B head_i_epoch 1 batch 200: avg loss -2.002419 avg loss no lamb -2.002419 time 2019-03-06 13:36:07.368439
last batch sz 160
Pre: time 2019-03-06 13:37:38.715649: 
 	std: 0.050779298
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51136667, 0.6149833, 0.61511666, 0.6150333, 0.5114167]
	train_accs: [0.51136667, 0.6149833, 0.61511666, 0.6150333, 0.5114167]
	best_train_sub_head: 2
	worst: 0.51136667
	avg: 0.5735833
	best: 0.61511666

Starting e_i: 1422
Model ind 640 epoch 1422 head A head_i_epoch 0 batch 0: avg loss -3.388650 avg loss no lamb -3.388650 time 2019-03-06 13:37:41.256627
Model ind 640 epoch 1422 head A head_i_epoch 0 batch 100: avg loss -3.232661 avg loss no lamb -3.232661 time 2019-03-06 13:39:21.415647
Model ind 640 epoch 1422 head A head_i_epoch 0 batch 200: avg loss -3.331807 avg loss no lamb -3.331807 time 2019-03-06 13:41:01.494185
last batch sz 160
Model ind 640 epoch 1422 head B head_i_epoch 0 batch 0: avg loss -1.995421 avg loss no lamb -1.995421 time 2019-03-06 13:42:14.280550
Model ind 640 epoch 1422 head B head_i_epoch 0 batch 100: avg loss -1.936796 avg loss no lamb -1.936796 time 2019-03-06 13:43:54.168239
Model ind 640 epoch 1422 head B head_i_epoch 0 batch 200: avg loss -1.997329 avg loss no lamb -1.997329 time 2019-03-06 13:45:34.109337
last batch sz 160
Model ind 640 epoch 1422 head B head_i_epoch 1 batch 0: avg loss -1.929613 avg loss no lamb -1.929613 time 2019-03-06 13:46:46.691762
Model ind 640 epoch 1422 head B head_i_epoch 1 batch 100: avg loss -1.933812 avg loss no lamb -1.933812 time 2019-03-06 13:48:26.659000
Model ind 640 epoch 1422 head B head_i_epoch 1 batch 200: avg loss -2.051952 avg loss no lamb -2.051952 time 2019-03-06 13:50:06.644550
last batch sz 160
Pre: time 2019-03-06 13:51:37.909871: 
 	std: 0.050285373
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51203334, 0.61455, 0.61481667, 0.6146167, 0.512]
	train_accs: [0.51203334, 0.61455, 0.61481667, 0.6146167, 0.512]
	best_train_sub_head: 2
	worst: 0.512
	avg: 0.57360333
	best: 0.61481667

Starting e_i: 1423
Model ind 640 epoch 1423 head A head_i_epoch 0 batch 0: avg loss -3.341107 avg loss no lamb -3.341107 time 2019-03-06 13:51:41.391036
Model ind 640 epoch 1423 head A head_i_epoch 0 batch 100: avg loss -3.303674 avg loss no lamb -3.303674 time 2019-03-06 13:53:21.504018
Model ind 640 epoch 1423 head A head_i_epoch 0 batch 200: avg loss -3.381858 avg loss no lamb -3.381858 time 2019-03-06 13:55:01.625178
last batch sz 160
Model ind 640 epoch 1423 head B head_i_epoch 0 batch 0: avg loss -1.978927 avg loss no lamb -1.978927 time 2019-03-06 13:56:14.397176
Model ind 640 epoch 1423 head B head_i_epoch 0 batch 100: avg loss -1.939915 avg loss no lamb -1.939915 time 2019-03-06 13:57:54.419470
Model ind 640 epoch 1423 head B head_i_epoch 0 batch 200: avg loss -1.996961 avg loss no lamb -1.996961 time 2019-03-06 13:59:34.394076
last batch sz 160
Model ind 640 epoch 1423 head B head_i_epoch 1 batch 0: avg loss -1.920373 avg loss no lamb -1.920373 time 2019-03-06 14:00:47.072758
Model ind 640 epoch 1423 head B head_i_epoch 1 batch 100: avg loss -1.943999 avg loss no lamb -1.943999 time 2019-03-06 14:02:27.055274
Model ind 640 epoch 1423 head B head_i_epoch 1 batch 200: avg loss -1.976192 avg loss no lamb -1.976192 time 2019-03-06 14:04:07.019375
last batch sz 160
Pre: time 2019-03-06 14:05:38.320966: 
 	std: 0.050764322
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51126665, 0.6149333, 0.61495, 0.6149333, 0.51136667]
	train_accs: [0.51126665, 0.6149333, 0.61495, 0.6149333, 0.51136667]
	best_train_sub_head: 2
	worst: 0.51126665
	avg: 0.57348996
	best: 0.61495

Starting e_i: 1424
Model ind 640 epoch 1424 head A head_i_epoch 0 batch 0: avg loss -3.400977 avg loss no lamb -3.400977 time 2019-03-06 14:05:40.907428
Model ind 640 epoch 1424 head A head_i_epoch 0 batch 100: avg loss -3.280352 avg loss no lamb -3.280352 time 2019-03-06 14:07:21.076641
Model ind 640 epoch 1424 head A head_i_epoch 0 batch 200: avg loss -3.409620 avg loss no lamb -3.409620 time 2019-03-06 14:09:01.262901
last batch sz 160
Model ind 640 epoch 1424 head B head_i_epoch 0 batch 0: avg loss -2.005805 avg loss no lamb -2.005805 time 2019-03-06 14:10:14.033644
Model ind 640 epoch 1424 head B head_i_epoch 0 batch 100: avg loss -2.004397 avg loss no lamb -2.004397 time 2019-03-06 14:11:53.952510
Model ind 640 epoch 1424 head B head_i_epoch 0 batch 200: avg loss -2.068790 avg loss no lamb -2.068790 time 2019-03-06 14:13:33.913492
last batch sz 160
Model ind 640 epoch 1424 head B head_i_epoch 1 batch 0: avg loss -1.982752 avg loss no lamb -1.982752 time 2019-03-06 14:14:46.648741
Model ind 640 epoch 1424 head B head_i_epoch 1 batch 100: avg loss -1.920621 avg loss no lamb -1.920621 time 2019-03-06 14:16:26.659331
Model ind 640 epoch 1424 head B head_i_epoch 1 batch 200: avg loss -2.051774 avg loss no lamb -2.051774 time 2019-03-06 14:18:06.520305
last batch sz 160
Pre: time 2019-03-06 14:19:37.491534: 
 	std: 0.050696287
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118833, 0.61543334, 0.61543334, 0.6153333, 0.51195]
	train_accs: [0.5118833, 0.61543334, 0.61543334, 0.6153333, 0.51195]
	best_train_sub_head: 1
	worst: 0.5118833
	avg: 0.5740067
	best: 0.61543334

Starting e_i: 1425
Model ind 640 epoch 1425 head A head_i_epoch 0 batch 0: avg loss -3.215423 avg loss no lamb -3.215423 time 2019-03-06 14:19:40.243070
Model ind 640 epoch 1425 head A head_i_epoch 0 batch 100: avg loss -3.346817 avg loss no lamb -3.346817 time 2019-03-06 14:21:19.937771
Model ind 640 epoch 1425 head A head_i_epoch 0 batch 200: avg loss -3.350079 avg loss no lamb -3.350079 time 2019-03-06 14:22:59.656310
last batch sz 160
Model ind 640 epoch 1425 head B head_i_epoch 0 batch 0: avg loss -2.013473 avg loss no lamb -2.013473 time 2019-03-06 14:24:12.268881
Model ind 640 epoch 1425 head B head_i_epoch 0 batch 100: avg loss -1.917451 avg loss no lamb -1.917451 time 2019-03-06 14:25:52.036628
Model ind 640 epoch 1425 head B head_i_epoch 0 batch 200: avg loss -1.927758 avg loss no lamb -1.927758 time 2019-03-06 14:27:31.843678
last batch sz 160
Model ind 640 epoch 1425 head B head_i_epoch 1 batch 0: avg loss -1.935189 avg loss no lamb -1.935189 time 2019-03-06 14:28:44.304799
Model ind 640 epoch 1425 head B head_i_epoch 1 batch 100: avg loss -1.955393 avg loss no lamb -1.955393 time 2019-03-06 14:30:23.988512
Model ind 640 epoch 1425 head B head_i_epoch 1 batch 200: avg loss -1.989248 avg loss no lamb -1.989248 time 2019-03-06 14:32:03.788120
last batch sz 160
Pre: time 2019-03-06 14:33:34.806226: 
 	std: 0.050643306
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51145, 0.6149667, 0.6150333, 0.61485, 0.5117]
	train_accs: [0.51145, 0.6149667, 0.6150333, 0.61485, 0.5117]
	best_train_sub_head: 2
	worst: 0.51145
	avg: 0.5736
	best: 0.6150333

Starting e_i: 1426
Model ind 640 epoch 1426 head A head_i_epoch 0 batch 0: avg loss -3.343267 avg loss no lamb -3.343267 time 2019-03-06 14:33:38.371827
Model ind 640 epoch 1426 head A head_i_epoch 0 batch 100: avg loss -3.302979 avg loss no lamb -3.302979 time 2019-03-06 14:35:18.514111
Model ind 640 epoch 1426 head A head_i_epoch 0 batch 200: avg loss -3.410958 avg loss no lamb -3.410958 time 2019-03-06 14:36:58.562350
last batch sz 160
Model ind 640 epoch 1426 head B head_i_epoch 0 batch 0: avg loss -2.034063 avg loss no lamb -2.034063 time 2019-03-06 14:38:11.354897
Model ind 640 epoch 1426 head B head_i_epoch 0 batch 100: avg loss -1.941512 avg loss no lamb -1.941512 time 2019-03-06 14:39:51.304081
Model ind 640 epoch 1426 head B head_i_epoch 0 batch 200: avg loss -2.015530 avg loss no lamb -2.015530 time 2019-03-06 14:41:31.210959
last batch sz 160
Model ind 640 epoch 1426 head B head_i_epoch 1 batch 0: avg loss -2.050392 avg loss no lamb -2.050392 time 2019-03-06 14:42:43.825376
Model ind 640 epoch 1426 head B head_i_epoch 1 batch 100: avg loss -1.884637 avg loss no lamb -1.884637 time 2019-03-06 14:44:23.818970
Model ind 640 epoch 1426 head B head_i_epoch 1 batch 200: avg loss -1.964561 avg loss no lamb -1.964561 time 2019-03-06 14:46:03.871965
last batch sz 160
Pre: time 2019-03-06 14:47:35.204216: 
 	std: 0.050661106
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51161665, 0.6151, 0.6152833, 0.6153, 0.51201665]
	train_accs: [0.51161665, 0.6151, 0.6152833, 0.6153, 0.51201665]
	best_train_sub_head: 3
	worst: 0.51161665
	avg: 0.5738633
	best: 0.6153

Starting e_i: 1427
Model ind 640 epoch 1427 head A head_i_epoch 0 batch 0: avg loss -3.361372 avg loss no lamb -3.361372 time 2019-03-06 14:47:37.832122
Model ind 640 epoch 1427 head A head_i_epoch 0 batch 100: avg loss -3.291848 avg loss no lamb -3.291848 time 2019-03-06 14:49:17.985675
Model ind 640 epoch 1427 head A head_i_epoch 0 batch 200: avg loss -3.309055 avg loss no lamb -3.309055 time 2019-03-06 14:50:58.109008
last batch sz 160
Model ind 640 epoch 1427 head B head_i_epoch 0 batch 0: avg loss -2.033512 avg loss no lamb -2.033512 time 2019-03-06 14:52:10.903618
Model ind 640 epoch 1427 head B head_i_epoch 0 batch 100: avg loss -1.883750 avg loss no lamb -1.883750 time 2019-03-06 14:53:50.838595
Model ind 640 epoch 1427 head B head_i_epoch 0 batch 200: avg loss -2.031921 avg loss no lamb -2.031921 time 2019-03-06 14:55:30.872296
last batch sz 160
Model ind 640 epoch 1427 head B head_i_epoch 1 batch 0: avg loss -2.041147 avg loss no lamb -2.041147 time 2019-03-06 14:56:43.466927
Model ind 640 epoch 1427 head B head_i_epoch 1 batch 100: avg loss -1.940557 avg loss no lamb -1.940557 time 2019-03-06 14:58:23.449724
Model ind 640 epoch 1427 head B head_i_epoch 1 batch 200: avg loss -1.943669 avg loss no lamb -1.943669 time 2019-03-06 15:00:03.420139
last batch sz 160
Pre: time 2019-03-06 15:01:34.716248: 
 	std: 0.05030856
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51173335, 0.61455, 0.6146333, 0.6146167, 0.51208335]
	train_accs: [0.51173335, 0.61455, 0.6146333, 0.6146167, 0.51208335]
	best_train_sub_head: 2
	worst: 0.51173335
	avg: 0.57352334
	best: 0.6146333

Starting e_i: 1428
Model ind 640 epoch 1428 head A head_i_epoch 0 batch 0: avg loss -3.325654 avg loss no lamb -3.325654 time 2019-03-06 15:01:37.318238
Model ind 640 epoch 1428 head A head_i_epoch 0 batch 100: avg loss -3.376933 avg loss no lamb -3.376933 time 2019-03-06 15:03:17.306856
Model ind 640 epoch 1428 head A head_i_epoch 0 batch 200: avg loss -3.377170 avg loss no lamb -3.377170 time 2019-03-06 15:04:57.328407
last batch sz 160
Model ind 640 epoch 1428 head B head_i_epoch 0 batch 0: avg loss -1.995028 avg loss no lamb -1.995028 time 2019-03-06 15:06:10.036318
Model ind 640 epoch 1428 head B head_i_epoch 0 batch 100: avg loss -1.969249 avg loss no lamb -1.969249 time 2019-03-06 15:07:49.771201
Model ind 640 epoch 1428 head B head_i_epoch 0 batch 200: avg loss -2.077067 avg loss no lamb -2.077067 time 2019-03-06 15:09:29.563179
last batch sz 160
Model ind 640 epoch 1428 head B head_i_epoch 1 batch 0: avg loss -1.980335 avg loss no lamb -1.980335 time 2019-03-06 15:10:42.105336
Model ind 640 epoch 1428 head B head_i_epoch 1 batch 100: avg loss -1.993947 avg loss no lamb -1.993947 time 2019-03-06 15:12:21.913282
Model ind 640 epoch 1428 head B head_i_epoch 1 batch 200: avg loss -2.057145 avg loss no lamb -2.057145 time 2019-03-06 15:14:01.703179
last batch sz 160
Pre: time 2019-03-06 15:15:32.765628: 
 	std: 0.050888166
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5111833, 0.61511666, 0.6151, 0.6150333, 0.5112333]
	train_accs: [0.5111833, 0.61511666, 0.6151, 0.6150333, 0.5112333]
	best_train_sub_head: 1
	worst: 0.5111833
	avg: 0.57353336
	best: 0.61511666

Starting e_i: 1429
Model ind 640 epoch 1429 head A head_i_epoch 0 batch 0: avg loss -3.334225 avg loss no lamb -3.334225 time 2019-03-06 15:15:36.369935
Model ind 640 epoch 1429 head A head_i_epoch 0 batch 100: avg loss -3.334551 avg loss no lamb -3.334551 time 2019-03-06 15:17:16.413880
Model ind 640 epoch 1429 head A head_i_epoch 0 batch 200: avg loss -3.414677 avg loss no lamb -3.414677 time 2019-03-06 15:18:56.498495
last batch sz 160
Model ind 640 epoch 1429 head B head_i_epoch 0 batch 0: avg loss -2.023156 avg loss no lamb -2.023156 time 2019-03-06 15:20:10.622903
Model ind 640 epoch 1429 head B head_i_epoch 0 batch 100: avg loss -1.956948 avg loss no lamb -1.956948 time 2019-03-06 15:21:52.742700
Model ind 640 epoch 1429 head B head_i_epoch 0 batch 200: avg loss -2.017045 avg loss no lamb -2.017045 time 2019-03-06 15:23:35.141942
last batch sz 160
Model ind 640 epoch 1429 head B head_i_epoch 1 batch 0: avg loss -2.021328 avg loss no lamb -2.021328 time 2019-03-06 15:24:48.687127
Model ind 640 epoch 1429 head B head_i_epoch 1 batch 100: avg loss -1.913145 avg loss no lamb -1.913145 time 2019-03-06 15:26:37.490243
Model ind 640 epoch 1429 head B head_i_epoch 1 batch 200: avg loss -2.017305 avg loss no lamb -2.017305 time 2019-03-06 15:28:26.848744
last batch sz 160
Pre: time 2019-03-06 15:30:07.034102: 
 	std: 0.050582
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51166666, 0.615, 0.6150333, 0.61501664, 0.5118667]
	train_accs: [0.51166666, 0.615, 0.6150333, 0.61501664, 0.5118667]
	best_train_sub_head: 2
	worst: 0.51166666
	avg: 0.57371664
	best: 0.6150333

Starting e_i: 1430
Model ind 640 epoch 1430 head A head_i_epoch 0 batch 0: avg loss -3.376144 avg loss no lamb -3.376144 time 2019-03-06 15:30:10.186623
Model ind 640 epoch 1430 head A head_i_epoch 0 batch 100: avg loss -3.352988 avg loss no lamb -3.352988 time 2019-03-06 15:31:59.508172
Model ind 640 epoch 1430 head A head_i_epoch 0 batch 200: avg loss -3.278929 avg loss no lamb -3.278929 time 2019-03-06 15:33:48.578907
last batch sz 160
Model ind 640 epoch 1430 head B head_i_epoch 0 batch 0: avg loss -2.069921 avg loss no lamb -2.069921 time 2019-03-06 15:35:07.214016
Model ind 640 epoch 1430 head B head_i_epoch 0 batch 100: avg loss -1.964601 avg loss no lamb -1.964601 time 2019-03-06 15:36:56.044218
Model ind 640 epoch 1430 head B head_i_epoch 0 batch 200: avg loss -1.996188 avg loss no lamb -1.996188 time 2019-03-06 15:38:44.879751
last batch sz 160
Model ind 640 epoch 1430 head B head_i_epoch 1 batch 0: avg loss -2.023348 avg loss no lamb -2.023348 time 2019-03-06 15:40:03.641787
Model ind 640 epoch 1430 head B head_i_epoch 1 batch 100: avg loss -2.006182 avg loss no lamb -2.006182 time 2019-03-06 15:41:52.401205
Model ind 640 epoch 1430 head B head_i_epoch 1 batch 200: avg loss -1.990273 avg loss no lamb -1.990273 time 2019-03-06 15:43:40.080987
last batch sz 160
Pre: time 2019-03-06 15:45:20.155778: 
 	std: 0.050613288
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51208335, 0.6153167, 0.61548334, 0.61536664, 0.51206666]
	train_accs: [0.51208335, 0.6153167, 0.61548334, 0.61536664, 0.51206666]
	best_train_sub_head: 2
	worst: 0.51206666
	avg: 0.57406336
	best: 0.61548334

Starting e_i: 1431
Model ind 640 epoch 1431 head A head_i_epoch 0 batch 0: avg loss -3.339842 avg loss no lamb -3.339842 time 2019-03-06 15:45:27.894661
Model ind 640 epoch 1431 head A head_i_epoch 0 batch 100: avg loss -3.322242 avg loss no lamb -3.322242 time 2019-03-06 15:47:17.607455
Model ind 640 epoch 1431 head A head_i_epoch 0 batch 200: avg loss -3.312739 avg loss no lamb -3.312739 time 2019-03-06 15:49:05.628344
last batch sz 160
Model ind 640 epoch 1431 head B head_i_epoch 0 batch 0: avg loss -2.066341 avg loss no lamb -2.066341 time 2019-03-06 15:50:24.555518
Model ind 640 epoch 1431 head B head_i_epoch 0 batch 100: avg loss -1.972899 avg loss no lamb -1.972899 time 2019-03-06 15:52:12.174589
Model ind 640 epoch 1431 head B head_i_epoch 0 batch 200: avg loss -2.054808 avg loss no lamb -2.054808 time 2019-03-06 15:53:59.957237
last batch sz 160
Model ind 640 epoch 1431 head B head_i_epoch 1 batch 0: avg loss -2.039290 avg loss no lamb -2.039290 time 2019-03-06 15:55:18.346814
Model ind 640 epoch 1431 head B head_i_epoch 1 batch 100: avg loss -1.976814 avg loss no lamb -1.976814 time 2019-03-06 15:57:07.291881
Model ind 640 epoch 1431 head B head_i_epoch 1 batch 200: avg loss -2.049974 avg loss no lamb -2.049974 time 2019-03-06 15:58:55.216056
last batch sz 160
Pre: time 2019-03-06 16:00:33.820534: 
 	std: 0.050684024
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5122667, 0.6157167, 0.6157, 0.6156833, 0.5122167]
	train_accs: [0.5122667, 0.6157167, 0.6157, 0.6156833, 0.5122167]
	best_train_sub_head: 1
	worst: 0.5122167
	avg: 0.5743167
	best: 0.6157167

Starting e_i: 1432
Model ind 640 epoch 1432 head A head_i_epoch 0 batch 0: avg loss -3.354111 avg loss no lamb -3.354111 time 2019-03-06 16:00:40.393154
Model ind 640 epoch 1432 head A head_i_epoch 0 batch 100: avg loss -3.299805 avg loss no lamb -3.299805 time 2019-03-06 16:02:28.619638
Model ind 640 epoch 1432 head A head_i_epoch 0 batch 200: avg loss -3.392245 avg loss no lamb -3.392245 time 2019-03-06 16:04:16.775108
last batch sz 160
Model ind 640 epoch 1432 head B head_i_epoch 0 batch 0: avg loss -2.016045 avg loss no lamb -2.016045 time 2019-03-06 16:05:36.477143
Model ind 640 epoch 1432 head B head_i_epoch 0 batch 100: avg loss -1.910303 avg loss no lamb -1.910303 time 2019-03-06 16:07:26.615346
Model ind 640 epoch 1432 head B head_i_epoch 0 batch 200: avg loss -2.006995 avg loss no lamb -2.006995 time 2019-03-06 16:09:15.634479
last batch sz 160
Model ind 640 epoch 1432 head B head_i_epoch 1 batch 0: avg loss -2.017815 avg loss no lamb -2.017815 time 2019-03-06 16:10:34.280242
Model ind 640 epoch 1432 head B head_i_epoch 1 batch 100: avg loss -2.010624 avg loss no lamb -2.010624 time 2019-03-06 16:12:23.758187
Model ind 640 epoch 1432 head B head_i_epoch 1 batch 200: avg loss -2.023296 avg loss no lamb -2.023296 time 2019-03-06 16:14:13.950459
last batch sz 160
Pre: time 2019-03-06 16:15:55.016277: 
 	std: 0.05051547
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51235, 0.6152, 0.61523336, 0.6153833, 0.51196665]
	train_accs: [0.51235, 0.6152, 0.61523336, 0.6153833, 0.51196665]
	best_train_sub_head: 3
	worst: 0.51196665
	avg: 0.5740267
	best: 0.6153833

Starting e_i: 1433
Model ind 640 epoch 1433 head A head_i_epoch 0 batch 0: avg loss -3.259424 avg loss no lamb -3.259424 time 2019-03-06 16:15:58.205074
Model ind 640 epoch 1433 head A head_i_epoch 0 batch 100: avg loss -3.241881 avg loss no lamb -3.241881 time 2019-03-06 16:17:49.200838
Model ind 640 epoch 1433 head A head_i_epoch 0 batch 200: avg loss -3.375685 avg loss no lamb -3.375685 time 2019-03-06 16:19:42.785302
last batch sz 160
Model ind 640 epoch 1433 head B head_i_epoch 0 batch 0: avg loss -2.053407 avg loss no lamb -2.053407 time 2019-03-06 16:21:03.034720
Model ind 640 epoch 1433 head B head_i_epoch 0 batch 100: avg loss -1.963835 avg loss no lamb -1.963835 time 2019-03-06 16:22:54.868179
Model ind 640 epoch 1433 head B head_i_epoch 0 batch 200: avg loss -1.998615 avg loss no lamb -1.998615 time 2019-03-06 16:24:46.448421
last batch sz 160
Model ind 640 epoch 1433 head B head_i_epoch 1 batch 0: avg loss -2.023008 avg loss no lamb -2.023008 time 2019-03-06 16:26:09.722964
Model ind 640 epoch 1433 head B head_i_epoch 1 batch 100: avg loss -1.998877 avg loss no lamb -1.998877 time 2019-03-06 16:28:01.323759
Model ind 640 epoch 1433 head B head_i_epoch 1 batch 200: avg loss -1.997074 avg loss no lamb -1.997074 time 2019-03-06 16:29:50.597545
last batch sz 160
Pre: time 2019-03-06 16:31:34.807346: 
 	std: 0.050703164
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118833, 0.61523336, 0.61525, 0.61523336, 0.5116]
	train_accs: [0.5118833, 0.61523336, 0.61525, 0.61523336, 0.5116]
	best_train_sub_head: 2
	worst: 0.5116
	avg: 0.57384
	best: 0.61525

Starting e_i: 1434
Model ind 640 epoch 1434 head A head_i_epoch 0 batch 0: avg loss -3.335734 avg loss no lamb -3.335734 time 2019-03-06 16:31:38.328970
Model ind 640 epoch 1434 head A head_i_epoch 0 batch 100: avg loss -3.311837 avg loss no lamb -3.311837 time 2019-03-06 16:33:29.574862
Model ind 640 epoch 1434 head A head_i_epoch 0 batch 200: avg loss -3.313700 avg loss no lamb -3.313700 time 2019-03-06 16:35:20.505310
last batch sz 160
Model ind 640 epoch 1434 head B head_i_epoch 0 batch 0: avg loss -1.989302 avg loss no lamb -1.989302 time 2019-03-06 16:36:40.100719
Model ind 640 epoch 1434 head B head_i_epoch 0 batch 100: avg loss -1.988069 avg loss no lamb -1.988069 time 2019-03-06 16:38:25.396370
Model ind 640 epoch 1434 head B head_i_epoch 0 batch 200: avg loss -2.059593 avg loss no lamb -2.059593 time 2019-03-06 16:40:13.951010
last batch sz 160
Model ind 640 epoch 1434 head B head_i_epoch 1 batch 0: avg loss -2.005943 avg loss no lamb -2.005943 time 2019-03-06 16:41:35.303820
Model ind 640 epoch 1434 head B head_i_epoch 1 batch 100: avg loss -1.968098 avg loss no lamb -1.968098 time 2019-03-06 16:43:24.978849
Model ind 640 epoch 1434 head B head_i_epoch 1 batch 200: avg loss -2.054512 avg loss no lamb -2.054512 time 2019-03-06 16:45:16.754187
last batch sz 160
Pre: time 2019-03-06 16:46:58.471769: 
 	std: 0.050702196
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5119, 0.61506665, 0.61506665, 0.615, 0.5112]
	train_accs: [0.5119, 0.61506665, 0.61506665, 0.615, 0.5112]
	best_train_sub_head: 1
	worst: 0.5112
	avg: 0.57364666
	best: 0.61506665

Starting e_i: 1435
Model ind 640 epoch 1435 head A head_i_epoch 0 batch 0: avg loss -3.409836 avg loss no lamb -3.409836 time 2019-03-06 16:47:03.328494
Model ind 640 epoch 1435 head A head_i_epoch 0 batch 100: avg loss -3.363836 avg loss no lamb -3.363836 time 2019-03-06 16:48:54.194262
Model ind 640 epoch 1435 head A head_i_epoch 0 batch 200: avg loss -3.370716 avg loss no lamb -3.370716 time 2019-03-06 16:50:46.535449
last batch sz 160
Model ind 640 epoch 1435 head B head_i_epoch 0 batch 0: avg loss -1.996141 avg loss no lamb -1.996141 time 2019-03-06 16:52:06.913712
Model ind 640 epoch 1435 head B head_i_epoch 0 batch 100: avg loss -1.915138 avg loss no lamb -1.915138 time 2019-03-06 16:53:58.444534
Model ind 640 epoch 1435 head B head_i_epoch 0 batch 200: avg loss -2.056167 avg loss no lamb -2.056167 time 2019-03-06 16:55:50.804656
last batch sz 160
Model ind 640 epoch 1435 head B head_i_epoch 1 batch 0: avg loss -2.022478 avg loss no lamb -2.022478 time 2019-03-06 16:57:10.648326
Model ind 640 epoch 1435 head B head_i_epoch 1 batch 100: avg loss -1.914140 avg loss no lamb -1.914140 time 2019-03-06 16:59:01.852630
Model ind 640 epoch 1435 head B head_i_epoch 1 batch 200: avg loss -1.973194 avg loss no lamb -1.973194 time 2019-03-06 17:00:53.518441
last batch sz 160
Pre: time 2019-03-06 17:02:33.394901: 
 	std: 0.050619308
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5118167, 0.61473334, 0.61478335, 0.61473334, 0.51103336]
	train_accs: [0.5118167, 0.61473334, 0.61478335, 0.61473334, 0.51103336]
	best_train_sub_head: 2
	worst: 0.51103336
	avg: 0.57342
	best: 0.61478335

Starting e_i: 1436
Model ind 640 epoch 1436 head A head_i_epoch 0 batch 0: avg loss -3.345152 avg loss no lamb -3.345152 time 2019-03-06 17:02:37.430363
Model ind 640 epoch 1436 head A head_i_epoch 0 batch 100: avg loss -3.326942 avg loss no lamb -3.326942 time 2019-03-06 17:04:27.573046
Model ind 640 epoch 1436 head A head_i_epoch 0 batch 200: avg loss -3.427037 avg loss no lamb -3.427037 time 2019-03-06 17:06:19.687023
last batch sz 160
Model ind 640 epoch 1436 head B head_i_epoch 0 batch 0: avg loss -2.018120 avg loss no lamb -2.018120 time 2019-03-06 17:07:39.668267
Model ind 640 epoch 1436 head B head_i_epoch 0 batch 100: avg loss -1.938386 avg loss no lamb -1.938386 time 2019-03-06 17:09:31.935677
Model ind 640 epoch 1436 head B head_i_epoch 0 batch 200: avg loss -2.032921 avg loss no lamb -2.032921 time 2019-03-06 17:11:22.781488
last batch sz 160
Model ind 640 epoch 1436 head B head_i_epoch 1 batch 0: avg loss -1.949132 avg loss no lamb -1.949132 time 2019-03-06 17:12:41.934277
Model ind 640 epoch 1436 head B head_i_epoch 1 batch 100: avg loss -1.889685 avg loss no lamb -1.889685 time 2019-03-06 17:14:33.833236
Model ind 640 epoch 1436 head B head_i_epoch 1 batch 200: avg loss -2.036560 avg loss no lamb -2.036560 time 2019-03-06 17:16:24.753537
last batch sz 160
Pre: time 2019-03-06 17:18:06.767065: 
 	std: 0.050647408
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.5118, 0.61501664, 0.61501664, 0.6149667, 0.51143336]
	train_accs: [0.5118, 0.61501664, 0.61501664, 0.6149667, 0.51143336]
	best_train_sub_head: 1
	worst: 0.51143336
	avg: 0.57364666
	best: 0.61501664

Starting e_i: 1437
Model ind 640 epoch 1437 head A head_i_epoch 0 batch 0: avg loss -3.331500 avg loss no lamb -3.331500 time 2019-03-06 17:18:10.058828
Model ind 640 epoch 1437 head A head_i_epoch 0 batch 100: avg loss -3.397831 avg loss no lamb -3.397831 time 2019-03-06 17:20:01.947915
Model ind 640 epoch 1437 head A head_i_epoch 0 batch 200: avg loss -3.378470 avg loss no lamb -3.378470 time 2019-03-06 17:21:51.814342
last batch sz 160
Model ind 640 epoch 1437 head B head_i_epoch 0 batch 0: avg loss -2.013835 avg loss no lamb -2.013835 time 2019-03-06 17:23:11.673714
Model ind 640 epoch 1437 head B head_i_epoch 0 batch 100: avg loss -1.925040 avg loss no lamb -1.925040 time 2019-03-06 17:25:02.854940
Model ind 640 epoch 1437 head B head_i_epoch 0 batch 200: avg loss -2.022166 avg loss no lamb -2.022166 time 2019-03-06 17:26:52.789385
last batch sz 160
Model ind 640 epoch 1437 head B head_i_epoch 1 batch 0: avg loss -2.035655 avg loss no lamb -2.035655 time 2019-03-06 17:28:14.841939
Model ind 640 epoch 1437 head B head_i_epoch 1 batch 100: avg loss -1.925153 avg loss no lamb -1.925153 time 2019-03-06 17:30:05.341867
Model ind 640 epoch 1437 head B head_i_epoch 1 batch 200: avg loss -1.968764 avg loss no lamb -1.968764 time 2019-03-06 17:31:54.381878
last batch sz 160
Pre: time 2019-03-06 17:33:36.360607: 
 	std: 0.050447628
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51238334, 0.61508334, 0.61516666, 0.61495, 0.5118]
	train_accs: [0.51238334, 0.61508334, 0.61516666, 0.61495, 0.5118]
	best_train_sub_head: 2
	worst: 0.5118
	avg: 0.5738767
	best: 0.61516666

Starting e_i: 1438
Model ind 640 epoch 1438 head A head_i_epoch 0 batch 0: avg loss -3.355204 avg loss no lamb -3.355204 time 2019-03-06 17:33:40.878044
Model ind 640 epoch 1438 head A head_i_epoch 0 batch 100: avg loss -3.292647 avg loss no lamb -3.292647 time 2019-03-06 17:35:32.317344
Model ind 640 epoch 1438 head A head_i_epoch 0 batch 200: avg loss -3.378395 avg loss no lamb -3.378395 time 2019-03-06 17:37:21.991171
last batch sz 160
Model ind 640 epoch 1438 head B head_i_epoch 0 batch 0: avg loss -2.049750 avg loss no lamb -2.049750 time 2019-03-06 17:38:44.579463
Model ind 640 epoch 1438 head B head_i_epoch 0 batch 100: avg loss -1.953066 avg loss no lamb -1.953066 time 2019-03-06 17:40:34.779880
Model ind 640 epoch 1438 head B head_i_epoch 0 batch 200: avg loss -1.987942 avg loss no lamb -1.987942 time 2019-03-06 17:42:26.505969
last batch sz 160
Model ind 640 epoch 1438 head B head_i_epoch 1 batch 0: avg loss -1.975533 avg loss no lamb -1.975533 time 2019-03-06 17:43:48.259056
Model ind 640 epoch 1438 head B head_i_epoch 1 batch 100: avg loss -1.960376 avg loss no lamb -1.960376 time 2019-03-06 17:45:38.368822
Model ind 640 epoch 1438 head B head_i_epoch 1 batch 200: avg loss -2.011099 avg loss no lamb -2.011099 time 2019-03-06 17:47:28.708404
last batch sz 160
Pre: time 2019-03-06 17:49:11.785676: 
 	std: 0.050722197
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5115333, 0.6149, 0.6149833, 0.61495, 0.51128334]
	train_accs: [0.5115333, 0.6149, 0.6149833, 0.61495, 0.51128334]
	best_train_sub_head: 2
	worst: 0.51128334
	avg: 0.57353
	best: 0.6149833

Starting e_i: 1439
Model ind 640 epoch 1439 head A head_i_epoch 0 batch 0: avg loss -3.348243 avg loss no lamb -3.348243 time 2019-03-06 17:49:15.140122
Model ind 640 epoch 1439 head A head_i_epoch 0 batch 100: avg loss -3.343681 avg loss no lamb -3.343681 time 2019-03-06 17:51:05.989183
Model ind 640 epoch 1439 head A head_i_epoch 0 batch 200: avg loss -3.374584 avg loss no lamb -3.374584 time 2019-03-06 17:52:58.058171
last batch sz 160
Model ind 640 epoch 1439 head B head_i_epoch 0 batch 0: avg loss -2.049521 avg loss no lamb -2.049521 time 2019-03-06 17:54:20.146348
Model ind 640 epoch 1439 head B head_i_epoch 0 batch 100: avg loss -1.947311 avg loss no lamb -1.947311 time 2019-03-06 17:56:09.214540
Model ind 640 epoch 1439 head B head_i_epoch 0 batch 200: avg loss -2.002894 avg loss no lamb -2.002894 time 2019-03-06 17:58:01.327495
last batch sz 160
Model ind 640 epoch 1439 head B head_i_epoch 1 batch 0: avg loss -1.909582 avg loss no lamb -1.909582 time 2019-03-06 17:59:21.293439
Model ind 640 epoch 1439 head B head_i_epoch 1 batch 100: avg loss -1.971838 avg loss no lamb -1.971838 time 2019-03-06 18:01:11.864297
Model ind 640 epoch 1439 head B head_i_epoch 1 batch 200: avg loss -2.000345 avg loss no lamb -2.000345 time 2019-03-06 18:03:03.911497
last batch sz 160
Pre: time 2019-03-06 18:04:44.281119: 
 	std: 0.05068543
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51205, 0.6153833, 0.61543334, 0.61546665, 0.5118833]
	train_accs: [0.51205, 0.6153833, 0.61543334, 0.61546665, 0.5118833]
	best_train_sub_head: 3
	worst: 0.5118833
	avg: 0.57404333
	best: 0.61546665

Starting e_i: 1440
Model ind 640 epoch 1440 head A head_i_epoch 0 batch 0: avg loss -3.310498 avg loss no lamb -3.310498 time 2019-03-06 18:04:48.004518
Model ind 640 epoch 1440 head A head_i_epoch 0 batch 100: avg loss -3.320916 avg loss no lamb -3.320916 time 2019-03-06 18:06:38.027187
Model ind 640 epoch 1440 head A head_i_epoch 0 batch 200: avg loss -3.384322 avg loss no lamb -3.384322 time 2019-03-06 18:08:29.761503
last batch sz 160
Model ind 640 epoch 1440 head B head_i_epoch 0 batch 0: avg loss -2.018570 avg loss no lamb -2.018570 time 2019-03-06 18:09:49.078937
Model ind 640 epoch 1440 head B head_i_epoch 0 batch 100: avg loss -1.940044 avg loss no lamb -1.940044 time 2019-03-06 18:11:39.564321
Model ind 640 epoch 1440 head B head_i_epoch 0 batch 200: avg loss -2.031817 avg loss no lamb -2.031817 time 2019-03-06 18:13:30.703912
last batch sz 160
Model ind 640 epoch 1440 head B head_i_epoch 1 batch 0: avg loss -1.942580 avg loss no lamb -1.942580 time 2019-03-06 18:14:50.405300
Model ind 640 epoch 1440 head B head_i_epoch 1 batch 100: avg loss -1.952670 avg loss no lamb -1.952670 time 2019-03-06 18:16:41.446108
Model ind 640 epoch 1440 head B head_i_epoch 1 batch 200: avg loss -2.021981 avg loss no lamb -2.021981 time 2019-03-06 18:18:31.625536
last batch sz 160
Pre: time 2019-03-06 18:20:11.974064: 
 	std: 0.050937194
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.5103667, 0.6141667, 0.61431664, 0.6142667, 0.51018333]
	train_accs: [0.5103667, 0.6141667, 0.61431664, 0.6142667, 0.51018333]
	best_train_sub_head: 2
	worst: 0.51018333
	avg: 0.57266
	best: 0.61431664

Starting e_i: 1441
Model ind 640 epoch 1441 head A head_i_epoch 0 batch 0: avg loss -3.302104 avg loss no lamb -3.302104 time 2019-03-06 18:20:22.944147
Model ind 640 epoch 1441 head A head_i_epoch 0 batch 100: avg loss -3.286901 avg loss no lamb -3.286901 time 2019-03-06 18:22:15.212773
Model ind 640 epoch 1441 head A head_i_epoch 0 batch 200: avg loss -3.429110 avg loss no lamb -3.429110 time 2019-03-06 18:24:06.743734
last batch sz 160
Model ind 640 epoch 1441 head B head_i_epoch 0 batch 0: avg loss -1.958809 avg loss no lamb -1.958809 time 2019-03-06 18:25:27.287435
Model ind 640 epoch 1441 head B head_i_epoch 0 batch 100: avg loss -1.963828 avg loss no lamb -1.963828 time 2019-03-06 18:27:18.805463
Model ind 640 epoch 1441 head B head_i_epoch 0 batch 200: avg loss -1.999347 avg loss no lamb -1.999347 time 2019-03-06 18:29:08.815987
last batch sz 160
Model ind 640 epoch 1441 head B head_i_epoch 1 batch 0: avg loss -2.022289 avg loss no lamb -2.022289 time 2019-03-06 18:30:28.274433
Model ind 640 epoch 1441 head B head_i_epoch 1 batch 100: avg loss -1.938396 avg loss no lamb -1.938396 time 2019-03-06 18:32:20.399526
Model ind 640 epoch 1441 head B head_i_epoch 1 batch 200: avg loss -2.003691 avg loss no lamb -2.003691 time 2019-03-06 18:34:12.004446
last batch sz 160
Pre: time 2019-03-06 18:35:53.394308: 
 	std: 0.05052352
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 1), (3, 9), (4, 5), (5, 0), (6, 6), (7, 7), (8, 4), (9, 3)]
	test_accs: [0.51231664, 0.6153, 0.6154, 0.6153167, 0.5121]
	train_accs: [0.51231664, 0.6153, 0.6154, 0.6153167, 0.5121]
	best_train_sub_head: 2
	worst: 0.5121
	avg: 0.57408667
	best: 0.6154

Starting e_i: 1442
Model ind 640 epoch 1442 head A head_i_epoch 0 batch 0: avg loss -3.405996 avg loss no lamb -3.405996 time 2019-03-06 18:35:56.599555
Model ind 640 epoch 1442 head A head_i_epoch 0 batch 100: avg loss -3.317303 avg loss no lamb -3.317303 time 2019-03-06 18:37:48.566899
Model ind 640 epoch 1442 head A head_i_epoch 0 batch 200: avg loss -3.297498 avg loss no lamb -3.297498 time 2019-03-06 18:39:38.441457
last batch sz 160
Model ind 640 epoch 1442 head B head_i_epoch 0 batch 0: avg loss -2.026631 avg loss no lamb -2.026631 time 2019-03-06 18:40:58.742135
Model ind 640 epoch 1442 head B head_i_epoch 0 batch 100: avg loss -1.949265 avg loss no lamb -1.949265 time 2019-03-06 18:42:49.425324
Model ind 640 epoch 1442 head B head_i_epoch 0 batch 200: avg loss -2.044774 avg loss no lamb -2.044774 time 2019-03-06 18:44:38.839503
last batch sz 160
Model ind 640 epoch 1442 head B head_i_epoch 1 batch 0: avg loss -1.953443 avg loss no lamb -1.953443 time 2019-03-06 18:46:01.711392
Model ind 640 epoch 1442 head B head_i_epoch 1 batch 100: avg loss -2.009603 avg loss no lamb -2.009603 time 2019-03-06 18:47:52.046111
Model ind 640 epoch 1442 head B head_i_epoch 1 batch 200: avg loss -2.044677 avg loss no lamb -2.044677 time 2019-03-06 18:49:42.146245
last batch sz 160
Pre: time 2019-03-06 18:51:24.920254: 
 	std: 0.050809637
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5110667, 0.61441666, 0.61446667, 0.61448336, 0.5104167]
	train_accs: [0.5110667, 0.61441666, 0.61446667, 0.61448336, 0.5104167]
	best_train_sub_head: 3
	worst: 0.5104167
	avg: 0.57297003
	best: 0.61448336

Starting e_i: 1443
Model ind 640 epoch 1443 head A head_i_epoch 0 batch 0: avg loss -3.419165 avg loss no lamb -3.419165 time 2019-03-06 18:51:28.196777
Model ind 640 epoch 1443 head A head_i_epoch 0 batch 100: avg loss -3.256416 avg loss no lamb -3.256416 time 2019-03-06 18:53:18.740602
Model ind 640 epoch 1443 head A head_i_epoch 0 batch 200: avg loss -3.362268 avg loss no lamb -3.362268 time 2019-03-06 18:55:07.953400
last batch sz 160
Model ind 640 epoch 1443 head B head_i_epoch 0 batch 0: avg loss -1.980178 avg loss no lamb -1.980178 time 2019-03-06 18:56:30.167143
Model ind 640 epoch 1443 head B head_i_epoch 0 batch 100: avg loss -1.947077 avg loss no lamb -1.947077 time 2019-03-06 18:58:20.632675
Model ind 640 epoch 1443 head B head_i_epoch 0 batch 200: avg loss -2.033748 avg loss no lamb -2.033748 time 2019-03-06 19:00:11.929447
last batch sz 160
Model ind 640 epoch 1443 head B head_i_epoch 1 batch 0: avg loss -1.975124 avg loss no lamb -1.975124 time 2019-03-06 19:01:32.846263
Model ind 640 epoch 1443 head B head_i_epoch 1 batch 100: avg loss -1.949043 avg loss no lamb -1.949043 time 2019-03-06 19:03:22.358547
Model ind 640 epoch 1443 head B head_i_epoch 1 batch 200: avg loss -2.027714 avg loss no lamb -2.027714 time 2019-03-06 19:05:13.538535
last batch sz 160
Pre: time 2019-03-06 19:06:55.907660: 
 	std: 0.050907373
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.51173335, 0.6154, 0.61541665, 0.6155, 0.51131666]
	train_accs: [0.51173335, 0.6154, 0.61541665, 0.6155, 0.51131666]
	best_train_sub_head: 3
	worst: 0.51131666
	avg: 0.57387334
	best: 0.6155

Starting e_i: 1444
Model ind 640 epoch 1444 head A head_i_epoch 0 batch 0: avg loss -3.324162 avg loss no lamb -3.324162 time 2019-03-06 19:07:00.864358
Model ind 640 epoch 1444 head A head_i_epoch 0 batch 100: avg loss -3.272086 avg loss no lamb -3.272086 time 2019-03-06 19:08:51.230982
Model ind 640 epoch 1444 head A head_i_epoch 0 batch 200: avg loss -3.377074 avg loss no lamb -3.377074 time 2019-03-06 19:10:43.576728
last batch sz 160
Model ind 640 epoch 1444 head B head_i_epoch 0 batch 0: avg loss -2.030703 avg loss no lamb -2.030703 time 2019-03-06 19:12:05.822168
Model ind 640 epoch 1444 head B head_i_epoch 0 batch 100: avg loss -1.958640 avg loss no lamb -1.958640 time 2019-03-06 19:13:56.708818
Model ind 640 epoch 1444 head B head_i_epoch 0 batch 200: avg loss -2.030299 avg loss no lamb -2.030299 time 2019-03-06 19:15:49.215957
last batch sz 160
Model ind 640 epoch 1444 head B head_i_epoch 1 batch 0: avg loss -1.912968 avg loss no lamb -1.912968 time 2019-03-06 19:17:09.456719
Model ind 640 epoch 1444 head B head_i_epoch 1 batch 100: avg loss -1.907319 avg loss no lamb -1.907319 time 2019-03-06 19:18:59.918985
Model ind 640 epoch 1444 head B head_i_epoch 1 batch 200: avg loss -2.042098 avg loss no lamb -2.042098 time 2019-03-06 19:20:51.849740
last batch sz 160
Pre: time 2019-03-06 19:22:33.884810: 
 	std: 0.050862383
	best_train_sub_head_match: [(0, 9), (1, 5), (2, 4), (3, 7), (4, 6), (5, 0), (6, 2), (7, 3), (8, 8), (9, 1)]
	test_accs: [0.51213336, 0.61585, 0.61578333, 0.61578333, 0.5118333]
	train_accs: [0.51213336, 0.61585, 0.61578333, 0.61578333, 0.5118333]
	best_train_sub_head: 1
	worst: 0.5118333
	avg: 0.5742766
	best: 0.61585

Starting e_i: 1445
Model ind 640 epoch 1445 head A head_i_epoch 0 batch 0: avg loss -3.316531 avg loss no lamb -3.316531 time 2019-03-06 19:22:37.382617
Model ind 640 epoch 1445 head A head_i_epoch 0 batch 100: avg loss -3.309333 avg loss no lamb -3.309333 time 2019-03-06 19:24:28.383886
Model ind 640 epoch 1445 head A head_i_epoch 0 batch 200: avg loss -3.304906 avg loss no lamb -3.304906 time 2019-03-06 19:26:19.992776
last batch sz 160
Model ind 640 epoch 1445 head B head_i_epoch 0 batch 0: avg loss -1.963371 avg loss no lamb -1.963371 time 2019-03-06 19:27:37.853275
Model ind 640 epoch 1445 head B head_i_epoch 0 batch 100: avg loss -1.888529 avg loss no lamb -1.888529 time 2019-03-06 19:29:28.858394
Model ind 640 epoch 1445 head B head_i_epoch 0 batch 200: avg loss -2.081100 avg loss no lamb -2.081100 time 2019-03-06 19:31:21.091117
last batch sz 160
Model ind 640 epoch 1445 head B head_i_epoch 1 batch 0: avg loss -1.928458 avg loss no lamb -1.928458 time 2019-03-06 19:32:42.297572
Model ind 640 epoch 1445 head B head_i_epoch 1 batch 100: avg loss -1.850223 avg loss no lamb -1.850223 time 2019-03-06 19:34:33.805354
Model ind 640 epoch 1445 head B head_i_epoch 1 batch 200: avg loss -1.985425 avg loss no lamb -1.985425 time 2019-03-06 19:36:23.630597
last batch sz 160
Pre: time 2019-03-06 19:38:04.632147: 
 	std: 0.050479952
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.512, 0.6148667, 0.61495, 0.6150333, 0.5118167]
	train_accs: [0.512, 0.6148667, 0.61495, 0.6150333, 0.5118167]
	best_train_sub_head: 3
	worst: 0.5118167
	avg: 0.5737334
	best: 0.6150333

Starting e_i: 1446
Model ind 640 epoch 1446 head A head_i_epoch 0 batch 0: avg loss -3.362240 avg loss no lamb -3.362240 time 2019-03-06 19:38:08.035470
Model ind 640 epoch 1446 head A head_i_epoch 0 batch 100: avg loss -3.331940 avg loss no lamb -3.331940 time 2019-03-06 19:39:59.845264
Model ind 640 epoch 1446 head A head_i_epoch 0 batch 200: avg loss -3.385075 avg loss no lamb -3.385075 time 2019-03-06 19:41:50.440434
last batch sz 160
Model ind 640 epoch 1446 head B head_i_epoch 0 batch 0: avg loss -2.049806 avg loss no lamb -2.049806 time 2019-03-06 19:43:10.402151
Model ind 640 epoch 1446 head B head_i_epoch 0 batch 100: avg loss -1.972713 avg loss no lamb -1.972713 time 2019-03-06 19:45:01.837327
Model ind 640 epoch 1446 head B head_i_epoch 0 batch 200: avg loss -2.027110 avg loss no lamb -2.027110 time 2019-03-06 19:46:52.260711
last batch sz 160
Model ind 640 epoch 1446 head B head_i_epoch 1 batch 0: avg loss -2.004456 avg loss no lamb -2.004456 time 2019-03-06 19:48:11.996388
Model ind 640 epoch 1446 head B head_i_epoch 1 batch 100: avg loss -1.935530 avg loss no lamb -1.935530 time 2019-03-06 19:50:03.913066
Model ind 640 epoch 1446 head B head_i_epoch 1 batch 200: avg loss -2.038818 avg loss no lamb -2.038818 time 2019-03-06 19:51:53.401921
last batch sz 160
Pre: time 2019-03-06 19:53:34.790600: 
 	std: 0.050942626
	best_train_sub_head_match: [(0, 1), (1, 8), (2, 7), (3, 4), (4, 5), (5, 2), (6, 9), (7, 3), (8, 6), (9, 0)]
	test_accs: [0.5107333, 0.6146167, 0.6146167, 0.61465, 0.51055]
	train_accs: [0.5107333, 0.6146167, 0.6146167, 0.61465, 0.51055]
	best_train_sub_head: 3
	worst: 0.51055
	avg: 0.57303333
	best: 0.61465

Starting e_i: 1447
Model ind 640 epoch 1447 head A head_i_epoch 0 batch 0: avg loss -3.403425 avg loss no lamb -3.403425 time 2019-03-06 19:53:39.022936
Model ind 640 epoch 1447 head A head_i_epoch 0 batch 100: avg loss -3.306276 avg loss no lamb -3.306276 time 2019-03-06 19:55:31.758911
Model ind 640 epoch 1447 head A head_i_epoch 0 batch 200: avg loss -3.388063 avg loss no lamb -3.388063 time 2019-03-06 19:57:22.846487
last batch sz 160
Model ind 640 epoch 1447 head B head_i_epoch 0 batch 0: avg loss -2.007613 avg loss no lamb -2.007613 time 2019-03-06 19:58:44.086105
Model ind 640 epoch 1447 head B head_i_epoch 0 batch 100: avg loss -1.998589 avg loss no lamb -1.998589 time 2019-03-06 20:00:35.258955
Model ind 640 epoch 1447 head B head_i_epoch 0 batch 200: avg loss -2.087693 avg loss no lamb -2.087693 time 2019-03-06 20:02:25.006089
last batch sz 160
Model ind 640 epoch 1447 head B head_i_epoch 1 batch 0: avg loss -1.965181 avg loss no lamb -1.965181 time 2019-03-06 20:03:47.188183
